nohup: 忽略输入
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_144620-x7982fhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-shape-2
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/x7982fhn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:04:43, 22.21s/it]  0%|          | 2/500 [00:33<2:10:56, 15.78s/it]  1%|          | 3/500 [00:43<1:50:35, 13.35s/it]  1%|          | 4/500 [00:54<1:40:29, 12.16s/it]  1%|          | 5/500 [01:04<1:35:38, 11.59s/it]  1%|          | 6/500 [01:15<1:32:15, 11.21s/it]  1%|▏         | 7/500 [01:25<1:30:26, 11.01s/it]  2%|▏         | 8/500 [01:36<1:28:43, 10.82s/it]  2%|▏         | 9/500 [01:46<1:26:53, 10.62s/it]  2%|▏         | 10/500 [01:56<1:26:10, 10.55s/it]  2%|▏         | 11/500 [02:07<1:25:07, 10.44s/it]  2%|▏         | 12/500 [02:17<1:24:26, 10.38s/it]  3%|▎         | 13/500 [02:27<1:23:28, 10.29s/it]  3%|▎         | 14/500 [02:37<1:22:53, 10.23s/it]  3%|▎         | 15/500 [02:47<1:22:40, 10.23s/it]  3%|▎         | 16/500 [02:57<1:22:19, 10.21s/it]  3%|▎         | 17/500 [03:08<1:22:05, 10.20s/it]  4%|▎         | 18/500 [03:18<1:22:19, 10.25s/it]  4%|▍         | 19/500 [03:28<1:21:32, 10.17s/it]  4%|▍         | 20/500 [03:38<1:20:46, 10.10s/it]  4%|▍         | 21/500 [03:48<1:19:37,  9.97s/it]  4%|▍         | 22/500 [03:57<1:18:52,  9.90s/it]  5%|▍         | 23/500 [04:07<1:18:12,  9.84s/it]  5%|▍         | 24/500 [04:17<1:17:48,  9.81s/it]  5%|▌         | 25/500 [04:26<1:17:27,  9.79s/it]  5%|▌         | 26/500 [04:36<1:17:05,  9.76s/it]  5%|▌         | 27/500 [04:46<1:16:42,  9.73s/it]  6%|▌         | 28/500 [04:56<1:17:28,  9.85s/it]  6%|▌         | 29/500 [05:06<1:18:13,  9.96s/it]  6%|▌         | 30/500 [05:16<1:18:19, 10.00s/it]  6%|▌         | 31/500 [05:26<1:18:27, 10.04s/it]  6%|▋         | 32/500 [05:37<1:18:35, 10.08s/it]  7%|▋         | 33/500 [05:47<1:18:28, 10.08s/it]  7%|▋         | 34/500 [05:57<1:18:32, 10.11s/it]  7%|▋         | 35/500 [06:07<1:18:17, 10.10s/it]  7%|▋         | 36/500 [06:17<1:17:51, 10.07s/it]  7%|▋         | 37/500 [06:27<1:18:00, 10.11s/it]  8%|▊         | 38/500 [06:37<1:16:53,  9.98s/it]  8%|▊         | 39/500 [06:47<1:16:42,  9.98s/it]  8%|▊         | 40/500 [06:57<1:16:23,  9.96s/it]  8%|▊         | 41/500 [07:06<1:15:32,  9.87s/it]  8%|▊         | 42/500 [07:16<1:15:08,  9.84s/it]  9%|▊         | 43/500 [07:26<1:14:33,  9.79s/it]  9%|▉         | 44/500 [07:35<1:13:56,  9.73s/it]  9%|▉         | 45/500 [07:45<1:13:40,  9.72s/it]  9%|▉         | 46/500 [07:55<1:13:55,  9.77s/it]  9%|▉         | 47/500 [08:05<1:13:43,  9.76s/it] 10%|▉         | 48/500 [08:14<1:13:32,  9.76s/it] 10%|▉         | 49/500 [08:24<1:13:10,  9.73s/it] 10%|█         | 50/500 [08:34<1:13:12,  9.76s/it] 10%|█         | 51/500 [08:44<1:13:21,  9.80s/it] 10%|█         | 52/500 [08:54<1:13:46,  9.88s/it] 11%|█         | 53/500 [09:04<1:14:10,  9.96s/it] 11%|█         | 54/500 [09:14<1:14:19, 10.00s/it] 11%|█         | 55/500 [09:24<1:14:27, 10.04s/it] 11%|█         | 56/500 [09:35<1:15:12, 10.16s/it] 11%|█▏        | 57/500 [09:45<1:15:36, 10.24s/it] 12%|█▏        | 58/500 [09:55<1:15:06, 10.20s/it] 12%|█▏        | 59/500 [10:06<1:15:09, 10.23s/it] 12%|█▏        | 60/500 [10:16<1:15:41, 10.32s/it] 12%|█▏        | 61/500 [10:26<1:15:15, 10.29s/it] 12%|█▏        | 62/500 [10:36<1:14:35, 10.22s/it] 13%|█▎        | 63/500 [10:46<1:13:44, 10.13s/it] 13%|█▎        | 64/500 [10:56<1:12:30,  9.98s/it] 13%|█▎        | 65/500 [11:06<1:11:42,  9.89s/it] 13%|█▎        | 66/500 [11:15<1:11:19,  9.86s/it] 13%|█▎        | 67/500 [11:25<1:11:00,  9.84s/it] 14%|█▎        | 68/500 [11:35<1:11:28,  9.93s/it] 14%|█▍        | 69/500 [11:46<1:12:17, 10.06s/it] 14%|█▍        | 70/500 [11:56<1:12:22, 10.10s/it] 14%|█▍        | 71/500 [12:06<1:12:27, 10.13s/it] 14%|█▍        | 72/500 [12:16<1:12:17, 10.13s/it] 15%|█▍        | 73/500 [12:26<1:11:59, 10.12s/it] 15%|█▍        | 74/500 [12:36<1:11:46, 10.11s/it] 15%|█▌        | 75/500 [12:47<1:12:05, 10.18s/it] 15%|█▌        | 76/500 [12:57<1:11:44, 10.15s/it] 15%|█▌        | 77/500 [13:07<1:11:47, 10.18s/it] 16%|█▌        | 78/500 [13:17<1:11:35, 10.18s/it] 16%|█▌        | 79/500 [13:27<1:11:26, 10.18s/it] 16%|█▌        | 80/500 [13:38<1:11:35, 10.23s/it] 16%|█▌        | 81/500 [13:48<1:11:21, 10.22s/it] 16%|█▋        | 82/500 [13:58<1:10:47, 10.16s/it] 17%|█▋        | 83/500 [14:08<1:10:10, 10.10s/it] 17%|█▋        | 84/500 [14:18<1:10:30, 10.17s/it] 17%|█▋        | 85/500 [14:29<1:10:37, 10.21s/it] 17%|█▋        | 86/500 [14:39<1:10:00, 10.15s/it] 17%|█▋        | 87/500 [14:49<1:09:39, 10.12s/it] 18%|█▊        | 88/500 [14:59<1:09:05, 10.06s/it] 18%|█▊        | 89/500 [15:09<1:09:06, 10.09s/it] 18%|█▊        | 90/500 [15:19<1:09:09, 10.12s/it] 18%|█▊        | 91/500 [15:29<1:09:17, 10.16s/it] 18%|█▊        | 92/500 [15:40<1:09:31, 10.22s/it] 19%|█▊        | 93/500 [15:50<1:09:30, 10.25s/it] 19%|█▉        | 94/500 [16:00<1:09:12, 10.23s/it] 19%|█▉        | 95/500 [16:10<1:08:51, 10.20s/it] 19%|█▉        | 96/500 [16:21<1:09:08, 10.27s/it] 19%|█▉        | 97/500 [16:31<1:08:49, 10.25s/it] 20%|█▉        | 98/500 [16:41<1:08:46, 10.27s/it] 20%|█▉        | 99/500 [16:51<1:08:26, 10.24s/it] 20%|██        | 100/500 [17:01<1:08:04, 10.21s/it] 20%|██        | 101/500 [17:12<1:07:41, 10.18s/it] 20%|██        | 102/500 [17:22<1:07:19, 10.15s/it] 21%|██        | 103/500 [17:32<1:06:59, 10.12s/it] 21%|██        | 104/500 [17:42<1:06:51, 10.13s/it] 21%|██        | 105/500 [17:52<1:06:31, 10.11s/it] 21%|██        | 106/500 [18:02<1:06:18, 10.10s/it] 21%|██▏       | 107/500 [18:12<1:06:02, 10.08s/it] 22%|██▏       | 108/500 [18:22<1:05:47, 10.07s/it] 22%|██▏       | 109/500 [18:32<1:05:39, 10.08s/it] 22%|██▏       | 110/500 [18:42<1:05:34, 10.09s/it] 22%|██▏       | 111/500 [18:52<1:05:25, 10.09s/it] 22%|██▏       | 112/500 [19:03<1:05:32, 10.14s/it] 23%|██▎       | 113/500 [19:13<1:05:27, 10.15s/it] 23%|██▎       | 114/500 [19:23<1:04:48, 10.07s/it] 23%|██▎       | 115/500 [19:33<1:04:27, 10.04s/it] 23%|██▎       | 116/500 [19:42<1:03:55,  9.99s/it] 23%|██▎       | 117/500 [19:53<1:04:35, 10.12s/it] 24%|██▎       | 118/500 [20:03<1:04:14, 10.09s/it] 24%|██▍       | 119/500 [20:13<1:03:54, 10.06s/it] 24%|██▍       | 120/500 [20:23<1:03:40, 10.05s/it] 24%|██▍       | 121/500 [20:33<1:03:25, 10.04s/it] 24%|██▍       | 122/500 [20:43<1:02:48,  9.97s/it] 25%|██▍       | 123/500 [20:52<1:01:57,  9.86s/it] 25%|██▍       | 124/500 [21:02<1:01:42,  9.85s/it] 25%|██▌       | 125/500 [21:12<1:02:14,  9.96s/it] 25%|██▌       | 126/500 [21:23<1:02:39, 10.05s/it] 25%|██▌       | 127/500 [21:33<1:02:57, 10.13s/it] 26%|██▌       | 128/500 [21:49<1:13:42, 11.89s/it] 26%|██▌       | 129/500 [21:59<1:10:37, 11.42s/it] 26%|██▌       | 130/500 [22:09<1:08:04, 11.04s/it] 26%|██▌       | 131/500 [22:21<1:08:13, 11.09s/it] 26%|██▋       | 132/500 [22:31<1:07:19, 10.98s/it] 26%|██▋       | 132/500 [22:31<1:02:48, 10.24s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.208 MB of 0.516 MB uploadedwandb: | 0.217 MB of 0.516 MB uploadedwandb: / 0.516 MB of 0.516 MB uploadedwandb: - 0.516 MB of 0.516 MB uploadedwandb: \ 0.516 MB of 0.516 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▄▁▆▇▇█████▇███▇████▇▇██▇███▇██▇█▇███████
wandb:     train_loss ▅█▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▆
wandb:   val_accuracy ▆▁▆▇▆▇██▇▇▇█▇█▇▇▇██▇▇▇█▇█▇▇▇▇▇▇▇▇██▇▇▇▇▇
wandb:       val_loss ▂▂▃▂▂▅▁▂▁▁█▃▂▇▇▁▁▁▂▁▁▁▅▄▅▆▁▁▁▇█▃▆▂▂▁▆▁▆▂
wandb: 
wandb: Run summary:
wandb:          epoch 131
wandb:  learning_rate 0.0
wandb: train_accuracy 0.97028
wandb:     train_loss 0.96513
wandb:   val_accuracy 0.59111
wandb:       val_loss 0.62258
wandb: 
wandb: 🚀 View run grateful-shape-2 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/x7982fhn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_144620-x7982fhn/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_151006-s81rif4n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-smoke-3
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/s81rif4n
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:27:10, 24.91s/it]  0%|          | 2/500 [00:36<2:22:19, 17.15s/it]  1%|          | 3/500 [00:48<2:01:10, 14.63s/it]  1%|          | 4/500 [00:59<1:50:39, 13.39s/it]  1%|          | 5/500 [01:10<1:43:27, 12.54s/it]  1%|          | 6/500 [01:22<1:39:36, 12.10s/it]  1%|▏         | 7/500 [01:33<1:36:29, 11.74s/it]  2%|▏         | 8/500 [01:44<1:34:22, 11.51s/it]  2%|▏         | 9/500 [01:55<1:33:16, 11.40s/it]  2%|▏         | 10/500 [02:06<1:32:22, 11.31s/it]  2%|▏         | 11/500 [02:17<1:31:49, 11.27s/it]  2%|▏         | 12/500 [02:28<1:31:05, 11.20s/it]  3%|▎         | 13/500 [02:39<1:29:52, 11.07s/it]  3%|▎         | 14/500 [02:50<1:29:39, 11.07s/it]  3%|▎         | 15/500 [03:01<1:29:34, 11.08s/it]  3%|▎         | 16/500 [03:13<1:31:44, 11.37s/it]  3%|▎         | 17/500 [03:24<1:31:11, 11.33s/it]  4%|▎         | 18/500 [03:37<1:33:22, 11.62s/it]  4%|▍         | 19/500 [03:47<1:31:06, 11.36s/it]  4%|▍         | 20/500 [03:58<1:28:48, 11.10s/it]  4%|▍         | 21/500 [04:08<1:27:05, 10.91s/it]  4%|▍         | 22/500 [04:20<1:28:24, 11.10s/it]  5%|▍         | 23/500 [04:31<1:27:48, 11.04s/it]  5%|▍         | 24/500 [04:41<1:25:36, 10.79s/it]  5%|▌         | 25/500 [04:51<1:24:17, 10.65s/it]  5%|▌         | 26/500 [05:02<1:23:49, 10.61s/it]  5%|▌         | 27/500 [05:13<1:24:05, 10.67s/it]  6%|▌         | 28/500 [05:23<1:23:08, 10.57s/it]  6%|▌         | 29/500 [05:33<1:22:51, 10.56s/it]  6%|▌         | 30/500 [05:44<1:21:53, 10.45s/it]  6%|▌         | 31/500 [05:54<1:21:05, 10.37s/it]  6%|▋         | 32/500 [06:04<1:21:26, 10.44s/it]  7%|▋         | 33/500 [06:15<1:21:44, 10.50s/it]  7%|▋         | 34/500 [06:26<1:22:04, 10.57s/it]  7%|▋         | 35/500 [06:36<1:22:01, 10.58s/it]  7%|▋         | 36/500 [06:47<1:21:41, 10.56s/it]  7%|▋         | 37/500 [06:58<1:22:27, 10.69s/it]  8%|▊         | 38/500 [07:08<1:21:52, 10.63s/it]  8%|▊         | 39/500 [07:19<1:21:40, 10.63s/it]  8%|▊         | 40/500 [07:29<1:20:56, 10.56s/it]  8%|▊         | 41/500 [07:40<1:20:16, 10.49s/it]  8%|▊         | 42/500 [07:50<1:19:41, 10.44s/it]  9%|▊         | 43/500 [08:00<1:18:33, 10.31s/it]  9%|▉         | 44/500 [08:10<1:18:03, 10.27s/it]  9%|▉         | 45/500 [08:20<1:17:12, 10.18s/it]  9%|▉         | 46/500 [08:30<1:16:58, 10.17s/it]  9%|▉         | 47/500 [08:41<1:17:49, 10.31s/it] 10%|▉         | 48/500 [08:52<1:18:09, 10.37s/it] 10%|▉         | 49/500 [09:02<1:18:50, 10.49s/it] 10%|█         | 50/500 [09:13<1:18:32, 10.47s/it] 10%|█         | 51/500 [09:23<1:18:45, 10.52s/it] 10%|█         | 52/500 [09:34<1:18:19, 10.49s/it] 11%|█         | 53/500 [09:44<1:18:09, 10.49s/it] 11%|█         | 54/500 [09:55<1:18:31, 10.56s/it] 11%|█         | 55/500 [10:05<1:18:02, 10.52s/it] 11%|█         | 56/500 [10:15<1:16:37, 10.36s/it] 11%|█▏        | 57/500 [10:25<1:15:37, 10.24s/it] 12%|█▏        | 58/500 [10:35<1:14:59, 10.18s/it] 12%|█▏        | 59/500 [10:45<1:14:26, 10.13s/it] 12%|█▏        | 60/500 [10:56<1:14:40, 10.18s/it] 12%|█▏        | 61/500 [11:06<1:15:02, 10.26s/it] 12%|█▏        | 62/500 [11:16<1:14:25, 10.20s/it] 13%|█▎        | 63/500 [11:26<1:13:55, 10.15s/it] 13%|█▎        | 64/500 [11:36<1:13:48, 10.16s/it] 13%|█▎        | 65/500 [11:46<1:13:08, 10.09s/it] 13%|█▎        | 66/500 [11:57<1:13:38, 10.18s/it] 13%|█▎        | 67/500 [12:07<1:14:29, 10.32s/it] 14%|█▎        | 68/500 [12:18<1:15:03, 10.42s/it] 14%|█▍        | 69/500 [12:29<1:15:18, 10.48s/it] 14%|█▍        | 70/500 [12:39<1:14:48, 10.44s/it] 14%|█▍        | 71/500 [12:49<1:14:23, 10.40s/it] 14%|█▍        | 72/500 [13:00<1:14:44, 10.48s/it] 15%|█▍        | 73/500 [13:11<1:16:03, 10.69s/it] 15%|█▍        | 74/500 [13:22<1:16:30, 10.78s/it] 15%|█▌        | 75/500 [13:33<1:15:50, 10.71s/it] 15%|█▌        | 76/500 [13:43<1:15:38, 10.70s/it] 15%|█▌        | 77/500 [13:54<1:15:17, 10.68s/it] 16%|█▌        | 78/500 [14:05<1:15:22, 10.72s/it] 16%|█▌        | 79/500 [14:15<1:14:40, 10.64s/it] 16%|█▌        | 80/500 [14:26<1:14:05, 10.58s/it] 16%|█▌        | 81/500 [14:36<1:12:44, 10.42s/it] 16%|█▋        | 82/500 [14:46<1:11:38, 10.28s/it] 17%|█▋        | 83/500 [14:56<1:10:55, 10.20s/it] 17%|█▋        | 84/500 [15:06<1:10:57, 10.24s/it] 17%|█▋        | 85/500 [15:17<1:11:59, 10.41s/it] 17%|█▋        | 86/500 [15:28<1:12:43, 10.54s/it] 17%|█▋        | 87/500 [15:38<1:12:08, 10.48s/it] 18%|█▊        | 88/500 [15:49<1:12:20, 10.54s/it] 18%|█▊        | 89/500 [15:59<1:11:11, 10.39s/it] 18%|█▊        | 90/500 [16:09<1:10:12, 10.27s/it] 18%|█▊        | 91/500 [16:19<1:09:33, 10.21s/it] 18%|█▊        | 92/500 [16:29<1:09:12, 10.18s/it] 19%|█▊        | 93/500 [16:39<1:08:48, 10.14s/it] 19%|█▉        | 94/500 [16:49<1:08:29, 10.12s/it] 19%|█▉        | 95/500 [16:59<1:08:28, 10.14s/it] 19%|█▉        | 96/500 [17:09<1:08:17, 10.14s/it] 19%|█▉        | 97/500 [17:19<1:07:46, 10.09s/it] 20%|█▉        | 98/500 [17:29<1:07:20, 10.05s/it] 20%|█▉        | 99/500 [17:40<1:07:57, 10.17s/it] 20%|██        | 100/500 [17:50<1:08:45, 10.31s/it] 20%|██        | 100/500 [17:50<1:11:23, 10.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.320 MB uploadedwandb: - 0.010 MB of 0.320 MB uploadedwandb: \ 0.235 MB of 0.320 MB uploadedwandb: | 0.320 MB of 0.320 MB uploadedwandb: / 0.320 MB of 0.320 MB uploadedwandb: - 0.320 MB of 0.320 MB uploadedwandb: \ 0.320 MB of 0.320 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▅▃▆▇▄▅▇▆▇▇▇▇█▇███▇▇██▇▇▇█▇█████▇█▇▇███
wandb:     train_loss █▇▇▄▇▆▄█▇▆▂▄▃▆▇▅▃▄▆█▂▄▁▃▃▅▅▅▄▂▅▇▁▃▂▅▅▆▃▅
wandb:   val_accuracy ▁▁▇█▄▄▂▂▂▁▃▄▄▃▅▂▅▅▄▄▃▄▄▃▃▃▄▃▅▅▃▃▅▄▅▃▃▅▅▅
wandb:       val_loss ▄▄▃▄▃▃▂▄▃▄▃▃▅▄▁▃▂▃▂▂█▆▂▄▃▁▂▅▄▃▄█▄▄▄▃▄▃▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb: train_accuracy 0.77266
wandb:     train_loss 0.77527
wandb:   val_accuracy 0.46444
wandb:       val_loss 1.34436
wandb: 
wandb: 🚀 View run avid-smoke-3 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/s81rif4n
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_151006-s81rif4n/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_152850-68icp3vs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-voice-4
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/68icp3vs
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:43:04, 26.82s/it]  0%|          | 2/500 [00:40<2:36:01, 18.80s/it]  1%|          | 3/500 [00:50<2:05:21, 15.13s/it]  1%|          | 4/500 [01:01<1:50:31, 13.37s/it]  1%|          | 5/500 [01:11<1:40:28, 12.18s/it]  1%|          | 6/500 [01:21<1:34:50, 11.52s/it]  1%|▏         | 7/500 [01:31<1:30:59, 11.07s/it]  2%|▏         | 8/500 [01:41<1:28:12, 10.76s/it]  2%|▏         | 9/500 [01:52<1:28:19, 10.79s/it]  2%|▏         | 10/500 [02:02<1:25:45, 10.50s/it]  2%|▏         | 11/500 [02:12<1:24:01, 10.31s/it]  2%|▏         | 12/500 [02:22<1:22:30, 10.15s/it]  3%|▎         | 13/500 [02:32<1:22:03, 10.11s/it]  3%|▎         | 14/500 [02:42<1:22:27, 10.18s/it]  3%|▎         | 15/500 [02:53<1:22:47, 10.24s/it]  3%|▎         | 16/500 [03:03<1:22:56, 10.28s/it]  3%|▎         | 17/500 [03:14<1:23:49, 10.41s/it]  4%|▎         | 18/500 [03:25<1:24:55, 10.57s/it]  4%|▍         | 19/500 [03:35<1:24:23, 10.53s/it]  4%|▍         | 20/500 [03:45<1:23:39, 10.46s/it]  4%|▍         | 21/500 [03:56<1:23:19, 10.44s/it]  4%|▍         | 22/500 [04:06<1:22:22, 10.34s/it]  5%|▍         | 23/500 [04:16<1:22:11, 10.34s/it]  5%|▍         | 24/500 [04:26<1:21:07, 10.23s/it]  5%|▌         | 25/500 [04:36<1:20:39, 10.19s/it]  5%|▌         | 26/500 [04:47<1:20:59, 10.25s/it]  5%|▌         | 27/500 [04:57<1:21:19, 10.32s/it]  6%|▌         | 28/500 [05:07<1:21:00, 10.30s/it]  6%|▌         | 29/500 [05:18<1:20:37, 10.27s/it]  6%|▌         | 30/500 [05:28<1:20:18, 10.25s/it]  6%|▌         | 31/500 [05:38<1:20:22, 10.28s/it]  6%|▋         | 32/500 [05:48<1:19:59, 10.26s/it]  7%|▋         | 33/500 [05:59<1:19:37, 10.23s/it]  7%|▋         | 34/500 [06:09<1:19:47, 10.27s/it]  7%|▋         | 35/500 [06:19<1:19:24, 10.25s/it]  7%|▋         | 36/500 [06:29<1:18:07, 10.10s/it]  7%|▋         | 37/500 [06:39<1:17:09, 10.00s/it]  8%|▊         | 38/500 [06:48<1:16:19,  9.91s/it]  8%|▊         | 39/500 [06:58<1:15:50,  9.87s/it]  8%|▊         | 40/500 [07:08<1:15:30,  9.85s/it]  8%|▊         | 41/500 [07:18<1:15:09,  9.83s/it]  8%|▊         | 42/500 [07:27<1:14:55,  9.82s/it]  9%|▊         | 43/500 [07:37<1:14:48,  9.82s/it]  9%|▉         | 44/500 [07:47<1:14:35,  9.81s/it]  9%|▉         | 45/500 [07:57<1:14:19,  9.80s/it]  9%|▉         | 46/500 [08:07<1:14:36,  9.86s/it]  9%|▉         | 47/500 [08:17<1:14:30,  9.87s/it] 10%|▉         | 48/500 [08:26<1:14:05,  9.83s/it] 10%|▉         | 49/500 [08:36<1:13:44,  9.81s/it] 10%|█         | 50/500 [08:46<1:13:35,  9.81s/it] 10%|█         | 50/500 [08:46<1:18:59, 10.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.319 MB uploadedwandb: | 0.010 MB of 0.319 MB uploadedwandb: / 0.169 MB of 0.319 MB uploadedwandb: - 0.169 MB of 0.319 MB uploadedwandb: \ 0.169 MB of 0.319 MB uploadedwandb: | 0.169 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▃▁▁▅
wandb:     train_loss ▆█▆█▁▇▆▁▅▂▂▂▆▅▅▅▄▇▅▆▇▅▃▅▆▆▅▅▅▂▆▄▂▆▃▅▅▅▆▄
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▂▅▅▅▅▆▅▅▇
wandb:       val_loss █▄█▃▃▇▃▆▅▇▆▁▅▇▄▅▅▃▃▃▃▄▄▂▇▅▄▆▄▅▄▄▂▂▂▆▆▄▇▄
wandb: 
wandb: Run summary:
wandb:          epoch 49
wandb:  learning_rate 0.0
wandb: train_accuracy 0.38782
wandb:     train_loss 1.07772
wandb:   val_accuracy 0.35556
wandb:       val_loss 1.09147
wandb: 
wandb: 🚀 View run dauntless-voice-4 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/68icp3vs
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_152850-68icp3vs/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_153824-ody0xmdj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-water-5
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ody0xmdj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:40:33, 26.52s/it]  0%|          | 2/500 [00:38<2:30:46, 18.17s/it]  1%|          | 3/500 [00:49<2:02:35, 14.80s/it]  1%|          | 4/500 [01:01<1:52:26, 13.60s/it]  1%|          | 5/500 [01:12<1:44:29, 12.67s/it]  1%|          | 6/500 [01:23<1:41:09, 12.29s/it]  1%|▏         | 7/500 [01:34<1:37:03, 11.81s/it]  2%|▏         | 8/500 [01:45<1:34:03, 11.47s/it]  2%|▏         | 9/500 [01:56<1:31:48, 11.22s/it]  2%|▏         | 10/500 [02:06<1:29:04, 10.91s/it]  2%|▏         | 11/500 [02:16<1:26:59, 10.67s/it]  2%|▏         | 12/500 [02:26<1:26:09, 10.59s/it]  3%|▎         | 13/500 [02:37<1:26:05, 10.61s/it]  3%|▎         | 14/500 [02:48<1:26:29, 10.68s/it]  3%|▎         | 15/500 [02:59<1:26:52, 10.75s/it]  3%|▎         | 16/500 [03:10<1:26:42, 10.75s/it]  3%|▎         | 17/500 [03:20<1:26:09, 10.70s/it]  4%|▎         | 18/500 [03:31<1:26:33, 10.78s/it]  4%|▍         | 19/500 [03:42<1:26:03, 10.74s/it]  4%|▍         | 20/500 [03:52<1:25:24, 10.68s/it]  4%|▍         | 21/500 [04:03<1:24:40, 10.61s/it]  4%|▍         | 22/500 [04:13<1:24:17, 10.58s/it]  5%|▍         | 23/500 [04:23<1:22:53, 10.43s/it]  5%|▍         | 24/500 [04:33<1:21:41, 10.30s/it]  5%|▌         | 25/500 [04:44<1:21:39, 10.32s/it]  5%|▌         | 26/500 [04:54<1:22:14, 10.41s/it]  5%|▌         | 27/500 [05:05<1:23:22, 10.58s/it]  6%|▌         | 28/500 [05:16<1:22:55, 10.54s/it]  6%|▌         | 29/500 [05:26<1:22:51, 10.55s/it]  6%|▌         | 30/500 [05:36<1:21:43, 10.43s/it]  6%|▌         | 31/500 [05:47<1:20:48, 10.34s/it]  6%|▋         | 32/500 [05:57<1:19:52, 10.24s/it]  7%|▋         | 33/500 [06:07<1:20:08, 10.30s/it]  7%|▋         | 34/500 [06:17<1:19:42, 10.26s/it]  7%|▋         | 35/500 [06:27<1:19:30, 10.26s/it]  7%|▋         | 36/500 [06:38<1:19:00, 10.22s/it]  7%|▋         | 37/500 [06:48<1:18:48, 10.21s/it]  8%|▊         | 38/500 [06:58<1:18:20, 10.17s/it]  8%|▊         | 39/500 [07:08<1:17:47, 10.13s/it]  8%|▊         | 40/500 [07:18<1:17:50, 10.15s/it]  8%|▊         | 41/500 [07:29<1:18:21, 10.24s/it]  8%|▊         | 42/500 [07:39<1:18:50, 10.33s/it]  9%|▊         | 43/500 [07:50<1:20:03, 10.51s/it]  9%|▉         | 44/500 [08:01<1:20:14, 10.56s/it]  9%|▉         | 45/500 [08:11<1:20:10, 10.57s/it]  9%|▉         | 46/500 [08:22<1:19:57, 10.57s/it]  9%|▉         | 47/500 [08:32<1:18:29, 10.40s/it] 10%|▉         | 48/500 [08:42<1:17:35, 10.30s/it] 10%|▉         | 49/500 [08:52<1:16:32, 10.18s/it] 10%|█         | 50/500 [09:02<1:16:05, 10.15s/it] 10%|█         | 51/500 [09:12<1:15:37, 10.11s/it] 10%|█         | 52/500 [09:22<1:15:36, 10.13s/it] 11%|█         | 53/500 [09:32<1:15:35, 10.15s/it] 11%|█         | 54/500 [09:43<1:16:19, 10.27s/it] 11%|█         | 55/500 [09:54<1:17:41, 10.48s/it] 11%|█         | 56/500 [10:04<1:17:15, 10.44s/it] 11%|█▏        | 57/500 [10:14<1:16:28, 10.36s/it] 12%|█▏        | 58/500 [10:24<1:15:20, 10.23s/it] 12%|█▏        | 59/500 [10:34<1:14:27, 10.13s/it] 12%|█▏        | 60/500 [10:44<1:14:12, 10.12s/it] 12%|█▏        | 61/500 [10:55<1:14:28, 10.18s/it] 12%|█▏        | 61/500 [10:55<1:18:34, 10.74s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.029 MB uploadedwandb: / 0.025 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▂▂▆▆▅▆█▇██▇▆▇███████▇█████████████████
wandb:     train_loss ▅▄▆▆█▁▁▁▁▁▁▁▁▁▃▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▁▁▅▇▂▇▆▇▇▇▇▇█▇█▆▇██▇▇███▇▇██████▇▇████
wandb:       val_loss ▃▃▃▄▃▂█▁▁▁▄▄▁▃▁▂▁▅▁▁▇▄▇▇▄▄▁▃▄▄▄█▁▅▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.99108
wandb:     train_loss 0.0
wandb:   val_accuracy 0.61111
wandb:       val_loss 0.00647
wandb: 
wandb: 🚀 View run golden-water-5 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ody0xmdj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_153824-ody0xmdj/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_155000-xdakgaee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-feather-6
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xdakgaee
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:24:13, 24.56s/it]  0%|          | 2/500 [00:36<2:23:55, 17.34s/it]  1%|          | 3/500 [00:48<2:03:14, 14.88s/it]  1%|          | 4/500 [01:00<1:52:15, 13.58s/it]  1%|          | 5/500 [01:11<1:45:07, 12.74s/it]  1%|          | 6/500 [01:23<1:41:08, 12.28s/it]  1%|▏         | 7/500 [01:34<1:37:23, 11.85s/it]  2%|▏         | 8/500 [01:44<1:33:59, 11.46s/it]  2%|▏         | 9/500 [01:55<1:32:18, 11.28s/it]  2%|▏         | 10/500 [02:06<1:30:27, 11.08s/it]  2%|▏         | 11/500 [02:17<1:30:08, 11.06s/it]  2%|▏         | 12/500 [02:28<1:30:44, 11.16s/it]  3%|▎         | 13/500 [02:39<1:30:04, 11.10s/it]  3%|▎         | 14/500 [02:51<1:31:34, 11.31s/it]  3%|▎         | 15/500 [03:04<1:35:09, 11.77s/it]  3%|▎         | 16/500 [03:15<1:34:57, 11.77s/it]  3%|▎         | 17/500 [03:27<1:33:20, 11.59s/it]  4%|▎         | 18/500 [03:43<1:44:33, 13.02s/it]  4%|▍         | 19/500 [03:54<1:40:48, 12.57s/it]  4%|▍         | 20/500 [04:05<1:36:29, 12.06s/it]  4%|▍         | 21/500 [04:17<1:34:25, 11.83s/it]  4%|▍         | 22/500 [04:29<1:34:40, 11.88s/it]  5%|▍         | 23/500 [04:41<1:34:29, 11.89s/it]  5%|▍         | 24/500 [04:52<1:34:02, 11.85s/it]  5%|▌         | 25/500 [05:04<1:33:44, 11.84s/it]  5%|▌         | 26/500 [05:16<1:32:40, 11.73s/it]  5%|▌         | 27/500 [05:26<1:30:24, 11.47s/it]  6%|▌         | 28/500 [05:43<1:42:15, 13.00s/it]  6%|▌         | 29/500 [05:55<1:38:32, 12.55s/it]  6%|▌         | 30/500 [06:06<1:35:33, 12.20s/it]  6%|▌         | 31/500 [06:17<1:32:20, 11.81s/it]  6%|▋         | 32/500 [06:28<1:30:29, 11.60s/it]  7%|▋         | 33/500 [06:40<1:30:29, 11.63s/it]  7%|▋         | 34/500 [06:51<1:29:51, 11.57s/it]  7%|▋         | 35/500 [07:03<1:31:19, 11.78s/it]  7%|▋         | 36/500 [07:15<1:29:45, 11.61s/it]  7%|▋         | 37/500 [07:26<1:29:54, 11.65s/it]  8%|▊         | 38/500 [07:38<1:30:05, 11.70s/it]  8%|▊         | 39/500 [07:50<1:30:06, 11.73s/it]  8%|▊         | 40/500 [08:01<1:29:34, 11.68s/it]  8%|▊         | 41/500 [08:13<1:29:14, 11.67s/it]  8%|▊         | 42/500 [08:25<1:29:03, 11.67s/it]  9%|▊         | 43/500 [08:37<1:29:05, 11.70s/it]  9%|▉         | 44/500 [08:48<1:29:04, 11.72s/it]  9%|▉         | 45/500 [09:00<1:28:39, 11.69s/it]  9%|▉         | 46/500 [09:12<1:28:40, 11.72s/it]  9%|▉         | 47/500 [09:23<1:28:18, 11.70s/it] 10%|▉         | 48/500 [09:35<1:28:34, 11.76s/it] 10%|▉         | 49/500 [09:47<1:28:05, 11.72s/it] 10%|█         | 50/500 [09:58<1:27:34, 11.68s/it] 10%|█         | 51/500 [10:10<1:26:48, 11.60s/it] 10%|█         | 52/500 [10:21<1:26:16, 11.55s/it] 11%|█         | 53/500 [10:33<1:26:05, 11.56s/it] 11%|█         | 54/500 [10:44<1:24:37, 11.38s/it] 11%|█         | 55/500 [10:56<1:26:07, 11.61s/it] 11%|█         | 56/500 [11:07<1:25:39, 11.58s/it] 11%|█▏        | 57/500 [11:19<1:24:19, 11.42s/it] 12%|█▏        | 58/500 [11:30<1:23:43, 11.36s/it] 12%|█▏        | 59/500 [11:41<1:22:40, 11.25s/it] 12%|█▏        | 60/500 [11:52<1:22:30, 11.25s/it] 12%|█▏        | 61/500 [12:03<1:21:38, 11.16s/it] 12%|█▏        | 62/500 [12:14<1:20:41, 11.05s/it] 13%|█▎        | 63/500 [12:25<1:20:36, 11.07s/it] 13%|█▎        | 64/500 [12:36<1:20:27, 11.07s/it] 13%|█▎        | 65/500 [12:47<1:19:52, 11.02s/it] 13%|█▎        | 66/500 [12:58<1:20:22, 11.11s/it] 13%|█▎        | 67/500 [13:09<1:20:14, 11.12s/it] 14%|█▎        | 68/500 [13:20<1:19:37, 11.06s/it] 14%|█▍        | 69/500 [13:32<1:20:30, 11.21s/it] 14%|█▍        | 70/500 [13:49<1:32:23, 12.89s/it] 14%|█▍        | 71/500 [14:01<1:30:18, 12.63s/it] 14%|█▍        | 72/500 [14:13<1:29:09, 12.50s/it] 15%|█▍        | 73/500 [14:25<1:27:40, 12.32s/it] 15%|█▍        | 74/500 [14:36<1:26:10, 12.14s/it] 15%|█▌        | 75/500 [14:53<1:35:05, 13.42s/it] 15%|█▌        | 76/500 [15:04<1:30:22, 12.79s/it] 15%|█▌        | 77/500 [15:15<1:26:00, 12.20s/it] 16%|█▌        | 78/500 [15:26<1:22:52, 11.78s/it] 16%|█▌        | 79/500 [15:37<1:20:34, 11.48s/it] 16%|█▌        | 80/500 [15:47<1:19:10, 11.31s/it] 16%|█▌        | 81/500 [15:58<1:17:55, 11.16s/it] 16%|█▋        | 82/500 [16:09<1:16:55, 11.04s/it] 17%|█▋        | 83/500 [16:20<1:16:07, 10.95s/it] 17%|█▋        | 84/500 [16:31<1:16:13, 10.99s/it] 17%|█▋        | 85/500 [16:42<1:15:47, 10.96s/it] 17%|█▋        | 86/500 [16:53<1:15:28, 10.94s/it] 17%|█▋        | 87/500 [17:04<1:15:14, 10.93s/it] 18%|█▊        | 88/500 [17:15<1:15:02, 10.93s/it] 18%|█▊        | 89/500 [17:26<1:15:26, 11.01s/it] 18%|█▊        | 90/500 [17:37<1:14:51, 10.96s/it] 18%|█▊        | 91/500 [17:48<1:14:57, 11.00s/it] 18%|█▊        | 92/500 [17:59<1:15:43, 11.14s/it] 19%|█▊        | 93/500 [18:11<1:16:14, 11.24s/it] 19%|█▉        | 94/500 [18:22<1:15:26, 11.15s/it] 19%|█▉        | 95/500 [18:32<1:14:41, 11.07s/it] 19%|█▉        | 96/500 [18:43<1:14:06, 11.01s/it] 19%|█▉        | 97/500 [18:54<1:14:14, 11.05s/it] 20%|█▉        | 98/500 [19:06<1:15:00, 11.20s/it] 20%|█▉        | 99/500 [19:18<1:15:47, 11.34s/it] 20%|██        | 100/500 [19:29<1:15:13, 11.28s/it] 20%|██        | 100/500 [19:29<1:17:57, 11.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.320 MB uploadedwandb: / 0.010 MB of 0.320 MB uploadedwandb: - 0.110 MB of 0.320 MB uploadedwandb: \ 0.320 MB of 0.320 MB uploadedwandb: | 0.320 MB of 0.320 MB uploadedwandb: / 0.320 MB of 0.320 MB uploadedwandb: - 0.320 MB of 0.320 MB uploadedwandb: \ 0.320 MB of 0.320 MB uploadedwandb: | 0.320 MB of 0.320 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▅▆▆▇▆▆▆▇▇▇▇▇▇▇████▇███▇▇█▇████████████
wandb:     train_loss █▇▇▄▇▅▅█▇▆▂▆▃▆▇▅▃▄▇▇▁▅▁▄▃▆▅▄▃▃▆▆▁▂▂▅▇█▄▆
wandb:   val_accuracy ▁▂▇▆▅▆▄▂▃▃▄▄▇▅▇▃▇▇▆▇▅▇▆▆▄▅▆▄▇▇▄▅▇▇█▅▆█▇█
wandb:       val_loss ▅▄▄▄▄▅▄▅▃▅▃▃▇▅▃▄▄▄▂▂█▄▃▃▃▁▃▆▆▄▅▆▃▄▅▄▄▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb: train_accuracy 0.77117
wandb:     train_loss 0.95993
wandb:   val_accuracy 0.50889
wandb:       val_loss 1.01754
wandb: 
wandb: 🚀 View run cerulean-feather-6 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xdakgaee
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_155000-xdakgaee/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_161012-ovctd8o8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-microwave-7
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ovctd8o8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:24:14, 24.56s/it]  0%|          | 2/500 [00:36<2:21:27, 17.04s/it]  1%|          | 3/500 [00:47<1:58:09, 14.26s/it]  1%|          | 4/500 [00:57<1:46:15, 12.85s/it]  1%|          | 5/500 [01:09<1:41:23, 12.29s/it]  1%|          | 6/500 [01:20<1:37:18, 11.82s/it]  1%|▏         | 7/500 [01:31<1:36:11, 11.71s/it]  2%|▏         | 8/500 [01:42<1:33:54, 11.45s/it]  2%|▏         | 9/500 [01:53<1:32:21, 11.29s/it]  2%|▏         | 10/500 [02:04<1:31:36, 11.22s/it]  2%|▏         | 11/500 [02:15<1:30:43, 11.13s/it]  2%|▏         | 12/500 [02:27<1:31:32, 11.26s/it]  3%|▎         | 13/500 [02:38<1:30:42, 11.18s/it]  3%|▎         | 14/500 [02:49<1:31:27, 11.29s/it]  3%|▎         | 15/500 [03:00<1:30:29, 11.19s/it]  3%|▎         | 16/500 [03:11<1:29:18, 11.07s/it]  3%|▎         | 17/500 [03:22<1:30:17, 11.22s/it]  4%|▎         | 18/500 [03:35<1:32:18, 11.49s/it]  4%|▍         | 19/500 [03:46<1:31:34, 11.42s/it]  4%|▍         | 20/500 [03:57<1:31:58, 11.50s/it]  4%|▍         | 21/500 [04:09<1:30:47, 11.37s/it]  4%|▍         | 22/500 [04:19<1:28:16, 11.08s/it]  5%|▍         | 23/500 [04:30<1:27:05, 10.96s/it]  5%|▍         | 24/500 [04:40<1:26:15, 10.87s/it]  5%|▌         | 25/500 [04:51<1:26:26, 10.92s/it]  5%|▌         | 26/500 [05:03<1:28:21, 11.18s/it]  5%|▌         | 27/500 [05:14<1:28:01, 11.17s/it]  6%|▌         | 28/500 [05:25<1:27:18, 11.10s/it]  6%|▌         | 29/500 [05:37<1:27:53, 11.20s/it]  6%|▌         | 30/500 [05:48<1:28:10, 11.26s/it]  6%|▌         | 31/500 [05:59<1:27:01, 11.13s/it]  6%|▋         | 32/500 [06:10<1:26:12, 11.05s/it]  7%|▋         | 33/500 [06:21<1:25:30, 10.99s/it]  7%|▋         | 34/500 [06:32<1:25:23, 11.00s/it]  7%|▋         | 35/500 [06:42<1:25:05, 10.98s/it]  7%|▋         | 36/500 [06:54<1:25:49, 11.10s/it]  7%|▋         | 37/500 [07:05<1:26:22, 11.19s/it]  8%|▊         | 38/500 [07:17<1:27:39, 11.38s/it]  8%|▊         | 39/500 [07:33<1:38:45, 12.85s/it]  8%|▊         | 40/500 [07:45<1:34:57, 12.39s/it]  8%|▊         | 41/500 [07:56<1:31:08, 11.91s/it]  8%|▊         | 42/500 [08:06<1:28:21, 11.58s/it]  9%|▊         | 43/500 [08:17<1:26:38, 11.38s/it]  9%|▉         | 44/500 [08:28<1:24:33, 11.13s/it]  9%|▉         | 45/500 [08:38<1:23:09, 10.97s/it]  9%|▉         | 46/500 [08:49<1:22:46, 10.94s/it]  9%|▉         | 47/500 [09:00<1:23:03, 11.00s/it] 10%|▉         | 48/500 [09:11<1:22:30, 10.95s/it] 10%|▉         | 49/500 [09:22<1:22:18, 10.95s/it] 10%|█         | 50/500 [09:33<1:21:29, 10.87s/it] 10%|█         | 50/500 [09:33<1:25:59, 11.47s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.319 MB uploadedwandb: \ 0.010 MB of 0.319 MB uploadedwandb: | 0.135 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb: \ 0.319 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▄▁▁▁▁▁█▁▁▁▁▁▁▁▄
wandb:     train_loss ▆█▆▇▁█▇▂▅▁▃▂▇▅▇▅▅▇▅▆▆▅▂▅▆▅▅▅▅▂▆▃▂▆▂▅▅▅▇▄
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄▁▁
wandb:       val_loss █▄█▃▂▇▃▅▅▇▅▁▆▆▄▄▂▃▃▃▃▄▄▃▇▄▄▆▄▄▄▄▁▁▁▆▇▄▇▄
wandb: 
wandb: Run summary:
wandb:          epoch 49
wandb:  learning_rate 0.0
wandb: train_accuracy 0.34027
wandb:     train_loss 1.09168
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.09646
wandb: 
wandb: 🚀 View run desert-microwave-7 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ovctd8o8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_161012-ovctd8o8/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_162024-62js59ex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-waterfall-8
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/62js59ex
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:21:32, 24.23s/it]  0%|          | 2/500 [00:35<2:17:34, 16.58s/it]  1%|          | 3/500 [00:47<2:00:30, 14.55s/it]  1%|          | 4/500 [01:03<2:05:31, 15.18s/it]  1%|          | 5/500 [01:15<1:55:05, 13.95s/it]  1%|          | 6/500 [01:26<1:45:48, 12.85s/it]  1%|▏         | 7/500 [01:37<1:40:40, 12.25s/it]  2%|▏         | 8/500 [01:48<1:36:58, 11.83s/it]  2%|▏         | 9/500 [01:59<1:34:19, 11.53s/it]  2%|▏         | 10/500 [02:09<1:32:07, 11.28s/it]  2%|▏         | 11/500 [02:20<1:31:03, 11.17s/it]  2%|▏         | 12/500 [02:31<1:29:51, 11.05s/it]  3%|▎         | 13/500 [02:42<1:29:10, 10.99s/it]  3%|▎         | 14/500 [02:53<1:28:35, 10.94s/it]  3%|▎         | 15/500 [03:04<1:28:24, 10.94s/it]  3%|▎         | 16/500 [03:15<1:28:31, 10.97s/it]  3%|▎         | 17/500 [03:26<1:28:26, 10.99s/it]  4%|▎         | 18/500 [03:37<1:28:03, 10.96s/it]  4%|▍         | 19/500 [03:48<1:28:04, 10.99s/it]  4%|▍         | 20/500 [03:58<1:27:42, 10.96s/it]  4%|▍         | 21/500 [04:10<1:27:50, 11.00s/it]  4%|▍         | 22/500 [04:21<1:27:52, 11.03s/it]  5%|▍         | 23/500 [04:32<1:27:43, 11.03s/it]  5%|▍         | 24/500 [04:43<1:27:30, 11.03s/it]  5%|▌         | 25/500 [05:00<1:41:01, 12.76s/it]  5%|▌         | 26/500 [05:12<1:39:14, 12.56s/it]  5%|▌         | 27/500 [05:23<1:35:01, 12.05s/it]  6%|▌         | 28/500 [05:33<1:32:03, 11.70s/it]  6%|▌         | 29/500 [05:44<1:29:51, 11.45s/it]  6%|▌         | 30/500 [05:55<1:28:50, 11.34s/it]  6%|▌         | 31/500 [06:06<1:27:55, 11.25s/it]  6%|▋         | 32/500 [06:18<1:28:33, 11.35s/it]  7%|▋         | 33/500 [06:30<1:28:58, 11.43s/it]  7%|▋         | 34/500 [06:41<1:29:41, 11.55s/it]  7%|▋         | 35/500 [06:53<1:29:52, 11.60s/it]  7%|▋         | 36/500 [07:05<1:29:42, 11.60s/it]  7%|▋         | 37/500 [07:16<1:29:42, 11.62s/it]  8%|▊         | 38/500 [07:28<1:29:26, 11.62s/it]  8%|▊         | 39/500 [07:40<1:29:31, 11.65s/it]  8%|▊         | 40/500 [07:51<1:28:59, 11.61s/it]  8%|▊         | 41/500 [08:03<1:29:40, 11.72s/it]  8%|▊         | 42/500 [08:20<1:40:16, 13.14s/it]  9%|▊         | 43/500 [08:31<1:36:45, 12.70s/it]  9%|▉         | 44/500 [08:42<1:32:58, 12.23s/it]  9%|▉         | 45/500 [08:53<1:29:47, 11.84s/it]  9%|▉         | 46/500 [09:04<1:27:34, 11.57s/it]  9%|▉         | 47/500 [09:16<1:26:51, 11.50s/it] 10%|▉         | 48/500 [09:27<1:25:46, 11.39s/it] 10%|▉         | 49/500 [09:38<1:25:39, 11.39s/it] 10%|█         | 50/500 [09:49<1:24:20, 11.25s/it] 10%|█         | 51/500 [10:00<1:23:38, 11.18s/it] 10%|█         | 52/500 [10:11<1:22:52, 11.10s/it] 11%|█         | 53/500 [10:22<1:22:12, 11.04s/it] 11%|█         | 54/500 [10:33<1:21:46, 11.00s/it] 11%|█         | 55/500 [10:44<1:21:38, 11.01s/it] 11%|█         | 56/500 [10:56<1:23:02, 11.22s/it] 11%|█▏        | 57/500 [11:07<1:22:31, 11.18s/it] 12%|█▏        | 58/500 [11:18<1:21:43, 11.09s/it] 12%|█▏        | 59/500 [11:28<1:21:04, 11.03s/it] 12%|█▏        | 60/500 [11:40<1:21:47, 11.15s/it] 12%|█▏        | 61/500 [11:51<1:22:09, 11.23s/it] 12%|█▏        | 62/500 [12:04<1:24:05, 11.52s/it] 13%|█▎        | 63/500 [12:15<1:24:31, 11.60s/it] 13%|█▎        | 64/500 [12:27<1:24:18, 11.60s/it] 13%|█▎        | 65/500 [12:39<1:24:40, 11.68s/it] 13%|█▎        | 66/500 [12:50<1:23:17, 11.52s/it] 13%|█▎        | 67/500 [13:02<1:24:01, 11.64s/it] 14%|█▎        | 68/500 [13:13<1:23:47, 11.64s/it] 14%|█▍        | 69/500 [13:25<1:24:11, 11.72s/it] 14%|█▍        | 70/500 [13:37<1:22:56, 11.57s/it] 14%|█▍        | 71/500 [13:47<1:21:03, 11.34s/it] 14%|█▍        | 72/500 [13:58<1:20:01, 11.22s/it] 15%|█▍        | 73/500 [14:09<1:19:21, 11.15s/it] 15%|█▍        | 74/500 [14:20<1:19:00, 11.13s/it] 15%|█▌        | 75/500 [14:31<1:18:41, 11.11s/it] 15%|█▌        | 76/500 [14:43<1:18:25, 11.10s/it] 15%|█▌        | 77/500 [14:54<1:18:06, 11.08s/it] 16%|█▌        | 78/500 [15:04<1:17:32, 11.02s/it] 16%|█▌        | 79/500 [15:16<1:17:37, 11.06s/it] 16%|█▌        | 80/500 [15:27<1:17:23, 11.06s/it] 16%|█▌        | 81/500 [15:38<1:17:22, 11.08s/it] 16%|█▋        | 82/500 [15:50<1:19:20, 11.39s/it] 17%|█▋        | 83/500 [16:03<1:22:14, 11.83s/it] 17%|█▋        | 84/500 [16:16<1:24:06, 12.13s/it] 17%|█▋        | 85/500 [16:28<1:24:09, 12.17s/it] 17%|█▋        | 86/500 [16:41<1:24:57, 12.31s/it] 17%|█▋        | 87/500 [16:53<1:24:24, 12.26s/it] 18%|█▊        | 88/500 [17:05<1:24:15, 12.27s/it] 18%|█▊        | 89/500 [17:18<1:25:00, 12.41s/it] 18%|█▊        | 90/500 [17:35<1:35:48, 14.02s/it] 18%|█▊        | 91/500 [17:48<1:32:22, 13.55s/it] 18%|█▊        | 91/500 [17:48<1:20:02, 11.74s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▂▃▆█▇▇█▇▇▇███████████████████████████
wandb:     train_loss ▂▂▂▃█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▃▂▄▄█▅▅▇▄▄▄▅▆▆▇▆▆▇▆▇▇▅▇▆▇▅▆▅▆▅▄▇▇▇██▆▇
wandb:       val_loss ▂▃▂▃▁▂▁▂▁▂▄▄▃▆▂▃▃▂▂▃▄▁▃▅▁▁▃█▃▂▃▅▁▄▄▁▁▁▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 90
wandb:  learning_rate 0.0
wandb: train_accuracy 0.98663
wandb:     train_loss 0.00078
wandb:   val_accuracy 0.54222
wandb:       val_loss 4.17404
wandb: 
wandb: 🚀 View run quiet-waterfall-8 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/62js59ex
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_162024-62js59ex/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_163853-5blkfk43
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-water-9
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5blkfk43
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:40:57, 26.57s/it]  0%|          | 2/500 [00:39<2:32:29, 18.37s/it]  1%|          | 3/500 [00:50<2:05:55, 15.20s/it]  1%|          | 4/500 [01:01<1:50:19, 13.35s/it]  1%|          | 5/500 [01:11<1:42:24, 12.41s/it]  1%|          | 6/500 [01:22<1:37:36, 11.85s/it]  1%|▏         | 7/500 [01:33<1:33:51, 11.42s/it]  2%|▏         | 8/500 [01:43<1:31:31, 11.16s/it]  2%|▏         | 9/500 [01:54<1:30:05, 11.01s/it]  2%|▏         | 10/500 [02:04<1:27:42, 10.74s/it]  2%|▏         | 11/500 [02:14<1:26:07, 10.57s/it]  2%|▏         | 12/500 [02:24<1:24:24, 10.38s/it]  3%|▎         | 13/500 [02:34<1:23:40, 10.31s/it]  3%|▎         | 14/500 [02:45<1:23:28, 10.31s/it]  3%|▎         | 15/500 [02:55<1:22:42, 10.23s/it]  3%|▎         | 16/500 [03:05<1:22:22, 10.21s/it]  3%|▎         | 17/500 [03:15<1:22:09, 10.21s/it]  4%|▎         | 18/500 [03:25<1:21:42, 10.17s/it]  4%|▍         | 19/500 [03:36<1:22:53, 10.34s/it]  4%|▍         | 20/500 [03:47<1:23:38, 10.46s/it]  4%|▍         | 21/500 [03:57<1:23:46, 10.49s/it]  4%|▍         | 22/500 [04:08<1:23:34, 10.49s/it]  5%|▍         | 23/500 [04:18<1:23:02, 10.45s/it]  5%|▍         | 24/500 [04:28<1:22:40, 10.42s/it]  5%|▌         | 25/500 [04:39<1:22:06, 10.37s/it]  5%|▌         | 26/500 [04:49<1:22:16, 10.41s/it]  5%|▌         | 27/500 [05:00<1:23:08, 10.55s/it]  6%|▌         | 28/500 [05:11<1:23:30, 10.62s/it]  6%|▌         | 29/500 [05:22<1:23:38, 10.66s/it]  6%|▌         | 30/500 [05:32<1:23:22, 10.64s/it]  6%|▌         | 31/500 [05:42<1:21:49, 10.47s/it]  6%|▋         | 32/500 [05:52<1:20:58, 10.38s/it]  7%|▋         | 33/500 [06:02<1:19:44, 10.25s/it]  7%|▋         | 34/500 [06:12<1:19:17, 10.21s/it]  7%|▋         | 35/500 [06:23<1:18:48, 10.17s/it]  7%|▋         | 36/500 [06:33<1:18:56, 10.21s/it]  7%|▋         | 37/500 [06:43<1:19:10, 10.26s/it]  8%|▊         | 38/500 [06:54<1:19:23, 10.31s/it]  8%|▊         | 39/500 [07:04<1:19:46, 10.38s/it]  8%|▊         | 40/500 [07:15<1:19:50, 10.41s/it]  8%|▊         | 41/500 [07:26<1:20:35, 10.54s/it]  8%|▊         | 42/500 [07:40<1:30:27, 11.85s/it]  9%|▊         | 43/500 [07:51<1:26:26, 11.35s/it]  9%|▉         | 44/500 [08:01<1:24:01, 11.06s/it]  9%|▉         | 45/500 [08:12<1:23:08, 10.96s/it]  9%|▉         | 46/500 [08:22<1:22:24, 10.89s/it]  9%|▉         | 47/500 [08:33<1:21:13, 10.76s/it] 10%|▉         | 48/500 [08:43<1:20:30, 10.69s/it] 10%|▉         | 49/500 [08:54<1:19:29, 10.58s/it] 10%|█         | 50/500 [09:04<1:19:17, 10.57s/it] 10%|█         | 51/500 [09:15<1:18:24, 10.48s/it] 10%|█         | 52/500 [09:24<1:16:53, 10.30s/it] 11%|█         | 53/500 [09:34<1:15:53, 10.19s/it] 11%|█         | 54/500 [09:44<1:14:54, 10.08s/it] 11%|█         | 55/500 [09:55<1:16:03, 10.26s/it] 11%|█         | 56/500 [10:05<1:16:34, 10.35s/it] 11%|█▏        | 57/500 [10:16<1:16:41, 10.39s/it] 12%|█▏        | 58/500 [10:26<1:15:48, 10.29s/it] 12%|█▏        | 59/500 [10:36<1:15:28, 10.27s/it] 12%|█▏        | 60/500 [10:47<1:15:28, 10.29s/it] 12%|█▏        | 61/500 [10:56<1:14:29, 10.18s/it] 12%|█▏        | 62/500 [11:06<1:13:54, 10.12s/it] 13%|█▎        | 63/500 [11:16<1:13:17, 10.06s/it] 13%|█▎        | 64/500 [11:26<1:12:55, 10.04s/it] 13%|█▎        | 65/500 [11:37<1:13:18, 10.11s/it] 13%|█▎        | 66/500 [11:47<1:12:57, 10.09s/it] 13%|█▎        | 67/500 [11:57<1:12:43, 10.08s/it] 13%|█▎        | 67/500 [11:57<1:17:15, 10.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.317 MB uploadedwandb: \ 0.025 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▅▅▄▆▆▅▅▂▂▁▃▁▃▁▃▁▃▂▅▂▇█▁▃▅▃███▇▇▆▇▇█▇█
wandb:     train_loss ▄▄▄▄▄▄▄▃▄▃▅▃▇▄▄▁█▃▁▄▃▄▄▃▃▇▇▃▁▃▂▄▂▂▃▄▂▂▂▄
wandb:   val_accuracy ▃▃▃█▃▃▅▆▂▃▃▂▂▂▁▂▂▃▂▄▂▄▂█▇▂▃▄▂▆▇▇█▅▄█▅▇▇▇
wandb:       val_loss ▄▃▃▃▄▃▃▄▄▃▃▃█▃▃▅▅▃▁▃▃▄▄▃▃▇▃▃▁▃▄▂▁▃▃▃▃▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 66
wandb:  learning_rate 0.0
wandb: train_accuracy 0.82615
wandb:     train_loss 0.98234
wandb:   val_accuracy 0.56667
wandb:       val_loss 0.96546
wandb: 
wandb: 🚀 View run ethereal-water-9 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5blkfk43
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_163853-5blkfk43/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_165129-nwd5r3w7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-darkness-10
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nwd5r3w7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:25:14, 24.68s/it]  0%|          | 2/500 [00:36<2:21:46, 17.08s/it]  1%|          | 3/500 [00:47<1:58:07, 14.26s/it]  1%|          | 4/500 [00:57<1:45:58, 12.82s/it]  1%|          | 5/500 [01:08<1:38:44, 11.97s/it]  1%|          | 6/500 [01:19<1:35:20, 11.58s/it]  1%|▏         | 7/500 [01:30<1:34:23, 11.49s/it]  2%|▏         | 8/500 [01:41<1:32:51, 11.32s/it]  2%|▏         | 9/500 [01:52<1:30:44, 11.09s/it]  2%|▏         | 10/500 [02:03<1:30:29, 11.08s/it]  2%|▏         | 11/500 [02:14<1:30:14, 11.07s/it]  2%|▏         | 12/500 [02:25<1:30:11, 11.09s/it]  3%|▎         | 13/500 [02:36<1:29:07, 10.98s/it]  3%|▎         | 14/500 [02:47<1:30:02, 11.12s/it]  3%|▎         | 15/500 [02:58<1:29:20, 11.05s/it]  3%|▎         | 16/500 [03:09<1:29:19, 11.07s/it]  3%|▎         | 17/500 [03:19<1:27:37, 10.88s/it]  4%|▎         | 18/500 [03:30<1:26:17, 10.74s/it]  4%|▍         | 19/500 [03:40<1:25:21, 10.65s/it]  4%|▍         | 20/500 [03:51<1:24:50, 10.60s/it]  4%|▍         | 21/500 [04:01<1:24:40, 10.61s/it]  4%|▍         | 22/500 [04:13<1:26:30, 10.86s/it]  5%|▍         | 23/500 [04:25<1:29:19, 11.24s/it]  5%|▍         | 24/500 [04:36<1:29:43, 11.31s/it]  5%|▌         | 25/500 [04:49<1:31:23, 11.54s/it]  5%|▌         | 26/500 [05:00<1:31:16, 11.55s/it]  5%|▌         | 27/500 [05:11<1:30:31, 11.48s/it]  6%|▌         | 28/500 [05:23<1:29:21, 11.36s/it]  6%|▌         | 29/500 [05:33<1:27:37, 11.16s/it]  6%|▌         | 30/500 [05:44<1:27:11, 11.13s/it]  6%|▌         | 31/500 [05:55<1:25:20, 10.92s/it]  6%|▋         | 32/500 [06:05<1:24:42, 10.86s/it]  7%|▋         | 33/500 [06:16<1:23:46, 10.76s/it]  7%|▋         | 34/500 [06:27<1:23:30, 10.75s/it]  7%|▋         | 35/500 [06:37<1:22:30, 10.65s/it]  7%|▋         | 36/500 [06:48<1:22:02, 10.61s/it]  7%|▋         | 37/500 [06:58<1:21:39, 10.58s/it]  8%|▊         | 38/500 [07:09<1:21:35, 10.60s/it]  8%|▊         | 39/500 [07:20<1:22:52, 10.79s/it]  8%|▊         | 40/500 [07:31<1:23:07, 10.84s/it]  8%|▊         | 41/500 [07:42<1:22:52, 10.83s/it]  8%|▊         | 42/500 [07:53<1:23:33, 10.95s/it]  9%|▊         | 43/500 [08:04<1:24:11, 11.05s/it]  9%|▉         | 44/500 [08:15<1:23:03, 10.93s/it]  9%|▉         | 45/500 [08:26<1:22:24, 10.87s/it]  9%|▉         | 46/500 [08:37<1:23:07, 10.99s/it]  9%|▉         | 47/500 [08:48<1:22:25, 10.92s/it] 10%|▉         | 48/500 [09:00<1:24:26, 11.21s/it] 10%|▉         | 49/500 [09:17<1:37:41, 13.00s/it] 10%|█         | 50/500 [09:29<1:36:14, 12.83s/it] 10%|█         | 50/500 [09:29<1:25:27, 11.39s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.319 MB uploadedwandb: \ 0.010 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb: \ 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁█▁▇▁█▁▁▁▁▂▅▁▁▁▁▁▁▁▄
wandb:     train_loss ▆█▆█▁█▇▂▅▂▂▂▆▅▆▅▅▇▅▆▇▄▃▅▆▅▅▅▅▂▆▃▂▅▃▅▅▅▇▅
wandb:   val_accuracy ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▄▂▅▂▇▂▇▂▂▂▂▃▄▂▂▂▂▁▂▂█
wandb:       val_loss █▄█▂▂▇▃▅▅▇▇▁▅▆▄▄▂▃▃▃▃▄▄▄▆▄▄▆▄▄▄▃▁▂▂▆▆▄▇▄
wandb: 
wandb: Run summary:
wandb:          epoch 49
wandb:  learning_rate 0.0
wandb: train_accuracy 0.3581
wandb:     train_loss 1.09023
wandb:   val_accuracy 0.36667
wandb:       val_loss 1.09144
wandb: 
wandb: 🚀 View run twilight-darkness-10 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nwd5r3w7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_165129-nwd5r3w7/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_170146-jz6oqkb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sun-11
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/jz6oqkb4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:11, 25.88s/it]  0%|          | 2/500 [00:37<2:23:50, 17.33s/it]  1%|          | 3/500 [00:47<1:56:32, 14.07s/it]  1%|          | 4/500 [00:57<1:43:47, 12.56s/it]  1%|          | 5/500 [01:08<1:38:27, 11.94s/it]  1%|          | 6/500 [01:19<1:35:49, 11.64s/it]  1%|▏         | 7/500 [01:30<1:34:12, 11.47s/it]  2%|▏         | 8/500 [01:41<1:33:19, 11.38s/it]  2%|▏         | 9/500 [01:52<1:32:10, 11.26s/it]  2%|▏         | 10/500 [02:04<1:31:54, 11.25s/it]  2%|▏         | 11/500 [02:15<1:32:07, 11.30s/it]  2%|▏         | 12/500 [02:26<1:30:58, 11.19s/it]  3%|▎         | 13/500 [02:37<1:30:12, 11.11s/it]  3%|▎         | 14/500 [02:48<1:30:01, 11.12s/it]  3%|▎         | 15/500 [02:59<1:29:36, 11.08s/it]  3%|▎         | 16/500 [03:10<1:28:27, 10.97s/it]  3%|▎         | 17/500 [03:21<1:28:22, 10.98s/it]  4%|▎         | 18/500 [03:31<1:27:31, 10.89s/it]  4%|▍         | 19/500 [03:42<1:26:21, 10.77s/it]  4%|▍         | 20/500 [03:53<1:26:07, 10.77s/it]  4%|▍         | 21/500 [04:04<1:27:33, 10.97s/it]  4%|▍         | 22/500 [04:15<1:27:26, 10.98s/it]  5%|▍         | 23/500 [04:26<1:26:47, 10.92s/it]  5%|▍         | 24/500 [04:37<1:26:41, 10.93s/it]  5%|▌         | 25/500 [04:48<1:26:38, 10.94s/it]  5%|▌         | 26/500 [04:59<1:26:06, 10.90s/it]  5%|▌         | 27/500 [05:09<1:24:56, 10.77s/it]  6%|▌         | 28/500 [05:20<1:24:19, 10.72s/it]  6%|▌         | 29/500 [05:30<1:23:51, 10.68s/it]  6%|▌         | 30/500 [05:41<1:24:49, 10.83s/it]  6%|▌         | 31/500 [05:52<1:24:40, 10.83s/it]  6%|▋         | 32/500 [06:03<1:24:39, 10.85s/it]  7%|▋         | 33/500 [06:14<1:24:37, 10.87s/it]  7%|▋         | 34/500 [06:25<1:24:54, 10.93s/it]  7%|▋         | 35/500 [06:36<1:25:04, 10.98s/it]  7%|▋         | 36/500 [06:48<1:27:05, 11.26s/it]  7%|▋         | 37/500 [07:00<1:27:41, 11.36s/it]  8%|▊         | 38/500 [07:11<1:26:57, 11.29s/it]  8%|▊         | 39/500 [07:22<1:26:54, 11.31s/it]  8%|▊         | 40/500 [07:34<1:26:35, 11.29s/it]  8%|▊         | 41/500 [07:45<1:26:12, 11.27s/it]  8%|▊         | 42/500 [07:56<1:25:39, 11.22s/it]  9%|▊         | 43/500 [08:07<1:25:34, 11.23s/it]  9%|▉         | 44/500 [08:18<1:24:57, 11.18s/it]  9%|▉         | 45/500 [08:29<1:24:31, 11.15s/it]  9%|▉         | 46/500 [08:41<1:25:13, 11.26s/it]  9%|▉         | 47/500 [08:52<1:24:56, 11.25s/it] 10%|▉         | 48/500 [09:03<1:25:09, 11.30s/it] 10%|▉         | 49/500 [09:14<1:24:17, 11.21s/it] 10%|█         | 50/500 [09:26<1:24:05, 11.21s/it] 10%|█         | 51/500 [09:37<1:24:06, 11.24s/it] 10%|█         | 52/500 [09:48<1:23:43, 11.21s/it] 11%|█         | 53/500 [09:59<1:22:24, 11.06s/it] 11%|█         | 54/500 [10:10<1:22:13, 11.06s/it] 11%|█         | 55/500 [10:21<1:21:52, 11.04s/it] 11%|█         | 56/500 [10:32<1:22:03, 11.09s/it] 11%|█▏        | 57/500 [10:43<1:21:49, 11.08s/it] 12%|█▏        | 58/500 [10:54<1:21:02, 11.00s/it] 12%|█▏        | 59/500 [11:05<1:21:15, 11.06s/it] 12%|█▏        | 60/500 [11:17<1:22:08, 11.20s/it] 12%|█▏        | 61/500 [11:28<1:22:11, 11.23s/it] 12%|█▏        | 62/500 [11:38<1:20:31, 11.03s/it] 13%|█▎        | 63/500 [11:49<1:19:11, 10.87s/it] 13%|█▎        | 64/500 [12:01<1:20:28, 11.07s/it] 13%|█▎        | 65/500 [12:12<1:20:05, 11.05s/it] 13%|█▎        | 66/500 [12:22<1:18:58, 10.92s/it] 13%|█▎        | 67/500 [12:33<1:18:25, 10.87s/it] 14%|█▎        | 68/500 [12:44<1:18:44, 10.94s/it] 14%|█▍        | 69/500 [12:54<1:17:35, 10.80s/it] 14%|█▍        | 70/500 [13:05<1:16:41, 10.70s/it] 14%|█▍        | 71/500 [13:15<1:16:10, 10.65s/it] 14%|█▍        | 72/500 [13:26<1:15:47, 10.62s/it] 15%|█▍        | 73/500 [13:37<1:15:27, 10.60s/it] 15%|█▍        | 74/500 [13:47<1:15:14, 10.60s/it] 15%|█▌        | 75/500 [13:58<1:15:42, 10.69s/it] 15%|█▌        | 76/500 [14:09<1:16:22, 10.81s/it] 15%|█▌        | 77/500 [14:20<1:15:35, 10.72s/it] 16%|█▌        | 78/500 [14:31<1:15:49, 10.78s/it] 16%|█▌        | 79/500 [14:42<1:16:55, 10.96s/it] 16%|█▌        | 80/500 [14:53<1:17:12, 11.03s/it] 16%|█▌        | 81/500 [15:05<1:17:55, 11.16s/it] 16%|█▋        | 82/500 [15:15<1:16:44, 11.02s/it] 17%|█▋        | 83/500 [15:26<1:15:53, 10.92s/it] 17%|█▋        | 84/500 [15:37<1:15:22, 10.87s/it] 17%|█▋        | 85/500 [15:48<1:15:16, 10.88s/it] 17%|█▋        | 86/500 [15:59<1:15:23, 10.93s/it] 17%|█▋        | 87/500 [16:09<1:14:13, 10.78s/it] 18%|█▊        | 88/500 [16:20<1:13:53, 10.76s/it] 18%|█▊        | 89/500 [16:30<1:13:12, 10.69s/it] 18%|█▊        | 90/500 [16:41<1:12:30, 10.61s/it] 18%|█▊        | 91/500 [16:51<1:12:09, 10.59s/it] 18%|█▊        | 91/500 [16:51<1:15:47, 11.12s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.234 MB of 0.316 MB uploadedwandb: - 0.234 MB of 0.316 MB uploadedwandb: \ 0.234 MB of 0.316 MB uploadedwandb: | 0.234 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▅▅▄▅▅▅▇▆▆▆▇▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇▇▇█████▇█
wandb:     train_loss ▂▂▂▁▁▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▅▅▅▂▄▅▅▇▅▅▅▅▅▆▇▆▆▇▆▇▆▆▇▆▇▆▆▆▆▆▆▆▆▆▇█▆▇
wandb:       val_loss ▂▃▂▃▆▅▇▃▁▅▆▅▅▇▅▄▄▁█▁▄▃▂▃▁▁▄▅▅▄▅▅▁▂▅▂▁▁▄▇
wandb: 
wandb: Run summary:
wandb:          epoch 90
wandb:  learning_rate 0.0
wandb: train_accuracy 0.89153
wandb:     train_loss 0.1265
wandb:   val_accuracy 0.49333
wandb:       val_loss 5.59437
wandb: 
wandb: 🚀 View run giddy-sun-11 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/jz6oqkb4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_170146-jz6oqkb4/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_171929-p4271j5i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-elevator-12
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/p4271j5i
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:41:36, 26.65s/it]  0%|          | 2/500 [00:39<2:34:22, 18.60s/it]  1%|          | 3/500 [00:51<2:07:27, 15.39s/it]  1%|          | 4/500 [01:02<1:54:31, 13.85s/it]  1%|          | 5/500 [01:13<1:45:01, 12.73s/it]  1%|          | 6/500 [01:23<1:38:32, 11.97s/it]  1%|▏         | 7/500 [01:34<1:35:24, 11.61s/it]  2%|▏         | 8/500 [01:45<1:32:52, 11.33s/it]  2%|▏         | 9/500 [01:56<1:31:26, 11.17s/it]  2%|▏         | 10/500 [02:07<1:32:05, 11.28s/it]  2%|▏         | 11/500 [02:18<1:31:22, 11.21s/it]  2%|▏         | 12/500 [02:30<1:32:45, 11.40s/it]  3%|▎         | 13/500 [02:41<1:31:38, 11.29s/it]  3%|▎         | 14/500 [02:52<1:29:51, 11.09s/it]  3%|▎         | 15/500 [03:03<1:29:06, 11.02s/it]  3%|▎         | 16/500 [03:14<1:28:18, 10.95s/it]  3%|▎         | 17/500 [03:25<1:28:35, 11.00s/it]  4%|▎         | 18/500 [03:36<1:28:35, 11.03s/it]  4%|▍         | 19/500 [03:47<1:29:27, 11.16s/it]  4%|▍         | 20/500 [03:58<1:28:55, 11.12s/it]  4%|▍         | 21/500 [04:10<1:31:16, 11.43s/it]  4%|▍         | 22/500 [04:22<1:31:51, 11.53s/it]  5%|▍         | 23/500 [04:34<1:32:44, 11.67s/it]  5%|▍         | 24/500 [04:46<1:32:18, 11.63s/it]  5%|▌         | 25/500 [04:57<1:31:13, 11.52s/it]  5%|▌         | 26/500 [05:09<1:31:31, 11.58s/it]  5%|▌         | 27/500 [05:20<1:30:30, 11.48s/it]  6%|▌         | 28/500 [05:31<1:29:40, 11.40s/it]  6%|▌         | 29/500 [05:42<1:29:02, 11.34s/it]  6%|▌         | 30/500 [05:54<1:28:47, 11.34s/it]  6%|▌         | 31/500 [06:05<1:29:31, 11.45s/it]  6%|▋         | 32/500 [06:17<1:29:37, 11.49s/it]  7%|▋         | 33/500 [06:29<1:29:39, 11.52s/it]  7%|▋         | 34/500 [06:41<1:30:31, 11.66s/it]  7%|▋         | 35/500 [06:52<1:29:45, 11.58s/it]  7%|▋         | 36/500 [07:03<1:29:03, 11.52s/it]  7%|▋         | 37/500 [07:15<1:28:11, 11.43s/it]  8%|▊         | 38/500 [07:26<1:27:10, 11.32s/it]  8%|▊         | 39/500 [07:37<1:27:21, 11.37s/it]  8%|▊         | 40/500 [07:50<1:30:09, 11.76s/it]  8%|▊         | 41/500 [08:01<1:29:41, 11.72s/it]  8%|▊         | 42/500 [08:13<1:28:21, 11.58s/it]  9%|▊         | 43/500 [08:24<1:27:22, 11.47s/it]  9%|▉         | 44/500 [08:35<1:26:29, 11.38s/it]  9%|▉         | 45/500 [08:46<1:25:39, 11.30s/it]  9%|▉         | 46/500 [08:57<1:24:47, 11.21s/it]  9%|▉         | 47/500 [09:08<1:24:35, 11.20s/it] 10%|▉         | 48/500 [09:19<1:24:15, 11.18s/it] 10%|▉         | 49/500 [09:31<1:23:45, 11.14s/it] 10%|█         | 50/500 [09:42<1:23:24, 11.12s/it] 10%|█         | 51/500 [09:53<1:23:17, 11.13s/it] 10%|█         | 52/500 [10:04<1:23:18, 11.16s/it] 11%|█         | 53/500 [10:15<1:22:20, 11.05s/it] 11%|█         | 54/500 [10:26<1:22:33, 11.11s/it] 11%|█         | 55/500 [10:37<1:22:42, 11.15s/it] 11%|█         | 56/500 [10:49<1:23:02, 11.22s/it] 11%|█▏        | 57/500 [11:00<1:22:56, 11.23s/it] 12%|█▏        | 58/500 [11:11<1:23:01, 11.27s/it] 12%|█▏        | 59/500 [11:22<1:22:18, 11.20s/it] 12%|█▏        | 60/500 [11:33<1:21:54, 11.17s/it] 12%|█▏        | 61/500 [11:44<1:20:47, 11.04s/it] 12%|█▏        | 62/500 [11:55<1:20:25, 11.02s/it] 13%|█▎        | 63/500 [12:06<1:20:31, 11.06s/it] 13%|█▎        | 64/500 [12:17<1:20:24, 11.06s/it] 13%|█▎        | 65/500 [12:28<1:20:03, 11.04s/it] 13%|█▎        | 66/500 [12:40<1:22:11, 11.36s/it] 13%|█▎        | 67/500 [12:53<1:24:31, 11.71s/it] 14%|█▎        | 68/500 [13:05<1:25:45, 11.91s/it] 14%|█▍        | 69/500 [13:18<1:27:11, 12.14s/it] 14%|█▍        | 70/500 [13:30<1:27:33, 12.22s/it] 14%|█▍        | 71/500 [13:42<1:25:36, 11.97s/it] 14%|█▍        | 72/500 [13:53<1:24:08, 11.79s/it] 15%|█▍        | 73/500 [14:04<1:22:35, 11.60s/it] 15%|█▍        | 74/500 [14:15<1:20:54, 11.39s/it] 15%|█▌        | 75/500 [14:26<1:20:07, 11.31s/it] 15%|█▌        | 76/500 [14:37<1:19:23, 11.24s/it] 15%|█▌        | 77/500 [14:49<1:19:19, 11.25s/it] 16%|█▌        | 78/500 [15:00<1:19:38, 11.32s/it] 16%|█▌        | 79/500 [15:12<1:19:56, 11.39s/it] 16%|█▌        | 80/500 [15:23<1:19:18, 11.33s/it] 16%|█▌        | 81/500 [15:34<1:18:23, 11.23s/it] 16%|█▋        | 82/500 [15:45<1:18:00, 11.20s/it] 17%|█▋        | 83/500 [15:57<1:19:08, 11.39s/it] 17%|█▋        | 84/500 [16:09<1:20:22, 11.59s/it] 17%|█▋        | 85/500 [16:25<1:29:19, 12.91s/it] 17%|█▋        | 86/500 [16:36<1:25:57, 12.46s/it] 17%|█▋        | 87/500 [16:48<1:24:08, 12.22s/it] 18%|█▊        | 88/500 [16:59<1:21:35, 11.88s/it] 18%|█▊        | 89/500 [17:10<1:20:19, 11.73s/it] 18%|█▊        | 90/500 [17:22<1:19:31, 11.64s/it] 18%|█▊        | 91/500 [17:33<1:18:41, 11.54s/it] 18%|█▊        | 91/500 [17:33<1:18:56, 11.58s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.319 MB uploadedwandb: / 0.010 MB of 0.319 MB uploadedwandb: - 0.234 MB of 0.319 MB uploadedwandb: \ 0.234 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▄▂▂▂▂▂▂▂▇█▁▂▄▅▂▂▄▄█▄▄▅▇▇▅███▅▅██▇▆▆▅▄█▆
wandb:     train_loss ▂▂▁▁▁▃█▂▃▁▁▂▂▂▁▁▄▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb:   val_accuracy ▃▄▃▃▃▃▃▃▃█▇▁▂▅▅▁▃▄▄▇▄▄▅▆▇▅█▇▆▅▅▇▇▆▅▆▅▅▇▆
wandb:       val_loss ▁▁▁▁▅▂█▁▃▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▁▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 90
wandb:  learning_rate 0.0
wandb: train_accuracy 0.69688
wandb:     train_loss 0.33331
wandb:   val_accuracy 0.45556
wandb:       val_loss 1.42734
wandb: 
wandb: 🚀 View run fallen-elevator-12 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/p4271j5i
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_171929-p4271j5i/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_173745-57pawr5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-hill-13
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/57pawr5z
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:59, 25.97s/it]  0%|          | 2/500 [00:37<2:22:46, 17.20s/it]  1%|          | 3/500 [00:46<1:54:22, 13.81s/it]  1%|          | 4/500 [00:56<1:42:03, 12.35s/it]  1%|          | 5/500 [01:06<1:34:25, 11.45s/it]  1%|          | 6/500 [01:16<1:30:06, 10.94s/it]  1%|▏         | 7/500 [01:26<1:27:42, 10.67s/it]  2%|▏         | 8/500 [01:37<1:26:34, 10.56s/it]  2%|▏         | 9/500 [01:48<1:27:20, 10.67s/it]  2%|▏         | 10/500 [01:58<1:26:30, 10.59s/it]  2%|▏         | 11/500 [02:09<1:26:29, 10.61s/it]  2%|▏         | 12/500 [02:19<1:25:22, 10.50s/it]  3%|▎         | 13/500 [02:29<1:24:56, 10.46s/it]  3%|▎         | 14/500 [02:40<1:25:30, 10.56s/it]  3%|▎         | 15/500 [02:50<1:24:53, 10.50s/it]  3%|▎         | 16/500 [03:01<1:23:48, 10.39s/it]  3%|▎         | 17/500 [03:10<1:22:11, 10.21s/it]  4%|▎         | 18/500 [03:21<1:22:08, 10.22s/it]  4%|▍         | 19/500 [03:31<1:21:39, 10.19s/it]  4%|▍         | 20/500 [03:40<1:20:31, 10.07s/it]  4%|▍         | 21/500 [03:51<1:20:20, 10.06s/it]  4%|▍         | 22/500 [04:01<1:20:40, 10.13s/it]  5%|▍         | 23/500 [04:11<1:20:55, 10.18s/it]  5%|▍         | 24/500 [04:22<1:21:39, 10.29s/it]  5%|▌         | 25/500 [04:32<1:21:47, 10.33s/it]  5%|▌         | 26/500 [04:42<1:21:24, 10.30s/it]  5%|▌         | 27/500 [04:52<1:20:41, 10.23s/it]  6%|▌         | 28/500 [05:02<1:19:35, 10.12s/it]  6%|▌         | 29/500 [05:12<1:18:35, 10.01s/it]  6%|▌         | 30/500 [05:22<1:17:49,  9.94s/it]  6%|▌         | 31/500 [05:32<1:17:30,  9.92s/it]  6%|▋         | 32/500 [05:42<1:17:17,  9.91s/it]  7%|▋         | 33/500 [05:52<1:18:29, 10.09s/it]  7%|▋         | 34/500 [06:02<1:18:17, 10.08s/it]  7%|▋         | 35/500 [06:12<1:18:41, 10.15s/it]  7%|▋         | 36/500 [06:23<1:18:48, 10.19s/it]  7%|▋         | 37/500 [06:33<1:17:47, 10.08s/it]  8%|▊         | 38/500 [06:42<1:16:47,  9.97s/it]  8%|▊         | 39/500 [06:53<1:17:38, 10.11s/it]  8%|▊         | 40/500 [07:03<1:17:53, 10.16s/it]  8%|▊         | 41/500 [07:14<1:19:16, 10.36s/it]  8%|▊         | 42/500 [07:24<1:19:46, 10.45s/it]  9%|▊         | 43/500 [07:35<1:19:38, 10.46s/it]  9%|▉         | 44/500 [07:46<1:19:55, 10.52s/it]  9%|▉         | 45/500 [07:56<1:20:07, 10.57s/it]  9%|▉         | 46/500 [08:07<1:19:23, 10.49s/it]  9%|▉         | 47/500 [08:17<1:18:33, 10.40s/it] 10%|▉         | 48/500 [08:27<1:17:31, 10.29s/it] 10%|▉         | 49/500 [08:37<1:16:24, 10.17s/it] 10%|█         | 50/500 [08:47<1:15:34, 10.08s/it] 10%|█         | 51/500 [08:57<1:16:27, 10.22s/it] 10%|█         | 52/500 [09:07<1:15:43, 10.14s/it] 11%|█         | 53/500 [09:17<1:15:35, 10.15s/it] 11%|█         | 54/500 [09:28<1:15:44, 10.19s/it] 11%|█         | 55/500 [09:38<1:15:21, 10.16s/it] 11%|█         | 56/500 [09:48<1:16:04, 10.28s/it] 11%|█▏        | 57/500 [09:58<1:15:25, 10.22s/it] 12%|█▏        | 58/500 [10:09<1:15:28, 10.24s/it] 12%|█▏        | 59/500 [10:19<1:15:18, 10.25s/it] 12%|█▏        | 60/500 [10:29<1:15:47, 10.34s/it] 12%|█▏        | 61/500 [10:40<1:15:33, 10.33s/it] 12%|█▏        | 62/500 [10:50<1:14:54, 10.26s/it] 13%|█▎        | 63/500 [11:00<1:13:47, 10.13s/it] 13%|█▎        | 64/500 [11:09<1:12:50, 10.02s/it] 13%|█▎        | 65/500 [11:19<1:12:04,  9.94s/it] 13%|█▎        | 66/500 [11:29<1:12:01,  9.96s/it] 13%|█▎        | 67/500 [11:40<1:12:56, 10.11s/it] 14%|█▎        | 68/500 [11:50<1:12:39, 10.09s/it] 14%|█▍        | 69/500 [12:00<1:12:32, 10.10s/it] 14%|█▍        | 70/500 [12:10<1:12:24, 10.10s/it] 14%|█▍        | 71/500 [12:20<1:12:08, 10.09s/it] 14%|█▍        | 72/500 [12:30<1:12:03, 10.10s/it] 15%|█▍        | 73/500 [12:40<1:11:49, 10.09s/it] 15%|█▍        | 74/500 [12:50<1:11:42, 10.10s/it] 15%|█▌        | 75/500 [13:00<1:11:49, 10.14s/it] 15%|█▌        | 76/500 [13:11<1:11:51, 10.17s/it] 15%|█▌        | 77/500 [13:21<1:11:08, 10.09s/it] 15%|█▌        | 77/500 [13:21<1:13:20, 10.40s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.320 MB uploadedwandb: | 0.010 MB of 0.320 MB uploadedwandb: / 0.140 MB of 0.320 MB uploadedwandb: - 0.320 MB of 0.320 MB uploadedwandb: \ 0.320 MB of 0.320 MB uploadedwandb: | 0.320 MB of 0.320 MB uploadedwandb: / 0.320 MB of 0.320 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss ▆██▁▇▆▂▃▇▆▅▆▆▇▄▇▆▅▅▆▃▂▂▂▅▅▆▅▅▆▃▂▆▆▅▂▆▅▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▄▃▃▃▃▆▂▅▄▄▃▃▃▅▃▅▆▄▄▄▁▂▆▄▅▅▃▇▃▆▄▄▃▄▆▄▄▆▆
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31204
wandb:     train_loss 1.12498
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.18093
wandb: 
wandb: 🚀 View run dutiful-hill-13 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/57pawr5z
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_173745-57pawr5z/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_175153-w0jn5i9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-yogurt-14
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w0jn5i9c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:34, 25.68s/it]  0%|          | 2/500 [00:36<2:21:15, 17.02s/it]  1%|          | 3/500 [00:47<1:57:43, 14.21s/it]  1%|          | 4/500 [00:58<1:48:26, 13.12s/it]  1%|          | 5/500 [01:10<1:44:09, 12.63s/it]  1%|          | 6/500 [01:26<1:53:14, 13.76s/it]  1%|▏         | 7/500 [01:38<1:46:43, 12.99s/it]  2%|▏         | 8/500 [01:49<1:42:18, 12.48s/it]  2%|▏         | 9/500 [02:00<1:39:10, 12.12s/it]  2%|▏         | 10/500 [02:11<1:36:14, 11.78s/it]  2%|▏         | 11/500 [02:23<1:36:00, 11.78s/it]  2%|▏         | 12/500 [02:34<1:33:58, 11.56s/it]  3%|▎         | 13/500 [02:45<1:32:28, 11.39s/it]  3%|▎         | 14/500 [02:56<1:31:09, 11.25s/it]  3%|▎         | 15/500 [03:07<1:30:56, 11.25s/it]  3%|▎         | 16/500 [03:19<1:30:44, 11.25s/it]  3%|▎         | 17/500 [03:30<1:31:16, 11.34s/it]  4%|▎         | 18/500 [03:41<1:30:49, 11.31s/it]  4%|▍         | 19/500 [03:52<1:29:12, 11.13s/it]  4%|▍         | 20/500 [04:02<1:27:16, 10.91s/it]  4%|▍         | 21/500 [04:14<1:28:38, 11.10s/it]  4%|▍         | 22/500 [04:25<1:27:57, 11.04s/it]  5%|▍         | 23/500 [04:36<1:28:41, 11.16s/it]  5%|▍         | 24/500 [04:48<1:29:29, 11.28s/it]  5%|▌         | 25/500 [05:00<1:30:38, 11.45s/it]  5%|▌         | 26/500 [05:11<1:30:21, 11.44s/it]  5%|▌         | 27/500 [05:22<1:29:45, 11.39s/it]  6%|▌         | 28/500 [05:34<1:29:21, 11.36s/it]  6%|▌         | 29/500 [05:45<1:28:14, 11.24s/it]  6%|▌         | 30/500 [06:01<1:39:56, 12.76s/it]  6%|▌         | 31/500 [06:12<1:36:02, 12.29s/it]  6%|▋         | 32/500 [06:24<1:34:15, 12.08s/it]  7%|▋         | 33/500 [06:35<1:32:26, 11.88s/it]  7%|▋         | 34/500 [06:46<1:30:34, 11.66s/it]  7%|▋         | 35/500 [06:58<1:30:01, 11.62s/it]  7%|▋         | 36/500 [07:09<1:28:26, 11.44s/it]  7%|▋         | 37/500 [07:20<1:27:35, 11.35s/it]  8%|▊         | 38/500 [07:31<1:26:48, 11.27s/it]  8%|▊         | 39/500 [07:42<1:25:47, 11.17s/it]  8%|▊         | 40/500 [07:53<1:25:19, 11.13s/it]  8%|▊         | 41/500 [08:04<1:24:30, 11.05s/it]  8%|▊         | 42/500 [08:15<1:23:52, 10.99s/it]  9%|▊         | 43/500 [08:26<1:23:49, 11.01s/it]  9%|▉         | 44/500 [08:37<1:24:04, 11.06s/it]  9%|▉         | 45/500 [08:48<1:23:15, 10.98s/it]  9%|▉         | 46/500 [08:59<1:23:24, 11.02s/it]  9%|▉         | 47/500 [09:09<1:22:13, 10.89s/it] 10%|▉         | 48/500 [09:20<1:20:57, 10.75s/it] 10%|▉         | 49/500 [09:30<1:20:20, 10.69s/it] 10%|█         | 50/500 [09:41<1:20:05, 10.68s/it] 10%|█         | 51/500 [09:52<1:19:58, 10.69s/it] 10%|█         | 52/500 [10:02<1:19:21, 10.63s/it] 11%|█         | 53/500 [10:13<1:18:44, 10.57s/it] 11%|█         | 54/500 [10:23<1:18:16, 10.53s/it] 11%|█         | 55/500 [10:34<1:17:56, 10.51s/it] 11%|█         | 56/500 [10:44<1:17:34, 10.48s/it] 11%|█▏        | 57/500 [10:55<1:17:58, 10.56s/it] 12%|█▏        | 58/500 [11:05<1:17:34, 10.53s/it] 12%|█▏        | 59/500 [11:16<1:17:12, 10.50s/it] 12%|█▏        | 60/500 [11:26<1:16:57, 10.49s/it] 12%|█▏        | 61/500 [11:37<1:16:50, 10.50s/it] 12%|█▏        | 62/500 [11:47<1:16:25, 10.47s/it] 13%|█▎        | 63/500 [11:58<1:16:11, 10.46s/it] 13%|█▎        | 64/500 [12:09<1:17:33, 10.67s/it] 13%|█▎        | 65/500 [12:20<1:17:50, 10.74s/it] 13%|█▎        | 66/500 [12:31<1:18:57, 10.92s/it] 13%|█▎        | 67/500 [12:42<1:19:37, 11.03s/it] 14%|█▎        | 68/500 [12:53<1:19:55, 11.10s/it] 14%|█▍        | 69/500 [13:05<1:20:15, 11.17s/it] 14%|█▍        | 70/500 [13:16<1:19:47, 11.13s/it] 14%|█▍        | 71/500 [13:26<1:18:29, 10.98s/it] 14%|█▍        | 72/500 [13:37<1:16:47, 10.77s/it] 15%|█▍        | 73/500 [13:48<1:17:09, 10.84s/it] 15%|█▍        | 74/500 [13:59<1:18:05, 11.00s/it] 15%|█▌        | 75/500 [14:10<1:17:07, 10.89s/it] 15%|█▌        | 76/500 [14:21<1:16:36, 10.84s/it] 15%|█▌        | 77/500 [14:31<1:15:48, 10.75s/it] 16%|█▌        | 78/500 [14:42<1:14:59, 10.66s/it] 16%|█▌        | 79/500 [14:52<1:15:00, 10.69s/it] 16%|█▌        | 80/500 [15:03<1:14:23, 10.63s/it] 16%|█▌        | 81/500 [15:14<1:14:45, 10.70s/it] 16%|█▋        | 82/500 [15:24<1:14:36, 10.71s/it] 17%|█▋        | 83/500 [15:35<1:14:43, 10.75s/it] 17%|█▋        | 84/500 [15:46<1:13:59, 10.67s/it] 17%|█▋        | 85/500 [15:56<1:13:16, 10.59s/it] 17%|█▋        | 86/500 [16:06<1:12:28, 10.50s/it] 17%|█▋        | 87/500 [16:17<1:12:08, 10.48s/it] 18%|█▊        | 88/500 [16:27<1:12:01, 10.49s/it] 18%|█▊        | 89/500 [16:38<1:11:55, 10.50s/it] 18%|█▊        | 90/500 [16:48<1:11:25, 10.45s/it] 18%|█▊        | 91/500 [16:59<1:11:33, 10.50s/it] 18%|█▊        | 91/500 [16:59<1:16:21, 11.20s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.141 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▆▁▅▅▆█▇▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▆█▆▇▇▇▇▆▇▇▇▇█▇▇
wandb:     train_loss ▂▂▂▁▂▄▄▁▁▁▁▁▁▁▁▁▃▃▁▁▁▁▁▁▁█▁▂▁▁▃▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▁▇▄▆▆▄█▇▇▆▆▅▆▆▆▇▆▆▅▆▆▆▅▅▆▆▆▆▆▆▅▆▆▆▆▇▆▆
wandb:       val_loss ▂▂▂▃▂▂▅▂▁▄▄▅▅▇▃▄▃▂▇▂▃▃▄▄▅▁▅▃▂▅▅█▁▃▃▂▁▁▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 90
wandb:  learning_rate 0.0
wandb: train_accuracy 0.76374
wandb:     train_loss 0.06831
wandb:   val_accuracy 0.45111
wandb:       val_loss 3.46896
wandb: 
wandb: 🚀 View run silvery-yogurt-14 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w0jn5i9c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_175153-w0jn5i9c/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_180936-wqwaq726
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-serenity-15
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wqwaq726
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:39:26, 19.17s/it]  0%|          | 2/500 [00:31<2:04:14, 14.97s/it]  1%|          | 3/500 [00:41<1:46:12, 12.82s/it]  1%|          | 4/500 [00:51<1:37:02, 11.74s/it]  1%|          | 5/500 [01:01<1:31:48, 11.13s/it]  1%|          | 6/500 [01:11<1:28:05, 10.70s/it]  1%|▏         | 7/500 [01:21<1:25:25, 10.40s/it]  2%|▏         | 8/500 [01:31<1:24:01, 10.25s/it]  2%|▏         | 9/500 [01:41<1:23:56, 10.26s/it]  2%|▏         | 10/500 [01:52<1:24:36, 10.36s/it]  2%|▏         | 11/500 [02:02<1:24:27, 10.36s/it]  2%|▏         | 12/500 [02:12<1:23:03, 10.21s/it]  3%|▎         | 13/500 [02:22<1:22:32, 10.17s/it]  3%|▎         | 14/500 [02:32<1:22:53, 10.23s/it]  3%|▎         | 15/500 [02:43<1:24:14, 10.42s/it]  3%|▎         | 16/500 [02:53<1:22:36, 10.24s/it]  3%|▎         | 17/500 [03:03<1:21:44, 10.16s/it]  4%|▎         | 18/500 [03:13<1:20:58, 10.08s/it]  4%|▍         | 19/500 [03:23<1:20:28, 10.04s/it]  4%|▍         | 20/500 [03:33<1:20:08, 10.02s/it]  4%|▍         | 21/500 [03:43<1:20:15, 10.05s/it]  4%|▍         | 22/500 [03:53<1:20:19, 10.08s/it]  5%|▍         | 23/500 [04:03<1:20:20, 10.10s/it]  5%|▍         | 24/500 [04:13<1:19:49, 10.06s/it]  5%|▌         | 25/500 [04:23<1:19:32, 10.05s/it]  5%|▌         | 26/500 [04:33<1:18:52,  9.98s/it]  5%|▌         | 27/500 [04:43<1:18:09,  9.91s/it]  6%|▌         | 28/500 [04:53<1:18:01,  9.92s/it]  6%|▌         | 29/500 [05:02<1:17:44,  9.90s/it]  6%|▌         | 30/500 [05:12<1:17:30,  9.89s/it]  6%|▌         | 31/500 [05:22<1:17:47,  9.95s/it]  6%|▋         | 32/500 [05:33<1:19:39, 10.21s/it]  7%|▋         | 33/500 [05:43<1:18:54, 10.14s/it]  7%|▋         | 34/500 [05:54<1:19:05, 10.18s/it]  7%|▋         | 35/500 [06:04<1:19:22, 10.24s/it]  7%|▋         | 36/500 [06:14<1:19:07, 10.23s/it]  7%|▋         | 37/500 [06:24<1:19:00, 10.24s/it]  8%|▊         | 38/500 [06:35<1:19:00, 10.26s/it]  8%|▊         | 39/500 [06:45<1:19:18, 10.32s/it]  8%|▊         | 40/500 [06:55<1:19:06, 10.32s/it]  8%|▊         | 41/500 [07:06<1:18:27, 10.26s/it]  8%|▊         | 42/500 [07:16<1:17:45, 10.19s/it]  9%|▊         | 43/500 [07:25<1:16:23, 10.03s/it]  9%|▉         | 44/500 [07:35<1:16:26, 10.06s/it]  9%|▉         | 45/500 [07:46<1:16:42, 10.11s/it]  9%|▉         | 46/500 [07:56<1:17:04, 10.19s/it]  9%|▉         | 47/500 [08:06<1:16:44, 10.16s/it] 10%|▉         | 48/500 [08:16<1:16:44, 10.19s/it] 10%|▉         | 49/500 [08:26<1:15:27, 10.04s/it] 10%|█         | 50/500 [08:36<1:14:49,  9.98s/it] 10%|█         | 51/500 [08:46<1:14:02,  9.89s/it] 10%|█         | 52/500 [08:55<1:13:41,  9.87s/it] 11%|█         | 53/500 [09:05<1:13:18,  9.84s/it] 11%|█         | 54/500 [09:15<1:12:53,  9.81s/it] 11%|█         | 55/500 [09:25<1:12:58,  9.84s/it] 11%|█         | 56/500 [09:35<1:12:37,  9.82s/it] 11%|█▏        | 57/500 [09:44<1:12:20,  9.80s/it] 12%|█▏        | 58/500 [09:54<1:12:26,  9.83s/it] 12%|█▏        | 59/500 [10:04<1:12:16,  9.83s/it] 12%|█▏        | 60/500 [10:14<1:11:43,  9.78s/it] 12%|█▏        | 61/500 [10:24<1:11:42,  9.80s/it] 12%|█▏        | 62/500 [10:33<1:11:36,  9.81s/it] 13%|█▎        | 63/500 [10:43<1:11:12,  9.78s/it] 13%|█▎        | 64/500 [10:53<1:11:42,  9.87s/it] 13%|█▎        | 65/500 [11:03<1:12:20,  9.98s/it] 13%|█▎        | 66/500 [11:13<1:12:10,  9.98s/it] 13%|█▎        | 67/500 [11:23<1:12:18, 10.02s/it] 14%|█▎        | 68/500 [11:33<1:11:41,  9.96s/it] 14%|█▍        | 69/500 [11:43<1:11:26,  9.95s/it] 14%|█▍        | 70/500 [11:53<1:10:51,  9.89s/it] 14%|█▍        | 71/500 [12:03<1:10:21,  9.84s/it] 14%|█▍        | 72/500 [12:12<1:10:00,  9.82s/it] 15%|█▍        | 73/500 [12:22<1:09:57,  9.83s/it] 15%|█▍        | 74/500 [12:32<1:09:25,  9.78s/it] 15%|█▌        | 75/500 [12:42<1:09:30,  9.81s/it] 15%|█▌        | 76/500 [12:52<1:09:12,  9.79s/it] 15%|█▌        | 77/500 [13:01<1:09:03,  9.80s/it] 16%|█▌        | 78/500 [13:11<1:08:45,  9.78s/it] 16%|█▌        | 79/500 [13:21<1:08:31,  9.77s/it] 16%|█▌        | 80/500 [13:31<1:08:25,  9.78s/it] 16%|█▌        | 81/500 [13:40<1:08:13,  9.77s/it] 16%|█▋        | 82/500 [13:50<1:08:13,  9.79s/it] 16%|█▋        | 82/500 [13:50<1:10:34, 10.13s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.010 MB of 0.318 MB uploadedwandb: - 0.130 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▄▄▃▃▃▃▃▇▃▁▃▃▂█▁▃▆▃▅▃▂▄▃▃▄▄▃▄▃▆▇▄▃▅▅▄▆▄▆
wandb:     train_loss ▅▅▅▁█▆▆▁▄▅▅▆▆▅▅▅▅▄▁▄▆▅▆▇▅▄▄█▄▄▃▆▄▄▆▃▅▅▅▄
wandb:   val_accuracy ▃▄▃▃▃▃▃▃█▃▁▃▃▃▇▁▃▆▃▄▃▃▃▄▃▃▄▃▃▄▅▅▃▄▄▄▃▄▄▄
wandb:       val_loss ▃▃▃█▄▄▃▃▃▃▃▃▄▃▃▃▃▃▇▅▃▃▁▄▂▄▄▃▄▄▂▃▃▃▄▄▄▃▆▄
wandb: 
wandb: Run summary:
wandb:          epoch 81
wandb:  learning_rate 0.0
wandb: train_accuracy 0.52897
wandb:     train_loss 0.90114
wandb:   val_accuracy 0.4
wandb:       val_loss 1.33274
wandb: 
wandb: 🚀 View run breezy-serenity-15 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wqwaq726
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_180936-wqwaq726/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_182418-rsu50986
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-haze-16
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rsu50986
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:52, 25.96s/it]  0%|          | 2/500 [00:37<2:23:16, 17.26s/it]  1%|          | 3/500 [00:47<1:57:46, 14.22s/it]  1%|          | 4/500 [00:58<1:45:44, 12.79s/it]  1%|          | 5/500 [01:08<1:38:58, 12.00s/it]  1%|          | 6/500 [01:19<1:34:59, 11.54s/it]  1%|▏         | 7/500 [01:30<1:32:33, 11.27s/it]  2%|▏         | 8/500 [01:40<1:30:58, 11.09s/it]  2%|▏         | 9/500 [01:51<1:29:31, 10.94s/it]  2%|▏         | 10/500 [02:02<1:28:05, 10.79s/it]  2%|▏         | 11/500 [02:12<1:27:11, 10.70s/it]  2%|▏         | 12/500 [02:23<1:26:31, 10.64s/it]  3%|▎         | 13/500 [02:33<1:25:41, 10.56s/it]  3%|▎         | 14/500 [02:43<1:25:23, 10.54s/it]  3%|▎         | 15/500 [02:54<1:24:52, 10.50s/it]  3%|▎         | 16/500 [03:04<1:24:49, 10.52s/it]  3%|▎         | 17/500 [03:15<1:24:37, 10.51s/it]  4%|▎         | 18/500 [03:26<1:24:44, 10.55s/it]  4%|▍         | 19/500 [03:36<1:24:27, 10.54s/it]  4%|▍         | 20/500 [03:46<1:23:51, 10.48s/it]  4%|▍         | 21/500 [03:57<1:24:00, 10.52s/it]  4%|▍         | 22/500 [04:08<1:24:02, 10.55s/it]  5%|▍         | 23/500 [04:19<1:24:45, 10.66s/it]  5%|▍         | 24/500 [04:30<1:26:08, 10.86s/it]  5%|▌         | 25/500 [04:40<1:24:59, 10.74s/it]  5%|▌         | 26/500 [04:51<1:23:42, 10.60s/it]  5%|▌         | 27/500 [05:01<1:23:32, 10.60s/it]  6%|▌         | 28/500 [05:11<1:22:37, 10.50s/it]  6%|▌         | 29/500 [05:22<1:22:38, 10.53s/it]  6%|▌         | 30/500 [05:32<1:21:56, 10.46s/it]  6%|▌         | 31/500 [05:43<1:21:38, 10.44s/it]  6%|▋         | 32/500 [05:54<1:23:16, 10.68s/it]  7%|▋         | 33/500 [06:05<1:23:33, 10.74s/it]  7%|▋         | 34/500 [06:16<1:23:31, 10.75s/it]  7%|▋         | 35/500 [06:27<1:24:09, 10.86s/it]  7%|▋         | 36/500 [06:38<1:23:49, 10.84s/it]  7%|▋         | 37/500 [06:48<1:22:30, 10.69s/it]  8%|▊         | 38/500 [06:58<1:21:48, 10.63s/it]  8%|▊         | 39/500 [07:09<1:20:53, 10.53s/it]  8%|▊         | 40/500 [07:19<1:20:28, 10.50s/it]  8%|▊         | 41/500 [07:30<1:20:27, 10.52s/it]  8%|▊         | 42/500 [07:41<1:21:27, 10.67s/it]  9%|▊         | 43/500 [07:52<1:22:53, 10.88s/it]  9%|▉         | 44/500 [08:03<1:22:22, 10.84s/it]  9%|▉         | 45/500 [08:14<1:22:02, 10.82s/it]  9%|▉         | 46/500 [08:25<1:22:30, 10.90s/it]  9%|▉         | 47/500 [08:36<1:23:23, 11.05s/it] 10%|▉         | 48/500 [08:47<1:23:05, 11.03s/it] 10%|▉         | 49/500 [08:57<1:21:32, 10.85s/it] 10%|█         | 50/500 [09:08<1:20:26, 10.72s/it] 10%|█         | 51/500 [09:18<1:19:48, 10.67s/it] 10%|█         | 52/500 [09:29<1:19:37, 10.66s/it] 11%|█         | 53/500 [09:40<1:18:58, 10.60s/it] 11%|█         | 54/500 [09:50<1:19:35, 10.71s/it] 11%|█         | 55/500 [10:01<1:20:01, 10.79s/it] 11%|█         | 56/500 [10:12<1:20:23, 10.86s/it] 11%|█▏        | 57/500 [10:23<1:20:29, 10.90s/it] 12%|█▏        | 58/500 [10:34<1:20:25, 10.92s/it] 12%|█▏        | 59/500 [10:45<1:19:18, 10.79s/it] 12%|█▏        | 60/500 [10:56<1:18:41, 10.73s/it] 12%|█▏        | 61/500 [11:06<1:17:56, 10.65s/it] 12%|█▏        | 62/500 [11:16<1:17:12, 10.58s/it] 13%|█▎        | 63/500 [11:27<1:17:06, 10.59s/it] 13%|█▎        | 64/500 [11:37<1:16:27, 10.52s/it] 13%|█▎        | 65/500 [11:49<1:18:16, 10.80s/it] 13%|█▎        | 66/500 [12:00<1:19:04, 10.93s/it] 13%|█▎        | 67/500 [12:11<1:18:49, 10.92s/it] 14%|█▎        | 68/500 [12:22<1:18:13, 10.87s/it] 14%|█▍        | 69/500 [12:33<1:18:46, 10.97s/it] 14%|█▍        | 70/500 [12:44<1:18:33, 10.96s/it] 14%|█▍        | 71/500 [12:55<1:18:19, 10.95s/it] 14%|█▍        | 72/500 [13:06<1:18:12, 10.96s/it] 15%|█▍        | 73/500 [13:17<1:18:09, 10.98s/it] 15%|█▍        | 74/500 [13:28<1:17:26, 10.91s/it] 15%|█▌        | 75/500 [13:39<1:17:27, 10.93s/it] 15%|█▌        | 76/500 [13:50<1:17:45, 11.00s/it] 15%|█▌        | 77/500 [14:00<1:17:04, 10.93s/it] 15%|█▌        | 77/500 [14:00<1:16:59, 10.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.320 MB uploadedwandb: / 0.010 MB of 0.320 MB uploadedwandb: - 0.233 MB of 0.320 MB uploadedwandb: \ 0.233 MB of 0.320 MB uploadedwandb: | 0.233 MB of 0.320 MB uploadedwandb: / 0.233 MB of 0.320 MB uploadedwandb: - 0.233 MB of 0.320 MB uploadedwandb: \ 0.320 MB of 0.320 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss ▆██▁▇▆▁▄▆▇▅▇▇▇▂▆▆▅▅▆▂▂▂▂▅▅▆▅▅▆▂▂▇▆▅▂▇▅▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▄▃▃▃▃▇▃▅▄▄▃▃▃▄▄▆▆▄▄▄▁▂▇▄▆▅▃▇▃▆▄▄▃▄▆▄▄▆▇
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31055
wandb:     train_loss 1.1334
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.2169
wandb: 
wandb: 🚀 View run ruby-haze-16 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rsu50986
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_182418-rsu50986/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_183904-7le85byr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-wind-17
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7le85byr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:08:52, 29.92s/it]  0%|          | 2/500 [00:44<2:53:13, 20.87s/it]  1%|          | 3/500 [00:58<2:28:55, 17.98s/it]  1%|          | 4/500 [01:18<2:32:58, 18.51s/it]  1%|          | 5/500 [01:32<2:20:56, 17.08s/it]  1%|          | 6/500 [01:51<2:25:41, 17.70s/it]  1%|▏         | 7/500 [02:15<2:40:39, 19.55s/it]  2%|▏         | 8/500 [02:29<2:26:28, 17.86s/it]  2%|▏         | 9/500 [02:47<2:27:48, 18.06s/it]  2%|▏         | 10/500 [03:01<2:17:11, 16.80s/it]  2%|▏         | 11/500 [03:20<2:22:15, 17.46s/it]  2%|▏         | 12/500 [03:35<2:14:26, 16.53s/it]  3%|▎         | 13/500 [03:49<2:08:39, 15.85s/it]  3%|▎         | 14/500 [04:03<2:04:37, 15.39s/it]  3%|▎         | 15/500 [04:18<2:01:47, 15.07s/it]  3%|▎         | 16/500 [04:37<2:11:10, 16.26s/it]  3%|▎         | 17/500 [04:55<2:16:56, 17.01s/it]  4%|▎         | 18/500 [05:10<2:09:38, 16.14s/it]  4%|▍         | 19/500 [05:24<2:04:51, 15.58s/it]  4%|▍         | 20/500 [05:38<2:01:33, 15.19s/it]  4%|▍         | 21/500 [05:57<2:10:46, 16.38s/it]  4%|▍         | 22/500 [06:16<2:15:17, 16.98s/it]  5%|▍         | 23/500 [06:34<2:18:38, 17.44s/it]  5%|▍         | 24/500 [06:48<2:10:40, 16.47s/it]  5%|▌         | 25/500 [07:03<2:05:19, 15.83s/it]  5%|▌         | 26/500 [07:22<2:12:44, 16.80s/it]  5%|▌         | 27/500 [07:41<2:17:21, 17.42s/it]  6%|▌         | 28/500 [07:55<2:09:05, 16.41s/it]  6%|▌         | 29/500 [08:09<2:03:59, 15.79s/it]  6%|▌         | 30/500 [08:23<1:59:53, 15.31s/it]  6%|▌         | 31/500 [08:42<2:07:29, 16.31s/it]  6%|▋         | 32/500 [09:01<2:12:48, 17.03s/it]  7%|▋         | 33/500 [09:19<2:16:15, 17.51s/it]  7%|▋         | 34/500 [09:38<2:19:34, 17.97s/it]  7%|▋         | 35/500 [09:57<2:21:24, 18.25s/it]  7%|▋         | 36/500 [10:16<2:22:04, 18.37s/it]  7%|▋         | 37/500 [10:35<2:22:56, 18.52s/it]  8%|▊         | 38/500 [10:53<2:22:59, 18.57s/it]  8%|▊         | 39/500 [11:12<2:22:57, 18.61s/it]  8%|▊         | 40/500 [11:31<2:22:46, 18.62s/it]  8%|▊         | 41/500 [11:49<2:21:51, 18.54s/it]  8%|▊         | 42/500 [12:03<2:11:36, 17.24s/it]  9%|▊         | 43/500 [12:18<2:04:35, 16.36s/it]  9%|▊         | 43/500 [12:18<2:10:43, 17.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.019 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▄▄▁▄▁▁▁▅▆▇███▇██▆▇▇▆█▇█▇██████████████
wandb:     train_loss ▁▁▁▁▃▁▅▃█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▃▆▁▁▁▁▁▆▅█▅▇▇▆▇▆▃▄▃▃▆▄▅▅▆▆▇▇▅▇▅▆▆▆▆▆▆▆
wandb:       val_loss ▂▂▂▁▆▃▁█▂▁▃▁▁▃▂▁▃▁▃▄▁▂▂▁▁▄▃▆▃▄▁▄▁▁▄▁▃▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.99851
wandb:     train_loss 0.01302
wandb:   val_accuracy 0.59333
wandb:       val_loss 0.08004
wandb: 
wandb: 🚀 View run blooming-wind-17 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7le85byr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_183904-7le85byr/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_185216-03kh5qr1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-moon-18
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/03kh5qr1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:21<3:01:30, 21.82s/it]  0%|          | 2/500 [00:36<2:28:18, 17.87s/it]  1%|          | 3/500 [01:01<2:52:22, 20.81s/it]  1%|          | 4/500 [01:16<2:33:48, 18.61s/it]  1%|          | 5/500 [01:31<2:23:00, 17.33s/it]  1%|          | 6/500 [01:46<2:15:23, 16.44s/it]  1%|▏         | 7/500 [02:00<2:10:13, 15.85s/it]  2%|▏         | 8/500 [02:15<2:06:51, 15.47s/it]  2%|▏         | 9/500 [02:34<2:16:17, 16.66s/it]  2%|▏         | 10/500 [02:54<2:23:46, 17.61s/it]  2%|▏         | 11/500 [03:18<2:38:08, 19.40s/it]  2%|▏         | 12/500 [03:33<2:26:57, 18.07s/it]  3%|▎         | 13/500 [03:47<2:18:26, 17.06s/it]  3%|▎         | 14/500 [04:02<2:12:18, 16.34s/it]  3%|▎         | 15/500 [04:21<2:19:21, 17.24s/it]  3%|▎         | 16/500 [04:41<2:24:10, 17.87s/it]  3%|▎         | 17/500 [05:00<2:27:28, 18.32s/it]  4%|▎         | 18/500 [05:20<2:30:48, 18.77s/it]  4%|▍         | 19/500 [05:44<2:44:19, 20.50s/it]  4%|▍         | 20/500 [06:00<2:32:59, 19.12s/it]  4%|▍         | 21/500 [06:21<2:36:45, 19.63s/it]  4%|▍         | 22/500 [06:42<2:39:56, 20.08s/it]  5%|▍         | 23/500 [07:02<2:38:43, 19.97s/it]  5%|▍         | 24/500 [07:22<2:37:59, 19.91s/it]  5%|▌         | 25/500 [07:46<2:47:11, 21.12s/it]  5%|▌         | 26/500 [08:01<2:32:14, 19.27s/it]  5%|▌         | 27/500 [08:20<2:32:46, 19.38s/it]  6%|▌         | 28/500 [08:39<2:31:50, 19.30s/it]  6%|▌         | 29/500 [08:54<2:21:02, 17.97s/it]  6%|▌         | 30/500 [09:13<2:23:27, 18.31s/it]  6%|▌         | 31/500 [09:32<2:25:03, 18.56s/it]  6%|▋         | 32/500 [09:52<2:27:41, 18.94s/it]  7%|▋         | 33/500 [10:12<2:28:15, 19.05s/it]  7%|▋         | 34/500 [10:31<2:28:16, 19.09s/it]  7%|▋         | 35/500 [10:50<2:28:21, 19.14s/it]  7%|▋         | 36/500 [11:06<2:20:39, 18.19s/it]  7%|▋         | 37/500 [11:27<2:26:04, 18.93s/it]  8%|▊         | 38/500 [11:47<2:29:41, 19.44s/it]  8%|▊         | 39/500 [12:08<2:32:01, 19.79s/it]  8%|▊         | 40/500 [12:28<2:33:31, 20.02s/it]  8%|▊         | 41/500 [12:49<2:33:39, 20.09s/it]  8%|▊         | 42/500 [13:08<2:32:10, 19.94s/it]  9%|▊         | 43/500 [13:23<2:20:15, 18.42s/it]  9%|▉         | 44/500 [13:42<2:21:37, 18.64s/it]  9%|▉         | 45/500 [14:01<2:22:42, 18.82s/it]  9%|▉         | 46/500 [14:21<2:24:52, 19.15s/it]  9%|▉         | 47/500 [14:41<2:25:03, 19.21s/it] 10%|▉         | 48/500 [15:00<2:24:40, 19.21s/it] 10%|▉         | 49/500 [15:19<2:24:15, 19.19s/it] 10%|█         | 50/500 [15:34<2:13:49, 17.84s/it] 10%|█         | 51/500 [15:53<2:16:33, 18.25s/it] 10%|█         | 52/500 [16:12<2:18:52, 18.60s/it] 11%|█         | 53/500 [16:32<2:19:55, 18.78s/it] 11%|█         | 54/500 [16:51<2:20:25, 18.89s/it] 11%|█         | 55/500 [17:11<2:22:32, 19.22s/it] 11%|█         | 56/500 [17:30<2:23:05, 19.34s/it] 11%|█▏        | 57/500 [17:45<2:12:17, 17.92s/it] 12%|█▏        | 58/500 [18:00<2:05:01, 16.97s/it] 12%|█▏        | 59/500 [18:19<2:10:09, 17.71s/it] 12%|█▏        | 60/500 [18:34<2:04:10, 16.93s/it] 12%|█▏        | 61/500 [18:54<2:09:21, 17.68s/it] 12%|█▏        | 61/500 [18:54<2:16:02, 18.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▄▆▇▆▇▆▇▇▇▇▇█▇▄▆▇▇▇▇▇▇▇▇▇██▇▇▇▁▇████▄█
wandb:     train_loss ▃▃▃▃▃▃▃▂▂▁▂▂▃▃▃▂▂▁▂▃▃▂▂▂▂▂▂▂▂▁▂▃█▁▂▃▂▃▄▂
wandb:   val_accuracy ▂▂██▄▅▂▃▃▄▃▃▃▃▃▄▂▃▄▄▄▄▄▄▄▄▄▄▄▄▅▄▁▅▄▄▄▅▁▅
wandb:       val_loss ▅▅▅▅▅▅▆▅▆▅▅▇▃▅▄▄▁▃▄▆█▂▅▄▃▄▃▄▃▆▆▄█▄▅▃▄▄█▃
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 0.0
wandb: train_accuracy 0.76374
wandb:     train_loss 0.91349
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.78306
wandb: 
wandb: 🚀 View run peach-moon-18 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/03kh5qr1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_185216-03kh5qr1/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_191156-r5lyql77
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-galaxy-19
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/r5lyql77
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:57:24, 28.55s/it]  0%|          | 2/500 [00:43<2:51:14, 20.63s/it]  1%|          | 3/500 [01:07<3:03:07, 22.11s/it]  1%|          | 4/500 [01:22<2:38:37, 19.19s/it]  1%|          | 5/500 [01:41<2:38:30, 19.21s/it]  1%|          | 6/500 [02:00<2:37:39, 19.15s/it]  1%|▏         | 7/500 [02:19<2:37:00, 19.11s/it]  2%|▏         | 8/500 [02:38<2:37:20, 19.19s/it]  2%|▏         | 9/500 [02:58<2:37:11, 19.21s/it]  2%|▏         | 10/500 [03:17<2:36:35, 19.18s/it]  2%|▏         | 11/500 [03:36<2:36:04, 19.15s/it]  2%|▏         | 12/500 [03:55<2:35:44, 19.15s/it]  3%|▎         | 13/500 [04:09<2:23:56, 17.73s/it]  3%|▎         | 14/500 [04:24<2:15:29, 16.73s/it]  3%|▎         | 15/500 [04:43<2:22:07, 17.58s/it]  3%|▎         | 16/500 [05:03<2:26:03, 18.11s/it]  3%|▎         | 17/500 [05:22<2:28:00, 18.39s/it]  4%|▎         | 18/500 [05:41<2:29:08, 18.56s/it]  4%|▍         | 19/500 [06:00<2:30:19, 18.75s/it]  4%|▍         | 20/500 [06:19<2:31:31, 18.94s/it]  4%|▍         | 21/500 [06:38<2:31:33, 18.99s/it]  4%|▍         | 22/500 [06:57<2:31:27, 19.01s/it]  5%|▍         | 23/500 [07:12<2:20:16, 17.64s/it]  5%|▍         | 24/500 [07:31<2:22:54, 18.01s/it]  5%|▌         | 25/500 [07:45<2:14:22, 16.97s/it]  5%|▌         | 26/500 [08:04<2:18:42, 17.56s/it]  5%|▌         | 27/500 [08:19<2:11:56, 16.74s/it]  6%|▌         | 28/500 [08:34<2:06:34, 16.09s/it]  6%|▌         | 29/500 [08:58<2:24:56, 18.46s/it]  6%|▌         | 30/500 [09:13<2:16:19, 17.40s/it]  6%|▌         | 31/500 [09:32<2:19:48, 17.89s/it]  6%|▋         | 32/500 [09:51<2:22:20, 18.25s/it]  7%|▋         | 33/500 [10:10<2:24:05, 18.51s/it]  7%|▋         | 34/500 [10:25<2:15:09, 17.40s/it]  7%|▋         | 35/500 [10:40<2:09:33, 16.72s/it]  7%|▋         | 36/500 [10:59<2:15:18, 17.50s/it]  7%|▋         | 37/500 [11:18<2:18:40, 17.97s/it]  8%|▊         | 38/500 [11:37<2:21:22, 18.36s/it]  8%|▊         | 39/500 [11:57<2:23:08, 18.63s/it]  8%|▊         | 40/500 [12:16<2:24:06, 18.80s/it]  8%|▊         | 41/500 [12:35<2:24:19, 18.86s/it]  8%|▊         | 42/500 [12:54<2:24:29, 18.93s/it]  9%|▊         | 43/500 [13:08<2:13:55, 17.58s/it]  9%|▉         | 44/500 [13:23<2:06:04, 16.59s/it]  9%|▉         | 45/500 [13:37<2:00:48, 15.93s/it]  9%|▉         | 46/500 [13:56<2:07:03, 16.79s/it]  9%|▉         | 47/500 [14:15<2:11:52, 17.47s/it] 10%|▉         | 48/500 [14:34<2:14:24, 17.84s/it] 10%|▉         | 49/500 [14:52<2:16:05, 18.10s/it] 10%|█         | 50/500 [15:12<2:18:48, 18.51s/it] 10%|█         | 51/500 [15:31<2:19:53, 18.69s/it] 10%|█         | 52/500 [15:50<2:19:43, 18.71s/it] 11%|█         | 53/500 [16:08<2:19:24, 18.71s/it] 11%|█         | 54/500 [16:23<2:09:59, 17.49s/it] 11%|█         | 55/500 [16:38<2:03:03, 16.59s/it] 11%|█         | 56/500 [16:52<1:58:40, 16.04s/it] 11%|█▏        | 57/500 [17:07<1:54:43, 15.54s/it] 12%|█▏        | 58/500 [17:26<2:01:54, 16.55s/it] 12%|█▏        | 59/500 [17:40<1:57:05, 15.93s/it] 12%|█▏        | 60/500 [17:55<1:53:56, 15.54s/it] 12%|█▏        | 61/500 [18:10<1:52:15, 15.34s/it] 12%|█▏        | 62/500 [18:29<2:01:22, 16.63s/it] 13%|█▎        | 63/500 [18:48<2:06:53, 17.42s/it] 13%|█▎        | 64/500 [19:08<2:10:13, 17.92s/it] 13%|█▎        | 65/500 [19:27<2:12:32, 18.28s/it] 13%|█▎        | 65/500 [19:31<2:10:40, 18.02s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.019 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▅▆▅▁▁▂▆▁▃█▁▂▂▇▄▇██▂█▇▃██▆▄▆▄▄█▇▇▇
wandb:     train_loss ▆▆▁▅▄▂▂▄▃▄▄▄█▃▂▄▂▅▄▂▃▃▃▂▃▄▃▃▃▃▃▃▂▂▄▃▃▄▃▃
wandb:   val_accuracy ▂▂▂▂▂▂▂▂▂▂▂▂▁▇▂▂▇▂▂▂▃▂▃▄█▂▄▃▂▃▃▂▂▂▂▂▃▂▂▂
wandb:       val_loss ▃▃█▄▇▆█▄▃▃▃▃▇▃▅▄▄▆▁▃▄▅▃▄▄▃▅▃▅▄▄▃▅▅▄▄▃▃▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 64
wandb:  learning_rate 0.0
wandb: train_accuracy 0.526
wandb:     train_loss 1.0546
wandb:   val_accuracy 0.37111
wandb:       val_loss 1.06777
wandb: 
wandb: 🚀 View run quiet-galaxy-19 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/r5lyql77
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_191156-r5lyql77/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_193216-tcpzgmlt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-glade-20
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tcpzgmlt
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:54:05, 28.15s/it]  0%|          | 2/500 [00:47<3:12:56, 23.25s/it]  1%|          | 3/500 [01:07<2:58:40, 21.57s/it]  1%|          | 4/500 [01:27<2:51:50, 20.79s/it]  1%|          | 5/500 [01:42<2:34:00, 18.67s/it]  1%|          | 6/500 [02:01<2:35:55, 18.94s/it]  1%|▏         | 7/500 [02:20<2:36:33, 19.05s/it]  2%|▏         | 8/500 [02:35<2:25:49, 17.78s/it]  2%|▏         | 9/500 [02:55<2:30:14, 18.36s/it]  2%|▏         | 10/500 [03:15<2:33:12, 18.76s/it]  2%|▏         | 11/500 [03:30<2:23:15, 17.58s/it]  2%|▏         | 12/500 [03:49<2:27:27, 18.13s/it]  3%|▎         | 13/500 [04:08<2:29:59, 18.48s/it]  3%|▎         | 14/500 [04:28<2:33:05, 18.90s/it]  3%|▎         | 15/500 [04:48<2:34:10, 19.07s/it]  3%|▎         | 16/500 [05:07<2:35:17, 19.25s/it]  3%|▎         | 17/500 [05:27<2:36:05, 19.39s/it]  4%|▎         | 18/500 [05:42<2:24:32, 17.99s/it]  4%|▍         | 19/500 [06:01<2:27:02, 18.34s/it]  4%|▍         | 20/500 [06:20<2:29:18, 18.66s/it]  4%|▍         | 21/500 [06:35<2:20:28, 17.60s/it]  4%|▍         | 22/500 [06:55<2:24:34, 18.15s/it]  5%|▍         | 23/500 [07:10<2:16:20, 17.15s/it]  5%|▍         | 24/500 [07:29<2:21:13, 17.80s/it]  5%|▌         | 25/500 [07:48<2:24:16, 18.22s/it]  5%|▌         | 26/500 [08:12<2:38:03, 20.01s/it]  5%|▌         | 27/500 [08:27<2:25:31, 18.46s/it]  6%|▌         | 28/500 [08:42<2:16:21, 17.33s/it]  6%|▌         | 29/500 [09:02<2:22:12, 18.12s/it]  6%|▌         | 30/500 [09:21<2:25:36, 18.59s/it]  6%|▌         | 31/500 [09:41<2:27:49, 18.91s/it]  6%|▋         | 32/500 [10:01<2:28:57, 19.10s/it]  7%|▋         | 33/500 [10:20<2:29:12, 19.17s/it]  7%|▋         | 34/500 [10:40<2:30:02, 19.32s/it]  7%|▋         | 35/500 [10:59<2:29:47, 19.33s/it]  7%|▋         | 36/500 [11:18<2:29:39, 19.35s/it]  7%|▋         | 37/500 [11:33<2:18:33, 17.96s/it]  8%|▊         | 38/500 [11:53<2:21:32, 18.38s/it]  8%|▊         | 39/500 [12:07<2:12:47, 17.28s/it]  8%|▊         | 40/500 [12:27<2:17:24, 17.92s/it]  8%|▊         | 41/500 [12:46<2:21:17, 18.47s/it]  8%|▊         | 42/500 [13:06<2:23:44, 18.83s/it]  9%|▊         | 43/500 [13:26<2:25:21, 19.08s/it]  9%|▉         | 44/500 [13:41<2:15:39, 17.85s/it]  9%|▉         | 45/500 [14:00<2:19:28, 18.39s/it]  9%|▉         | 46/500 [14:24<2:30:45, 19.92s/it]  9%|▉         | 47/500 [14:39<2:19:47, 18.51s/it] 10%|▉         | 48/500 [14:59<2:22:31, 18.92s/it] 10%|▉         | 49/500 [15:19<2:23:59, 19.16s/it] 10%|█         | 50/500 [15:38<2:24:04, 19.21s/it] 10%|█         | 51/500 [15:58<2:24:23, 19.30s/it] 10%|█         | 52/500 [16:17<2:24:14, 19.32s/it] 11%|█         | 53/500 [16:36<2:24:34, 19.41s/it] 11%|█         | 54/500 [16:56<2:25:33, 19.58s/it] 11%|█         | 55/500 [17:11<2:14:37, 18.15s/it] 11%|█         | 56/500 [17:31<2:17:02, 18.52s/it] 11%|█▏        | 57/500 [17:50<2:19:10, 18.85s/it] 12%|█▏        | 58/500 [18:10<2:21:23, 19.19s/it] 12%|█▏        | 59/500 [18:30<2:22:41, 19.41s/it] 12%|█▏        | 60/500 [18:45<2:12:57, 18.13s/it] 12%|█▏        | 61/500 [19:00<2:05:41, 17.18s/it] 12%|█▏        | 62/500 [19:15<2:00:52, 16.56s/it] 12%|█▏        | 62/500 [19:20<2:16:40, 18.72s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.010 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▃▁▃▁▁▇▆▆▄▆▆██▆▇▇██████████████████████
wandb:     train_loss ▂▂▁▃▁▄█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▅▁▁▁▁▇▄▃▂▅▆██▆▇▆▇▇█▇▇▇▇██▇▇▇▆▇▇▆▇▇▆▆▇▇
wandb:       val_loss ▂▂▁▅▁█▁▃▁▂▄▃▃▅▂▄▁▅▃▄█▁▅▃▁▁▁▁▃▄▃▄▅▂▂▃▄▃▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.98366
wandb:     train_loss 0.00455
wandb:   val_accuracy 0.53556
wandb:       val_loss 5.49571
wandb: 
wandb: 🚀 View run celestial-glade-20 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tcpzgmlt
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_193216-tcpzgmlt/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_195216-tm3bkomo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-aardvark-21
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tm3bkomo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:01:49, 29.08s/it]  0%|          | 2/500 [00:44<2:55:04, 21.09s/it]  1%|          | 3/500 [01:09<3:07:39, 22.66s/it]  1%|          | 4/500 [01:28<2:56:53, 21.40s/it]  1%|          | 5/500 [01:43<2:38:09, 19.17s/it]  1%|          | 6/500 [02:03<2:38:57, 19.31s/it]  1%|▏         | 7/500 [02:27<2:51:11, 20.83s/it]  2%|▏         | 8/500 [02:42<2:36:47, 19.12s/it]  2%|▏         | 9/500 [03:07<2:49:45, 20.74s/it]  2%|▏         | 10/500 [03:22<2:36:58, 19.22s/it]  2%|▏         | 11/500 [03:43<2:40:27, 19.69s/it]  2%|▏         | 12/500 [04:03<2:40:24, 19.72s/it]  3%|▎         | 13/500 [04:23<2:40:40, 19.80s/it]  3%|▎         | 14/500 [04:47<2:51:57, 21.23s/it]  3%|▎         | 15/500 [05:02<2:36:16, 19.33s/it]  3%|▎         | 16/500 [05:23<2:38:08, 19.60s/it]  3%|▎         | 17/500 [05:44<2:41:51, 20.11s/it]  4%|▎         | 18/500 [06:04<2:42:33, 20.24s/it]  4%|▍         | 19/500 [06:25<2:43:37, 20.41s/it]  4%|▍         | 20/500 [06:41<2:32:44, 19.09s/it]  4%|▍         | 21/500 [07:02<2:37:02, 19.67s/it]  4%|▍         | 22/500 [07:18<2:26:46, 18.42s/it]  5%|▍         | 23/500 [07:34<2:20:55, 17.73s/it]  5%|▍         | 24/500 [07:49<2:15:03, 17.02s/it]  5%|▌         | 25/500 [08:09<2:20:46, 17.78s/it]  5%|▌         | 26/500 [08:28<2:24:39, 18.31s/it]  5%|▌         | 27/500 [08:48<2:27:32, 18.72s/it]  6%|▌         | 28/500 [09:08<2:30:25, 19.12s/it]  6%|▌         | 29/500 [09:28<2:31:32, 19.30s/it]  6%|▌         | 30/500 [09:43<2:20:51, 17.98s/it]  6%|▌         | 31/500 [10:02<2:23:32, 18.36s/it]  6%|▋         | 32/500 [10:21<2:25:11, 18.61s/it]  7%|▋         | 33/500 [10:45<2:37:45, 20.27s/it]  7%|▋         | 34/500 [11:00<2:24:57, 18.66s/it]  7%|▋         | 35/500 [11:15<2:15:44, 17.52s/it]  7%|▋         | 36/500 [11:30<2:08:49, 16.66s/it]  7%|▋         | 37/500 [11:45<2:04:39, 16.15s/it]  8%|▊         | 38/500 [11:59<2:00:55, 15.70s/it]  8%|▊         | 39/500 [12:19<2:09:08, 16.81s/it]  8%|▊         | 40/500 [12:38<2:14:16, 17.52s/it]  8%|▊         | 41/500 [12:57<2:17:39, 17.99s/it]  8%|▊         | 42/500 [13:12<2:09:58, 17.03s/it]  9%|▊         | 43/500 [13:26<2:03:57, 16.27s/it]  9%|▉         | 44/500 [13:46<2:10:48, 17.21s/it]  9%|▉         | 45/500 [14:05<2:15:54, 17.92s/it]  9%|▉         | 46/500 [14:25<2:18:39, 18.32s/it]  9%|▉         | 47/500 [14:44<2:20:15, 18.58s/it] 10%|▉         | 48/500 [15:03<2:21:42, 18.81s/it] 10%|▉         | 49/500 [15:22<2:21:37, 18.84s/it] 10%|█         | 50/500 [15:41<2:21:30, 18.87s/it] 10%|█         | 51/500 [16:00<2:22:13, 19.00s/it] 10%|█         | 52/500 [16:15<2:11:32, 17.62s/it] 11%|█         | 53/500 [16:30<2:05:27, 16.84s/it] 11%|█         | 54/500 [16:44<2:00:29, 16.21s/it] 11%|█         | 55/500 [17:04<2:08:07, 17.28s/it] 11%|█         | 56/500 [17:23<2:11:48, 17.81s/it] 11%|█▏        | 57/500 [17:42<2:14:09, 18.17s/it] 12%|█▏        | 58/500 [18:02<2:16:19, 18.51s/it] 12%|█▏        | 59/500 [18:16<2:07:20, 17.33s/it] 12%|█▏        | 60/500 [18:31<2:00:43, 16.46s/it] 12%|█▏        | 61/500 [18:46<1:57:07, 16.01s/it] 12%|█▏        | 62/500 [19:05<2:03:43, 16.95s/it] 13%|█▎        | 63/500 [19:19<1:58:13, 16.23s/it] 13%|█▎        | 64/500 [19:38<2:04:22, 17.12s/it] 13%|█▎        | 65/500 [19:58<2:08:41, 17.75s/it] 13%|█▎        | 66/500 [20:17<2:12:37, 18.33s/it] 13%|█▎        | 67/500 [20:37<2:14:59, 18.71s/it] 14%|█▎        | 68/500 [20:57<2:17:01, 19.03s/it] 14%|█▍        | 69/500 [21:12<2:07:43, 17.78s/it] 14%|█▍        | 70/500 [21:31<2:10:38, 18.23s/it] 14%|█▍        | 71/500 [21:46<2:03:48, 17.31s/it] 14%|█▍        | 72/500 [22:01<1:58:43, 16.64s/it] 15%|█▍        | 73/500 [22:16<1:54:22, 16.07s/it] 15%|█▍        | 73/500 [22:16<2:10:16, 18.31s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.010 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████
wandb:     train_loss █▇█▆▇▆▆▃▅▃▆▆▅▆▅▃▆▃▄▄▅▄▄▅▁▄▆▃▁▆▃▂▆▆▇▄▃▂▃▄
wandb:   val_accuracy ▁▁▁▇▆▄▄▄█▅▄▅▆▅▆▅▅▆▅▅▅▅▅▆▆▅▄▆▆▅▅▅▇▆▇▅▅▆▆▆
wandb:       val_loss ▅▆▅▆▅▅▅▅▅▅▂▇▆▅▇▆▂▄▆▄▁▅▃▅▇▇▄▅▆▃▅▅▂▃▅▇▃█▇▇
wandb: 
wandb: Run summary:
wandb:          epoch 72
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81724
wandb:     train_loss 0.56876
wandb:   val_accuracy 0.48
wandb:       val_loss 1.2314
wandb: 
wandb: 🚀 View run apricot-aardvark-21 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tm3bkomo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_195216-tm3bkomo/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_201517-h83k4nb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-tree-22
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/h83k4nb1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:10:49, 30.16s/it]  0%|          | 2/500 [00:45<2:57:34, 21.39s/it]  1%|          | 3/500 [01:04<2:49:09, 20.42s/it]  1%|          | 4/500 [01:19<2:31:45, 18.36s/it]  1%|          | 5/500 [01:34<2:21:47, 17.19s/it]  1%|          | 6/500 [01:49<2:15:24, 16.45s/it]  1%|▏         | 7/500 [02:09<2:24:33, 17.59s/it]  2%|▏         | 8/500 [02:29<2:29:41, 18.25s/it]  2%|▏         | 9/500 [02:49<2:32:26, 18.63s/it]  2%|▏         | 10/500 [03:03<2:22:22, 17.43s/it]  2%|▏         | 11/500 [03:23<2:26:47, 18.01s/it]  2%|▏         | 12/500 [03:38<2:18:50, 17.07s/it]  3%|▎         | 13/500 [03:57<2:23:57, 17.74s/it]  3%|▎         | 14/500 [04:16<2:27:25, 18.20s/it]  3%|▎         | 15/500 [04:31<2:18:55, 17.19s/it]  3%|▎         | 16/500 [04:50<2:24:17, 17.89s/it]  3%|▎         | 17/500 [05:10<2:27:45, 18.36s/it]  4%|▎         | 18/500 [05:30<2:30:56, 18.79s/it]  4%|▍         | 19/500 [05:50<2:33:18, 19.12s/it]  4%|▍         | 20/500 [06:05<2:23:16, 17.91s/it]  4%|▍         | 21/500 [06:25<2:27:42, 18.50s/it]  4%|▍         | 22/500 [06:44<2:30:39, 18.91s/it]  5%|▍         | 23/500 [07:05<2:33:06, 19.26s/it]  5%|▍         | 24/500 [07:24<2:33:36, 19.36s/it]  5%|▌         | 25/500 [07:40<2:24:29, 18.25s/it]  5%|▌         | 26/500 [07:56<2:19:08, 17.61s/it]  5%|▌         | 27/500 [08:17<2:26:19, 18.56s/it]  6%|▌         | 28/500 [08:37<2:31:12, 19.22s/it]  6%|▌         | 29/500 [09:03<2:45:17, 21.06s/it]  6%|▌         | 30/500 [09:18<2:31:13, 19.30s/it]  6%|▌         | 31/500 [09:33<2:20:47, 18.01s/it]  6%|▋         | 32/500 [09:52<2:23:51, 18.44s/it]  7%|▋         | 33/500 [10:12<2:25:52, 18.74s/it]  7%|▋         | 34/500 [10:31<2:27:27, 18.99s/it]  7%|▋         | 35/500 [10:46<2:18:01, 17.81s/it]  7%|▋         | 36/500 [11:01<2:11:04, 16.95s/it]  7%|▋         | 37/500 [11:21<2:16:01, 17.63s/it]  8%|▊         | 38/500 [11:40<2:20:32, 18.25s/it]  8%|▊         | 39/500 [12:00<2:23:08, 18.63s/it]  8%|▊         | 40/500 [12:15<2:13:54, 17.47s/it]  8%|▊         | 41/500 [12:29<2:07:08, 16.62s/it]  8%|▊         | 42/500 [12:49<2:13:19, 17.47s/it]  9%|▊         | 43/500 [13:04<2:07:11, 16.70s/it]  9%|▉         | 44/500 [13:19<2:03:45, 16.28s/it]  9%|▉         | 45/500 [13:34<2:01:09, 15.98s/it]  9%|▉         | 46/500 [13:54<2:09:23, 17.10s/it]  9%|▉         | 47/500 [14:14<2:16:07, 18.03s/it] 10%|▉         | 48/500 [14:34<2:19:59, 18.58s/it] 10%|▉         | 49/500 [14:54<2:21:58, 18.89s/it] 10%|█         | 50/500 [15:08<2:12:39, 17.69s/it] 10%|█         | 51/500 [15:28<2:16:18, 18.21s/it] 10%|█         | 52/500 [15:47<2:18:48, 18.59s/it] 11%|█         | 53/500 [16:03<2:10:57, 17.58s/it] 11%|█         | 54/500 [16:22<2:15:09, 18.18s/it] 11%|█         | 55/500 [16:37<2:07:25, 17.18s/it] 11%|█         | 56/500 [16:57<2:12:32, 17.91s/it] 11%|█▏        | 57/500 [17:16<2:16:13, 18.45s/it] 12%|█▏        | 58/500 [17:36<2:19:30, 18.94s/it] 12%|█▏        | 59/500 [17:56<2:21:20, 19.23s/it] 12%|█▏        | 60/500 [18:11<2:11:59, 18.00s/it] 12%|█▏        | 61/500 [18:31<2:15:11, 18.48s/it] 12%|█▏        | 62/500 [18:51<2:17:09, 18.79s/it] 13%|█▎        | 63/500 [19:10<2:18:15, 18.98s/it] 13%|█▎        | 64/500 [19:25<2:08:48, 17.73s/it] 13%|█▎        | 65/500 [19:44<2:12:15, 18.24s/it] 13%|█▎        | 66/500 [20:04<2:14:40, 18.62s/it] 13%|█▎        | 66/500 [20:08<2:12:28, 18.31s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.312 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▅▁▁▄▃▆▃▅▃▅▅▁█▇▇▄▇▃▇▂█▇▆█▃▃▆▇▆▇▇▇
wandb:     train_loss ██▁▇▆▄▇▅▄▂▅▅▅▄▅▄▂▅▄▃▄▄▄▄▃▅▄▃▅▄▃▄▅▅▆▅▅▄▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▂▁▁▂▂█▂▆▁▂▂▁▃▃▂▁▂▂▃▂▃▃▂▃▂▂▂▂▂▂▂▂
wandb:       val_loss ▃▃▇▆█▄▃▄▃▃▃▂▄▂▂▄▆▁▄▂▂▂▃▄▄▂▂▅▄▃▃▃▂▃▃▂▂▃▄▂
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.53046
wandb:     train_loss 1.10613
wandb:   val_accuracy 0.36667
wandb:       val_loss 1.05485
wandb: 
wandb: 🚀 View run blooming-tree-22 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/h83k4nb1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_201517-h83k4nb1/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_203615-a7puigr9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-donkey-23
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a7puigr9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<4:00:45, 28.95s/it]  0%|          | 2/500 [00:49<3:17:18, 23.77s/it]  1%|          | 3/500 [01:08<3:00:48, 21.83s/it]  1%|          | 4/500 [01:23<2:37:28, 19.05s/it]  1%|          | 5/500 [01:42<2:37:15, 19.06s/it]  1%|          | 6/500 [01:57<2:24:17, 17.53s/it]  1%|▏         | 7/500 [02:20<2:40:55, 19.59s/it]  2%|▏         | 8/500 [02:35<2:27:47, 18.02s/it]  2%|▏         | 9/500 [02:54<2:30:24, 18.38s/it]  2%|▏         | 10/500 [03:13<2:31:56, 18.61s/it]  2%|▏         | 11/500 [03:28<2:21:29, 17.36s/it]  2%|▏         | 12/500 [03:47<2:25:43, 17.92s/it]  3%|▎         | 13/500 [04:06<2:28:00, 18.23s/it]  3%|▎         | 14/500 [04:25<2:29:54, 18.51s/it]  3%|▎         | 15/500 [04:40<2:19:49, 17.30s/it]  3%|▎         | 16/500 [04:59<2:24:18, 17.89s/it]  3%|▎         | 17/500 [05:18<2:26:47, 18.23s/it]  4%|▎         | 18/500 [05:37<2:28:21, 18.47s/it]  4%|▍         | 19/500 [05:56<2:29:13, 18.61s/it]  4%|▍         | 20/500 [06:10<2:18:41, 17.34s/it]  4%|▍         | 21/500 [06:25<2:12:25, 16.59s/it]  4%|▍         | 22/500 [06:40<2:07:03, 15.95s/it]  5%|▍         | 23/500 [06:59<2:14:47, 16.95s/it]  5%|▍         | 24/500 [07:18<2:18:59, 17.52s/it]  5%|▌         | 25/500 [07:37<2:22:04, 17.95s/it]  5%|▌         | 26/500 [07:51<2:13:45, 16.93s/it]  5%|▌         | 27/500 [08:06<2:08:04, 16.25s/it]  6%|▌         | 28/500 [08:20<2:03:36, 15.71s/it]  6%|▌         | 29/500 [08:39<2:10:52, 16.67s/it]  6%|▌         | 30/500 [08:59<2:17:05, 17.50s/it]  6%|▌         | 31/500 [09:18<2:20:38, 17.99s/it]  6%|▋         | 32/500 [09:37<2:22:46, 18.30s/it]  7%|▋         | 33/500 [09:56<2:24:06, 18.52s/it]  7%|▋         | 34/500 [10:15<2:25:15, 18.70s/it]  7%|▋         | 35/500 [10:34<2:25:20, 18.75s/it]  7%|▋         | 36/500 [10:53<2:25:16, 18.79s/it]  7%|▋         | 37/500 [11:07<2:15:13, 17.52s/it]  8%|▊         | 38/500 [11:26<2:18:39, 18.01s/it]  8%|▊         | 39/500 [11:41<2:10:06, 16.93s/it]  8%|▊         | 40/500 [11:55<2:04:30, 16.24s/it]  8%|▊         | 41/500 [12:10<2:00:38, 15.77s/it]  8%|▊         | 42/500 [12:25<1:58:00, 15.46s/it]  9%|▊         | 43/500 [12:40<1:56:36, 15.31s/it]  9%|▉         | 44/500 [12:54<1:54:35, 15.08s/it]  9%|▉         | 45/500 [13:14<2:04:34, 16.43s/it]  9%|▉         | 46/500 [13:29<2:00:02, 15.87s/it]  9%|▉         | 47/500 [13:48<2:06:54, 16.81s/it] 10%|▉         | 48/500 [14:07<2:11:39, 17.48s/it] 10%|▉         | 49/500 [14:26<2:15:23, 18.01s/it] 10%|█         | 50/500 [14:40<2:07:19, 16.98s/it] 10%|█         | 51/500 [14:55<2:01:25, 16.23s/it] 10%|█         | 52/500 [15:09<1:57:19, 15.71s/it] 11%|█         | 53/500 [15:24<1:55:16, 15.47s/it] 11%|█         | 54/500 [15:44<2:03:22, 16.60s/it] 11%|█         | 55/500 [16:03<2:08:45, 17.36s/it] 11%|█         | 56/500 [16:22<2:12:53, 17.96s/it] 11%|█▏        | 57/500 [16:41<2:14:42, 18.24s/it] 12%|█▏        | 58/500 [16:56<2:06:27, 17.17s/it] 12%|█▏        | 59/500 [17:10<2:00:19, 16.37s/it] 12%|█▏        | 60/500 [17:29<2:06:17, 17.22s/it] 12%|█▏        | 61/500 [17:44<1:59:50, 16.38s/it] 12%|█▏        | 62/500 [18:03<2:05:11, 17.15s/it] 13%|█▎        | 63/500 [18:17<1:59:09, 16.36s/it] 13%|█▎        | 64/500 [18:36<2:04:36, 17.15s/it] 13%|█▎        | 65/500 [18:55<2:08:58, 17.79s/it] 13%|█▎        | 66/500 [19:10<2:01:59, 16.86s/it] 13%|█▎        | 67/500 [19:25<1:56:46, 16.18s/it] 14%|█▎        | 68/500 [19:44<2:03:14, 17.12s/it] 14%|█▍        | 69/500 [20:03<2:07:18, 17.72s/it] 14%|█▍        | 70/500 [20:22<2:09:36, 18.08s/it] 14%|█▍        | 71/500 [20:41<2:11:04, 18.33s/it] 14%|█▍        | 72/500 [20:56<2:02:57, 17.24s/it] 15%|█▍        | 73/500 [21:11<1:57:34, 16.52s/it] 15%|█▍        | 74/500 [21:25<1:53:24, 15.97s/it] 15%|█▍        | 74/500 [21:25<2:03:21, 17.37s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.030 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.233 MB of 0.315 MB uploadedwandb: | 0.233 MB of 0.315 MB uploadedwandb: / 0.233 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▁▁▅▁▄▆▇▇▇▇▆▇██████████████████████████
wandb:     train_loss ▂▁▆▁▃█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▁▁▇▁▃▆▅▆▅▆▅▆▆▆▆███▇▇▇▆▆▆▇▇▇█▇▇█▇▇▇████
wandb:       val_loss ▂▂▃▃▅▁▃▄▃█▄▆▁▂▅▃▆▅▄▁▁▁▃▄▄▃▄▄▆▇▁▄▃▆▄▅▃▃▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 73
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 0.00033
wandb:   val_accuracy 0.59333
wandb:       val_loss 0.08576
wandb: 
wandb: 🚀 View run celestial-donkey-23 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a7puigr9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_203615-a7puigr9/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_205821-0a8jtno7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-frog-24
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/0a8jtno7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:02:15, 29.13s/it]  0%|          | 2/500 [00:43<2:50:38, 20.56s/it]  1%|          | 3/500 [01:07<3:00:44, 21.82s/it]  1%|          | 4/500 [01:25<2:50:33, 20.63s/it]  1%|          | 5/500 [01:40<2:31:58, 18.42s/it]  1%|          | 6/500 [01:59<2:33:22, 18.63s/it]  1%|▏         | 7/500 [02:22<2:45:42, 20.17s/it]  2%|▏         | 8/500 [02:36<2:29:54, 18.28s/it]  2%|▏         | 9/500 [03:00<2:42:56, 19.91s/it]  2%|▏         | 10/500 [03:14<2:27:53, 18.11s/it]  2%|▏         | 11/500 [03:33<2:29:20, 18.33s/it]  2%|▏         | 12/500 [03:51<2:29:47, 18.42s/it]  3%|▎         | 13/500 [04:10<2:30:28, 18.54s/it]  3%|▎         | 14/500 [04:25<2:19:54, 17.27s/it]  3%|▎         | 15/500 [04:39<2:12:22, 16.38s/it]  3%|▎         | 16/500 [04:53<2:06:46, 15.72s/it]  3%|▎         | 17/500 [05:12<2:14:26, 16.70s/it]  4%|▎         | 18/500 [05:26<2:08:22, 15.98s/it]  4%|▍         | 19/500 [05:41<2:04:33, 15.54s/it]  4%|▍         | 20/500 [05:56<2:02:52, 15.36s/it]  4%|▍         | 21/500 [06:11<2:02:18, 15.32s/it]  4%|▍         | 22/500 [06:25<1:59:33, 15.01s/it]  5%|▍         | 23/500 [06:40<1:57:37, 14.79s/it]  5%|▍         | 24/500 [06:54<1:56:10, 14.64s/it]  5%|▌         | 25/500 [07:10<2:00:17, 15.19s/it]  5%|▌         | 26/500 [07:24<1:57:18, 14.85s/it]  5%|▌         | 27/500 [07:39<1:56:09, 14.74s/it]  6%|▌         | 28/500 [07:58<2:05:25, 15.94s/it]  6%|▌         | 29/500 [08:21<2:22:45, 18.19s/it]  6%|▌         | 30/500 [08:35<2:12:53, 16.96s/it]  6%|▌         | 31/500 [08:54<2:16:10, 17.42s/it]  6%|▋         | 32/500 [09:08<2:08:02, 16.41s/it]  7%|▋         | 33/500 [09:31<2:24:45, 18.60s/it]  7%|▋         | 34/500 [09:46<2:14:46, 17.35s/it]  7%|▋         | 35/500 [10:05<2:17:37, 17.76s/it]  7%|▋         | 36/500 [10:23<2:18:33, 17.92s/it]  7%|▋         | 37/500 [10:42<2:20:12, 18.17s/it]  8%|▊         | 38/500 [10:56<2:10:28, 16.94s/it]  8%|▊         | 39/500 [11:10<2:03:48, 16.11s/it]  8%|▊         | 40/500 [11:24<1:59:00, 15.52s/it]  8%|▊         | 41/500 [11:38<1:56:09, 15.18s/it]  8%|▊         | 42/500 [11:53<1:54:23, 14.99s/it]  9%|▊         | 43/500 [12:07<1:51:40, 14.66s/it]  9%|▉         | 44/500 [12:21<1:50:13, 14.50s/it]  9%|▉         | 45/500 [12:36<1:51:06, 14.65s/it]  9%|▉         | 46/500 [12:52<1:52:48, 14.91s/it]  9%|▉         | 47/500 [13:10<2:01:04, 16.04s/it] 10%|▉         | 48/500 [13:29<2:06:53, 16.84s/it] 10%|▉         | 49/500 [13:47<2:10:01, 17.30s/it] 10%|█         | 50/500 [14:06<2:13:14, 17.77s/it] 10%|█         | 51/500 [14:25<2:14:50, 18.02s/it] 10%|█         | 52/500 [14:39<2:05:23, 16.79s/it] 11%|█         | 53/500 [14:53<1:59:27, 16.04s/it] 11%|█         | 54/500 [15:07<1:54:37, 15.42s/it] 11%|█         | 55/500 [15:21<1:51:34, 15.04s/it] 11%|█         | 56/500 [15:35<1:49:27, 14.79s/it] 11%|█▏        | 57/500 [15:54<1:58:09, 16.00s/it] 12%|█▏        | 58/500 [16:13<2:04:11, 16.86s/it] 12%|█▏        | 59/500 [16:27<1:58:15, 16.09s/it] 12%|█▏        | 60/500 [16:42<1:54:20, 15.59s/it] 12%|█▏        | 61/500 [16:56<1:51:46, 15.28s/it] 12%|█▏        | 62/500 [17:15<1:58:28, 16.23s/it] 13%|█▎        | 63/500 [17:29<1:53:20, 15.56s/it] 13%|█▎        | 64/500 [17:47<1:59:09, 16.40s/it] 13%|█▎        | 65/500 [18:01<1:53:37, 15.67s/it] 13%|█▎        | 66/500 [18:20<1:59:54, 16.58s/it] 13%|█▎        | 67/500 [18:34<1:54:26, 15.86s/it] 14%|█▎        | 68/500 [18:48<1:50:55, 15.41s/it] 14%|█▍        | 69/500 [19:03<1:48:30, 15.11s/it] 14%|█▍        | 70/500 [19:17<1:46:58, 14.93s/it] 14%|█▍        | 71/500 [19:36<1:54:48, 16.06s/it] 14%|█▍        | 72/500 [19:54<1:59:44, 16.79s/it] 15%|█▍        | 73/500 [20:09<1:55:00, 16.16s/it] 15%|█▍        | 73/500 [20:14<1:58:21, 16.63s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.019 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇█▇▇████████████████████
wandb:     train_loss █▇█▆▇▆▆▃▅▃▆▆▅▇▅▃▆▃▄▄▅▄▄▅▁▄▅▃▁▆▃▂▆▆▆▄▃▂▃▄
wandb:   val_accuracy ▁▁▁▇▆▃▅▄█▅▄▅▆▅▆▅▅▆▅▅▅▅▅▅▅▅▅▆▅▅▅▆▇▆▇▆▅▆▅▆
wandb:       val_loss ▅▅▅▅▅▅▅▅▅▄▂▆▅▅▇▅▃▃▆▃▁▄▃▅▆▇▃▄▅▃▅▄▁▃▄▆▃█▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 72
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81724
wandb:     train_loss 0.51969
wandb:   val_accuracy 0.48444
wandb:       val_loss 1.12809
wandb: 
wandb: 🚀 View run golden-frog-24 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/0a8jtno7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_205821-0a8jtno7/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_211920-gyjdmh5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-night-25
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gyjdmh5c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:54:34, 28.20s/it]  0%|          | 2/500 [00:43<2:50:16, 20.52s/it]  1%|          | 3/500 [01:07<3:03:35, 22.16s/it]  1%|          | 4/500 [01:23<2:42:24, 19.65s/it]  1%|          | 5/500 [01:43<2:43:32, 19.82s/it]  1%|          | 6/500 [02:03<2:43:50, 19.90s/it]  1%|▏         | 7/500 [02:23<2:44:11, 19.98s/it]  2%|▏         | 8/500 [02:43<2:44:24, 20.05s/it]  2%|▏         | 9/500 [03:04<2:45:46, 20.26s/it]  2%|▏         | 10/500 [03:24<2:45:34, 20.27s/it]  2%|▏         | 11/500 [03:41<2:35:47, 19.11s/it]  2%|▏         | 12/500 [04:01<2:38:24, 19.48s/it]  3%|▎         | 13/500 [04:22<2:42:08, 19.98s/it]  3%|▎         | 14/500 [04:43<2:44:52, 20.36s/it]  3%|▎         | 15/500 [05:05<2:46:46, 20.63s/it]  3%|▎         | 16/500 [05:26<2:47:08, 20.72s/it]  3%|▎         | 17/500 [05:42<2:35:35, 19.33s/it]  4%|▎         | 18/500 [06:02<2:37:24, 19.59s/it]  4%|▍         | 19/500 [06:22<2:37:49, 19.69s/it]  4%|▍         | 20/500 [06:42<2:37:49, 19.73s/it]  4%|▍         | 21/500 [07:06<2:49:13, 21.20s/it]  4%|▍         | 22/500 [07:22<2:35:14, 19.49s/it]  5%|▍         | 23/500 [07:37<2:24:25, 18.17s/it]  5%|▍         | 24/500 [07:52<2:17:37, 17.35s/it]  5%|▌         | 25/500 [08:08<2:13:11, 16.82s/it]  5%|▌         | 26/500 [08:24<2:10:00, 16.46s/it]  5%|▌         | 27/500 [08:44<2:18:46, 17.60s/it]  6%|▌         | 28/500 [09:04<2:25:14, 18.46s/it]  6%|▌         | 29/500 [09:24<2:28:55, 18.97s/it]  6%|▌         | 30/500 [09:40<2:20:07, 17.89s/it]  6%|▌         | 31/500 [10:00<2:24:41, 18.51s/it]  6%|▋         | 32/500 [10:15<2:16:48, 17.54s/it]  7%|▋         | 33/500 [10:30<2:11:27, 16.89s/it]  7%|▋         | 34/500 [10:53<2:23:35, 18.49s/it]  7%|▋         | 35/500 [11:08<2:16:34, 17.62s/it]  7%|▋         | 36/500 [11:28<2:22:07, 18.38s/it]  7%|▋         | 37/500 [11:49<2:25:59, 18.92s/it]  8%|▊         | 38/500 [12:09<2:28:08, 19.24s/it]  8%|▊         | 39/500 [12:28<2:29:25, 19.45s/it]  8%|▊         | 40/500 [12:44<2:19:34, 18.21s/it]  8%|▊         | 41/500 [13:04<2:23:10, 18.72s/it]  8%|▊         | 42/500 [13:19<2:15:41, 17.78s/it]  9%|▊         | 43/500 [13:40<2:21:02, 18.52s/it]  9%|▉         | 44/500 [14:00<2:24:11, 18.97s/it]  9%|▉         | 45/500 [14:15<2:15:52, 17.92s/it]  9%|▉         | 46/500 [14:31<2:10:23, 17.23s/it]  9%|▉         | 47/500 [14:46<2:05:19, 16.60s/it] 10%|▉         | 48/500 [15:01<2:01:43, 16.16s/it] 10%|▉         | 49/500 [15:16<1:59:19, 15.88s/it] 10%|█         | 50/500 [15:31<1:57:38, 15.69s/it] 10%|█         | 51/500 [15:47<1:56:38, 15.59s/it] 10%|█         | 52/500 [16:02<1:55:17, 15.44s/it] 11%|█         | 53/500 [16:18<1:56:54, 15.69s/it] 11%|█         | 54/500 [16:38<2:06:24, 17.01s/it] 11%|█         | 55/500 [16:58<2:12:32, 17.87s/it] 11%|█         | 56/500 [17:18<2:17:15, 18.55s/it] 11%|█▏        | 57/500 [17:38<2:19:54, 18.95s/it] 11%|█▏        | 57/500 [17:43<2:17:42, 18.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▄▁▁▆▄▁▆▁▆▁▁▃▄▄▆▃▆▃▂▆▃▄▇▄▆▄█▇▇▇█
wandb:     train_loss ██▆▆█▆▁▇▅▅▃▂▃▄▅▄▂▄▅▆▅▃▇▄▄▄▂▄▂▅▄▄▂▄▅▃▄▅▂▅
wandb:   val_accuracy ▃▃▃▃▃▃▃▃▃▁▃▃▄▂▃▅▃▆▃▃▃▂▂▅▄▆▄▃▅▃▅▇▄▆▄▇▆█▇▇
wandb:       val_loss ▄▄▃▄█▆▇▄▆▆▆▄▄▄▁▃▅▄▄▇▂▃▅▃▅▄▄▄▄▄▄▄▆▅▄▄▄▄▅▅
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.5156
wandb:     train_loss 1.14252
wandb:   val_accuracy 0.37556
wandb:       val_loss 1.09681
wandb: 
wandb: 🚀 View run blooming-night-25 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gyjdmh5c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_211920-gyjdmh5c/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_213749-lfxzr69m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-hill-26
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lfxzr69m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:06:34, 29.65s/it]  0%|          | 2/500 [00:46<3:02:57, 22.04s/it]  1%|          | 3/500 [01:06<2:55:33, 21.19s/it]  1%|          | 4/500 [01:26<2:50:18, 20.60s/it]  1%|          | 5/500 [01:45<2:47:23, 20.29s/it]  1%|          | 6/500 [02:05<2:45:58, 20.16s/it]  1%|▏         | 7/500 [02:30<2:56:43, 21.51s/it]  2%|▏         | 8/500 [02:45<2:39:30, 19.45s/it]  2%|▏         | 9/500 [03:04<2:39:29, 19.49s/it]  2%|▏         | 10/500 [03:30<2:53:58, 21.30s/it]  2%|▏         | 11/500 [03:46<2:40:54, 19.74s/it]  2%|▏         | 12/500 [04:01<2:29:53, 18.43s/it]  3%|▎         | 13/500 [04:16<2:21:02, 17.38s/it]  3%|▎         | 14/500 [04:36<2:26:20, 18.07s/it]  3%|▎         | 15/500 [04:56<2:29:56, 18.55s/it]  3%|▎         | 16/500 [05:15<2:32:26, 18.90s/it]  3%|▎         | 17/500 [05:30<2:22:50, 17.74s/it]  4%|▎         | 18/500 [05:45<2:15:59, 16.93s/it]  4%|▍         | 19/500 [06:00<2:10:40, 16.30s/it]  4%|▍         | 20/500 [06:15<2:06:58, 15.87s/it]  4%|▍         | 21/500 [06:35<2:16:02, 17.04s/it]  4%|▍         | 22/500 [06:50<2:11:37, 16.52s/it]  5%|▍         | 23/500 [07:10<2:18:31, 17.42s/it]  5%|▍         | 24/500 [07:29<2:22:51, 18.01s/it]  5%|▌         | 25/500 [07:44<2:15:41, 17.14s/it]  5%|▌         | 26/500 [08:04<2:21:04, 17.86s/it]  5%|▌         | 27/500 [08:19<2:15:12, 17.15s/it]  6%|▌         | 28/500 [08:34<2:09:26, 16.46s/it]  6%|▌         | 29/500 [08:54<2:17:14, 17.48s/it]  6%|▌         | 30/500 [09:13<2:21:40, 18.09s/it]  6%|▌         | 31/500 [09:33<2:24:16, 18.46s/it]  6%|▋         | 32/500 [09:53<2:28:09, 19.00s/it]  7%|▋         | 33/500 [10:12<2:28:59, 19.14s/it]  7%|▋         | 34/500 [10:32<2:29:29, 19.25s/it]  7%|▋         | 35/500 [10:51<2:28:19, 19.14s/it]  7%|▋         | 36/500 [11:10<2:27:38, 19.09s/it]  7%|▋         | 37/500 [11:29<2:27:21, 19.10s/it]  8%|▊         | 38/500 [11:43<2:16:28, 17.72s/it]  8%|▊         | 39/500 [12:03<2:19:14, 18.12s/it]  8%|▊         | 40/500 [12:22<2:21:05, 18.40s/it]  8%|▊         | 41/500 [12:41<2:22:03, 18.57s/it]  8%|▊         | 42/500 [13:00<2:22:44, 18.70s/it]  9%|▊         | 43/500 [13:14<2:12:42, 17.42s/it]  9%|▉         | 44/500 [13:28<2:05:04, 16.46s/it]  9%|▉         | 45/500 [13:43<2:00:21, 15.87s/it]  9%|▉         | 46/500 [14:02<2:07:18, 16.83s/it]  9%|▉         | 46/500 [14:06<2:19:16, 18.41s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▂▁▁▁▁▄▂█▅▇▆▇▇▆█▇▆▇▇█▆▇▆▇███████▇██████
wandb:     train_loss ▂▂▂█▃▁▃▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▂▁▁▁▁▁▁▇▃▆▅▆▅▅▇▅▅▆▅▆▆▅▅▆█▆▆▆▆█▅▅▇▇▅▆▅▅
wandb:       val_loss ▂▂▃▄▆▄▁▄▁▄▂▁▃▂▆▄▁▄▅▁▂▁▃▆▁▃▆▂▁▅█▁▄▅▁▂▄▁▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 45
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.9688
wandb:     train_loss 0.00038
wandb:   val_accuracy 0.50444
wandb:       val_loss 2.22638
wandb: 
wandb: 🚀 View run neat-hill-26 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lfxzr69m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_213749-lfxzr69m/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_215243-7t123gqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-thunder-27
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7t123gqg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:11:27, 30.24s/it]  0%|          | 2/500 [00:45<2:56:34, 21.27s/it]  1%|          | 3/500 [01:00<2:31:52, 18.33s/it]  1%|          | 4/500 [01:14<2:19:14, 16.84s/it]  1%|          | 5/500 [01:28<2:11:32, 15.94s/it]  1%|          | 6/500 [01:48<2:20:34, 17.07s/it]  1%|▏         | 7/500 [02:11<2:37:23, 19.16s/it]  2%|▏         | 8/500 [02:26<2:25:09, 17.70s/it]  2%|▏         | 9/500 [02:45<2:28:03, 18.09s/it]  2%|▏         | 10/500 [02:59<2:18:34, 16.97s/it]  2%|▏         | 11/500 [03:14<2:11:56, 16.19s/it]  2%|▏         | 12/500 [03:28<2:07:28, 15.67s/it]  3%|▎         | 13/500 [03:47<2:15:04, 16.64s/it]  3%|▎         | 14/500 [04:06<2:19:40, 17.24s/it]  3%|▎         | 15/500 [04:24<2:22:57, 17.68s/it]  3%|▎         | 16/500 [04:43<2:24:36, 17.93s/it]  3%|▎         | 17/500 [04:57<2:14:59, 16.77s/it]  4%|▎         | 18/500 [05:16<2:19:32, 17.37s/it]  4%|▍         | 19/500 [05:30<2:11:17, 16.38s/it]  4%|▍         | 20/500 [05:48<2:16:05, 17.01s/it]  4%|▍         | 21/500 [06:03<2:09:35, 16.23s/it]  4%|▍         | 22/500 [06:22<2:16:03, 17.08s/it]  5%|▍         | 23/500 [06:40<2:19:40, 17.57s/it]  5%|▍         | 24/500 [06:59<2:22:52, 18.01s/it]  5%|▌         | 25/500 [07:18<2:24:48, 18.29s/it]  5%|▌         | 26/500 [07:37<2:26:07, 18.50s/it]  5%|▌         | 27/500 [07:56<2:26:19, 18.56s/it]  6%|▌         | 28/500 [08:10<2:15:50, 17.27s/it]  6%|▌         | 29/500 [08:29<2:18:55, 17.70s/it]  6%|▌         | 30/500 [08:43<2:11:02, 16.73s/it]  6%|▌         | 31/500 [08:58<2:05:03, 16.00s/it]  6%|▋         | 32/500 [09:12<2:00:45, 15.48s/it]  7%|▋         | 33/500 [09:26<1:58:00, 15.16s/it]  7%|▋         | 34/500 [09:43<2:01:24, 15.63s/it]  7%|▋         | 35/500 [09:58<1:58:41, 15.31s/it]  7%|▋         | 36/500 [10:12<1:56:11, 15.03s/it]  7%|▋         | 37/500 [10:27<1:55:02, 14.91s/it]  8%|▊         | 38/500 [10:46<2:04:53, 16.22s/it]  8%|▊         | 39/500 [11:00<2:00:08, 15.64s/it]  8%|▊         | 40/500 [11:15<1:56:58, 15.26s/it]  8%|▊         | 41/500 [11:29<1:55:00, 15.03s/it]  8%|▊         | 42/500 [11:48<2:03:27, 16.17s/it]  9%|▊         | 43/500 [12:07<2:09:21, 16.98s/it]  9%|▉         | 44/500 [12:26<2:13:20, 17.54s/it]  9%|▉         | 45/500 [12:45<2:16:22, 17.98s/it]  9%|▉         | 46/500 [13:03<2:17:37, 18.19s/it]  9%|▉         | 47/500 [13:22<2:18:30, 18.34s/it] 10%|▉         | 48/500 [13:41<2:19:15, 18.49s/it] 10%|▉         | 49/500 [13:55<2:08:59, 17.16s/it] 10%|█         | 50/500 [14:09<2:02:36, 16.35s/it] 10%|█         | 51/500 [14:24<1:58:08, 15.79s/it] 10%|█         | 52/500 [14:38<1:54:47, 15.37s/it] 11%|█         | 53/500 [14:57<2:02:24, 16.43s/it] 11%|█         | 54/500 [15:12<1:57:24, 15.80s/it] 11%|█         | 55/500 [15:26<1:53:43, 15.33s/it] 11%|█         | 56/500 [15:40<1:51:30, 15.07s/it] 11%|█▏        | 57/500 [15:59<1:59:30, 16.19s/it] 12%|█▏        | 58/500 [16:18<2:04:55, 16.96s/it] 12%|█▏        | 59/500 [16:36<2:08:13, 17.45s/it] 12%|█▏        | 60/500 [16:55<2:10:19, 17.77s/it] 12%|█▏        | 61/500 [17:09<2:02:09, 16.69s/it] 12%|█▏        | 62/500 [17:28<2:06:31, 17.33s/it] 13%|█▎        | 63/500 [17:46<2:08:54, 17.70s/it] 13%|█▎        | 64/500 [18:05<2:10:30, 17.96s/it] 13%|█▎        | 65/500 [18:19<2:01:54, 16.81s/it] 13%|█▎        | 66/500 [18:33<1:56:01, 16.04s/it] 13%|█▎        | 67/500 [18:52<2:01:34, 16.85s/it] 14%|█▎        | 68/500 [19:06<1:55:49, 16.09s/it] 14%|█▍        | 69/500 [19:21<1:52:28, 15.66s/it] 14%|█▍        | 70/500 [19:37<1:52:42, 15.73s/it] 14%|█▍        | 71/500 [19:56<1:59:09, 16.67s/it] 14%|█▍        | 71/500 [19:56<2:00:28, 16.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.233 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▅▃▁▄▁▃▂▁▆▁▃▁▁▃▄▁▂▇▁▂███▄▃▃▄▄▇▄▄▄▄▄▄▇▄
wandb:     train_loss ▂▂▅▂▂▂▁▂▂▁▆▂▂▁█▁▁▁▁▁▂▃▂▁▁▁▂▂▃▁▂▁▁▁▁▂▂▂▁▂
wandb:   val_accuracy ▂▂▁▄▂▁█▁▂▂▁▄▁▂▁▁▃▃▁▁▅▁▁▆▆▆▃▂▂▄▃▅▄▃▄▃▃▃▅▄
wandb:       val_loss ▂▂▂▂▂▁▂▁▂▂▄▁▂▂▅█▂▂▂▁▂▂▃▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 70
wandb:  learning_rate 0.0
wandb: train_accuracy 0.56464
wandb:     train_loss 1.10201
wandb:   val_accuracy 0.44667
wandb:       val_loss 1.55652
wandb: 
wandb: 🚀 View run radiant-thunder-27 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7t123gqg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_215243-7t123gqg/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_221323-j0c4oqcs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-deluge-28
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j0c4oqcs
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<4:00:47, 28.95s/it]  0%|          | 2/500 [00:44<2:55:36, 21.16s/it]  1%|          | 3/500 [01:04<2:48:57, 20.40s/it]  1%|          | 4/500 [01:18<2:29:17, 18.06s/it]  1%|          | 5/500 [01:37<2:32:00, 18.43s/it]  1%|          | 6/500 [01:52<2:21:23, 17.17s/it]  1%|▏         | 7/500 [02:06<2:14:03, 16.32s/it]  2%|▏         | 8/500 [02:21<2:09:23, 15.78s/it]  2%|▏         | 9/500 [02:36<2:06:19, 15.44s/it]  2%|▏         | 10/500 [02:51<2:04:27, 15.24s/it]  2%|▏         | 11/500 [03:10<2:13:57, 16.44s/it]  2%|▏         | 12/500 [03:29<2:21:11, 17.36s/it]  3%|▎         | 13/500 [03:48<2:25:29, 17.92s/it]  3%|▎         | 14/500 [04:07<2:27:21, 18.19s/it]  3%|▎         | 15/500 [04:26<2:28:43, 18.40s/it]  3%|▎         | 16/500 [04:45<2:29:47, 18.57s/it]  3%|▎         | 17/500 [04:59<2:19:13, 17.30s/it]  4%|▎         | 18/500 [05:19<2:23:55, 17.92s/it]  4%|▍         | 19/500 [05:38<2:26:35, 18.29s/it]  4%|▍         | 20/500 [05:57<2:28:27, 18.56s/it]  4%|▍         | 21/500 [06:16<2:29:33, 18.73s/it]  4%|▍         | 22/500 [06:31<2:20:12, 17.60s/it]  5%|▍         | 23/500 [06:50<2:23:32, 18.06s/it]  5%|▍         | 24/500 [07:05<2:15:03, 17.02s/it]  5%|▌         | 25/500 [07:24<2:19:51, 17.67s/it]  5%|▌         | 26/500 [07:43<2:23:27, 18.16s/it]  5%|▌         | 27/500 [08:07<2:36:15, 19.82s/it]  6%|▌         | 28/500 [08:22<2:23:55, 18.29s/it]  6%|▌         | 29/500 [08:36<2:14:53, 17.18s/it]  6%|▌         | 30/500 [08:51<2:08:14, 16.37s/it]  6%|▌         | 31/500 [09:10<2:15:06, 17.28s/it]  6%|▋         | 32/500 [09:29<2:18:55, 17.81s/it]  7%|▋         | 33/500 [09:44<2:10:36, 16.78s/it]  7%|▋         | 34/500 [10:03<2:15:54, 17.50s/it]  7%|▋         | 35/500 [10:18<2:09:01, 16.65s/it]  7%|▋         | 36/500 [10:32<2:04:26, 16.09s/it]  7%|▋         | 37/500 [10:51<2:10:34, 16.92s/it]  8%|▊         | 38/500 [11:06<2:04:33, 16.18s/it]  8%|▊         | 39/500 [11:21<2:01:07, 15.76s/it]  8%|▊         | 40/500 [11:36<2:00:18, 15.69s/it]  8%|▊         | 41/500 [11:51<1:58:19, 15.47s/it]  8%|▊         | 42/500 [12:05<1:55:38, 15.15s/it]  9%|▊         | 43/500 [12:20<1:53:41, 14.93s/it]  9%|▉         | 44/500 [12:39<2:02:52, 16.17s/it]  9%|▉         | 45/500 [12:58<2:08:31, 16.95s/it]  9%|▉         | 46/500 [13:16<2:12:28, 17.51s/it]  9%|▉         | 47/500 [13:31<2:05:13, 16.59s/it] 10%|▉         | 48/500 [13:45<1:59:49, 15.91s/it] 10%|▉         | 49/500 [14:02<2:00:50, 16.08s/it] 10%|█         | 50/500 [14:21<2:07:05, 16.94s/it] 10%|█         | 51/500 [14:35<2:00:47, 16.14s/it] 10%|█         | 52/500 [14:49<1:56:20, 15.58s/it] 11%|█         | 53/500 [15:04<1:53:33, 15.24s/it] 11%|█         | 54/500 [15:22<2:01:07, 16.30s/it] 11%|█         | 55/500 [15:37<1:57:04, 15.78s/it] 11%|█         | 56/500 [15:51<1:53:46, 15.37s/it] 11%|█▏        | 57/500 [16:07<1:54:16, 15.48s/it] 12%|█▏        | 58/500 [16:23<1:54:04, 15.48s/it] 12%|█▏        | 59/500 [16:37<1:51:37, 15.19s/it] 12%|█▏        | 60/500 [16:56<2:00:08, 16.38s/it] 12%|█▏        | 61/500 [17:15<2:05:53, 17.21s/it] 12%|█▏        | 62/500 [17:35<2:10:39, 17.90s/it] 13%|█▎        | 63/500 [17:54<2:12:54, 18.25s/it] 13%|█▎        | 63/500 [17:58<2:04:44, 17.13s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.019 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▄▁▁▃▁▁▁▇▂█▂▂▇▃▇▇█▃▄▄▅▆▇▄▃▄▃▄▅▅▆
wandb:     train_loss ██▁▆▆▆▃▅▆▄▂▂▃▅▃▅▄▆▄▅▃▄▅▄▄▄▅▄▄▅▄▃▄▂▃▅▃▅▅▄
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▄▁▁▂▂█▂▄▁▁▁▁▂▂▁▁▁▁▂▁▁▁
wandb:       val_loss ▂▂▇▂█▅▅▄▆▃▃▂▂▁▄▃▃▅▂▅▂▁▅▂▃▃▂▃▂▅▃▄▃▅▄▃▃▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 62
wandb:  learning_rate 0.0
wandb: train_accuracy 0.50371
wandb:     train_loss 1.04874
wandb:   val_accuracy 0.34889
wandb:       val_loss 1.09573
wandb: 
wandb: 🚀 View run fragrant-deluge-28 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j0c4oqcs
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_221323-j0c4oqcs/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_223214-6yjfaoms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-fire-29
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6yjfaoms
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:03:17, 29.25s/it]  0%|          | 2/500 [00:45<2:57:30, 21.39s/it]  1%|          | 3/500 [01:08<3:06:25, 22.51s/it]  1%|          | 4/500 [01:23<2:40:11, 19.38s/it]  1%|          | 5/500 [01:42<2:39:37, 19.35s/it]  1%|          | 6/500 [02:02<2:39:12, 19.34s/it]  1%|▏         | 7/500 [02:21<2:39:24, 19.40s/it]  2%|▏         | 8/500 [02:36<2:27:24, 17.98s/it]  2%|▏         | 9/500 [02:55<2:30:11, 18.35s/it]  2%|▏         | 10/500 [03:10<2:20:40, 17.22s/it]  2%|▏         | 11/500 [03:29<2:25:07, 17.81s/it]  2%|▏         | 12/500 [03:44<2:17:24, 16.89s/it]  3%|▎         | 13/500 [03:58<2:10:55, 16.13s/it]  3%|▎         | 14/500 [04:13<2:06:00, 15.56s/it]  3%|▎         | 15/500 [04:31<2:12:48, 16.43s/it]  3%|▎         | 16/500 [04:45<2:06:54, 15.73s/it]  3%|▎         | 17/500 [04:59<2:02:51, 15.26s/it]  4%|▎         | 18/500 [05:19<2:12:16, 16.47s/it]  4%|▍         | 19/500 [05:37<2:17:09, 17.11s/it]  4%|▍         | 20/500 [05:51<2:09:54, 16.24s/it]  4%|▍         | 21/500 [06:10<2:15:35, 16.98s/it]  4%|▍         | 22/500 [06:25<2:09:42, 16.28s/it]  5%|▍         | 23/500 [06:39<2:05:11, 15.75s/it]  5%|▍         | 24/500 [06:54<2:01:35, 15.33s/it]  5%|▌         | 25/500 [07:13<2:09:53, 16.41s/it]  5%|▌         | 26/500 [07:27<2:03:56, 15.69s/it]  5%|▌         | 27/500 [07:45<2:10:59, 16.62s/it]  6%|▌         | 28/500 [08:04<2:15:56, 17.28s/it]  6%|▌         | 29/500 [08:23<2:19:25, 17.76s/it]  6%|▌         | 30/500 [08:37<2:11:03, 16.73s/it]  6%|▌         | 31/500 [08:56<2:15:34, 17.34s/it]  6%|▋         | 32/500 [09:10<2:07:56, 16.40s/it]  7%|▋         | 33/500 [09:34<2:24:46, 18.60s/it]  7%|▋         | 34/500 [09:48<2:14:06, 17.27s/it]  7%|▋         | 35/500 [10:07<2:16:56, 17.67s/it]  7%|▋         | 36/500 [10:25<2:18:41, 17.93s/it]  7%|▋         | 37/500 [10:39<2:09:25, 16.77s/it]  8%|▊         | 38/500 [11:03<2:24:21, 18.75s/it]  8%|▊         | 39/500 [11:17<2:13:32, 17.38s/it]  8%|▊         | 40/500 [11:36<2:16:38, 17.82s/it]  8%|▊         | 41/500 [11:55<2:18:21, 18.09s/it]  8%|▊         | 42/500 [12:09<2:09:29, 16.96s/it]  9%|▊         | 43/500 [12:23<2:03:31, 16.22s/it]  9%|▉         | 44/500 [12:37<1:58:15, 15.56s/it]  9%|▉         | 45/500 [12:51<1:54:41, 15.12s/it]  9%|▉         | 46/500 [13:10<2:02:14, 16.15s/it]  9%|▉         | 47/500 [13:29<2:07:43, 16.92s/it] 10%|▉         | 48/500 [13:43<2:00:46, 16.03s/it] 10%|▉         | 49/500 [14:02<2:06:46, 16.87s/it] 10%|█         | 50/500 [14:16<2:00:37, 16.08s/it] 10%|█         | 51/500 [14:30<1:56:00, 15.50s/it] 10%|█         | 52/500 [14:45<1:54:00, 15.27s/it] 11%|█         | 53/500 [14:59<1:52:34, 15.11s/it] 11%|█         | 54/500 [15:18<2:01:10, 16.30s/it] 11%|█         | 55/500 [15:37<2:06:16, 17.03s/it] 11%|█         | 56/500 [15:56<2:09:33, 17.51s/it] 11%|█▏        | 57/500 [16:10<2:02:42, 16.62s/it] 12%|█▏        | 58/500 [16:25<1:58:12, 16.05s/it] 12%|█▏        | 59/500 [16:39<1:54:09, 15.53s/it] 12%|█▏        | 60/500 [16:58<2:01:04, 16.51s/it] 12%|█▏        | 61/500 [17:12<1:55:49, 15.83s/it] 12%|█▏        | 62/500 [17:31<2:02:25, 16.77s/it] 13%|█▎        | 63/500 [17:46<1:56:20, 15.97s/it] 13%|█▎        | 64/500 [18:00<1:52:03, 15.42s/it] 13%|█▎        | 65/500 [18:14<1:49:36, 15.12s/it] 13%|█▎        | 66/500 [18:29<1:48:25, 14.99s/it] 13%|█▎        | 67/500 [18:43<1:46:26, 14.75s/it] 14%|█▎        | 68/500 [19:02<1:54:42, 15.93s/it] 14%|█▍        | 69/500 [19:20<2:00:25, 16.76s/it] 14%|█▍        | 70/500 [19:39<2:04:36, 17.39s/it] 14%|█▍        | 71/500 [19:58<2:08:00, 17.90s/it] 14%|█▍        | 72/500 [20:13<2:00:01, 16.83s/it] 15%|█▍        | 73/500 [20:27<1:54:51, 16.14s/it] 15%|█▍        | 74/500 [20:46<2:00:32, 16.98s/it] 15%|█▍        | 74/500 [20:51<2:00:01, 16.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.019 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▂▅▄▄▇▇▆▆▅█▅██▇▇████▇█▇▇████▇██▇▇▇████
wandb:     train_loss ▃▃▃█▂▁▃▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▁▁▁▆▆▇▄▄▇▅▇▄▇▇█▅▇▇██▇█▇▇████▇▇▇█▆▆▇▇██
wandb:       val_loss ▂▂▃▃▄▃▁▃▃█▁▅▁▁▂▃▄▄▄▁▁▄▃▃▄▅▃▆▃▆▁▄▄▆▄▄▄▄▆▂
wandb: 
wandb: Run summary:
wandb:          epoch 73
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.97474
wandb:     train_loss 0.00037
wandb:   val_accuracy 0.59778
wandb:       val_loss 0.68919
wandb: 
wandb: 🚀 View run cool-fire-29 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6yjfaoms
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_223214-6yjfaoms/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_225347-bq28xp2v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-yogurt-30
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bq28xp2v
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:05:27, 29.51s/it]  0%|          | 2/500 [00:44<2:52:45, 20.81s/it]  1%|          | 3/500 [01:07<3:02:48, 22.07s/it]  1%|          | 4/500 [01:22<2:38:02, 19.12s/it]  1%|          | 5/500 [01:36<2:23:45, 17.43s/it]  1%|          | 6/500 [01:55<2:27:34, 17.92s/it]  1%|▏         | 7/500 [02:10<2:18:04, 16.80s/it]  2%|▏         | 8/500 [02:24<2:11:33, 16.04s/it]  2%|▏         | 9/500 [02:38<2:06:58, 15.52s/it]  2%|▏         | 10/500 [02:53<2:03:30, 15.12s/it]  2%|▏         | 11/500 [03:12<2:12:32, 16.26s/it]  2%|▏         | 12/500 [03:35<2:29:13, 18.35s/it]  3%|▎         | 13/500 [03:49<2:19:08, 17.14s/it]  3%|▎         | 14/500 [04:08<2:22:21, 17.57s/it]  3%|▎         | 15/500 [04:26<2:25:01, 17.94s/it]  3%|▎         | 16/500 [04:45<2:26:32, 18.17s/it]  3%|▎         | 17/500 [05:00<2:17:37, 17.10s/it]  4%|▎         | 18/500 [05:14<2:10:54, 16.30s/it]  4%|▍         | 19/500 [05:29<2:06:15, 15.75s/it]  4%|▍         | 20/500 [05:43<2:03:04, 15.38s/it]  4%|▍         | 21/500 [05:58<2:01:41, 15.24s/it]  4%|▍         | 22/500 [06:17<2:10:49, 16.42s/it]  5%|▍         | 23/500 [06:36<2:15:55, 17.10s/it]  5%|▍         | 24/500 [06:54<2:18:54, 17.51s/it]  5%|▌         | 25/500 [07:08<2:10:27, 16.48s/it]  5%|▌         | 26/500 [07:23<2:05:01, 15.83s/it]  5%|▌         | 27/500 [07:37<2:01:51, 15.46s/it]  6%|▌         | 28/500 [07:56<2:09:20, 16.44s/it]  6%|▌         | 29/500 [08:10<2:03:50, 15.78s/it]  6%|▌         | 30/500 [08:29<2:09:52, 16.58s/it]  6%|▌         | 31/500 [08:47<2:14:27, 17.20s/it]  6%|▋         | 32/500 [09:06<2:17:17, 17.60s/it]  7%|▋         | 33/500 [09:25<2:19:24, 17.91s/it]  7%|▋         | 34/500 [09:39<2:10:37, 16.82s/it]  7%|▋         | 35/500 [09:53<2:04:43, 16.09s/it]  7%|▋         | 36/500 [10:07<1:59:44, 15.48s/it]  7%|▋         | 37/500 [10:30<2:17:05, 17.77s/it]  8%|▊         | 38/500 [10:45<2:09:31, 16.82s/it]  8%|▊         | 39/500 [10:59<2:03:25, 16.06s/it]  8%|▊         | 40/500 [11:18<2:09:22, 16.88s/it]  8%|▊         | 41/500 [11:32<2:02:53, 16.06s/it]  8%|▊         | 42/500 [11:46<1:58:12, 15.49s/it]  9%|▊         | 43/500 [12:05<2:05:56, 16.53s/it]  9%|▉         | 44/500 [12:24<2:09:55, 17.09s/it]  9%|▉         | 45/500 [12:38<2:02:56, 16.21s/it]  9%|▉         | 46/500 [12:52<1:58:25, 15.65s/it]  9%|▉         | 47/500 [13:07<1:55:43, 15.33s/it] 10%|▉         | 48/500 [13:25<2:02:54, 16.31s/it] 10%|▉         | 49/500 [13:44<2:08:30, 17.10s/it] 10%|█         | 50/500 [14:03<2:12:18, 17.64s/it] 10%|█         | 51/500 [14:22<2:14:49, 18.02s/it] 10%|█         | 52/500 [14:41<2:16:13, 18.25s/it] 11%|█         | 53/500 [15:00<2:17:15, 18.42s/it] 11%|█         | 54/500 [15:19<2:17:48, 18.54s/it] 11%|█         | 55/500 [15:37<2:17:56, 18.60s/it] 11%|█         | 56/500 [15:56<2:18:13, 18.68s/it] 11%|█▏        | 57/500 [16:15<2:17:37, 18.64s/it] 12%|█▏        | 58/500 [16:34<2:17:31, 18.67s/it] 12%|█▏        | 59/500 [16:52<2:17:14, 18.67s/it] 12%|█▏        | 60/500 [17:06<2:07:13, 17.35s/it] 12%|█▏        | 61/500 [17:21<1:59:56, 16.39s/it] 12%|█▏        | 62/500 [17:35<1:55:03, 15.76s/it] 13%|█▎        | 63/500 [17:54<2:01:11, 16.64s/it] 13%|█▎        | 64/500 [18:12<2:05:48, 17.31s/it] 13%|█▎        | 65/500 [18:31<2:07:53, 17.64s/it] 13%|█▎        | 66/500 [18:45<1:59:36, 16.54s/it] 13%|█▎        | 67/500 [18:59<1:54:08, 15.82s/it] 14%|█▎        | 68/500 [19:18<2:00:21, 16.72s/it] 14%|█▍        | 69/500 [19:37<2:05:10, 17.43s/it] 14%|█▍        | 70/500 [19:56<2:07:39, 17.81s/it] 14%|█▍        | 71/500 [20:10<1:59:30, 16.71s/it] 14%|█▍        | 72/500 [20:24<1:54:50, 16.10s/it] 15%|█▍        | 73/500 [20:39<1:51:05, 15.61s/it]wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
 15%|█▍        | 74/500 [20:54<1:48:50, 15.33s/it] 15%|█▌        | 75/500 [21:08<1:46:47, 15.08s/it] 15%|█▌        | 76/500 [21:27<1:54:09, 16.15s/it] 15%|█▌        | 77/500 [21:46<1:59:29, 16.95s/it] 16%|█▌        | 78/500 [22:04<2:02:21, 17.40s/it] 16%|█▌        | 79/500 [22:23<2:04:29, 17.74s/it] 16%|█▌        | 80/500 [22:42<2:06:51, 18.12s/it] 16%|█▌        | 81/500 [23:00<2:08:01, 18.33s/it] 16%|█▋        | 82/500 [23:19<2:08:34, 18.46s/it] 17%|█▋        | 83/500 [23:38<2:08:57, 18.55s/it] 17%|█▋        | 84/500 [23:57<2:08:50, 18.58s/it] 17%|█▋        | 85/500 [24:11<1:59:09, 17.23s/it] 17%|█▋        | 86/500 [24:25<1:52:52, 16.36s/it] 17%|█▋        | 87/500 [24:44<1:58:10, 17.17s/it] 18%|█▊        | 88/500 [25:03<2:01:08, 17.64s/it] 18%|█▊        | 89/500 [25:21<2:02:55, 17.95s/it] 18%|█▊        | 90/500 [25:40<2:04:03, 18.15s/it] 18%|█▊        | 91/500 [25:54<1:55:12, 16.90s/it] 18%|█▊        | 92/500 [26:13<1:59:04, 17.51s/it] 19%|█▊        | 93/500 [26:32<2:01:13, 17.87s/it] 19%|█▉        | 94/500 [26:50<2:02:14, 18.07s/it] 19%|█▉        | 95/500 [27:09<2:02:49, 18.20s/it] 19%|█▉        | 96/500 [27:28<2:03:51, 18.39s/it] 19%|█▉        | 97/500 [27:42<1:54:54, 17.11s/it] 20%|█▉        | 98/500 [27:56<1:48:38, 16.22s/it] 20%|█▉        | 99/500 [28:14<1:52:51, 16.89s/it] 20%|██        | 100/500 [28:28<1:47:10, 16.08s/it] 20%|██        | 101/500 [28:47<1:52:06, 16.86s/it] 20%|██        | 102/500 [29:06<1:55:19, 17.38s/it] 21%|██        | 103/500 [29:24<1:57:12, 17.71s/it] 21%|██        | 104/500 [29:38<1:49:30, 16.59s/it] 21%|██        | 105/500 [29:52<1:44:10, 15.82s/it] 21%|██        | 106/500 [30:06<1:40:48, 15.35s/it] 21%|██▏       | 107/500 [30:25<1:47:15, 16.38s/it] 22%|██▏       | 108/500 [30:40<1:42:59, 15.76s/it] 22%|██▏       | 109/500 [30:54<1:39:20, 15.24s/it] 22%|██▏       | 110/500 [31:08<1:37:00, 14.92s/it] 22%|██▏       | 111/500 [31:26<1:43:59, 16.04s/it] 22%|██▏       | 112/500 [31:45<1:48:53, 16.84s/it] 23%|██▎       | 113/500 [32:04<1:52:13, 17.40s/it] 23%|██▎       | 114/500 [32:23<1:54:31, 17.80s/it] 23%|██▎       | 115/500 [32:37<1:47:02, 16.68s/it] 23%|██▎       | 116/500 [32:51<1:41:38, 15.88s/it] 23%|██▎       | 117/500 [33:08<1:43:17, 16.18s/it] 24%|██▎       | 118/500 [33:22<1:39:52, 15.69s/it] 24%|██▍       | 119/500 [33:36<1:36:35, 15.21s/it] 24%|██▍       | 120/500 [33:50<1:34:00, 14.84s/it] 24%|██▍       | 121/500 [34:04<1:32:09, 14.59s/it] 24%|██▍       | 122/500 [34:18<1:31:23, 14.51s/it] 25%|██▍       | 123/500 [34:37<1:38:39, 15.70s/it] 25%|██▍       | 124/500 [34:56<1:43:51, 16.57s/it] 25%|██▌       | 125/500 [35:10<1:38:53, 15.82s/it] 25%|██▌       | 126/500 [35:28<1:44:04, 16.70s/it] 25%|██▌       | 127/500 [35:47<1:47:15, 17.25s/it] 26%|██▌       | 128/500 [36:01<1:41:06, 16.31s/it] 26%|██▌       | 129/500 [36:15<1:37:16, 15.73s/it] 26%|██▌       | 130/500 [36:34<1:42:16, 16.59s/it] 26%|██▌       | 131/500 [36:53<1:45:43, 17.19s/it] 26%|██▋       | 132/500 [37:07<1:39:49, 16.28s/it] 26%|██▋       | 132/500 [37:11<1:43:41, 16.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.320 MB uploadedwandb: \ 0.019 MB of 0.320 MB uploadedwandb: | 0.320 MB of 0.320 MB uploadedwandb: / 0.320 MB of 0.320 MB uploadedwandb: - 0.320 MB of 0.320 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▅▆▆▅▇▇▇███▇▇████████████▇▇█████████▇███
wandb:     train_loss ▇█▇▆▄▆▆▆▃▅▄▅▄▄▅▁▇▄▄▇▅▅▃▄▅▇▅▄▇▄▅▃▇▅▅▅▆▆▃▂
wandb:   val_accuracy ▁▅▃▃▄▇▆▆▇▇▆▆▆▇▇█▆▇▇▇█▇▇█▇▆▇▆▇███▇██▇▆▆█▇
wandb:       val_loss ▅▄▄▄▄▄▃▆▄▆▆▂▄▂▄▅▅▆▆█▂▁▆▄▆▃▃▂▄▂▄▅▆▃▄▃█▄▆▄
wandb: 
wandb: Run summary:
wandb:          epoch 131
wandb:  learning_rate 0.0
wandb: train_accuracy 0.80832
wandb:     train_loss 0.23483
wandb:   val_accuracy 0.52667
wandb:       val_loss 0.93495
wandb: 
wandb: 🚀 View run hardy-yogurt-30 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bq28xp2v
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_225347-bq28xp2v/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240916_233145-j3iuqh06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-night-31
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j3iuqh06
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:59:41, 28.82s/it]  0%|          | 2/500 [00:43<2:51:55, 20.71s/it]  1%|          | 3/500 [01:03<2:47:01, 20.16s/it]  1%|          | 4/500 [01:18<2:29:34, 18.09s/it]  1%|          | 5/500 [01:32<2:19:05, 16.86s/it]  1%|          | 6/500 [01:52<2:26:03, 17.74s/it]  1%|▏         | 7/500 [02:07<2:17:33, 16.74s/it]  2%|▏         | 8/500 [02:26<2:24:28, 17.62s/it]  2%|▏         | 9/500 [02:46<2:28:47, 18.18s/it]  2%|▏         | 10/500 [03:05<2:30:54, 18.48s/it]  2%|▏         | 11/500 [03:24<2:32:58, 18.77s/it]  2%|▏         | 12/500 [03:44<2:34:44, 19.02s/it]  3%|▎         | 13/500 [03:58<2:24:00, 17.74s/it]  3%|▎         | 14/500 [04:18<2:28:14, 18.30s/it]  3%|▎         | 15/500 [04:33<2:19:55, 17.31s/it]  3%|▎         | 16/500 [04:48<2:13:37, 16.57s/it]  3%|▎         | 17/500 [05:03<2:10:19, 16.19s/it]  4%|▎         | 18/500 [05:18<2:07:25, 15.86s/it]  4%|▍         | 19/500 [05:34<2:06:35, 15.79s/it]  4%|▍         | 20/500 [05:49<2:04:36, 15.58s/it]  4%|▍         | 21/500 [06:13<2:24:34, 18.11s/it]  4%|▍         | 22/500 [06:28<2:17:22, 17.24s/it]  5%|▍         | 23/500 [06:43<2:11:26, 16.53s/it]  5%|▍         | 24/500 [06:58<2:07:08, 16.03s/it]  5%|▌         | 25/500 [07:13<2:03:50, 15.64s/it]  5%|▌         | 26/500 [07:28<2:02:26, 15.50s/it]  5%|▌         | 27/500 [07:53<2:24:20, 18.31s/it]  6%|▌         | 28/500 [08:08<2:16:26, 17.34s/it]  6%|▌         | 29/500 [08:27<2:20:59, 17.96s/it]  6%|▌         | 30/500 [08:47<2:23:54, 18.37s/it]  6%|▌         | 31/500 [09:01<2:14:55, 17.26s/it]  6%|▋         | 32/500 [09:21<2:19:21, 17.87s/it]  7%|▋         | 33/500 [09:40<2:23:14, 18.40s/it]  7%|▋         | 34/500 [09:59<2:24:57, 18.66s/it]  7%|▋         | 35/500 [10:19<2:26:42, 18.93s/it]  7%|▋         | 36/500 [10:39<2:28:07, 19.15s/it]  7%|▋         | 37/500 [10:58<2:28:57, 19.30s/it]  8%|▊         | 38/500 [11:18<2:29:10, 19.37s/it]  8%|▊         | 39/500 [11:33<2:18:13, 17.99s/it]  8%|▊         | 40/500 [11:52<2:21:34, 18.47s/it]  8%|▊         | 41/500 [12:12<2:24:52, 18.94s/it]  8%|▊         | 42/500 [12:27<2:15:12, 17.71s/it]  9%|▊         | 43/500 [12:47<2:19:46, 18.35s/it]  9%|▉         | 44/500 [13:02<2:11:53, 17.35s/it]  9%|▉         | 45/500 [13:22<2:17:03, 18.07s/it]  9%|▉         | 46/500 [13:37<2:09:44, 17.15s/it]  9%|▉         | 47/500 [13:56<2:15:08, 17.90s/it] 10%|▉         | 48/500 [14:16<2:18:24, 18.37s/it] 10%|▉         | 49/500 [14:35<2:19:58, 18.62s/it] 10%|█         | 50/500 [14:55<2:22:02, 18.94s/it] 10%|█         | 51/500 [15:15<2:23:55, 19.23s/it] 10%|█         | 52/500 [15:34<2:24:33, 19.36s/it] 11%|█         | 53/500 [15:54<2:25:02, 19.47s/it] 11%|█         | 54/500 [16:14<2:25:40, 19.60s/it] 11%|█         | 55/500 [16:34<2:25:21, 19.60s/it] 11%|█         | 56/500 [16:48<2:14:30, 18.18s/it] 11%|█▏        | 57/500 [17:04<2:08:05, 17.35s/it] 12%|█▏        | 58/500 [17:19<2:02:47, 16.67s/it] 12%|█▏        | 59/500 [17:39<2:09:02, 17.56s/it] 12%|█▏        | 60/500 [17:58<2:13:41, 18.23s/it] 12%|█▏        | 61/500 [18:18<2:17:03, 18.73s/it] 12%|█▏        | 62/500 [18:33<2:08:30, 17.60s/it] 13%|█▎        | 63/500 [18:52<2:11:51, 18.10s/it] 13%|█▎        | 64/500 [19:12<2:14:22, 18.49s/it] 13%|█▎        | 65/500 [19:32<2:18:28, 19.10s/it] 13%|█▎        | 66/500 [19:53<2:22:23, 19.69s/it] 13%|█▎        | 67/500 [20:13<2:22:02, 19.68s/it] 14%|█▎        | 68/500 [20:28<2:11:32, 18.27s/it] 14%|█▍        | 69/500 [20:48<2:13:48, 18.63s/it] 14%|█▍        | 70/500 [21:07<2:15:52, 18.96s/it] 14%|█▍        | 71/500 [21:27<2:17:22, 19.21s/it] 14%|█▍        | 72/500 [21:42<2:07:37, 17.89s/it] 15%|█▍        | 73/500 [22:02<2:11:07, 18.42s/it] 15%|█▍        | 74/500 [22:16<2:03:06, 17.34s/it] 15%|█▌        | 75/500 [22:36<2:06:52, 17.91s/it] 15%|█▌        | 76/500 [22:55<2:10:00, 18.40s/it] 15%|█▌        | 77/500 [23:14<2:11:30, 18.65s/it] 16%|█▌        | 78/500 [23:34<2:12:16, 18.81s/it] 16%|█▌        | 79/500 [23:48<2:03:40, 17.63s/it] 16%|█▌        | 80/500 [24:08<2:07:19, 18.19s/it] 16%|█▌        | 81/500 [24:27<2:09:10, 18.50s/it] 16%|█▋        | 82/500 [24:46<2:10:26, 18.72s/it] 17%|█▋        | 83/500 [25:01<2:02:06, 17.57s/it] 17%|█▋        | 84/500 [25:16<1:56:15, 16.77s/it] 17%|█▋        | 85/500 [25:31<1:52:01, 16.20s/it] 17%|█▋        | 86/500 [25:46<1:49:39, 15.89s/it] 17%|█▋        | 87/500 [26:06<1:57:18, 17.04s/it] 18%|█▊        | 88/500 [26:25<2:01:48, 17.74s/it] 18%|█▊        | 89/500 [26:40<1:55:55, 16.92s/it] 18%|█▊        | 90/500 [27:00<2:00:49, 17.68s/it] 18%|█▊        | 91/500 [27:19<2:03:34, 18.13s/it] 18%|█▊        | 92/500 [27:39<2:06:35, 18.62s/it] 19%|█▊        | 93/500 [27:54<1:58:49, 17.52s/it] 19%|█▉        | 94/500 [28:13<2:02:56, 18.17s/it] 19%|█▉        | 95/500 [28:28<1:56:15, 17.22s/it] 19%|█▉        | 96/500 [28:48<2:00:57, 17.96s/it] 19%|█▉        | 97/500 [29:04<1:56:32, 17.35s/it] 19%|█▉        | 97/500 [29:04<2:00:47, 17.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.234 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁█▁▂▁▁▁▂▇▇▁▂▄▃▅▁▅▂▃▂▁▁▁▂▂▃▂▁▁▂▂▂▂
wandb:     train_loss █▆▆▅▂▆▁▂▄▂▄▆▄▆▄▄▄▄▅▅▂▄▂▅▄▄▅▆▅▂▆▃▂▄▄▄▂▃▄▄
wandb:   val_accuracy ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss ▃▁▃▆▆█▇▂▃▅▅▆▂▆▆▃▄▃▄▆▆▂▆▂▆▃▂▃▅▆▄▅▂▅▆▃▁▇▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 96
wandb:  learning_rate 0.0
wandb: train_accuracy 0.3373
wandb:     train_loss 1.0958
wandb:   val_accuracy 0.34444
wandb:       val_loss 1.01665
wandb: 
wandb: 🚀 View run vague-night-31 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j3iuqh06
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240916_233145-j3iuqh06/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_000146-3g8jh3kh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-resonance-32
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3g8jh3kh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:12:48, 37.61s/it]  0%|          | 2/500 [01:07<4:32:35, 32.84s/it]  1%|          | 3/500 [01:31<3:58:47, 28.83s/it]  1%|          | 4/500 [01:59<3:58:14, 28.82s/it]  1%|          | 5/500 [02:29<3:58:49, 28.95s/it]  1%|          | 6/500 [03:01<4:08:23, 30.17s/it]  1%|▏         | 7/500 [03:32<4:09:17, 30.34s/it]  2%|▏         | 8/500 [04:03<4:10:04, 30.50s/it]  2%|▏         | 9/500 [04:36<4:15:33, 31.23s/it]  2%|▏         | 10/500 [05:13<4:31:13, 33.21s/it]  2%|▏         | 11/500 [05:37<4:07:58, 30.43s/it]  2%|▏         | 12/500 [06:10<4:13:45, 31.20s/it]  3%|▎         | 13/500 [06:43<4:15:52, 31.52s/it]  3%|▎         | 14/500 [07:22<4:34:07, 33.84s/it]  3%|▎         | 15/500 [07:45<4:08:39, 30.76s/it]  3%|▎         | 16/500 [08:15<4:04:58, 30.37s/it]  3%|▎         | 17/500 [08:44<4:02:09, 30.08s/it]  4%|▎         | 18/500 [09:16<4:05:55, 30.61s/it]  4%|▍         | 19/500 [09:47<4:05:21, 30.61s/it]  4%|▍         | 20/500 [10:18<4:06:54, 30.86s/it]  4%|▍         | 21/500 [10:51<4:11:13, 31.47s/it]  4%|▍         | 22/500 [11:23<4:11:35, 31.58s/it]  5%|▍         | 23/500 [11:55<4:11:35, 31.65s/it]  5%|▍         | 24/500 [12:27<4:11:30, 31.70s/it]  5%|▌         | 25/500 [12:56<4:04:41, 30.91s/it]  5%|▌         | 26/500 [13:27<4:06:34, 31.21s/it]  5%|▌         | 27/500 [13:58<4:05:10, 31.10s/it]  6%|▌         | 28/500 [14:30<4:04:57, 31.14s/it]  6%|▌         | 29/500 [15:04<4:13:06, 32.24s/it]  6%|▌         | 30/500 [15:39<4:17:17, 32.85s/it]  6%|▌         | 31/500 [16:10<4:13:32, 32.44s/it]  6%|▋         | 32/500 [16:51<4:33:43, 35.09s/it]  7%|▋         | 33/500 [17:21<4:19:45, 33.37s/it]  7%|▋         | 34/500 [17:46<3:59:51, 30.88s/it]  7%|▋         | 35/500 [18:19<4:05:04, 31.62s/it]  7%|▋         | 36/500 [18:51<4:05:20, 31.73s/it]  7%|▋         | 37/500 [19:23<4:05:20, 31.79s/it]  8%|▊         | 38/500 [19:56<4:06:15, 31.98s/it]  8%|▊         | 39/500 [20:24<3:58:37, 31.06s/it]  8%|▊         | 40/500 [20:57<4:00:31, 31.37s/it]  8%|▊         | 41/500 [21:32<4:10:06, 32.69s/it]  8%|▊         | 42/500 [22:01<3:59:17, 31.35s/it]  9%|▊         | 43/500 [22:34<4:04:24, 32.09s/it]  9%|▉         | 44/500 [23:04<3:59:07, 31.46s/it]  9%|▉         | 45/500 [23:37<4:01:40, 31.87s/it]  9%|▉         | 46/500 [24:09<4:01:21, 31.90s/it]  9%|▉         | 47/500 [24:41<4:01:04, 31.93s/it] 10%|▉         | 48/500 [25:13<4:00:21, 31.91s/it] 10%|▉         | 49/500 [25:45<3:59:48, 31.90s/it] 10%|█         | 50/500 [26:19<4:03:57, 32.53s/it] 10%|█         | 51/500 [26:52<4:05:29, 32.81s/it] 10%|█         | 52/500 [27:25<4:03:55, 32.67s/it] 11%|█         | 53/500 [27:57<4:01:43, 32.45s/it] 11%|█         | 54/500 [28:30<4:04:15, 32.86s/it] 11%|█         | 55/500 [29:05<4:06:32, 33.24s/it] 11%|█         | 56/500 [29:36<4:02:48, 32.81s/it] 11%|█▏        | 57/500 [30:08<3:59:49, 32.48s/it] 12%|█▏        | 58/500 [30:41<3:59:38, 32.53s/it] 12%|█▏        | 59/500 [31:15<4:03:16, 33.10s/it] 12%|█▏        | 60/500 [31:45<3:56:22, 32.23s/it] 12%|█▏        | 61/500 [32:16<3:53:06, 31.86s/it] 12%|█▏        | 62/500 [32:46<3:47:02, 31.10s/it] 13%|█▎        | 63/500 [33:16<3:44:25, 30.81s/it] 13%|█▎        | 64/500 [33:47<3:44:54, 30.95s/it] 13%|█▎        | 65/500 [34:19<3:47:04, 31.32s/it] 13%|█▎        | 66/500 [34:51<3:48:24, 31.58s/it] 13%|█▎        | 67/500 [35:25<3:52:17, 32.19s/it] 14%|█▎        | 68/500 [36:00<3:57:30, 32.99s/it] 14%|█▍        | 69/500 [36:29<3:48:43, 31.84s/it] 14%|█▍        | 70/500 [37:02<3:50:26, 32.16s/it] 14%|█▍        | 71/500 [37:34<3:49:26, 32.09s/it] 14%|█▍        | 72/500 [38:06<3:48:09, 31.98s/it] 15%|█▍        | 73/500 [38:38<3:47:24, 31.95s/it] 15%|█▍        | 74/500 [39:09<3:45:23, 31.74s/it] 15%|█▌        | 75/500 [39:40<3:44:23, 31.68s/it] 15%|█▌        | 76/500 [40:12<3:44:18, 31.74s/it] 15%|█▌        | 77/500 [40:44<3:43:54, 31.76s/it] 15%|█▌        | 77/500 [40:52<3:44:34, 31.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.019 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▃▁▇▆▇█▇███▆██████████████████████████
wandb:     train_loss ▃▂▃▄▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▆▃▅▂▆█▆▇█▆█▇▅██▆▇▆▇▆█▆▆▇▆▆▇▇▆▇▆▇▆▇▆▆▆▇▇
wandb:       val_loss ▃▂▂▄▃▁▇▁▃▃▁▁▄▁▂█▁▄▁▁█▁▁▃▅▄▁▁▂▆▁▁▅▁▁▁▁▅▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 1e-05
wandb:   val_accuracy 0.66
wandb:       val_loss 2.58024
wandb: 
wandb: 🚀 View run classic-resonance-32 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3g8jh3kh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_000146-3g8jh3kh/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_004329-a2604vxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-brook-33
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a2604vxx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:17:11, 38.14s/it]  0%|          | 2/500 [01:00<4:01:38, 29.11s/it]  1%|          | 3/500 [01:39<4:35:52, 33.31s/it]  1%|          | 4/500 [02:09<4:26:42, 32.26s/it]  1%|          | 5/500 [02:32<3:58:39, 28.93s/it]  1%|          | 6/500 [03:04<4:04:58, 29.75s/it]  1%|▏         | 7/500 [03:34<4:07:05, 30.07s/it]  2%|▏         | 8/500 [04:07<4:12:42, 30.82s/it]  2%|▏         | 9/500 [04:37<4:10:32, 30.62s/it]  2%|▏         | 10/500 [05:13<4:22:57, 32.20s/it]  2%|▏         | 11/500 [05:42<4:14:43, 31.25s/it]  2%|▏         | 12/500 [06:15<4:19:42, 31.93s/it]  3%|▎         | 13/500 [06:53<4:33:14, 33.66s/it]  3%|▎         | 14/500 [07:16<4:06:46, 30.47s/it]  3%|▎         | 15/500 [07:47<4:07:19, 30.60s/it]  3%|▎         | 16/500 [08:18<4:08:14, 30.77s/it]  3%|▎         | 17/500 [08:48<4:06:10, 30.58s/it]  4%|▎         | 18/500 [09:20<4:07:29, 30.81s/it]  4%|▍         | 19/500 [09:51<4:07:11, 30.84s/it]  4%|▍         | 20/500 [10:21<4:05:30, 30.69s/it]  4%|▍         | 21/500 [10:52<4:05:02, 30.69s/it]  4%|▍         | 22/500 [11:28<4:18:05, 32.40s/it]  5%|▍         | 23/500 [11:52<3:57:18, 29.85s/it]  5%|▍         | 24/500 [12:23<4:00:23, 30.30s/it]  5%|▌         | 25/500 [12:53<3:57:36, 30.01s/it]  5%|▌         | 26/500 [13:23<3:58:32, 30.19s/it]  5%|▌         | 27/500 [13:56<4:03:08, 30.84s/it]  6%|▌         | 28/500 [14:23<3:55:31, 29.94s/it]  6%|▌         | 29/500 [14:56<4:02:15, 30.86s/it]  6%|▌         | 30/500 [15:27<4:00:29, 30.70s/it]  6%|▌         | 31/500 [15:57<3:59:00, 30.58s/it]  6%|▋         | 32/500 [16:28<3:58:29, 30.58s/it]  7%|▋         | 33/500 [17:09<4:23:47, 33.89s/it]  7%|▋         | 34/500 [17:32<3:57:31, 30.58s/it]  7%|▋         | 35/500 [18:00<3:51:48, 29.91s/it]  7%|▋         | 36/500 [18:32<3:55:36, 30.47s/it]  7%|▋         | 37/500 [19:05<3:59:27, 31.03s/it]  8%|▊         | 38/500 [19:32<3:51:07, 30.02s/it]  8%|▊         | 39/500 [20:06<3:59:09, 31.13s/it]  8%|▊         | 40/500 [20:39<4:02:47, 31.67s/it]  8%|▊         | 41/500 [21:13<4:08:26, 32.48s/it]  8%|▊         | 42/500 [21:36<3:46:11, 29.63s/it]  9%|▊         | 43/500 [22:08<3:50:34, 30.27s/it]  9%|▉         | 44/500 [22:39<3:52:28, 30.59s/it]  9%|▉         | 45/500 [23:10<3:52:46, 30.70s/it]  9%|▉         | 46/500 [23:39<3:48:27, 30.19s/it]  9%|▉         | 47/500 [24:08<3:45:27, 29.86s/it] 10%|▉         | 48/500 [24:36<3:40:02, 29.21s/it] 10%|▉         | 49/500 [25:06<3:41:50, 29.51s/it] 10%|█         | 50/500 [25:37<3:44:44, 29.97s/it] 10%|█         | 51/500 [26:06<3:40:48, 29.51s/it] 10%|█         | 52/500 [26:37<3:44:00, 30.00s/it] 11%|█         | 53/500 [27:11<3:51:36, 31.09s/it] 11%|█         | 54/500 [27:41<3:49:58, 30.94s/it] 11%|█         | 55/500 [28:12<3:49:29, 30.94s/it] 11%|█         | 56/500 [28:43<3:48:41, 30.90s/it] 11%|█▏        | 57/500 [29:14<3:48:46, 30.99s/it] 12%|█▏        | 58/500 [29:45<3:47:44, 30.91s/it] 12%|█▏        | 59/500 [30:18<3:53:00, 31.70s/it] 12%|█▏        | 60/500 [30:51<3:53:31, 31.84s/it] 12%|█▏        | 61/500 [31:20<3:48:38, 31.25s/it] 12%|█▏        | 62/500 [31:51<3:46:10, 30.98s/it] 13%|█▎        | 63/500 [32:25<3:53:03, 32.00s/it] 13%|█▎        | 64/500 [32:52<3:40:50, 30.39s/it] 13%|█▎        | 65/500 [33:24<3:43:53, 30.88s/it] 13%|█▎        | 66/500 [33:52<3:37:28, 30.06s/it] 13%|█▎        | 67/500 [34:23<3:38:39, 30.30s/it] 14%|█▎        | 68/500 [34:52<3:34:59, 29.86s/it] 14%|█▍        | 69/500 [35:24<3:40:33, 30.71s/it] 14%|█▍        | 70/500 [35:52<3:33:46, 29.83s/it] 14%|█▍        | 71/500 [36:25<3:39:00, 30.63s/it] 14%|█▍        | 72/500 [36:54<3:36:21, 30.33s/it] 15%|█▍        | 73/500 [37:24<3:34:57, 30.20s/it] 15%|█▍        | 74/500 [37:54<3:33:33, 30.08s/it] 15%|█▌        | 75/500 [38:25<3:35:43, 30.45s/it] 15%|█▌        | 76/500 [38:55<3:33:28, 30.21s/it] 15%|█▌        | 77/500 [39:29<3:40:14, 31.24s/it] 15%|█▌        | 77/500 [39:36<3:37:33, 30.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.019 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▆▆▆▆▇▇▇▇▇▇▇▇█▇▇█████████████████████
wandb:     train_loss ▆▆▆▅▃█▂▅▅▂▆▅▃▃▃▂▂▁▁▇▆▂▅▃▂▅▃▄▄▂▄▃▂▄▇▅▅▂▂▁
wandb:   val_accuracy ▁▁▆█▅▃▅▄▄▅▄▄▆▅▅▄▄▄▅▄▄▅▅▄▄▄▅▅▅▄▅▅▅▅▅▄▅▄▅▄
wandb:       val_loss ▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▂▁▂▃▁▁▂▃█▁▃▁▂▂▁▃▃▂▂▄▄▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81278
wandb:     train_loss 0.02483
wandb:   val_accuracy 0.51111
wandb:       val_loss 1.26331
wandb: 
wandb: 🚀 View run dashing-brook-33 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a2604vxx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_004329-a2604vxx/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_012345-wn7vimip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-elevator-34
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wn7vimip
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:20:11, 38.50s/it]  0%|          | 2/500 [01:02<4:10:43, 30.21s/it]  1%|          | 3/500 [01:38<4:32:15, 32.87s/it]  1%|          | 4/500 [02:03<4:04:52, 29.62s/it]  1%|          | 5/500 [02:34<4:09:08, 30.20s/it]  1%|          | 6/500 [03:08<4:19:28, 31.51s/it]  1%|▏         | 7/500 [03:40<4:19:35, 31.59s/it]  2%|▏         | 8/500 [04:14<4:24:27, 32.25s/it]  2%|▏         | 9/500 [04:56<4:49:07, 35.33s/it]  2%|▏         | 10/500 [05:20<4:21:02, 31.96s/it]  2%|▏         | 11/500 [05:54<4:24:19, 32.43s/it]  2%|▏         | 12/500 [06:27<4:24:34, 32.53s/it]  3%|▎         | 13/500 [06:56<4:16:44, 31.63s/it]  3%|▎         | 14/500 [07:30<4:20:43, 32.19s/it]  3%|▎         | 15/500 [08:03<4:21:59, 32.41s/it]  3%|▎         | 16/500 [08:36<4:24:06, 32.74s/it]  3%|▎         | 17/500 [09:10<4:25:23, 32.97s/it]  4%|▎         | 18/500 [09:40<4:18:04, 32.13s/it]  4%|▍         | 19/500 [10:13<4:20:58, 32.55s/it]  4%|▍         | 20/500 [10:47<4:23:16, 32.91s/it]  4%|▍         | 21/500 [11:21<4:24:14, 33.10s/it]  4%|▍         | 22/500 [11:54<4:25:29, 33.32s/it]  5%|▍         | 23/500 [12:28<4:25:12, 33.36s/it]  5%|▍         | 24/500 [12:58<4:16:09, 32.29s/it]  5%|▌         | 25/500 [13:28<4:10:32, 31.65s/it]  5%|▌         | 26/500 [14:00<4:11:10, 31.79s/it]  5%|▌         | 27/500 [14:34<4:15:12, 32.37s/it]  6%|▌         | 28/500 [15:05<4:12:47, 32.13s/it]  6%|▌         | 29/500 [15:39<4:15:43, 32.58s/it]  6%|▌         | 30/500 [16:12<4:15:39, 32.64s/it]  6%|▌         | 31/500 [16:44<4:14:33, 32.57s/it]  6%|▋         | 32/500 [17:18<4:17:52, 33.06s/it]  7%|▋         | 33/500 [17:52<4:19:24, 33.33s/it]  7%|▋         | 34/500 [18:25<4:17:52, 33.20s/it]  7%|▋         | 35/500 [18:59<4:19:15, 33.45s/it]  7%|▋         | 36/500 [19:33<4:18:51, 33.47s/it]  7%|▋         | 37/500 [20:06<4:18:08, 33.45s/it]  8%|▊         | 38/500 [20:41<4:19:58, 33.76s/it]  8%|▊         | 39/500 [21:14<4:19:51, 33.82s/it]  8%|▊         | 40/500 [21:48<4:18:46, 33.75s/it]  8%|▊         | 41/500 [22:21<4:16:59, 33.59s/it]  8%|▊         | 42/500 [22:55<4:15:46, 33.51s/it]  9%|▊         | 43/500 [23:28<4:14:39, 33.43s/it]  9%|▉         | 44/500 [24:01<4:13:43, 33.39s/it]  9%|▉         | 45/500 [24:31<4:05:18, 32.35s/it]  9%|▉         | 45/500 [24:40<4:09:30, 32.90s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.019 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▄▆▆▇▇▇▇█████▇████▇███████
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▆▅▅▅▅▆▄▅▄▆▅▃▆▄▄▄▃▄▅▆▄▅▃▆▅▅▄▄▄▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▃▃▅▅▄▅▄▅▄▃▄▄▃▆▇▄▆▇▆▇█▇
wandb:       val_loss ▃█▂▇▂▂▂▆▄▄▁▅▆▄▃▅▃▃▂▂▄▃▄▃▃▄▄▅▃▃▃▄▄▂▃▅▁▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.60921
wandb:     train_loss 1.06211
wandb:   val_accuracy 0.43556
wandb:       val_loss 1.06304
wandb: 
wandb: 🚀 View run likely-elevator-34 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wn7vimip
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_012345-wn7vimip/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_014910-uii3n1xv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-rain-35
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uii3n1xv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:15:34, 37.94s/it]  0%|          | 2/500 [01:10<4:50:10, 34.96s/it]  1%|          | 3/500 [01:34<4:06:57, 29.81s/it]  1%|          | 4/500 [02:03<4:02:51, 29.38s/it]  1%|          | 5/500 [02:32<4:01:10, 29.23s/it]  1%|          | 6/500 [03:04<4:09:43, 30.33s/it]  1%|▏         | 7/500 [03:35<4:09:47, 30.40s/it]  2%|▏         | 8/500 [04:05<4:10:03, 30.49s/it]  2%|▏         | 9/500 [04:38<4:14:02, 31.04s/it]  2%|▏         | 10/500 [05:09<4:15:19, 31.26s/it]  2%|▏         | 11/500 [05:44<4:22:38, 32.23s/it]  2%|▏         | 12/500 [06:17<4:23:50, 32.44s/it]  3%|▎         | 13/500 [06:48<4:20:29, 32.09s/it]  3%|▎         | 14/500 [07:19<4:16:30, 31.67s/it]  3%|▎         | 15/500 [07:50<4:14:36, 31.50s/it]  3%|▎         | 16/500 [08:31<4:38:00, 34.46s/it]  3%|▎         | 17/500 [08:55<4:11:11, 31.20s/it]  4%|▎         | 18/500 [09:27<4:12:46, 31.47s/it]  4%|▍         | 19/500 [09:58<4:12:37, 31.51s/it]  4%|▍         | 20/500 [10:27<4:05:32, 30.69s/it]  4%|▍         | 21/500 [10:57<4:03:05, 30.45s/it]  4%|▍         | 22/500 [11:34<4:17:05, 32.27s/it]  5%|▍         | 23/500 [12:00<4:03:15, 30.60s/it]  5%|▍         | 24/500 [12:26<3:50:52, 29.10s/it]  5%|▌         | 25/500 [12:58<3:58:11, 30.09s/it]  5%|▌         | 26/500 [13:27<3:53:22, 29.54s/it]  5%|▌         | 27/500 [14:04<4:11:36, 31.92s/it]  6%|▌         | 28/500 [14:28<3:52:08, 29.51s/it]  6%|▌         | 29/500 [15:00<3:57:53, 30.30s/it]  6%|▌         | 30/500 [15:31<3:57:57, 30.38s/it]  6%|▌         | 31/500 [16:02<3:59:17, 30.61s/it]  6%|▋         | 32/500 [16:34<4:01:17, 30.93s/it]  7%|▋         | 33/500 [17:07<4:05:41, 31.57s/it]  7%|▋         | 34/500 [17:41<4:12:01, 32.45s/it]  7%|▋         | 35/500 [18:09<4:00:32, 31.04s/it]  7%|▋         | 36/500 [18:37<3:54:00, 30.26s/it]  7%|▋         | 37/500 [19:03<3:41:49, 28.75s/it]  8%|▊         | 38/500 [19:38<3:56:40, 30.74s/it]  8%|▊         | 39/500 [20:02<3:40:03, 28.64s/it]  8%|▊         | 40/500 [20:34<3:47:09, 29.63s/it]  8%|▊         | 41/500 [21:03<3:46:12, 29.57s/it]  8%|▊         | 42/500 [21:31<3:41:11, 28.98s/it]  9%|▊         | 43/500 [22:02<3:46:56, 29.80s/it]  9%|▉         | 44/500 [22:35<3:52:26, 30.58s/it]  9%|▉         | 45/500 [23:05<3:52:12, 30.62s/it]  9%|▉         | 46/500 [23:38<3:55:01, 31.06s/it]  9%|▉         | 47/500 [24:03<3:41:49, 29.38s/it] 10%|▉         | 48/500 [24:36<3:49:27, 30.46s/it] 10%|▉         | 49/500 [25:07<3:50:50, 30.71s/it] 10%|█         | 50/500 [25:39<3:52:19, 30.98s/it] 10%|█         | 51/500 [26:10<3:53:19, 31.18s/it] 10%|█         | 52/500 [26:41<3:51:06, 30.95s/it] 11%|█         | 53/500 [27:16<3:59:06, 32.10s/it] 11%|█         | 54/500 [27:47<3:57:21, 31.93s/it] 11%|█         | 55/500 [28:19<3:56:19, 31.86s/it] 11%|█         | 56/500 [28:49<3:51:09, 31.24s/it] 11%|█▏        | 57/500 [29:18<3:47:01, 30.75s/it] 12%|█▏        | 58/500 [29:48<3:43:30, 30.34s/it] 12%|█▏        | 59/500 [30:19<3:44:14, 30.51s/it] 12%|█▏        | 60/500 [30:51<3:48:41, 31.19s/it] 12%|█▏        | 61/500 [31:23<3:48:54, 31.29s/it] 12%|█▏        | 62/500 [31:54<3:48:21, 31.28s/it] 13%|█▎        | 63/500 [32:23<3:41:33, 30.42s/it] 13%|█▎        | 64/500 [32:56<3:47:28, 31.30s/it] 13%|█▎        | 65/500 [33:27<3:46:40, 31.27s/it] 13%|█▎        | 66/500 [33:59<3:48:04, 31.53s/it] 13%|█▎        | 67/500 [34:30<3:46:03, 31.32s/it] 14%|█▎        | 68/500 [35:00<3:41:27, 30.76s/it] 14%|█▍        | 69/500 [35:28<3:36:12, 30.10s/it] 14%|█▍        | 70/500 [36:01<3:42:16, 31.02s/it] 14%|█▍        | 71/500 [36:35<3:48:11, 31.92s/it] 14%|█▍        | 72/500 [37:07<3:47:05, 31.84s/it] 15%|█▍        | 73/500 [37:39<3:46:20, 31.80s/it] 15%|█▍        | 74/500 [38:07<3:37:40, 30.66s/it] 15%|█▌        | 75/500 [38:38<3:38:02, 30.78s/it] 15%|█▌        | 76/500 [39:10<3:39:41, 31.09s/it] 15%|█▌        | 77/500 [39:41<3:40:18, 31.25s/it] 15%|█▌        | 77/500 [39:49<3:38:46, 31.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.297 MB of 0.313 MB uploadedwandb: | 0.297 MB of 0.313 MB uploadedwandb: / 0.297 MB of 0.313 MB uploadedwandb: - 0.297 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▅▃▁▂█▇████████████████████████████████
wandb:     train_loss ▃▂▃█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy █▅▅▃▁▃▅▅▆▅▇▇▇▇▇▆▆▆▇▆▆▆▆▆▇▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆
wandb:       val_loss ▂▂▂▅█▃▃▃▁▂▁▁▂▁▁▇▁▇▁▁▇▁▁▁▇▂▁▂▄▇▁▁▆▅▁▂▂▆▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.99851
wandb:     train_loss 0.00019
wandb:   val_accuracy 0.58222
wandb:       val_loss 2.83998
wandb: 
wandb: 🚀 View run whole-rain-35 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uii3n1xv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_014910-uii3n1xv/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_023017-dbk3flfe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-valley-36
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dbk3flfe
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:11:28, 37.45s/it]  0%|          | 2/500 [01:01<4:03:58, 29.40s/it]  1%|          | 3/500 [01:36<4:27:35, 32.31s/it]  1%|          | 4/500 [02:09<4:28:18, 32.46s/it]  1%|          | 5/500 [02:33<4:02:28, 29.39s/it]  1%|          | 6/500 [03:05<4:09:51, 30.35s/it]  1%|▏         | 7/500 [03:49<4:45:18, 34.72s/it]  2%|▏         | 8/500 [04:22<4:39:20, 34.07s/it]  2%|▏         | 9/500 [04:46<4:12:32, 30.86s/it]  2%|▏         | 10/500 [05:27<4:38:46, 34.13s/it]  2%|▏         | 11/500 [05:51<4:13:06, 31.06s/it]  2%|▏         | 12/500 [06:21<4:09:10, 30.64s/it]  3%|▎         | 13/500 [06:56<4:20:19, 32.07s/it]  3%|▎         | 14/500 [07:20<4:00:55, 29.74s/it]  3%|▎         | 15/500 [07:50<4:00:46, 29.79s/it]  3%|▎         | 16/500 [08:22<4:03:52, 30.23s/it]  3%|▎         | 17/500 [08:55<4:09:56, 31.05s/it]  4%|▎         | 18/500 [09:28<4:14:55, 31.73s/it]  4%|▍         | 19/500 [09:56<4:05:32, 30.63s/it]  4%|▍         | 20/500 [10:30<4:13:05, 31.64s/it]  4%|▍         | 21/500 [11:02<4:14:39, 31.90s/it]  4%|▍         | 22/500 [11:34<4:14:26, 31.94s/it]  5%|▍         | 23/500 [12:06<4:13:00, 31.83s/it]  5%|▍         | 24/500 [12:37<4:11:22, 31.69s/it]  5%|▌         | 25/500 [13:08<4:07:55, 31.32s/it]  5%|▌         | 26/500 [13:40<4:10:08, 31.66s/it]  5%|▌         | 27/500 [14:19<4:26:29, 33.80s/it]  6%|▌         | 28/500 [14:45<4:06:07, 31.29s/it]  6%|▌         | 29/500 [15:18<4:10:15, 31.88s/it]  6%|▌         | 30/500 [15:50<4:10:11, 31.94s/it]  6%|▌         | 31/500 [16:21<4:07:34, 31.67s/it]  6%|▋         | 32/500 [16:54<4:09:57, 32.05s/it]  7%|▋         | 33/500 [17:29<4:15:35, 32.84s/it]  7%|▋         | 34/500 [18:01<4:14:05, 32.72s/it]  7%|▋         | 35/500 [18:30<4:05:39, 31.70s/it]  7%|▋         | 36/500 [19:03<4:07:17, 31.98s/it]  7%|▋         | 37/500 [19:35<4:07:05, 32.02s/it]  8%|▊         | 38/500 [20:09<4:10:14, 32.50s/it]  8%|▊         | 39/500 [20:43<4:14:12, 33.09s/it]  8%|▊         | 40/500 [21:15<4:11:31, 32.81s/it]  8%|▊         | 41/500 [21:48<4:09:50, 32.66s/it]  8%|▊         | 42/500 [22:20<4:08:11, 32.51s/it]  9%|▊         | 43/500 [23:01<4:28:40, 35.27s/it]  9%|▉         | 44/500 [23:26<4:02:32, 31.91s/it]  9%|▉         | 45/500 [23:58<4:03:20, 32.09s/it]  9%|▉         | 46/500 [24:27<3:56:14, 31.22s/it]  9%|▉         | 47/500 [25:02<4:03:26, 32.24s/it] 10%|▉         | 48/500 [25:34<4:01:37, 32.07s/it] 10%|▉         | 49/500 [26:03<3:54:19, 31.17s/it] 10%|█         | 50/500 [26:34<3:55:12, 31.36s/it] 10%|█         | 51/500 [27:13<4:10:24, 33.46s/it] 10%|█         | 52/500 [27:37<3:48:03, 30.54s/it] 11%|█         | 53/500 [28:09<3:50:56, 31.00s/it] 11%|█         | 54/500 [28:41<3:53:36, 31.43s/it] 11%|█         | 55/500 [29:13<3:55:00, 31.69s/it] 11%|█         | 56/500 [29:46<3:56:19, 31.93s/it] 11%|█▏        | 57/500 [30:18<3:56:22, 32.01s/it] 12%|█▏        | 58/500 [30:50<3:56:34, 32.11s/it] 12%|█▏        | 59/500 [31:23<3:57:43, 32.34s/it] 12%|█▏        | 60/500 [31:58<4:01:27, 32.93s/it] 12%|█▏        | 61/500 [32:30<3:59:23, 32.72s/it] 12%|█▏        | 62/500 [33:02<3:57:55, 32.59s/it] 13%|█▎        | 63/500 [33:31<3:49:28, 31.51s/it] 13%|█▎        | 64/500 [34:02<3:47:51, 31.36s/it] 13%|█▎        | 65/500 [34:36<3:53:41, 32.23s/it] 13%|█▎        | 66/500 [35:10<3:55:48, 32.60s/it] 13%|█▎        | 67/500 [35:41<3:51:48, 32.12s/it] 14%|█▎        | 68/500 [36:12<3:50:18, 31.99s/it] 14%|█▍        | 69/500 [36:45<3:50:20, 32.07s/it] 14%|█▍        | 70/500 [37:17<3:50:42, 32.19s/it] 14%|█▍        | 71/500 [37:47<3:45:28, 31.54s/it] 14%|█▍        | 72/500 [38:21<3:49:48, 32.22s/it] 15%|█▍        | 73/500 [38:51<3:44:03, 31.48s/it] 15%|█▍        | 74/500 [39:25<3:50:02, 32.40s/it] 15%|█▌        | 75/500 [40:00<3:54:55, 33.17s/it] 15%|█▌        | 76/500 [40:33<3:53:26, 33.03s/it] 15%|█▌        | 77/500 [41:06<3:52:46, 33.02s/it] 16%|█▌        | 78/500 [41:36<3:46:27, 32.20s/it] 16%|█▌        | 79/500 [42:04<3:37:03, 30.94s/it] 16%|█▌        | 80/500 [42:39<3:45:04, 32.15s/it] 16%|█▌        | 81/500 [43:12<3:45:39, 32.31s/it] 16%|█▋        | 82/500 [43:45<3:47:07, 32.60s/it] 17%|█▋        | 83/500 [44:18<3:47:08, 32.68s/it] 17%|█▋        | 84/500 [44:48<3:39:55, 31.72s/it] 17%|█▋        | 85/500 [45:21<3:41:59, 32.10s/it] 17%|█▋        | 86/500 [45:52<3:41:05, 32.04s/it] 17%|█▋        | 87/500 [46:25<3:40:54, 32.09s/it] 17%|█▋        | 87/500 [46:34<3:41:03, 32.12s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▆▇▇███▁▃▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇
wandb:     train_loss ▄▄▂▄▃▁▃▃▁▄▃▅▅▄▃▂▂█▄▂▂▂▃▁▆▅▆▃▁▃▄▄▃▁▁▄▃▁▂▃
wandb:   val_accuracy ▂▅█▇▃▄▄▄▄▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▄▄▃▄▄▄▄
wandb:       val_loss ▄▄▄▃▃▄▄▅▃▄▅▃▂▆▇▄▃▃▆▅▃▆▄▁▄▃▂▃▂▅▂▆▄█▃▅▃▂▆▃
wandb: 
wandb: Run summary:
wandb:          epoch 86
wandb:  learning_rate 0.0
wandb: train_accuracy 0.66865
wandb:     train_loss 0.80319
wandb:   val_accuracy 0.45778
wandb:       val_loss 0.8027
wandb: 
wandb: 🚀 View run winter-valley-36 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dbk3flfe
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_023017-dbk3flfe/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_031734-gcblxyvo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-hill-37
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gcblxyvo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:06:21, 36.84s/it]  0%|          | 2/500 [00:59<3:57:00, 28.56s/it]  1%|          | 3/500 [01:38<4:34:16, 33.11s/it]  1%|          | 4/500 [02:01<4:00:31, 29.10s/it]  1%|          | 5/500 [02:29<3:57:35, 28.80s/it]  1%|          | 6/500 [03:00<4:03:05, 29.52s/it]  1%|▏         | 7/500 [03:32<4:10:08, 30.44s/it]  2%|▏         | 8/500 [04:03<4:11:04, 30.62s/it]  2%|▏         | 9/500 [04:39<4:24:42, 32.35s/it]  2%|▏         | 10/500 [05:03<4:03:25, 29.81s/it]  2%|▏         | 11/500 [05:32<3:59:36, 29.40s/it]  2%|▏         | 12/500 [06:05<4:08:32, 30.56s/it]  3%|▎         | 13/500 [06:37<4:12:29, 31.11s/it]  3%|▎         | 14/500 [07:07<4:09:07, 30.76s/it]  3%|▎         | 15/500 [07:38<4:07:06, 30.57s/it]  3%|▎         | 16/500 [08:08<4:06:58, 30.62s/it]  3%|▎         | 17/500 [08:39<4:06:55, 30.67s/it]  4%|▎         | 18/500 [09:10<4:07:08, 30.76s/it]  4%|▍         | 19/500 [09:42<4:10:15, 31.22s/it]  4%|▍         | 20/500 [10:20<4:25:31, 33.19s/it]  4%|▍         | 21/500 [10:43<4:00:44, 30.15s/it]  4%|▍         | 22/500 [11:15<4:03:28, 30.56s/it]  5%|▍         | 23/500 [11:47<4:08:02, 31.20s/it]  5%|▍         | 24/500 [12:17<4:02:59, 30.63s/it]  5%|▌         | 25/500 [12:45<3:57:23, 29.99s/it]  5%|▌         | 26/500 [13:16<3:58:14, 30.16s/it]  5%|▌         | 27/500 [13:47<3:59:59, 30.44s/it]  6%|▌         | 28/500 [14:18<4:01:52, 30.75s/it]  6%|▌         | 29/500 [14:43<3:46:46, 28.89s/it]  6%|▌         | 30/500 [15:15<3:53:01, 29.75s/it]  6%|▌         | 31/500 [15:46<3:56:01, 30.19s/it]  6%|▋         | 32/500 [16:18<4:00:29, 30.83s/it]  7%|▋         | 33/500 [16:47<3:55:24, 30.25s/it]  7%|▋         | 34/500 [17:16<3:52:00, 29.87s/it]  7%|▋         | 35/500 [17:48<3:56:40, 30.54s/it]  7%|▋         | 36/500 [18:18<3:53:35, 30.21s/it]  7%|▋         | 37/500 [18:45<3:46:30, 29.35s/it]  8%|▊         | 38/500 [19:18<3:53:53, 30.38s/it]  8%|▊         | 39/500 [19:48<3:53:17, 30.36s/it]  8%|▊         | 40/500 [20:18<3:53:08, 30.41s/it]  8%|▊         | 41/500 [20:49<3:52:20, 30.37s/it]  8%|▊         | 42/500 [21:20<3:53:18, 30.56s/it]  9%|▊         | 43/500 [21:50<3:53:05, 30.60s/it]  9%|▉         | 44/500 [22:19<3:47:55, 29.99s/it]  9%|▉         | 45/500 [22:52<3:54:40, 30.95s/it]  9%|▉         | 45/500 [22:58<3:52:17, 30.63s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▆▇▆▇▇▇▇███▇▇███▇█████▇█
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▆▅▅▅▅▆▄▅▄▆▅▄▆▄▄▄▃▄▅▆▄▅▃▆▅▅▄▄▄▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▃▂▅▄▄▄▄▄▄▃▃▄▄▆▇▄▅▇▆▇█▇
wandb:       val_loss ▃█▂▇▂▂▂▆▄▄▁▅▆▄▃▅▃▃▂▂▄▃▄▃▃▄▄▅▃▃▃▄▄▂▃▅▁▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.60624
wandb:     train_loss 1.06332
wandb:   val_accuracy 0.43111
wandb:       val_loss 1.06427
wandb: 
wandb: 🚀 View run feasible-hill-37 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gcblxyvo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_031734-gcblxyvo/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_034120-2fi93okk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-thunder-38
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2fi93okk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:17:40, 38.20s/it]  0%|          | 2/500 [01:09<4:45:21, 34.38s/it]  1%|          | 3/500 [01:33<4:04:02, 29.46s/it]  1%|          | 4/500 [02:03<4:06:31, 29.82s/it]  1%|          | 5/500 [02:33<4:06:17, 29.85s/it]  1%|          | 6/500 [03:04<4:06:54, 29.99s/it]  1%|▏         | 7/500 [03:36<4:12:23, 30.72s/it]  2%|▏         | 8/500 [04:09<4:18:03, 31.47s/it]  2%|▏         | 9/500 [04:39<4:14:17, 31.07s/it]  2%|▏         | 10/500 [05:17<4:31:12, 33.21s/it]  2%|▏         | 11/500 [05:40<4:05:29, 30.12s/it]  2%|▏         | 12/500 [06:13<4:11:06, 30.87s/it]  3%|▎         | 13/500 [06:52<4:31:39, 33.47s/it]  3%|▎         | 14/500 [07:23<4:23:33, 32.54s/it]  3%|▎         | 15/500 [07:46<4:00:08, 29.71s/it]  3%|▎         | 16/500 [08:17<4:02:47, 30.10s/it]  3%|▎         | 17/500 [08:49<4:08:22, 30.85s/it]  4%|▎         | 18/500 [09:17<3:59:06, 29.76s/it]  4%|▍         | 19/500 [09:50<4:07:01, 30.81s/it]  4%|▍         | 20/500 [10:20<4:04:40, 30.59s/it]  4%|▍         | 21/500 [10:51<4:05:23, 30.74s/it]  4%|▍         | 22/500 [11:25<4:11:32, 31.58s/it]  5%|▍         | 23/500 [11:53<4:03:28, 30.63s/it]  5%|▍         | 24/500 [12:22<3:59:11, 30.15s/it]  5%|▌         | 25/500 [12:54<4:03:38, 30.77s/it]  5%|▌         | 26/500 [13:32<4:20:08, 32.93s/it]  5%|▌         | 27/500 [13:55<3:56:21, 29.98s/it]  6%|▌         | 28/500 [14:26<3:58:23, 30.30s/it]  6%|▌         | 29/500 [14:57<3:59:29, 30.51s/it]  6%|▌         | 30/500 [15:29<4:00:55, 30.76s/it]  6%|▌         | 31/500 [15:59<3:58:50, 30.55s/it]  6%|▋         | 32/500 [16:31<4:01:31, 30.96s/it]  7%|▋         | 33/500 [17:01<4:00:20, 30.88s/it]  7%|▋         | 34/500 [17:32<4:00:09, 30.92s/it]  7%|▋         | 35/500 [18:12<4:21:03, 33.68s/it]  7%|▋         | 36/500 [18:35<3:55:43, 30.48s/it]  7%|▋         | 37/500 [19:06<3:55:18, 30.49s/it]  8%|▊         | 38/500 [19:38<3:58:52, 31.02s/it]  8%|▊         | 39/500 [20:11<4:01:58, 31.49s/it]  8%|▊         | 40/500 [20:43<4:02:59, 31.69s/it]  8%|▊         | 41/500 [21:21<4:17:28, 33.66s/it]  8%|▊         | 42/500 [21:45<3:53:19, 30.57s/it]  9%|▊         | 43/500 [22:16<3:53:50, 30.70s/it]  9%|▉         | 44/500 [22:47<3:54:37, 30.87s/it]  9%|▉         | 45/500 [23:18<3:55:06, 31.00s/it]  9%|▉         | 46/500 [23:51<3:59:09, 31.61s/it]  9%|▉         | 47/500 [24:25<4:03:22, 32.24s/it] 10%|▉         | 48/500 [24:56<4:00:37, 31.94s/it] 10%|▉         | 49/500 [25:27<3:58:09, 31.68s/it] 10%|█         | 50/500 [25:58<3:56:00, 31.47s/it] 10%|█         | 51/500 [26:31<3:58:23, 31.86s/it] 10%|█         | 52/500 [27:02<3:56:31, 31.68s/it] 11%|█         | 53/500 [27:33<3:54:59, 31.54s/it] 11%|█         | 54/500 [28:03<3:48:56, 30.80s/it] 11%|█         | 55/500 [28:35<3:51:58, 31.28s/it] 11%|█         | 56/500 [29:06<3:51:55, 31.34s/it] 11%|█▏        | 57/500 [29:36<3:46:51, 30.73s/it] 12%|█▏        | 58/500 [30:07<3:47:01, 30.82s/it] 12%|█▏        | 59/500 [30:38<3:46:37, 30.83s/it] 12%|█▏        | 60/500 [31:08<3:44:49, 30.66s/it] 12%|█▏        | 61/500 [31:39<3:44:36, 30.70s/it] 12%|█▏        | 62/500 [32:09<3:43:40, 30.64s/it] 13%|█▎        | 63/500 [32:42<3:48:24, 31.36s/it] 13%|█▎        | 64/500 [33:07<3:32:46, 29.28s/it] 13%|█▎        | 65/500 [33:38<3:37:14, 29.97s/it] 13%|█▎        | 66/500 [34:08<3:37:02, 30.01s/it] 13%|█▎        | 67/500 [34:38<3:36:29, 30.00s/it] 14%|█▎        | 68/500 [35:11<3:41:57, 30.83s/it] 14%|█▍        | 69/500 [35:43<3:43:59, 31.18s/it] 14%|█▍        | 70/500 [36:13<3:41:15, 30.87s/it] 14%|█▍        | 71/500 [36:44<3:39:41, 30.73s/it] 14%|█▍        | 72/500 [37:16<3:42:00, 31.12s/it] 15%|█▍        | 73/500 [37:50<3:47:30, 31.97s/it] 15%|█▍        | 74/500 [38:16<3:35:19, 30.33s/it] 15%|█▌        | 75/500 [38:49<3:39:55, 31.05s/it] 15%|█▌        | 76/500 [39:19<3:38:03, 30.86s/it] 15%|█▌        | 77/500 [39:50<3:36:53, 30.76s/it] 15%|█▌        | 77/500 [39:50<3:38:51, 31.04s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.140 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▆▁▃▅█▅▄▅███▆█████████████████████████
wandb:     train_loss █▂▅▁▁▁▁▁▄█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▃▇▁▃▄▇▄▂▄███▅▇▇▇▇▇▇██▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇
wandb:       val_loss ▄▃▃▅▅▁▆▁▂▇▆▂▄▁▇▆▃▅▂▁▃▁▁▄█▂▅▂▃▄▁▁▂▁▃▃▁▃▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.99851
wandb:     train_loss 0.0
wandb:   val_accuracy 0.60222
wandb:       val_loss 1.48212
wandb: 
wandb: 🚀 View run fluent-thunder-38 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2fi93okk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_034120-2fi93okk/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_042155-69573fcg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sea-39
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/69573fcg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:41:45, 26.66s/it]  0%|          | 2/500 [00:49<3:24:39, 24.66s/it]  1%|          | 3/500 [01:19<3:41:07, 26.69s/it]  1%|          | 4/500 [01:50<3:56:43, 28.64s/it]  1%|          | 5/500 [02:13<3:40:19, 26.71s/it]  1%|          | 6/500 [02:45<3:52:47, 28.27s/it]  1%|▏         | 7/500 [03:16<4:00:24, 29.26s/it]  2%|▏         | 8/500 [03:47<4:04:19, 29.80s/it]  2%|▏         | 9/500 [04:26<4:26:40, 32.59s/it]  2%|▏         | 10/500 [04:49<4:03:18, 29.79s/it]  2%|▏         | 11/500 [05:20<4:03:59, 29.94s/it]  2%|▏         | 12/500 [05:43<3:47:16, 27.94s/it]  3%|▎         | 13/500 [06:20<4:08:41, 30.64s/it]  3%|▎         | 14/500 [06:43<3:49:29, 28.33s/it]  3%|▎         | 15/500 [07:11<3:49:31, 28.39s/it]  3%|▎         | 16/500 [07:40<3:50:32, 28.58s/it]  3%|▎         | 17/500 [08:11<3:55:46, 29.29s/it]  4%|▎         | 18/500 [08:43<4:01:36, 30.07s/it]  4%|▍         | 19/500 [09:15<4:04:47, 30.54s/it]  4%|▍         | 20/500 [09:45<4:02:52, 30.36s/it]  4%|▍         | 21/500 [10:15<4:03:21, 30.48s/it]  4%|▍         | 22/500 [10:48<4:06:37, 30.96s/it]  5%|▍         | 23/500 [11:18<4:04:24, 30.74s/it]  5%|▍         | 24/500 [11:49<4:04:10, 30.78s/it]  5%|▌         | 25/500 [12:19<4:03:00, 30.70s/it]  5%|▌         | 26/500 [12:50<4:02:00, 30.63s/it]  5%|▌         | 27/500 [13:21<4:02:31, 30.76s/it]  6%|▌         | 28/500 [13:52<4:04:23, 31.07s/it]  6%|▌         | 29/500 [14:21<3:57:20, 30.23s/it]  6%|▌         | 30/500 [14:52<3:58:09, 30.40s/it]  6%|▌         | 31/500 [15:22<3:57:43, 30.41s/it]  6%|▋         | 32/500 [15:52<3:55:19, 30.17s/it]  7%|▋         | 33/500 [16:22<3:54:37, 30.15s/it]  7%|▋         | 34/500 [16:53<3:55:57, 30.38s/it]  7%|▋         | 35/500 [17:23<3:54:27, 30.25s/it]  7%|▋         | 36/500 [18:02<4:15:06, 32.99s/it]  7%|▋         | 37/500 [18:25<3:51:39, 30.02s/it]  8%|▊         | 38/500 [18:55<3:50:48, 29.97s/it]  8%|▊         | 39/500 [19:28<3:57:50, 30.96s/it]  8%|▊         | 40/500 [19:58<3:55:22, 30.70s/it]  8%|▊         | 41/500 [20:41<4:21:48, 34.22s/it]  8%|▊         | 42/500 [21:04<3:55:25, 30.84s/it]  9%|▊         | 43/500 [21:31<3:47:42, 29.90s/it]  9%|▉         | 44/500 [22:03<3:52:19, 30.57s/it]  9%|▉         | 45/500 [22:33<3:49:09, 30.22s/it]  9%|▉         | 46/500 [23:02<3:46:28, 29.93s/it]  9%|▉         | 47/500 [23:33<3:48:39, 30.29s/it] 10%|▉         | 48/500 [24:06<3:53:42, 31.02s/it] 10%|▉         | 49/500 [24:45<4:10:08, 33.28s/it] 10%|█         | 50/500 [25:08<3:46:42, 30.23s/it] 10%|█         | 51/500 [25:43<3:57:04, 31.68s/it] 10%|█         | 52/500 [26:06<3:38:38, 29.28s/it] 11%|█         | 53/500 [26:36<3:37:55, 29.25s/it] 11%|█         | 54/500 [27:04<3:35:28, 28.99s/it] 11%|█         | 55/500 [27:34<3:36:49, 29.23s/it] 11%|█         | 56/500 [28:06<3:43:15, 30.17s/it] 11%|█▏        | 57/500 [28:37<3:43:25, 30.26s/it] 12%|█▏        | 58/500 [29:05<3:37:58, 29.59s/it] 12%|█▏        | 59/500 [29:35<3:40:04, 29.94s/it] 12%|█▏        | 60/500 [30:06<3:40:54, 30.12s/it] 12%|█▏        | 61/500 [30:38<3:44:23, 30.67s/it] 12%|█▏        | 62/500 [31:09<3:45:31, 30.89s/it] 13%|█▎        | 63/500 [31:40<3:45:27, 30.96s/it] 13%|█▎        | 64/500 [32:09<3:39:09, 30.16s/it] 13%|█▎        | 65/500 [32:40<3:40:08, 30.36s/it] 13%|█▎        | 66/500 [33:08<3:36:13, 29.89s/it] 13%|█▎        | 67/500 [33:37<3:34:01, 29.66s/it] 14%|█▎        | 68/500 [34:08<3:35:26, 29.92s/it] 14%|█▍        | 69/500 [34:40<3:39:31, 30.56s/it] 14%|█▍        | 70/500 [35:10<3:38:11, 30.45s/it] 14%|█▍        | 71/500 [35:40<3:36:52, 30.33s/it] 14%|█▍        | 72/500 [36:11<3:37:35, 30.50s/it] 15%|█▍        | 73/500 [36:41<3:35:34, 30.29s/it] 15%|█▍        | 74/500 [37:10<3:32:23, 29.92s/it] 15%|█▌        | 75/500 [37:43<3:38:15, 30.81s/it] 15%|█▌        | 76/500 [38:13<3:35:53, 30.55s/it] 15%|█▌        | 77/500 [38:43<3:35:09, 30.52s/it] 16%|█▌        | 78/500 [39:14<3:35:14, 30.60s/it] 16%|█▌        | 79/500 [39:45<3:35:56, 30.77s/it] 16%|█▌        | 80/500 [40:19<3:41:04, 31.58s/it] 16%|█▌        | 81/500 [40:52<3:43:28, 32.00s/it] 16%|█▋        | 82/500 [41:20<3:35:19, 30.91s/it] 17%|█▋        | 83/500 [41:51<3:35:10, 30.96s/it] 17%|█▋        | 84/500 [42:22<3:34:09, 30.89s/it] 17%|█▋        | 85/500 [42:54<3:36:23, 31.29s/it] 17%|█▋        | 86/500 [43:25<3:34:44, 31.12s/it] 17%|█▋        | 87/500 [43:55<3:33:08, 30.96s/it] 17%|█▋        | 87/500 [44:03<3:29:08, 30.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.019 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▇▇██▂▁▅▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:     train_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▂
wandb:   val_accuracy ▃▇█▇▄▅▅▂▁▄▅▅▄▄▄▃▅▃▄▅▄▄▄▅▄▄▄▄▄▅▄▄▄▄▅▃▄▃▄▄
wandb:       val_loss ▁▁▁▁▁▁▁█▂▁▂▁▁▂▂▁▁▁▂▂▁▂▁▁▂▁▁▁▁▂▁▂▂▂▁▂▁▁▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 86
wandb:  learning_rate 0.0
wandb: train_accuracy 0.69094
wandb:     train_loss 1.41199
wandb:   val_accuracy 0.4
wandb:       val_loss 0.82726
wandb: 
wandb: 🚀 View run ruby-sea-39 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/69573fcg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_042155-69573fcg/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_050642-46iwrqfx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-terrain-40
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/46iwrqfx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:13:03, 37.64s/it]  0%|          | 2/500 [01:00<4:00:06, 28.93s/it]  1%|          | 3/500 [01:29<3:58:34, 28.80s/it]  1%|          | 4/500 [01:52<3:39:12, 26.52s/it]  1%|          | 5/500 [02:20<3:44:48, 27.25s/it]  1%|          | 6/500 [02:53<3:59:46, 29.12s/it]  1%|▏         | 7/500 [03:25<4:07:36, 30.13s/it]  2%|▏         | 8/500 [03:55<4:07:29, 30.18s/it]  2%|▏         | 9/500 [04:37<4:34:51, 33.59s/it]  2%|▏         | 10/500 [05:00<4:07:40, 30.33s/it]  2%|▏         | 11/500 [05:30<4:08:14, 30.46s/it]  2%|▏         | 12/500 [06:01<4:08:20, 30.53s/it]  3%|▎         | 13/500 [06:34<4:14:59, 31.42s/it]  3%|▎         | 14/500 [07:08<4:19:11, 32.00s/it]  3%|▎         | 15/500 [07:36<4:09:42, 30.89s/it]  3%|▎         | 16/500 [08:07<4:09:22, 30.91s/it]  3%|▎         | 17/500 [08:36<4:03:35, 30.26s/it]  4%|▎         | 18/500 [09:07<4:05:04, 30.51s/it]  4%|▍         | 19/500 [09:36<4:00:39, 30.02s/it]  4%|▍         | 20/500 [10:09<4:08:26, 31.06s/it]  4%|▍         | 21/500 [10:38<4:01:56, 30.31s/it]  4%|▍         | 22/500 [11:07<3:59:32, 30.07s/it]  5%|▍         | 23/500 [11:39<4:03:41, 30.65s/it]  5%|▍         | 24/500 [12:10<4:02:40, 30.59s/it]  5%|▌         | 25/500 [12:41<4:04:09, 30.84s/it]  5%|▌         | 26/500 [13:12<4:02:55, 30.75s/it]  5%|▌         | 27/500 [13:43<4:02:58, 30.82s/it]  6%|▌         | 28/500 [14:15<4:05:18, 31.18s/it]  6%|▌         | 29/500 [14:46<4:05:50, 31.32s/it]  6%|▌         | 30/500 [15:15<3:58:58, 30.51s/it]  6%|▌         | 31/500 [15:45<3:58:22, 30.49s/it]  6%|▋         | 32/500 [16:17<4:01:15, 30.93s/it]  7%|▋         | 33/500 [16:47<3:58:27, 30.64s/it]  7%|▋         | 34/500 [17:19<3:59:08, 30.79s/it]  7%|▋         | 35/500 [17:50<3:59:19, 30.88s/it]  7%|▋         | 36/500 [18:26<4:12:07, 32.60s/it]  7%|▋         | 37/500 [18:50<3:50:03, 29.81s/it]  8%|▊         | 38/500 [19:22<3:56:15, 30.68s/it]  8%|▊         | 39/500 [19:56<4:03:34, 31.70s/it]  8%|▊         | 40/500 [20:24<3:54:45, 30.62s/it]  8%|▊         | 41/500 [20:54<3:52:20, 30.37s/it]  8%|▊         | 42/500 [21:24<3:51:20, 30.31s/it]  9%|▊         | 43/500 [21:55<3:51:44, 30.43s/it]  9%|▉         | 44/500 [22:26<3:52:07, 30.54s/it]  9%|▉         | 45/500 [22:58<3:55:29, 31.05s/it]  9%|▉         | 45/500 [23:09<3:54:11, 30.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▅▆▇▆▇▇▇▇███▇████▇███████
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▇▅▅▅▅▆▄▅▄▆▅▄▆▄▄▄▃▄▅▆▄▅▃▆▅▅▄▄▄▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▃▃▅▄▄▄▄▄▄▃▃▄▄▆▇▄▅▇▆▆█▇
wandb:       val_loss ▃█▂▇▂▂▂▆▄▄▁▅▆▄▃▅▃▃▂▂▄▃▄▃▃▄▄▅▃▃▃▄▄▂▃▅▁▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.60624
wandb:     train_loss 1.06373
wandb:   val_accuracy 0.43333
wandb:       val_loss 1.06466
wandb: 
wandb: 🚀 View run woven-terrain-40 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/46iwrqfx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_050642-46iwrqfx/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_053035-t88u93ev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-universe-41
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/t88u93ev
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:14:52, 37.86s/it]  0%|          | 2/500 [01:00<4:01:41, 29.12s/it]  1%|          | 3/500 [01:31<4:08:08, 29.96s/it]  1%|          | 4/500 [02:02<4:09:49, 30.22s/it]  1%|          | 5/500 [02:32<4:10:07, 30.32s/it]  1%|          | 6/500 [03:03<4:09:55, 30.35s/it]  1%|▏         | 7/500 [03:40<4:27:06, 32.51s/it]  2%|▏         | 8/500 [04:03<4:02:37, 29.59s/it]  2%|▏         | 9/500 [04:34<4:04:51, 29.92s/it]  2%|▏         | 10/500 [05:06<4:10:44, 30.70s/it]  2%|▏         | 11/500 [05:34<4:04:03, 29.95s/it]  2%|▏         | 12/500 [06:04<4:02:11, 29.78s/it]  3%|▎         | 13/500 [06:33<4:00:38, 29.65s/it]  3%|▎         | 14/500 [07:01<3:56:46, 29.23s/it]  3%|▎         | 15/500 [07:32<4:00:23, 29.74s/it]  3%|▎         | 16/500 [08:03<4:02:24, 30.05s/it]  3%|▎         | 17/500 [08:33<4:01:01, 29.94s/it]  4%|▎         | 18/500 [09:03<4:02:03, 30.13s/it]  4%|▍         | 19/500 [09:32<3:58:09, 29.71s/it]  4%|▍         | 20/500 [10:06<4:06:35, 30.82s/it]  4%|▍         | 21/500 [10:39<4:13:12, 31.72s/it]  4%|▍         | 22/500 [11:18<4:28:09, 33.66s/it]  5%|▍         | 23/500 [11:40<4:01:58, 30.44s/it]  5%|▍         | 24/500 [12:13<4:06:46, 31.11s/it]  5%|▌         | 25/500 [12:46<4:09:15, 31.49s/it]  5%|▌         | 26/500 [13:16<4:05:21, 31.06s/it]  5%|▌         | 27/500 [13:46<4:03:41, 30.91s/it]  6%|▌         | 28/500 [14:18<4:05:41, 31.23s/it]  6%|▌         | 29/500 [14:50<4:06:17, 31.37s/it]  6%|▌         | 30/500 [15:20<4:02:47, 30.99s/it]  6%|▌         | 31/500 [15:55<4:10:42, 32.07s/it]  6%|▋         | 32/500 [16:27<4:12:02, 32.31s/it]  7%|▋         | 33/500 [16:57<4:05:58, 31.60s/it]  7%|▋         | 34/500 [17:28<4:02:12, 31.19s/it]  7%|▋         | 35/500 [18:00<4:03:45, 31.45s/it]  7%|▋         | 36/500 [18:37<4:15:52, 33.09s/it]  7%|▋         | 37/500 [19:00<3:53:43, 30.29s/it]  8%|▊         | 38/500 [19:29<3:49:54, 29.86s/it]  8%|▊         | 39/500 [20:01<3:53:16, 30.36s/it]  8%|▊         | 40/500 [20:33<3:56:24, 30.83s/it]  8%|▊         | 41/500 [21:08<4:05:54, 32.15s/it]  8%|▊         | 42/500 [21:31<3:44:59, 29.48s/it]  9%|▊         | 43/500 [22:01<3:45:16, 29.58s/it]  9%|▉         | 44/500 [22:33<3:50:06, 30.28s/it]  9%|▉         | 45/500 [23:01<3:44:51, 29.65s/it]  9%|▉         | 46/500 [23:31<3:44:22, 29.65s/it]  9%|▉         | 47/500 [24:02<3:48:24, 30.25s/it] 10%|▉         | 48/500 [24:31<3:43:57, 29.73s/it] 10%|▉         | 49/500 [25:01<3:45:32, 30.01s/it] 10%|█         | 50/500 [25:32<3:46:24, 30.19s/it] 10%|█         | 51/500 [26:04<3:50:39, 30.82s/it] 10%|█         | 52/500 [26:33<3:44:24, 30.05s/it] 11%|█         | 53/500 [27:02<3:42:45, 29.90s/it] 11%|█         | 54/500 [27:32<3:42:40, 29.96s/it] 11%|█         | 55/500 [28:01<3:39:00, 29.53s/it] 11%|█         | 56/500 [28:32<3:41:12, 29.89s/it] 11%|█▏        | 57/500 [29:03<3:44:03, 30.35s/it] 12%|█▏        | 58/500 [29:34<3:46:11, 30.71s/it] 12%|█▏        | 59/500 [30:06<3:46:21, 30.80s/it] 12%|█▏        | 60/500 [30:36<3:44:59, 30.68s/it] 12%|█▏        | 61/500 [31:07<3:44:45, 30.72s/it] 12%|█▏        | 62/500 [31:39<3:47:37, 31.18s/it] 13%|█▎        | 63/500 [32:09<3:45:01, 30.90s/it] 13%|█▎        | 64/500 [32:41<3:46:24, 31.16s/it] 13%|█▎        | 65/500 [33:12<3:46:06, 31.19s/it] 13%|█▎        | 66/500 [33:43<3:44:01, 30.97s/it] 13%|█▎        | 67/500 [34:13<3:41:49, 30.74s/it] 14%|█▎        | 68/500 [34:41<3:36:26, 30.06s/it] 14%|█▍        | 69/500 [35:14<3:40:28, 30.69s/it] 14%|█▍        | 70/500 [35:44<3:39:16, 30.60s/it] 14%|█▍        | 71/500 [36:14<3:37:16, 30.39s/it] 14%|█▍        | 72/500 [36:45<3:37:43, 30.52s/it] 15%|█▍        | 73/500 [37:15<3:36:05, 30.36s/it] 15%|█▍        | 74/500 [37:46<3:38:00, 30.71s/it] 15%|█▌        | 75/500 [38:17<3:38:32, 30.85s/it] 15%|█▌        | 76/500 [38:48<3:36:43, 30.67s/it] 15%|█▌        | 77/500 [39:18<3:34:55, 30.49s/it] 15%|█▌        | 77/500 [39:25<3:36:34, 30.72s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.019 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▁▂▇▇▆▅█▆▅███▅███▇▇███████████████████
wandb:     train_loss ▂▂▃▃▄▆▁▁▁▁▁█▁▁▁▁▁▁▁▇▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy █▁▂▂▃▆▅▅▆▆▅▅▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆
wandb:       val_loss ▂▃▂▃▄▁▂▅█▁▂▁▃▁▁▆▁▅▁▁▇▁▁▃▅▆▁▁▅▅▁▁▇▃▁▁▂▄▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.97177
wandb:     train_loss 3e-05
wandb:   val_accuracy 0.57778
wandb:       val_loss 2.91312
wandb: 
wandb: 🚀 View run electric-universe-41 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/t88u93ev
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_053035-t88u93ev/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_061046-vdwjlv4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-water-42
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vdwjlv4w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:10:16, 37.31s/it]  0%|          | 2/500 [01:00<3:59:50, 28.90s/it]  1%|          | 3/500 [01:39<4:38:47, 33.66s/it]  1%|          | 4/500 [02:10<4:29:23, 32.59s/it]  1%|          | 5/500 [02:33<4:00:11, 29.11s/it]  1%|          | 6/500 [03:04<4:04:53, 29.74s/it]  1%|▏         | 7/500 [03:32<4:00:39, 29.29s/it]  2%|▏         | 8/500 [04:03<4:03:31, 29.70s/it]  2%|▏         | 9/500 [04:35<4:07:52, 30.29s/it]  2%|▏         | 10/500 [05:09<4:18:56, 31.71s/it]  2%|▏         | 11/500 [05:36<4:04:34, 30.01s/it]  2%|▏         | 12/500 [06:08<4:10:17, 30.77s/it]  3%|▎         | 13/500 [06:36<4:03:51, 30.04s/it]  3%|▎         | 14/500 [07:07<4:04:19, 30.16s/it]  3%|▎         | 15/500 [07:37<4:04:46, 30.28s/it]  3%|▎         | 16/500 [08:08<4:05:11, 30.40s/it]  3%|▎         | 17/500 [08:37<4:02:11, 30.09s/it]  4%|▎         | 18/500 [09:07<4:00:41, 29.96s/it]  4%|▍         | 19/500 [09:45<4:18:40, 32.27s/it]  4%|▍         | 20/500 [10:13<4:08:58, 31.12s/it]  4%|▍         | 21/500 [10:41<4:01:15, 30.22s/it]  4%|▍         | 22/500 [11:22<4:24:48, 33.24s/it]  5%|▍         | 23/500 [11:44<3:59:29, 30.12s/it]  5%|▍         | 24/500 [12:15<3:59:26, 30.18s/it]  5%|▌         | 25/500 [12:45<3:58:15, 30.10s/it]  5%|▌         | 26/500 [13:15<3:57:05, 30.01s/it]  5%|▌         | 27/500 [13:44<3:56:21, 29.98s/it]  6%|▌         | 28/500 [14:15<3:57:40, 30.21s/it]  6%|▌         | 29/500 [14:46<3:58:59, 30.44s/it]  6%|▌         | 30/500 [15:16<3:57:47, 30.36s/it]  6%|▌         | 31/500 [15:48<4:00:26, 30.76s/it]  6%|▋         | 32/500 [16:18<3:57:50, 30.49s/it]  7%|▋         | 33/500 [16:48<3:57:11, 30.47s/it]  7%|▋         | 34/500 [17:19<3:57:44, 30.61s/it]  7%|▋         | 35/500 [17:50<3:57:32, 30.65s/it]  7%|▋         | 36/500 [18:28<4:14:37, 32.93s/it]  7%|▋         | 37/500 [18:52<3:52:54, 30.18s/it]  8%|▊         | 38/500 [19:23<3:55:07, 30.54s/it]  8%|▊         | 39/500 [19:53<3:51:38, 30.15s/it]  8%|▊         | 40/500 [20:25<3:55:12, 30.68s/it]  8%|▊         | 41/500 [21:02<4:10:19, 32.72s/it]  8%|▊         | 42/500 [21:25<3:48:25, 29.93s/it]  9%|▊         | 43/500 [21:57<3:51:00, 30.33s/it]  9%|▉         | 44/500 [22:28<3:51:49, 30.50s/it]  9%|▉         | 45/500 [22:59<3:52:31, 30.66s/it]  9%|▉         | 46/500 [23:30<3:52:28, 30.72s/it]  9%|▉         | 47/500 [24:04<3:59:23, 31.71s/it] 10%|▉         | 48/500 [24:36<4:01:17, 32.03s/it] 10%|▉         | 49/500 [25:08<3:59:21, 31.84s/it] 10%|█         | 50/500 [25:36<3:51:44, 30.90s/it] 10%|█         | 51/500 [26:08<3:52:22, 31.05s/it] 10%|█         | 52/500 [26:39<3:52:54, 31.19s/it] 11%|█         | 53/500 [27:09<3:48:53, 30.72s/it] 11%|█         | 54/500 [27:37<3:43:28, 30.06s/it] 11%|█         | 55/500 [28:09<3:45:30, 30.41s/it] 11%|█         | 56/500 [28:41<3:50:14, 31.11s/it] 11%|█▏        | 57/500 [29:07<3:36:50, 29.37s/it] 12%|█▏        | 58/500 [29:38<3:40:58, 30.00s/it] 12%|█▏        | 59/500 [30:07<3:36:57, 29.52s/it] 12%|█▏        | 60/500 [30:39<3:41:51, 30.25s/it] 12%|█▏        | 61/500 [31:11<3:46:51, 31.01s/it] 12%|█▏        | 62/500 [31:44<3:50:44, 31.61s/it] 13%|█▎        | 63/500 [32:13<3:43:18, 30.66s/it] 13%|█▎        | 64/500 [32:44<3:43:16, 30.73s/it] 13%|█▎        | 65/500 [33:16<3:45:53, 31.16s/it] 13%|█▎        | 66/500 [33:48<3:48:34, 31.60s/it] 13%|█▎        | 67/500 [34:18<3:43:00, 30.90s/it] 14%|█▎        | 68/500 [34:47<3:38:37, 30.36s/it] 14%|█▍        | 69/500 [35:21<3:46:22, 31.51s/it] 14%|█▍        | 70/500 [35:54<3:49:02, 31.96s/it] 14%|█▍        | 71/500 [36:22<3:40:55, 30.90s/it] 14%|█▍        | 72/500 [36:53<3:39:00, 30.70s/it] 15%|█▍        | 73/500 [37:25<3:41:42, 31.15s/it] 15%|█▍        | 74/500 [37:55<3:39:39, 30.94s/it] 15%|█▌        | 75/500 [38:26<3:38:41, 30.88s/it] 15%|█▌        | 76/500 [38:57<3:38:01, 30.85s/it] 15%|█▌        | 77/500 [39:28<3:38:51, 31.04s/it] 15%|█▌        | 77/500 [39:28<3:36:53, 30.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.140 MB of 0.318 MB uploadedwandb: - 0.140 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇███▇███▇██████▁▁▂▂▂▂▃▂▂▂▂
wandb:     train_loss ▄▄▇▄▂▅▂▄▄▂▅▄▂▃▂▂▂▁▁▅▃▂▂▃▁▃▂▃▂█▆█▇▄▂█▃▆▃▇
wandb:   val_accuracy ▄▅███▅▆▆▅▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▂▁▄▄▃▄▅▄▃▃▄
wandb:       val_loss ▃▄▃▃▃▃▃▄▄▃▂▂▃▃▃▅▃▄▂▂▅▁▂▃▄▄▂▃▃▆▆▂▇▆▂▆▄█▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.18871
wandb:     train_loss 2.02256
wandb:   val_accuracy 0.34444
wandb:       val_loss 2.15145
wandb: 
wandb: 🚀 View run quiet-water-42 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vdwjlv4w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_061046-vdwjlv4w/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_065052-qw58n22t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-meadow-43
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qw58n22t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:19:24, 38.41s/it]  0%|          | 2/500 [01:01<4:05:35, 29.59s/it]  1%|          | 3/500 [01:38<4:32:04, 32.85s/it]  1%|          | 4/500 [02:01<4:00:39, 29.11s/it]  1%|          | 5/500 [02:31<4:01:44, 29.30s/it]  1%|          | 6/500 [03:00<4:01:33, 29.34s/it]  1%|▏         | 7/500 [03:31<4:04:43, 29.78s/it]  2%|▏         | 8/500 [04:05<4:13:35, 30.93s/it]  2%|▏         | 9/500 [04:41<4:28:27, 32.81s/it]  2%|▏         | 10/500 [05:07<4:09:18, 30.53s/it]  2%|▏         | 11/500 [05:40<4:14:08, 31.18s/it]  2%|▏         | 12/500 [06:10<4:12:46, 31.08s/it]  3%|▎         | 13/500 [06:42<4:14:18, 31.33s/it]  3%|▎         | 14/500 [07:11<4:08:07, 30.63s/it]  3%|▎         | 15/500 [07:43<4:11:07, 31.07s/it]  3%|▎         | 16/500 [08:14<4:10:26, 31.05s/it]  3%|▎         | 17/500 [08:46<4:10:04, 31.06s/it]  4%|▎         | 18/500 [09:17<4:10:48, 31.22s/it]  4%|▍         | 19/500 [09:49<4:12:20, 31.48s/it]  4%|▍         | 20/500 [10:22<4:16:08, 32.02s/it]  4%|▍         | 21/500 [10:48<4:00:31, 30.13s/it]  4%|▍         | 22/500 [11:17<3:56:39, 29.71s/it]  5%|▍         | 23/500 [11:49<4:01:56, 30.43s/it]  5%|▍         | 24/500 [12:22<4:08:32, 31.33s/it]  5%|▌         | 25/500 [12:51<4:00:13, 30.34s/it]  5%|▌         | 26/500 [13:22<4:02:08, 30.65s/it]  5%|▌         | 27/500 [13:51<3:57:12, 30.09s/it]  6%|▌         | 28/500 [14:23<4:02:01, 30.77s/it]  6%|▌         | 29/500 [14:51<3:55:30, 30.00s/it]  6%|▌         | 30/500 [15:24<4:01:53, 30.88s/it]  6%|▌         | 31/500 [15:55<4:00:19, 30.74s/it]  6%|▋         | 32/500 [16:27<4:02:35, 31.10s/it]  7%|▋         | 33/500 [16:55<3:56:55, 30.44s/it]  7%|▋         | 34/500 [17:28<4:01:26, 31.09s/it]  7%|▋         | 35/500 [18:01<4:05:13, 31.64s/it]  7%|▋         | 36/500 [18:29<3:56:59, 30.64s/it]  7%|▋         | 37/500 [18:53<3:41:28, 28.70s/it]  8%|▊         | 38/500 [19:24<3:46:00, 29.35s/it]  8%|▊         | 39/500 [19:55<3:48:52, 29.79s/it]  8%|▊         | 40/500 [20:26<3:51:18, 30.17s/it]  8%|▊         | 41/500 [20:55<3:48:40, 29.89s/it]  8%|▊         | 42/500 [21:27<3:51:07, 30.28s/it]  9%|▊         | 43/500 [21:59<3:56:05, 31.00s/it]  9%|▉         | 44/500 [22:23<3:39:51, 28.93s/it]  9%|▉         | 45/500 [22:53<3:40:51, 29.12s/it]  9%|▉         | 45/500 [23:00<3:52:34, 30.67s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.028 MB of 0.315 MB uploadedwandb: \ 0.028 MB of 0.315 MB uploadedwandb: | 0.028 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.028 MB of 0.315 MB uploadedwandb: \ 0.028 MB of 0.315 MB uploadedwandb: | 0.028 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.028 MB of 0.315 MB uploadedwandb: \ 0.028 MB of 0.315 MB uploadedwandb: | 0.028 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.028 MB of 0.315 MB uploadedwandb: \ 0.028 MB of 0.315 MB uploadedwandb: | 0.028 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.028 MB of 0.315 MB uploadedwandb: \ 0.028 MB of 0.315 MB uploadedwandb: | 0.028 MB of 0.315 MB uploadedwandb: / 0.028 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇██▇██████
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▇▅▅▅▅▆▄▅▄▆▅▄▆▄▄▄▃▄▅▆▄▅▃▆▅▅▅▅▄▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▂▃▃▅▅▅▄▅▅▅▃▄▅▅▇▇▅▅▇▇██▇
wandb:       val_loss ▃█▂▇▂▂▂▆▅▄▁▅▆▄▃▅▃▃▂▂▄▃▄▃▃▄▄▅▃▃▃▄▄▂▃▅▁▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.60624
wandb:     train_loss 1.06679
wandb:   val_accuracy 0.41111
wandb:       val_loss 1.06724
wandb: 
wandb: 🚀 View run drawn-meadow-43 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qw58n22t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_065052-qw58n22t/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_071455-bxf4jqne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-night-44
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bxf4jqne
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:06:17, 36.83s/it]  0%|          | 2/500 [01:09<4:44:57, 34.33s/it]  1%|          | 3/500 [01:33<4:04:17, 29.49s/it]  1%|          | 4/500 [02:05<4:12:25, 30.54s/it]  1%|          | 5/500 [02:34<4:06:51, 29.92s/it]  1%|          | 6/500 [03:04<4:08:38, 30.20s/it]  1%|▏         | 7/500 [03:34<4:06:00, 29.94s/it]  2%|▏         | 8/500 [04:07<4:15:03, 31.10s/it]  2%|▏         | 9/500 [04:42<4:24:25, 32.31s/it]  2%|▏         | 10/500 [05:11<4:15:30, 31.29s/it]  2%|▏         | 11/500 [05:43<4:16:33, 31.48s/it]  2%|▏         | 12/500 [06:13<4:10:40, 30.82s/it]  3%|▎         | 13/500 [06:44<4:11:40, 31.01s/it]  3%|▎         | 14/500 [07:16<4:14:18, 31.40s/it]  3%|▎         | 15/500 [07:45<4:07:24, 30.61s/it]  3%|▎         | 16/500 [08:19<4:14:10, 31.51s/it]  3%|▎         | 17/500 [08:47<4:06:20, 30.60s/it]  4%|▎         | 18/500 [09:18<4:05:17, 30.53s/it]  4%|▍         | 19/500 [09:52<4:14:43, 31.78s/it]  4%|▍         | 20/500 [10:24<4:15:01, 31.88s/it]  4%|▍         | 21/500 [10:48<3:55:22, 29.48s/it]  4%|▍         | 22/500 [11:23<4:08:31, 31.20s/it]  5%|▍         | 23/500 [11:48<3:51:41, 29.14s/it]  5%|▍         | 24/500 [12:20<3:59:11, 30.15s/it]  5%|▌         | 25/500 [12:52<4:02:36, 30.65s/it]  5%|▌         | 26/500 [13:24<4:05:39, 31.10s/it]  5%|▌         | 27/500 [13:56<4:07:51, 31.44s/it]  6%|▌         | 28/500 [14:29<4:09:52, 31.76s/it]  6%|▌         | 29/500 [14:58<4:03:51, 31.07s/it]  6%|▌         | 30/500 [15:28<4:00:29, 30.70s/it]  6%|▌         | 31/500 [15:58<3:57:47, 30.42s/it]  6%|▋         | 32/500 [16:28<3:55:26, 30.19s/it]  7%|▋         | 33/500 [17:00<4:00:58, 30.96s/it]  7%|▋         | 34/500 [17:33<4:03:20, 31.33s/it]  7%|▋         | 35/500 [18:07<4:10:53, 32.37s/it]  7%|▋         | 36/500 [18:32<3:51:57, 29.99s/it]  7%|▋         | 37/500 [19:04<3:56:38, 30.67s/it]  8%|▊         | 38/500 [19:38<4:04:08, 31.71s/it]  8%|▊         | 39/500 [20:07<3:57:47, 30.95s/it]  8%|▊         | 40/500 [20:37<3:53:25, 30.45s/it]  8%|▊         | 41/500 [21:14<4:09:23, 32.60s/it]  8%|▊         | 42/500 [21:38<3:49:06, 30.01s/it]  9%|▊         | 43/500 [22:13<3:58:52, 31.36s/it]  9%|▉         | 44/500 [22:47<4:05:19, 32.28s/it]  9%|▉         | 45/500 [23:19<4:03:39, 32.13s/it]  9%|▉         | 46/500 [23:48<3:56:04, 31.20s/it]  9%|▉         | 47/500 [24:18<3:52:08, 30.75s/it] 10%|▉         | 48/500 [24:52<3:58:29, 31.66s/it] 10%|▉         | 49/500 [25:26<4:04:18, 32.50s/it] 10%|█         | 50/500 [25:58<4:03:08, 32.42s/it] 10%|█         | 51/500 [26:28<3:55:41, 31.50s/it] 10%|█         | 52/500 [27:00<3:57:08, 31.76s/it] 11%|█         | 53/500 [27:33<4:00:21, 32.26s/it] 11%|█         | 54/500 [28:08<4:06:00, 33.10s/it] 11%|█         | 55/500 [28:38<3:57:07, 31.97s/it] 11%|█         | 56/500 [29:07<3:51:09, 31.24s/it] 11%|█▏        | 57/500 [29:41<3:56:05, 31.98s/it] 12%|█▏        | 58/500 [30:13<3:56:14, 32.07s/it] 12%|█▏        | 59/500 [30:43<3:50:44, 31.39s/it] 12%|█▏        | 60/500 [31:15<3:50:36, 31.45s/it] 12%|█▏        | 61/500 [31:48<3:54:56, 32.11s/it] 12%|█▏        | 62/500 [32:16<3:45:31, 30.89s/it] 13%|█▎        | 63/500 [32:41<3:31:19, 29.02s/it] 13%|█▎        | 64/500 [33:14<3:39:45, 30.24s/it] 13%|█▎        | 65/500 [33:43<3:36:18, 29.84s/it] 13%|█▎        | 66/500 [34:12<3:34:40, 29.68s/it] 13%|█▎        | 67/500 [34:43<3:35:59, 29.93s/it] 14%|█▎        | 68/500 [35:16<3:41:59, 30.83s/it] 14%|█▍        | 69/500 [35:48<3:44:11, 31.21s/it] 14%|█▍        | 70/500 [36:17<3:39:40, 30.65s/it] 14%|█▍        | 71/500 [36:50<3:43:24, 31.25s/it] 14%|█▍        | 72/500 [37:19<3:37:57, 30.56s/it] 15%|█▍        | 73/500 [37:49<3:37:10, 30.52s/it] 15%|█▍        | 74/500 [38:20<3:36:13, 30.45s/it] 15%|█▌        | 75/500 [38:48<3:32:16, 29.97s/it] 15%|█▌        | 76/500 [39:21<3:37:36, 30.79s/it] 15%|█▌        | 77/500 [39:53<3:39:49, 31.18s/it] 15%|█▌        | 77/500 [39:59<3:39:39, 31.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.318 MB uploadedwandb: - 0.019 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▃▃█▃███▇████████████████████████████
wandb:     train_loss ▅▅█▆▁▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▅▃▃▄▃▇▄▇▇▇███▇▆▇▇█▇▇████████▇█████▇▇███
wandb:       val_loss ▃▂▃▂▃▄▂█▃▂▂▁▅▁▂▆▂█▁▁▅▁▁▁▄▄▁▁▁▅▁▁▄▂▁▁▃▆▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.99554
wandb:     train_loss 0.00037
wandb:   val_accuracy 0.71778
wandb:       val_loss 2.43956
wandb: 
wandb: 🚀 View run olive-night-44 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bxf4jqne
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_071455-bxf4jqne/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_075536-kagi06xz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sunset-45
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kagi06xz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:06:13, 36.82s/it]  0%|          | 2/500 [00:59<3:57:50, 28.66s/it]  1%|          | 3/500 [01:37<4:32:05, 32.85s/it]  1%|          | 4/500 [02:08<4:25:04, 32.07s/it]  1%|          | 5/500 [02:31<3:57:22, 28.77s/it]  1%|          | 6/500 [03:09<4:22:36, 31.90s/it]  1%|▏         | 7/500 [03:44<4:30:13, 32.89s/it]  2%|▏         | 8/500 [04:16<4:27:30, 32.62s/it]  2%|▏         | 9/500 [04:39<4:02:20, 29.61s/it]  2%|▏         | 10/500 [05:16<4:19:43, 31.80s/it]  2%|▏         | 11/500 [05:39<3:57:15, 29.11s/it]  2%|▏         | 12/500 [06:07<3:54:28, 28.83s/it]  3%|▎         | 13/500 [06:46<4:19:58, 32.03s/it]  3%|▎         | 14/500 [07:09<3:57:41, 29.34s/it]  3%|▎         | 15/500 [07:38<3:55:53, 29.18s/it]  3%|▎         | 16/500 [08:08<3:56:37, 29.33s/it]  3%|▎         | 17/500 [08:39<4:01:31, 30.00s/it]  4%|▎         | 18/500 [09:08<3:58:25, 29.68s/it]  4%|▍         | 19/500 [09:45<4:15:13, 31.84s/it]  4%|▍         | 20/500 [10:09<3:55:34, 29.45s/it]  4%|▍         | 21/500 [10:39<3:56:29, 29.62s/it]  4%|▍         | 22/500 [11:13<4:06:02, 30.88s/it]  5%|▍         | 23/500 [11:41<3:59:16, 30.10s/it]  5%|▍         | 24/500 [12:12<3:59:39, 30.21s/it]  5%|▌         | 25/500 [12:42<3:59:31, 30.25s/it]  5%|▌         | 26/500 [13:12<3:59:31, 30.32s/it]  5%|▌         | 27/500 [13:47<4:07:58, 31.46s/it]  6%|▌         | 28/500 [14:20<4:11:12, 31.93s/it]  6%|▌         | 29/500 [14:48<4:01:46, 30.80s/it]  6%|▌         | 30/500 [15:20<4:04:23, 31.20s/it]  6%|▌         | 31/500 [15:48<3:57:24, 30.37s/it]  6%|▋         | 32/500 [16:16<3:51:41, 29.70s/it]  7%|▋         | 33/500 [16:54<4:10:07, 32.14s/it]  7%|▋         | 34/500 [17:17<3:48:13, 29.39s/it]  7%|▋         | 35/500 [17:48<3:50:18, 29.72s/it]  7%|▋         | 36/500 [18:26<4:10:37, 32.41s/it]  7%|▋         | 37/500 [18:49<3:48:15, 29.58s/it]  8%|▊         | 38/500 [19:18<3:44:39, 29.18s/it]  8%|▊         | 39/500 [19:48<3:46:21, 29.46s/it]  8%|▊         | 40/500 [20:19<3:50:28, 30.06s/it]  8%|▊         | 41/500 [20:54<4:00:51, 31.48s/it]  8%|▊         | 42/500 [21:17<3:41:39, 29.04s/it]  9%|▊         | 43/500 [21:48<3:45:25, 29.60s/it]  9%|▉         | 44/500 [22:20<3:49:19, 30.17s/it]  9%|▉         | 45/500 [22:52<3:54:29, 30.92s/it]  9%|▉         | 46/500 [23:22<3:51:31, 30.60s/it]  9%|▉         | 47/500 [23:52<3:50:03, 30.47s/it] 10%|▉         | 48/500 [24:22<3:48:23, 30.32s/it] 10%|▉         | 49/500 [24:52<3:46:57, 30.19s/it] 10%|█         | 50/500 [25:26<3:54:44, 31.30s/it] 10%|█         | 51/500 [25:59<3:57:24, 31.72s/it] 10%|█         | 52/500 [26:27<3:47:54, 30.52s/it] 11%|█         | 53/500 [26:57<3:47:44, 30.57s/it] 11%|█         | 54/500 [27:28<3:46:53, 30.52s/it] 11%|█         | 55/500 [27:58<3:46:30, 30.54s/it] 11%|█         | 56/500 [28:32<3:52:11, 31.38s/it] 11%|█▏        | 57/500 [29:00<3:44:06, 30.35s/it] 12%|█▏        | 58/500 [29:32<3:47:59, 30.95s/it] 12%|█▏        | 59/500 [30:02<3:45:10, 30.64s/it] 12%|█▏        | 60/500 [30:32<3:42:57, 30.40s/it] 12%|█▏        | 61/500 [31:05<3:49:02, 31.30s/it] 12%|█▏        | 62/500 [31:37<3:50:43, 31.61s/it] 13%|█▎        | 63/500 [32:07<3:46:45, 31.13s/it] 13%|█▎        | 64/500 [32:38<3:44:20, 30.87s/it] 13%|█▎        | 65/500 [33:08<3:42:47, 30.73s/it] 13%|█▎        | 66/500 [33:38<3:39:33, 30.35s/it] 13%|█▎        | 67/500 [34:12<3:47:52, 31.58s/it] 14%|█▎        | 68/500 [34:36<3:29:55, 29.16s/it] 14%|█▍        | 69/500 [35:05<3:30:58, 29.37s/it] 14%|█▍        | 70/500 [35:31<3:23:11, 28.35s/it] 14%|█▍        | 71/500 [36:00<3:23:04, 28.40s/it] 14%|█▍        | 72/500 [36:28<3:21:16, 28.22s/it] 15%|█▍        | 73/500 [36:58<3:26:22, 29.00s/it] 15%|█▍        | 74/500 [37:29<3:28:58, 29.43s/it] 15%|█▌        | 75/500 [38:01<3:34:22, 30.27s/it] 15%|█▌        | 76/500 [38:34<3:39:42, 31.09s/it] 15%|█▌        | 77/500 [39:02<3:32:36, 30.16s/it] 15%|█▌        | 77/500 [39:10<3:35:12, 30.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.019 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▅▅▅▅▆▆▅▆▇▇▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇▇▇██▇▇▇██▇
wandb:     train_loss ▄▅▄▄▂▆▂▄▄█▄▃▁▃▃▂▂▁▁▄▄▁▄▄▃▆▂▄▄▂▄▃▁▄▃▃▄▁▃▁
wandb:   val_accuracy ▁▁▄█▆▄▃▃▃█▆▅▇▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       val_loss ▆▇▆▆▅▅▆▆█▅▃▅▅▅▅█▅█▃▅█▁▅▅█▇▅▄▃█▅▄▇▅▅▄██▅▆
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.79495
wandb:     train_loss 0.03844
wandb:   val_accuracy 0.50667
wandb:       val_loss 1.09645
wandb: 
wandb: 🚀 View run gentle-sunset-45 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kagi06xz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_075536-kagi06xz/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_083525-atosthsq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-violet-46
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/atosthsq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:18:34, 38.31s/it]  0%|          | 2/500 [01:01<4:02:50, 29.26s/it]  1%|          | 3/500 [01:37<4:29:40, 32.56s/it]  1%|          | 4/500 [02:00<3:57:42, 28.76s/it]  1%|          | 5/500 [02:31<4:02:25, 29.39s/it]  1%|          | 6/500 [03:01<4:04:54, 29.75s/it]  1%|▏         | 7/500 [03:31<4:05:58, 29.94s/it]  2%|▏         | 8/500 [04:03<4:10:35, 30.56s/it]  2%|▏         | 9/500 [04:43<4:33:06, 33.37s/it]  2%|▏         | 10/500 [05:06<4:07:01, 30.25s/it]  2%|▏         | 11/500 [05:35<4:01:52, 29.68s/it]  2%|▏         | 12/500 [06:05<4:04:03, 30.01s/it]  3%|▎         | 13/500 [06:36<4:06:17, 30.34s/it]  3%|▎         | 14/500 [07:05<4:01:35, 29.83s/it]  3%|▎         | 15/500 [07:35<4:02:31, 30.00s/it]  3%|▎         | 16/500 [08:05<3:59:49, 29.73s/it]  3%|▎         | 17/500 [08:35<4:01:36, 30.01s/it]  4%|▎         | 18/500 [09:09<4:09:29, 31.06s/it]  4%|▍         | 19/500 [09:39<4:06:09, 30.71s/it]  4%|▍         | 20/500 [10:12<4:12:24, 31.55s/it]  4%|▍         | 21/500 [10:46<4:17:34, 32.26s/it]  4%|▍         | 22/500 [11:14<4:07:32, 31.07s/it]  5%|▍         | 23/500 [11:48<4:12:24, 31.75s/it]  5%|▍         | 24/500 [12:15<4:02:06, 30.52s/it]  5%|▌         | 25/500 [12:46<4:01:03, 30.45s/it]  5%|▌         | 26/500 [13:17<4:03:16, 30.79s/it]  5%|▌         | 27/500 [13:49<4:04:33, 31.02s/it]  6%|▌         | 28/500 [14:17<3:56:51, 30.11s/it]  6%|▌         | 29/500 [14:54<4:12:10, 32.12s/it]  6%|▌         | 30/500 [15:17<3:52:01, 29.62s/it]  6%|▌         | 31/500 [15:51<4:00:40, 30.79s/it]  6%|▋         | 32/500 [16:23<4:02:29, 31.09s/it]  7%|▋         | 33/500 [16:52<3:58:59, 30.71s/it]  7%|▋         | 34/500 [17:23<3:58:13, 30.67s/it]  7%|▋         | 35/500 [17:55<4:01:26, 31.15s/it]  7%|▋         | 36/500 [18:25<3:57:14, 30.68s/it]  7%|▋         | 37/500 [18:55<3:54:39, 30.41s/it]  8%|▊         | 38/500 [19:24<3:51:50, 30.11s/it]  8%|▊         | 39/500 [19:57<3:57:50, 30.96s/it]  8%|▊         | 40/500 [20:25<3:51:00, 30.13s/it]  8%|▊         | 41/500 [20:57<3:54:11, 30.61s/it]  8%|▊         | 42/500 [21:30<3:58:23, 31.23s/it]  9%|▊         | 43/500 [21:58<3:52:12, 30.49s/it]  9%|▉         | 44/500 [22:29<3:52:38, 30.61s/it]  9%|▉         | 45/500 [23:03<3:59:48, 31.62s/it]  9%|▉         | 45/500 [23:10<3:54:21, 30.90s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▅▆▆▆▇▆▆▇▇▇██▇█▇████
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▇▆▅▅▅▆▅▅▅▆▄▄▆▃▄▄▄▅▅▆▄▅▃▆▅▆▅▅▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▃▂▃▄▄▃▅▆▄▄▄▃▄▆▄▄▃▄▇▃▄▅▇██▆
wandb:       val_loss ▃█▂▇▂▂▂▆▅▃▁▆▇▄▃▅▃▄▃▂▄▃▅▃▃▅▄▅▄▄▃▄▃▂▃▆▂▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.54681
wandb:     train_loss 1.08109
wandb:   val_accuracy 0.36889
wandb:       val_loss 1.07901
wandb: 
wandb: 🚀 View run pleasant-violet-46 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/atosthsq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_083525-atosthsq/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_085915-9f3l58o2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-darkness-47
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9f3l58o2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:31, 28.44s/it]  0%|          | 2/500 [00:44<2:54:06, 20.98s/it]  1%|          | 3/500 [01:03<2:47:16, 20.19s/it]  1%|          | 4/500 [01:22<2:43:49, 19.82s/it]  1%|          | 5/500 [01:37<2:27:53, 17.93s/it]  1%|          | 6/500 [01:56<2:30:21, 18.26s/it]  1%|▏         | 7/500 [02:15<2:31:47, 18.47s/it]  2%|▏         | 8/500 [02:29<2:20:04, 17.08s/it]  2%|▏         | 9/500 [02:43<2:13:19, 16.29s/it]  2%|▏         | 10/500 [03:02<2:19:59, 17.14s/it]  2%|▏         | 11/500 [03:21<2:22:56, 17.54s/it]  2%|▏         | 12/500 [03:40<2:25:52, 17.94s/it]  3%|▎         | 13/500 [03:58<2:27:10, 18.13s/it]  3%|▎         | 14/500 [04:13<2:17:53, 17.02s/it]  3%|▎         | 15/500 [04:31<2:21:39, 17.52s/it]  3%|▎         | 16/500 [04:46<2:13:19, 16.53s/it]  3%|▎         | 17/500 [05:00<2:07:44, 15.87s/it]  4%|▎         | 18/500 [05:19<2:14:59, 16.80s/it]  4%|▍         | 19/500 [05:38<2:19:25, 17.39s/it]  4%|▍         | 20/500 [05:56<2:21:49, 17.73s/it]  4%|▍         | 21/500 [06:15<2:24:07, 18.05s/it]  4%|▍         | 22/500 [06:34<2:25:31, 18.27s/it]  5%|▍         | 23/500 [06:52<2:26:21, 18.41s/it]  5%|▍         | 24/500 [07:07<2:16:10, 17.16s/it]  5%|▌         | 25/500 [07:25<2:18:59, 17.56s/it]  5%|▌         | 26/500 [07:40<2:11:28, 16.64s/it]  5%|▌         | 27/500 [07:54<2:05:29, 15.92s/it]  6%|▌         | 28/500 [08:08<2:01:14, 15.41s/it]  6%|▌         | 29/500 [08:23<1:58:43, 15.12s/it]  6%|▌         | 30/500 [08:37<1:57:00, 14.94s/it]  6%|▌         | 31/500 [08:52<1:55:56, 14.83s/it]  6%|▋         | 32/500 [09:06<1:54:48, 14.72s/it]  7%|▋         | 33/500 [09:21<1:53:50, 14.63s/it]  7%|▋         | 34/500 [09:35<1:52:54, 14.54s/it]  7%|▋         | 35/500 [09:49<1:52:28, 14.51s/it]  7%|▋         | 36/500 [10:03<1:51:16, 14.39s/it]  7%|▋         | 37/500 [10:18<1:50:38, 14.34s/it]  8%|▊         | 38/500 [10:32<1:50:27, 14.34s/it]  8%|▊         | 39/500 [10:47<1:51:03, 14.45s/it]  8%|▊         | 40/500 [11:01<1:50:59, 14.48s/it]  8%|▊         | 41/500 [11:20<2:01:03, 15.83s/it]  8%|▊         | 42/500 [11:39<2:07:17, 16.67s/it]  9%|▊         | 43/500 [11:58<2:11:43, 17.29s/it]  9%|▉         | 44/500 [12:17<2:15:14, 17.80s/it]  9%|▉         | 45/500 [12:35<2:17:05, 18.08s/it]  9%|▉         | 46/500 [12:54<2:18:35, 18.32s/it]  9%|▉         | 47/500 [13:13<2:19:33, 18.49s/it] 10%|▉         | 48/500 [13:27<2:09:11, 17.15s/it] 10%|▉         | 49/500 [13:41<2:02:39, 16.32s/it] 10%|█         | 50/500 [14:01<2:08:39, 17.16s/it] 10%|█         | 51/500 [14:15<2:02:17, 16.34s/it] 10%|█         | 52/500 [14:34<2:08:18, 17.18s/it] 11%|█         | 53/500 [14:53<2:11:56, 17.71s/it] 11%|█         | 54/500 [15:12<2:14:06, 18.04s/it] 11%|█         | 55/500 [15:31<2:16:07, 18.35s/it] 11%|█         | 56/500 [15:50<2:16:24, 18.43s/it] 11%|█▏        | 57/500 [16:08<2:16:26, 18.48s/it] 12%|█▏        | 58/500 [16:27<2:16:38, 18.55s/it] 12%|█▏        | 59/500 [16:41<2:06:36, 17.23s/it] 12%|█▏        | 60/500 [17:00<2:09:00, 17.59s/it] 12%|█▏        | 61/500 [17:18<2:11:05, 17.92s/it] 12%|█▏        | 62/500 [17:37<2:12:25, 18.14s/it] 13%|█▎        | 63/500 [17:51<2:03:59, 17.02s/it] 13%|█▎        | 64/500 [18:05<1:57:13, 16.13s/it] 13%|█▎        | 65/500 [18:24<2:02:52, 16.95s/it] 13%|█▎        | 66/500 [18:43<2:06:54, 17.55s/it] 13%|█▎        | 67/500 [18:57<1:59:43, 16.59s/it] 14%|█▎        | 68/500 [19:12<1:54:24, 15.89s/it] 14%|█▍        | 69/500 [19:26<1:50:24, 15.37s/it] 14%|█▍        | 70/500 [19:45<1:57:28, 16.39s/it] 14%|█▍        | 71/500 [20:01<1:57:41, 16.46s/it] 14%|█▍        | 72/500 [20:15<1:52:22, 15.75s/it] 15%|█▍        | 73/500 [20:34<1:58:15, 16.62s/it] 15%|█▍        | 74/500 [20:53<2:02:20, 17.23s/it] 15%|█▍        | 74/500 [20:57<2:00:39, 16.99s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▁▄▅▄▆▆▆█▇████████████████████████████
wandb:     train_loss ▂▂▅▁▁█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁█▄▁▅▂▂▂▂▂▅▃▄▄▅▅▆▅▅▅▆▇▆▅▅▅▅▅▅▅▆▇▄▅▅▅▅▅▅▅
wandb:       val_loss ▂▂▁▃▂▁▂▅▃█▄█▁▁▆▄▇▃▅▁▁▁▃▂▆▄▄▆▃▄▁▅▄▆▄▄▁▃▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 73
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 1e-05
wandb:   val_accuracy 0.50667
wandb:       val_loss 0.25965
wandb: 
wandb: 🚀 View run drawn-darkness-47 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9f3l58o2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_085915-9f3l58o2/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_092053-wx3qqtzf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-galaxy-48
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wx3qqtzf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:28, 28.43s/it]  0%|          | 2/500 [00:44<2:55:45, 21.18s/it]  1%|          | 3/500 [01:09<3:08:40, 22.78s/it]  1%|          | 4/500 [01:29<2:59:13, 21.68s/it]  1%|          | 5/500 [01:44<2:39:31, 19.34s/it]  1%|          | 6/500 [01:59<2:26:48, 17.83s/it]  1%|▏         | 7/500 [02:18<2:31:26, 18.43s/it]  2%|▏         | 8/500 [02:38<2:33:49, 18.76s/it]  2%|▏         | 9/500 [03:02<2:47:01, 20.41s/it]  2%|▏         | 10/500 [03:17<2:33:02, 18.74s/it]  2%|▏         | 11/500 [03:37<2:34:46, 18.99s/it]  2%|▏         | 12/500 [03:56<2:35:31, 19.12s/it]  3%|▎         | 13/500 [04:11<2:24:53, 17.85s/it]  3%|▎         | 14/500 [04:30<2:28:47, 18.37s/it]  3%|▎         | 15/500 [04:50<2:31:35, 18.75s/it]  3%|▎         | 16/500 [05:10<2:32:54, 18.95s/it]  3%|▎         | 17/500 [05:29<2:34:06, 19.14s/it]  4%|▎         | 18/500 [05:45<2:24:53, 18.04s/it]  4%|▍         | 19/500 [06:10<2:41:43, 20.17s/it]  4%|▍         | 20/500 [06:25<2:30:46, 18.85s/it]  4%|▍         | 21/500 [06:41<2:23:18, 17.95s/it]  4%|▍         | 22/500 [06:57<2:18:03, 17.33s/it]  5%|▍         | 23/500 [07:13<2:14:11, 16.88s/it]  5%|▍         | 24/500 [07:29<2:10:56, 16.50s/it]  5%|▌         | 25/500 [07:49<2:20:11, 17.71s/it]  5%|▌         | 26/500 [08:10<2:27:19, 18.65s/it]  5%|▌         | 27/500 [08:30<2:31:10, 19.18s/it]  6%|▌         | 28/500 [08:45<2:20:56, 17.92s/it]  6%|▌         | 29/500 [09:11<2:38:09, 20.15s/it]  6%|▌         | 30/500 [09:26<2:26:33, 18.71s/it]  6%|▌         | 31/500 [09:41<2:17:34, 17.60s/it]  6%|▋         | 32/500 [09:56<2:11:05, 16.81s/it]  7%|▋         | 33/500 [10:11<2:06:08, 16.21s/it]  7%|▋         | 34/500 [10:26<2:03:18, 15.88s/it]  7%|▋         | 35/500 [10:41<2:00:53, 15.60s/it]  7%|▋         | 36/500 [10:57<2:00:51, 15.63s/it]  7%|▋         | 37/500 [11:12<2:01:02, 15.69s/it]  8%|▊         | 38/500 [11:28<1:59:27, 15.52s/it]  8%|▊         | 39/500 [11:43<1:58:32, 15.43s/it]  8%|▊         | 40/500 [11:58<1:56:44, 15.23s/it]  8%|▊         | 41/500 [12:13<1:56:02, 15.17s/it]  8%|▊         | 42/500 [12:27<1:55:07, 15.08s/it]  9%|▊         | 43/500 [12:47<2:05:49, 16.52s/it]  9%|▉         | 44/500 [13:02<2:02:16, 16.09s/it]  9%|▉         | 45/500 [13:18<1:59:57, 15.82s/it]  9%|▉         | 46/500 [13:37<2:07:40, 16.87s/it]  9%|▉         | 47/500 [13:56<2:13:18, 17.66s/it] 10%|▉         | 48/500 [14:16<2:17:21, 18.23s/it] 10%|▉         | 49/500 [14:36<2:20:37, 18.71s/it] 10%|█         | 50/500 [14:56<2:22:47, 19.04s/it] 10%|█         | 51/500 [15:11<2:13:38, 17.86s/it] 10%|█         | 52/500 [15:31<2:18:14, 18.52s/it] 11%|█         | 53/500 [15:50<2:20:04, 18.80s/it] 11%|█         | 54/500 [16:05<2:11:08, 17.64s/it] 11%|█         | 55/500 [16:20<2:04:49, 16.83s/it] 11%|█         | 56/500 [16:35<2:00:03, 16.22s/it] 11%|█▏        | 57/500 [16:50<1:56:49, 15.82s/it] 12%|█▏        | 58/500 [17:09<2:04:58, 16.96s/it] 12%|█▏        | 59/500 [17:29<2:10:29, 17.75s/it] 12%|█▏        | 60/500 [17:44<2:04:00, 16.91s/it] 12%|█▏        | 61/500 [18:00<2:01:49, 16.65s/it] 12%|█▏        | 62/500 [18:16<2:00:20, 16.49s/it] 13%|█▎        | 63/500 [18:36<2:07:26, 17.50s/it] 13%|█▎        | 64/500 [18:56<2:12:21, 18.21s/it] 13%|█▎        | 65/500 [19:11<2:05:15, 17.28s/it] 13%|█▎        | 66/500 [19:30<2:09:45, 17.94s/it] 13%|█▎        | 67/500 [19:50<2:13:11, 18.46s/it] 14%|█▎        | 68/500 [20:10<2:15:11, 18.78s/it] 14%|█▍        | 69/500 [20:25<2:07:10, 17.70s/it] 14%|█▍        | 70/500 [20:45<2:11:32, 18.36s/it] 14%|█▍        | 71/500 [21:00<2:03:44, 17.31s/it] 14%|█▍        | 72/500 [21:14<1:58:18, 16.59s/it] 15%|█▍        | 73/500 [21:34<2:05:14, 17.60s/it] 15%|█▍        | 74/500 [21:49<1:59:00, 16.76s/it] 15%|█▌        | 75/500 [22:04<1:55:18, 16.28s/it] 15%|█▌        | 76/500 [22:20<1:52:42, 15.95s/it] 15%|█▌        | 77/500 [22:35<1:50:24, 15.66s/it] 16%|█▌        | 78/500 [22:52<1:54:33, 16.29s/it] 16%|█▌        | 79/500 [23:07<1:51:32, 15.90s/it] 16%|█▌        | 80/500 [23:27<1:59:35, 17.08s/it] 16%|█▌        | 81/500 [23:42<1:55:00, 16.47s/it] 16%|█▋        | 82/500 [23:58<1:52:33, 16.16s/it] 17%|█▋        | 83/500 [24:13<1:51:14, 16.01s/it] 17%|█▋        | 84/500 [24:33<1:59:04, 17.17s/it] 17%|█▋        | 85/500 [24:53<2:03:38, 17.88s/it] 17%|█▋        | 86/500 [25:08<1:57:52, 17.08s/it] 17%|█▋        | 87/500 [25:28<2:03:24, 17.93s/it] 18%|█▊        | 88/500 [25:47<2:06:10, 18.37s/it] 18%|█▊        | 89/500 [26:02<1:59:21, 17.42s/it] 18%|█▊        | 90/500 [26:18<1:54:12, 16.71s/it] 18%|█▊        | 91/500 [26:33<1:50:49, 16.26s/it] 18%|█▊        | 92/500 [26:52<1:57:31, 17.28s/it] 19%|█▊        | 93/500 [27:12<2:02:57, 18.13s/it] 19%|█▉        | 94/500 [27:32<2:05:45, 18.58s/it] 19%|█▉        | 95/500 [27:52<2:07:59, 18.96s/it] 19%|█▉        | 96/500 [28:12<2:09:54, 19.29s/it] 19%|█▉        | 97/500 [28:27<2:01:50, 18.14s/it] 20%|█▉        | 98/500 [28:43<1:55:27, 17.23s/it] 20%|█▉        | 99/500 [28:58<1:50:34, 16.55s/it] 20%|██        | 100/500 [29:17<1:56:38, 17.50s/it] 20%|██        | 101/500 [29:37<2:01:10, 18.22s/it] 20%|██        | 102/500 [29:57<2:03:45, 18.66s/it] 21%|██        | 103/500 [30:16<2:05:10, 18.92s/it] 21%|██        | 104/500 [30:32<1:57:32, 17.81s/it] 21%|██        | 105/500 [30:51<2:00:52, 18.36s/it] 21%|██        | 106/500 [31:11<2:03:02, 18.74s/it] 21%|██▏       | 107/500 [31:31<2:05:33, 19.17s/it] 22%|██▏       | 108/500 [31:54<2:12:56, 20.35s/it] 22%|██▏       | 109/500 [32:09<2:02:32, 18.80s/it] 22%|██▏       | 110/500 [32:24<1:54:40, 17.64s/it] 22%|██▏       | 111/500 [32:40<1:49:48, 16.94s/it] 22%|██▏       | 112/500 [32:59<1:55:09, 17.81s/it] 23%|██▎       | 113/500 [33:14<1:49:24, 16.96s/it] 23%|██▎       | 114/500 [33:29<1:45:09, 16.35s/it] 23%|██▎       | 115/500 [33:46<1:45:33, 16.45s/it] 23%|██▎       | 116/500 [34:05<1:51:05, 17.36s/it] 23%|██▎       | 117/500 [34:25<1:55:22, 18.08s/it] 24%|██▎       | 118/500 [34:45<1:58:43, 18.65s/it] 24%|██▍       | 119/500 [35:00<1:51:56, 17.63s/it] 24%|██▍       | 120/500 [35:20<1:55:37, 18.26s/it] 24%|██▍       | 121/500 [35:36<1:50:00, 17.42s/it] 24%|██▍       | 122/500 [35:51<1:45:17, 16.71s/it] 25%|██▍       | 123/500 [36:06<1:41:28, 16.15s/it] 25%|██▍       | 124/500 [36:21<1:39:54, 15.94s/it] 25%|██▌       | 125/500 [36:37<1:39:05, 15.86s/it] 25%|██▌       | 126/500 [36:52<1:38:08, 15.74s/it] 25%|██▌       | 127/500 [37:07<1:36:58, 15.60s/it] 26%|██▌       | 128/500 [37:23<1:35:55, 15.47s/it] 26%|██▌       | 129/500 [37:42<1:43:34, 16.75s/it] 26%|██▌       | 130/500 [38:02<1:49:02, 17.68s/it] 26%|██▌       | 131/500 [38:22<1:52:31, 18.30s/it] 26%|██▋       | 132/500 [38:37<1:46:43, 17.40s/it] 27%|██▋       | 133/500 [38:57<1:50:16, 18.03s/it] 27%|██▋       | 134/500 [39:11<1:43:58, 17.04s/it] 27%|██▋       | 135/500 [39:26<1:39:29, 16.36s/it] 27%|██▋       | 136/500 [39:41<1:36:14, 15.86s/it] 27%|██▋       | 137/500 [39:56<1:34:56, 15.69s/it] 28%|██▊       | 138/500 [40:11<1:33:37, 15.52s/it] 28%|██▊       | 139/500 [40:31<1:40:46, 16.75s/it] 28%|██▊       | 140/500 [40:50<1:45:26, 17.57s/it] 28%|██▊       | 141/500 [41:10<1:48:58, 18.21s/it] 28%|██▊       | 142/500 [41:30<1:51:18, 18.65s/it] 29%|██▊       | 143/500 [41:45<1:44:31, 17.57s/it] 29%|██▉       | 144/500 [42:00<1:39:39, 16.80s/it] 29%|██▉       | 145/500 [42:15<1:36:03, 16.24s/it] 29%|██▉       | 146/500 [42:30<1:33:24, 15.83s/it] 29%|██▉       | 147/500 [42:45<1:32:06, 15.66s/it] 30%|██▉       | 148/500 [43:00<1:31:31, 15.60s/it] 30%|██▉       | 149/500 [43:21<1:39:11, 16.96s/it] 30%|███       | 150/500 [43:35<1:35:15, 16.33s/it] 30%|███       | 151/500 [43:51<1:33:17, 16.04s/it] 30%|███       | 152/500 [44:06<1:31:20, 15.75s/it] 31%|███       | 153/500 [44:25<1:37:01, 16.78s/it] 31%|███       | 154/500 [44:39<1:32:46, 16.09s/it] 31%|███       | 155/500 [44:59<1:38:57, 17.21s/it] 31%|███       | 156/500 [45:15<1:35:34, 16.67s/it] 31%|███       | 156/500 [45:20<1:39:58, 17.44s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.322 MB uploadedwandb: \ 0.010 MB of 0.322 MB uploadedwandb: | 0.322 MB of 0.322 MB uploadedwandb: / 0.322 MB of 0.322 MB uploadedwandb: - 0.322 MB of 0.322 MB uploadedwandb: \ 0.322 MB of 0.322 MB uploadedwandb: | 0.322 MB of 0.322 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███▄▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆▆▆▇▇▇▇▇▇▇▇█▇███▇▇███▇███████████▇████
wandb:     train_loss ▇█▇▆▅▃▆▃▃▃▇▃▆▁▃▇▇▅▃▄▂▇▄▃▃▃▃▅▄▂▃▅▅▁▃▄▄▃▆▄
wandb:   val_accuracy ▁█▅▃▃▄▅▅▅▅▅▅▅▆▄▇▆▆▄▅▅▅▅▅▅▆▆▆▆▅▅▅▆▆▆▅▆▅▆▆
wandb:       val_loss ▅▄▄▃▅▅▃▄▆▃▄▂▄▄▄▂▆▂▅▃▅▅▃▆▁▄▃▃▅▅▁▃▄▆█▇▅▅▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 155
wandb:  learning_rate 0.0
wandb: train_accuracy 0.82467
wandb:     train_loss 0.64681
wandb:   val_accuracy 0.52222
wandb:       val_loss 0.80164
wandb: 
wandb: 🚀 View run neat-galaxy-48 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wx3qqtzf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_092053-wx3qqtzf/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_100653-l48cyrta
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-aardvark-49
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/l48cyrta
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:55:59, 28.37s/it]  0%|          | 2/500 [00:43<2:51:11, 20.63s/it]  1%|          | 3/500 [01:07<3:04:24, 22.26s/it]  1%|          | 4/500 [01:22<2:40:48, 19.45s/it]  1%|          | 5/500 [01:42<2:41:03, 19.52s/it]  1%|          | 6/500 [02:01<2:40:06, 19.45s/it]  1%|▏         | 7/500 [02:21<2:39:36, 19.43s/it]  2%|▏         | 8/500 [02:40<2:39:26, 19.44s/it]  2%|▏         | 9/500 [03:00<2:39:26, 19.48s/it]  2%|▏         | 10/500 [03:15<2:27:17, 18.04s/it]  2%|▏         | 11/500 [03:34<2:30:07, 18.42s/it]  2%|▏         | 12/500 [03:49<2:21:29, 17.40s/it]  3%|▎         | 13/500 [04:09<2:26:41, 18.07s/it]  3%|▎         | 14/500 [04:28<2:29:57, 18.51s/it]  3%|▎         | 15/500 [04:48<2:31:54, 18.79s/it]  3%|▎         | 16/500 [05:07<2:33:42, 19.05s/it]  3%|▎         | 17/500 [05:27<2:35:09, 19.27s/it]  4%|▎         | 18/500 [05:46<2:35:13, 19.32s/it]  4%|▍         | 19/500 [06:06<2:35:44, 19.43s/it]  4%|▍         | 20/500 [06:26<2:35:59, 19.50s/it]  4%|▍         | 21/500 [06:45<2:36:01, 19.54s/it]  4%|▍         | 22/500 [07:01<2:25:08, 18.22s/it]  5%|▍         | 23/500 [07:15<2:16:36, 17.18s/it]  5%|▍         | 24/500 [07:30<2:11:09, 16.53s/it]  5%|▌         | 25/500 [07:50<2:18:01, 17.43s/it]  5%|▌         | 26/500 [08:10<2:23:14, 18.13s/it]  5%|▌         | 27/500 [08:34<2:37:09, 19.94s/it]  6%|▌         | 28/500 [08:49<2:25:21, 18.48s/it]  6%|▌         | 29/500 [09:09<2:28:07, 18.87s/it]  6%|▌         | 30/500 [09:33<2:40:49, 20.53s/it]  6%|▌         | 31/500 [09:48<2:28:27, 18.99s/it]  6%|▋         | 32/500 [10:08<2:30:03, 19.24s/it]  7%|▋         | 33/500 [10:28<2:30:09, 19.29s/it]  7%|▋         | 34/500 [10:47<2:30:31, 19.38s/it]  7%|▋         | 35/500 [11:07<2:30:26, 19.41s/it]  7%|▋         | 36/500 [11:26<2:30:32, 19.47s/it]  7%|▋         | 37/500 [11:41<2:19:48, 18.12s/it]  8%|▊         | 38/500 [12:01<2:22:53, 18.56s/it]  8%|▊         | 39/500 [12:21<2:25:34, 18.95s/it]  8%|▊         | 40/500 [12:40<2:26:19, 19.09s/it]  8%|▊         | 41/500 [13:00<2:27:16, 19.25s/it]  8%|▊         | 42/500 [13:15<2:17:32, 18.02s/it]  9%|▊         | 43/500 [13:35<2:20:50, 18.49s/it]  9%|▉         | 44/500 [13:54<2:23:28, 18.88s/it]  9%|▉         | 45/500 [14:14<2:25:13, 19.15s/it]  9%|▉         | 46/500 [14:34<2:26:24, 19.35s/it]  9%|▉         | 47/500 [14:54<2:26:57, 19.47s/it] 10%|▉         | 48/500 [15:13<2:27:22, 19.56s/it] 10%|▉         | 49/500 [15:33<2:27:07, 19.57s/it] 10%|█         | 50/500 [15:53<2:26:46, 19.57s/it] 10%|█         | 51/500 [16:12<2:26:43, 19.61s/it] 10%|█         | 52/500 [16:32<2:26:49, 19.67s/it] 11%|█         | 53/500 [16:52<2:26:59, 19.73s/it] 11%|█         | 54/500 [17:12<2:27:39, 19.86s/it] 11%|█         | 55/500 [17:27<2:16:37, 18.42s/it] 11%|█         | 56/500 [17:42<2:08:45, 17.40s/it] 11%|█▏        | 57/500 [18:02<2:13:01, 18.02s/it] 12%|█▏        | 58/500 [18:21<2:16:18, 18.50s/it] 12%|█▏        | 59/500 [18:36<2:08:08, 17.43s/it] 12%|█▏        | 60/500 [18:51<2:02:52, 16.76s/it] 12%|█▏        | 61/500 [19:06<1:58:52, 16.25s/it] 12%|█▏        | 62/500 [19:21<1:55:36, 15.84s/it] 13%|█▎        | 63/500 [19:36<1:53:26, 15.58s/it] 13%|█▎        | 64/500 [19:51<1:51:52, 15.40s/it] 13%|█▎        | 65/500 [20:11<2:01:23, 16.74s/it] 13%|█▎        | 66/500 [20:26<1:56:57, 16.17s/it] 13%|█▎        | 66/500 [20:30<2:14:54, 18.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.019 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▄▄▄▄▅▄▄▅▅▅▅▆▅▇▆██▆▆█▇██▇█
wandb:     train_loss ██▁█▆▂▇▅▆▂▅▅▅▅▅▅▄▅▅▃▄▄▄▅▂▅▄▃▅▅▄▄▅▅▆▅▅▄▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▂▂▁▄▃▅▅▅▅▅▅▅▂█▇▂▆▄▃▄▄▄▅▁▃▅▄▄▃
wandb:       val_loss ▃▃▇▇█▆▃▅▂▄▃▃▂▄▃▃▁▁▅▃▃▃▃▄▄▃▃▅▅▃▃▃▃▄▄▃▂▃▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.51412
wandb:     train_loss 1.1052
wandb:   val_accuracy 0.35333
wandb:       val_loss 1.05558
wandb: 
wandb: 🚀 View run devoted-aardvark-49 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/l48cyrta
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_100653-l48cyrta/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_102802-1ri7wuk6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-dust-50
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1ri7wuk6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:07:16, 29.73s/it]  0%|          | 2/500 [00:50<3:22:27, 24.39s/it]  1%|          | 3/500 [01:10<3:04:29, 22.27s/it]  1%|          | 4/500 [01:29<2:54:52, 21.15s/it]  1%|          | 5/500 [01:44<2:36:08, 18.93s/it]  1%|          | 6/500 [01:59<2:24:34, 17.56s/it]  1%|▏         | 7/500 [02:23<2:41:33, 19.66s/it]  2%|▏         | 8/500 [02:38<2:29:04, 18.18s/it]  2%|▏         | 9/500 [02:58<2:32:27, 18.63s/it]  2%|▏         | 10/500 [03:17<2:33:41, 18.82s/it]  2%|▏         | 11/500 [03:32<2:23:42, 17.63s/it]  2%|▏         | 12/500 [03:51<2:27:53, 18.18s/it]  3%|▎         | 13/500 [04:10<2:30:10, 18.50s/it]  3%|▎         | 14/500 [04:30<2:31:44, 18.73s/it]  3%|▎         | 15/500 [04:49<2:32:41, 18.89s/it]  3%|▎         | 16/500 [05:03<2:21:45, 17.57s/it]  3%|▎         | 17/500 [05:23<2:25:27, 18.07s/it]  4%|▎         | 18/500 [05:42<2:27:54, 18.41s/it]  4%|▍         | 19/500 [06:01<2:29:30, 18.65s/it]  4%|▍         | 20/500 [06:20<2:30:22, 18.80s/it]  4%|▍         | 21/500 [06:40<2:32:00, 19.04s/it]  4%|▍         | 22/500 [06:55<2:21:12, 17.73s/it]  5%|▍         | 23/500 [07:09<2:13:37, 16.81s/it]  5%|▍         | 24/500 [07:29<2:19:56, 17.64s/it]  5%|▌         | 25/500 [07:49<2:24:40, 18.28s/it]  5%|▌         | 26/500 [08:13<2:38:27, 20.06s/it]  5%|▌         | 27/500 [08:28<2:26:15, 18.55s/it]  6%|▌         | 28/500 [08:47<2:27:22, 18.73s/it]  6%|▌         | 29/500 [09:06<2:28:18, 18.89s/it]  6%|▌         | 30/500 [09:26<2:29:08, 19.04s/it]  6%|▌         | 31/500 [09:40<2:18:14, 17.69s/it]  6%|▋         | 32/500 [09:59<2:21:26, 18.13s/it]  7%|▋         | 33/500 [10:14<2:13:12, 17.11s/it]  7%|▋         | 34/500 [10:33<2:17:49, 17.75s/it]  7%|▋         | 35/500 [10:57<2:32:22, 19.66s/it]  7%|▋         | 36/500 [11:12<2:20:47, 18.21s/it]  7%|▋         | 37/500 [11:32<2:23:26, 18.59s/it]  8%|▊         | 38/500 [11:51<2:24:46, 18.80s/it]  8%|▊         | 39/500 [12:11<2:26:38, 19.09s/it]  8%|▊         | 40/500 [12:30<2:27:07, 19.19s/it]  8%|▊         | 41/500 [12:45<2:16:35, 17.86s/it]  8%|▊         | 42/500 [13:00<2:09:09, 16.92s/it]  9%|▊         | 43/500 [13:14<2:03:33, 16.22s/it]  9%|▉         | 44/500 [13:34<2:10:48, 17.21s/it]  9%|▉         | 45/500 [13:53<2:14:37, 17.75s/it]  9%|▉         | 46/500 [14:07<2:06:51, 16.77s/it]  9%|▉         | 47/500 [14:22<2:01:28, 16.09s/it] 10%|▉         | 48/500 [14:41<2:07:59, 16.99s/it] 10%|▉         | 49/500 [15:00<2:13:05, 17.71s/it] 10%|█         | 50/500 [15:15<2:06:19, 16.84s/it] 10%|█         | 51/500 [15:30<2:01:02, 16.18s/it] 10%|█         | 52/500 [15:44<1:57:14, 15.70s/it] 11%|█         | 53/500 [16:04<2:05:35, 16.86s/it] 11%|█         | 54/500 [16:18<2:00:18, 16.18s/it] 11%|█         | 55/500 [16:38<2:06:53, 17.11s/it] 11%|█         | 56/500 [16:57<2:11:18, 17.75s/it] 11%|█▏        | 57/500 [17:11<2:03:53, 16.78s/it] 12%|█▏        | 58/500 [17:31<2:09:42, 17.61s/it] 12%|█▏        | 59/500 [17:50<2:12:43, 18.06s/it] 12%|█▏        | 60/500 [18:09<2:14:32, 18.35s/it] 12%|█▏        | 61/500 [18:24<2:05:47, 17.19s/it] 12%|█▏        | 62/500 [18:38<1:59:41, 16.40s/it] 13%|█▎        | 63/500 [18:55<1:59:44, 16.44s/it] 13%|█▎        | 64/500 [19:09<1:55:36, 15.91s/it] 13%|█▎        | 65/500 [19:29<2:03:05, 16.98s/it] 13%|█▎        | 66/500 [19:43<1:57:47, 16.28s/it] 13%|█▎        | 67/500 [20:03<2:04:30, 17.25s/it] 14%|█▎        | 68/500 [20:23<2:09:07, 17.93s/it] 14%|█▍        | 69/500 [20:42<2:11:23, 18.29s/it] 14%|█▍        | 70/500 [20:56<2:02:50, 17.14s/it] 14%|█▍        | 71/500 [21:11<1:56:52, 16.35s/it] 14%|█▍        | 71/500 [21:15<2:08:27, 17.97s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▃▁▂▃▇▆▅▇▆▆▇▇████▇▇████████████████████
wandb:     train_loss ▂▂▃▁▂█▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▂▁▁▂▇▅▅▇▆▅▆▅█▇▄▇▄▄▅▅▆▅▅▅▅▅▄▅▅▆▄▅▆▄▄▅▅▅
wandb:       val_loss ▂▂▂▄█▂▃▁▂▅▁▂▁▆▁▇▅▄▁▁▃▃▁▄▄▇▄▄▇▅▄▁▆▄█▂▃▄▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 70
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.48222
wandb:       val_loss 2.93178
wandb: 
wandb: 🚀 View run clear-dust-50 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1ri7wuk6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_102802-1ri7wuk6/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_104957-36pe2uh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-dawn-51
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/36pe2uh8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:06:03, 29.59s/it]  0%|          | 2/500 [00:43<2:51:17, 20.64s/it]  1%|          | 3/500 [00:58<2:27:02, 17.75s/it]  1%|          | 4/500 [01:12<2:16:05, 16.46s/it]  1%|          | 5/500 [01:27<2:10:02, 15.76s/it]  1%|          | 6/500 [01:41<2:05:31, 15.24s/it]  1%|▏         | 7/500 [02:00<2:15:28, 16.49s/it]  2%|▏         | 8/500 [02:19<2:21:11, 17.22s/it]  2%|▏         | 9/500 [02:33<2:12:30, 16.19s/it]  2%|▏         | 10/500 [02:52<2:18:54, 17.01s/it]  2%|▏         | 11/500 [03:10<2:22:26, 17.48s/it]  2%|▏         | 12/500 [03:33<2:35:39, 19.14s/it]  3%|▎         | 13/500 [03:47<2:22:39, 17.58s/it]  3%|▎         | 14/500 [04:06<2:24:40, 17.86s/it]  3%|▎         | 15/500 [04:24<2:25:55, 18.05s/it]  3%|▎         | 16/500 [04:43<2:27:05, 18.23s/it]  3%|▎         | 17/500 [05:01<2:26:52, 18.25s/it]  4%|▎         | 18/500 [05:15<2:16:04, 16.94s/it]  4%|▍         | 19/500 [05:38<2:30:25, 18.76s/it]  4%|▍         | 20/500 [05:52<2:18:50, 17.35s/it]  4%|▍         | 21/500 [06:06<2:10:43, 16.38s/it]  4%|▍         | 22/500 [06:20<2:05:23, 15.74s/it]  5%|▍         | 23/500 [06:39<2:11:54, 16.59s/it]  5%|▍         | 24/500 [06:57<2:16:15, 17.18s/it]  5%|▌         | 25/500 [07:12<2:08:46, 16.27s/it]  5%|▌         | 26/500 [07:30<2:14:17, 17.00s/it]  5%|▌         | 27/500 [07:49<2:18:02, 17.51s/it]  6%|▌         | 28/500 [08:03<2:10:06, 16.54s/it]  6%|▌         | 29/500 [08:18<2:05:11, 15.95s/it]  6%|▌         | 30/500 [08:32<2:00:13, 15.35s/it]  6%|▌         | 31/500 [08:50<2:07:13, 16.28s/it]  6%|▋         | 32/500 [09:09<2:12:17, 16.96s/it]  7%|▋         | 33/500 [09:27<2:15:44, 17.44s/it]  7%|▋         | 34/500 [09:41<2:07:21, 16.40s/it]  7%|▋         | 35/500 [09:56<2:03:44, 15.97s/it]  7%|▋         | 36/500 [10:15<2:08:47, 16.65s/it]  7%|▋         | 37/500 [10:33<2:12:56, 17.23s/it]  8%|▊         | 38/500 [10:51<2:14:59, 17.53s/it]  8%|▊         | 39/500 [11:10<2:16:47, 17.80s/it]  8%|▊         | 40/500 [11:28<2:18:02, 18.01s/it]  8%|▊         | 41/500 [11:43<2:09:10, 16.89s/it]  8%|▊         | 42/500 [12:01<2:12:48, 17.40s/it]  9%|▊         | 43/500 [12:20<2:15:07, 17.74s/it]  9%|▉         | 44/500 [12:38<2:17:06, 18.04s/it]  9%|▉         | 45/500 [12:52<2:07:45, 16.85s/it]  9%|▉         | 46/500 [13:07<2:01:08, 16.01s/it]  9%|▉         | 47/500 [13:24<2:04:48, 16.53s/it] 10%|▉         | 48/500 [13:39<2:00:02, 15.93s/it] 10%|▉         | 49/500 [13:53<1:55:40, 15.39s/it] 10%|█         | 50/500 [14:12<2:03:43, 16.50s/it] 10%|█         | 51/500 [14:26<1:58:46, 15.87s/it] 10%|█         | 52/500 [14:41<1:55:12, 15.43s/it] 11%|█         | 53/500 [14:55<1:52:36, 15.12s/it] 11%|█         | 54/500 [15:11<1:52:57, 15.20s/it] 11%|█         | 55/500 [15:25<1:50:23, 14.88s/it] 11%|█         | 56/500 [15:43<1:58:11, 15.97s/it] 11%|█▏        | 57/500 [15:58<1:54:07, 15.46s/it] 12%|█▏        | 58/500 [16:16<2:00:21, 16.34s/it] 12%|█▏        | 59/500 [16:35<2:05:15, 17.04s/it] 12%|█▏        | 60/500 [16:53<2:08:37, 17.54s/it] 12%|█▏        | 61/500 [17:08<2:01:22, 16.59s/it] 12%|█▏        | 62/500 [17:22<1:56:00, 15.89s/it] 13%|█▎        | 63/500 [17:37<1:54:18, 15.69s/it] 13%|█▎        | 64/500 [17:56<2:01:23, 16.71s/it] 13%|█▎        | 65/500 [18:15<2:06:13, 17.41s/it] 13%|█▎        | 66/500 [18:34<2:08:46, 17.80s/it] 13%|█▎        | 67/500 [18:53<2:10:31, 18.09s/it] 14%|█▎        | 68/500 [19:07<2:01:59, 16.94s/it] 14%|█▍        | 69/500 [19:23<1:58:33, 16.51s/it] 14%|█▍        | 70/500 [19:39<1:59:03, 16.61s/it] 14%|█▍        | 71/500 [19:58<2:03:28, 17.27s/it] 14%|█▍        | 72/500 [20:17<2:06:12, 17.69s/it] 15%|█▍        | 73/500 [20:35<2:07:49, 17.96s/it] 15%|█▍        | 74/500 [20:54<2:09:05, 18.18s/it] 15%|█▌        | 75/500 [21:13<2:09:52, 18.34s/it] 15%|█▌        | 76/500 [21:27<2:01:24, 17.18s/it] 15%|█▌        | 77/500 [21:46<2:04:41, 17.69s/it] 16%|█▌        | 78/500 [22:05<2:06:47, 18.03s/it] 16%|█▌        | 79/500 [22:24<2:07:57, 18.24s/it] 16%|█▌        | 80/500 [22:39<2:00:25, 17.20s/it] 16%|█▌        | 81/500 [22:57<2:03:30, 17.69s/it] 16%|█▋        | 82/500 [23:16<2:05:04, 17.95s/it] 17%|█▋        | 83/500 [23:30<1:56:31, 16.77s/it] 17%|█▋        | 84/500 [23:44<1:50:34, 15.95s/it] 17%|█▋        | 85/500 [24:01<1:51:58, 16.19s/it] 17%|█▋        | 86/500 [24:19<1:56:57, 16.95s/it] 17%|█▋        | 87/500 [24:34<1:51:21, 16.18s/it] 18%|█▊        | 88/500 [24:53<1:57:04, 17.05s/it] 18%|█▊        | 89/500 [25:07<1:51:13, 16.24s/it] 18%|█▊        | 90/500 [25:21<1:46:48, 15.63s/it] 18%|█▊        | 91/500 [25:37<1:46:03, 15.56s/it] 18%|█▊        | 92/500 [25:52<1:44:12, 15.32s/it] 19%|█▊        | 93/500 [26:10<1:50:51, 16.34s/it] 19%|█▉        | 94/500 [26:24<1:46:07, 15.68s/it] 19%|█▉        | 95/500 [26:39<1:43:03, 15.27s/it] 19%|█▉        | 96/500 [26:58<1:50:17, 16.38s/it] 19%|█▉        | 97/500 [27:12<1:45:52, 15.76s/it] 20%|█▉        | 98/500 [27:31<1:51:51, 16.69s/it] 20%|█▉        | 99/500 [27:50<1:55:49, 17.33s/it] 20%|██        | 100/500 [28:09<1:58:21, 17.75s/it] 20%|██        | 101/500 [28:28<2:00:39, 18.14s/it] 20%|██        | 102/500 [28:46<2:01:19, 18.29s/it] 21%|██        | 103/500 [29:01<1:55:03, 17.39s/it] 21%|██        | 104/500 [29:16<1:48:40, 16.47s/it] 21%|██        | 105/500 [29:30<1:44:15, 15.84s/it] 21%|██        | 106/500 [29:45<1:41:29, 15.46s/it] 21%|██▏       | 107/500 [30:04<1:47:45, 16.45s/it] 22%|██▏       | 108/500 [30:18<1:42:54, 15.75s/it] 22%|██▏       | 109/500 [30:36<1:48:20, 16.63s/it] 22%|██▏       | 110/500 [30:50<1:43:14, 15.88s/it] 22%|██▏       | 111/500 [31:05<1:40:12, 15.46s/it] 22%|██▏       | 112/500 [31:20<1:38:18, 15.20s/it] 23%|██▎       | 113/500 [31:34<1:37:26, 15.11s/it] 23%|██▎       | 114/500 [31:49<1:36:14, 14.96s/it] 23%|██▎       | 115/500 [32:03<1:33:57, 14.64s/it] 23%|██▎       | 116/500 [32:17<1:32:11, 14.40s/it] 23%|██▎       | 117/500 [32:33<1:35:35, 14.98s/it] 24%|██▎       | 118/500 [32:47<1:33:38, 14.71s/it] 24%|██▍       | 119/500 [33:05<1:40:12, 15.78s/it] 24%|██▍       | 120/500 [33:20<1:36:50, 15.29s/it] 24%|██▍       | 121/500 [33:34<1:35:21, 15.10s/it] 24%|██▍       | 122/500 [33:53<1:41:24, 16.10s/it] 25%|██▍       | 123/500 [34:07<1:37:01, 15.44s/it] 25%|██▍       | 124/500 [34:21<1:34:16, 15.04s/it] 25%|██▌       | 125/500 [34:39<1:40:12, 16.03s/it] 25%|██▌       | 126/500 [34:53<1:35:59, 15.40s/it] 25%|██▌       | 127/500 [35:07<1:33:28, 15.04s/it] 26%|██▌       | 128/500 [35:21<1:31:46, 14.80s/it] 26%|██▌       | 129/500 [35:40<1:38:35, 15.94s/it] 26%|██▌       | 130/500 [35:59<1:43:14, 16.74s/it] 26%|██▌       | 131/500 [36:17<1:46:15, 17.28s/it] 26%|██▋       | 132/500 [36:36<1:48:18, 17.66s/it] 27%|██▋       | 133/500 [36:54<1:49:42, 17.93s/it] 27%|██▋       | 134/500 [37:13<1:50:27, 18.11s/it] 27%|██▋       | 135/500 [37:31<1:51:11, 18.28s/it] 27%|██▋       | 136/500 [37:50<1:51:29, 18.38s/it] 27%|██▋       | 137/500 [38:04<1:43:07, 17.04s/it] 28%|██▊       | 138/500 [38:18<1:37:12, 16.11s/it] 28%|██▊       | 139/500 [38:32<1:33:06, 15.47s/it] 28%|██▊       | 140/500 [38:51<1:38:58, 16.50s/it] 28%|██▊       | 141/500 [39:09<1:42:30, 17.13s/it] 28%|██▊       | 142/500 [39:28<1:44:46, 17.56s/it] 29%|██▊       | 143/500 [39:47<1:46:18, 17.87s/it] 29%|██▉       | 144/500 [40:05<1:47:03, 18.04s/it] 29%|██▉       | 145/500 [40:24<1:48:04, 18.27s/it] 29%|██▉       | 146/500 [40:43<1:48:35, 18.40s/it] 29%|██▉       | 147/500 [41:01<1:48:54, 18.51s/it] 30%|██▉       | 148/500 [41:20<1:48:51, 18.55s/it] 30%|██▉       | 149/500 [41:39<1:48:42, 18.58s/it] 30%|███       | 150/500 [41:58<1:49:02, 18.69s/it] 30%|███       | 151/500 [42:16<1:48:27, 18.65s/it] 30%|███       | 152/500 [42:35<1:48:02, 18.63s/it] 31%|███       | 153/500 [42:53<1:47:36, 18.61s/it] 31%|███       | 154/500 [43:07<1:39:42, 17.29s/it] 31%|███       | 155/500 [43:21<1:33:39, 16.29s/it] 31%|███       | 156/500 [43:40<1:37:03, 16.93s/it] 31%|███       | 156/500 [43:44<1:36:28, 16.83s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.323 MB uploadedwandb: | 0.019 MB of 0.323 MB uploadedwandb: / 0.323 MB of 0.323 MB uploadedwandb: - 0.323 MB of 0.323 MB uploadedwandb: \ 0.323 MB of 0.323 MB uploadedwandb: | 0.323 MB of 0.323 MB uploadedwandb: / 0.323 MB of 0.323 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███▄▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆▅▆▆▆▆▄▂▁▃▂▁▂▇▅▇█▇█▆▇▇▇▇▇▆▇▆▇▆▇▆▇▆▇▇▇▇
wandb:     train_loss ▃▃▃▃▂▂▂▃▂▃█▂▂▁▁▂▃▃▂▂▁▃▂▃▂▂▂▂▂▂▁▂▂▂▂▃▃▂▂▂
wandb:   val_accuracy ▁▇▄▃▃▃▃▃▂▁▁▂▁▁▂▆█▅▆▇▆▃▄▃▄▄▅▄▄▃▄▄▄▄▄▃▄▄▄▄
wandb:       val_loss ▃▂▃▂▃▃▃▃▄▃█▄▂▆▃▁▃▂▂▂▂▄▂▄▁▃▂▂▅▃▁▂▃▄▅▅▃▃▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 155
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81575
wandb:     train_loss 0.86025
wandb:   val_accuracy 0.46444
wandb:       val_loss 0.73739
wandb: 
wandb: 🚀 View run devoted-dawn-51 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/36pe2uh8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_104957-36pe2uh8/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_113431-7skmxf6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-dust-52
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7skmxf6h
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:05:20, 29.50s/it]  0%|          | 2/500 [00:44<2:54:09, 20.98s/it]  1%|          | 3/500 [01:03<2:47:31, 20.23s/it]  1%|          | 4/500 [01:19<2:31:24, 18.32s/it]  1%|          | 5/500 [01:34<2:22:17, 17.25s/it]  1%|          | 6/500 [01:54<2:28:53, 18.08s/it]  1%|▏         | 7/500 [02:09<2:20:44, 17.13s/it]  2%|▏         | 8/500 [02:29<2:26:52, 17.91s/it]  2%|▏         | 9/500 [02:48<2:31:11, 18.48s/it]  2%|▏         | 10/500 [03:03<2:22:16, 17.42s/it]  2%|▏         | 11/500 [03:23<2:27:34, 18.11s/it]  2%|▏         | 12/500 [03:38<2:19:35, 17.16s/it]  3%|▎         | 13/500 [03:53<2:14:01, 16.51s/it]  3%|▎         | 14/500 [04:12<2:20:21, 17.33s/it]  3%|▎         | 15/500 [04:31<2:24:42, 17.90s/it]  3%|▎         | 16/500 [04:51<2:27:37, 18.30s/it]  3%|▎         | 17/500 [05:06<2:18:58, 17.26s/it]  4%|▎         | 18/500 [05:25<2:23:33, 17.87s/it]  4%|▍         | 19/500 [05:44<2:26:58, 18.33s/it]  4%|▍         | 20/500 [06:03<2:28:34, 18.57s/it]  4%|▍         | 21/500 [06:23<2:30:09, 18.81s/it]  4%|▍         | 22/500 [06:43<2:32:35, 19.15s/it]  5%|▍         | 23/500 [07:02<2:33:00, 19.25s/it]  5%|▍         | 24/500 [07:22<2:34:06, 19.42s/it]  5%|▌         | 25/500 [07:42<2:34:21, 19.50s/it]  5%|▌         | 26/500 [08:01<2:33:23, 19.42s/it]  5%|▌         | 27/500 [08:20<2:33:00, 19.41s/it]  6%|▌         | 28/500 [08:35<2:22:19, 18.09s/it]  6%|▌         | 29/500 [08:55<2:25:25, 18.53s/it]  6%|▌         | 30/500 [09:19<2:37:27, 20.10s/it]  6%|▌         | 31/500 [09:33<2:24:33, 18.49s/it]  6%|▋         | 32/500 [09:53<2:25:58, 18.72s/it]  7%|▋         | 33/500 [10:07<2:16:28, 17.54s/it]  7%|▋         | 34/500 [10:22<2:10:01, 16.74s/it]  7%|▋         | 35/500 [10:37<2:05:22, 16.18s/it]  7%|▋         | 36/500 [10:52<2:02:24, 15.83s/it]  7%|▋         | 37/500 [11:12<2:11:14, 17.01s/it]  8%|▊         | 38/500 [11:32<2:17:05, 17.80s/it]  8%|▊         | 39/500 [11:51<2:20:27, 18.28s/it]  8%|▊         | 40/500 [12:10<2:22:46, 18.62s/it]  8%|▊         | 41/500 [12:30<2:24:08, 18.84s/it]  8%|▊         | 42/500 [12:45<2:14:48, 17.66s/it]  9%|▊         | 43/500 [13:04<2:19:05, 18.26s/it]  9%|▉         | 44/500 [13:24<2:21:48, 18.66s/it]  9%|▉         | 45/500 [13:39<2:12:26, 17.46s/it]  9%|▉         | 46/500 [13:53<2:06:16, 16.69s/it]  9%|▉         | 47/500 [14:08<2:01:25, 16.08s/it] 10%|▉         | 48/500 [14:23<1:58:18, 15.71s/it] 10%|▉         | 49/500 [14:38<1:56:06, 15.45s/it] 10%|█         | 50/500 [14:53<1:54:59, 15.33s/it] 10%|█         | 51/500 [15:13<2:04:54, 16.69s/it] 10%|█         | 52/500 [15:32<2:10:33, 17.49s/it] 11%|█         | 53/500 [15:52<2:14:53, 18.11s/it] 11%|█         | 54/500 [16:11<2:17:17, 18.47s/it] 11%|█         | 55/500 [16:26<2:08:52, 17.38s/it] 11%|█         | 56/500 [16:40<2:02:44, 16.59s/it] 11%|█▏        | 57/500 [17:00<2:08:40, 17.43s/it] 12%|█▏        | 58/500 [17:15<2:02:29, 16.63s/it] 12%|█▏        | 59/500 [17:34<2:07:51, 17.40s/it] 12%|█▏        | 60/500 [17:53<2:12:01, 18.00s/it] 12%|█▏        | 61/500 [18:08<2:04:31, 17.02s/it] 12%|█▏        | 62/500 [18:23<1:59:22, 16.35s/it] 13%|█▎        | 63/500 [18:43<2:06:49, 17.41s/it] 13%|█▎        | 64/500 [19:02<2:11:16, 18.06s/it] 13%|█▎        | 65/500 [19:22<2:14:23, 18.54s/it] 13%|█▎        | 66/500 [19:41<2:15:56, 18.79s/it] 13%|█▎        | 66/500 [19:46<2:10:02, 17.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.019 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▄█▂▂▃▂▄▅█▄▃▅▃▆█▅▆▃▃▄▆▄▆▆▆
wandb:     train_loss ██▁█▆▃▇▆▆▂▅▅▅▅▅▅▃▅▅▃▄▄▄▄▃▅▄▃▅▄▄▄▅▅▆▅▅▄▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▇▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss ▂▂▇▇█▄▃▄▁▃▂▂▁▃▃▂▃▁▅▂▂▂▂▃▃▂▂▄▄▂▃▂▂▃▃▂▁▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.51412
wandb:     train_loss 1.1027
wandb:   val_accuracy 0.35111
wandb:       val_loss 1.05248
wandb: 
wandb: 🚀 View run misty-dust-52 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7skmxf6h
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_113431-7skmxf6h/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_115506-wxjpb6p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-meadow-53
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wxjpb6p1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:42, 28.46s/it]  0%|          | 2/500 [00:42<2:48:03, 20.25s/it]  1%|          | 3/500 [01:06<2:59:27, 21.66s/it]  1%|          | 4/500 [01:20<2:34:55, 18.74s/it]  1%|          | 5/500 [01:39<2:33:56, 18.66s/it]  1%|          | 6/500 [01:57<2:34:04, 18.71s/it]  1%|▏         | 7/500 [02:16<2:34:05, 18.75s/it]  2%|▏         | 8/500 [02:40<2:45:42, 20.21s/it]  2%|▏         | 9/500 [02:54<2:30:18, 18.37s/it]  2%|▏         | 10/500 [03:08<2:19:57, 17.14s/it]  2%|▏         | 11/500 [03:23<2:12:30, 16.26s/it]  2%|▏         | 12/500 [03:37<2:07:29, 15.68s/it]  3%|▎         | 13/500 [03:51<2:03:43, 15.24s/it]  3%|▎         | 14/500 [04:05<2:00:34, 14.89s/it]  3%|▎         | 15/500 [04:24<2:09:13, 15.99s/it]  3%|▎         | 16/500 [04:38<2:04:24, 15.42s/it]  3%|▎         | 17/500 [04:56<2:11:42, 16.36s/it]  4%|▎         | 18/500 [05:15<2:16:42, 17.02s/it]  4%|▍         | 19/500 [05:34<2:20:13, 17.49s/it]  4%|▍         | 20/500 [05:52<2:22:22, 17.80s/it]  4%|▍         | 21/500 [06:11<2:25:16, 18.20s/it]  4%|▍         | 22/500 [06:30<2:27:25, 18.50s/it]  5%|▍         | 23/500 [06:49<2:28:21, 18.66s/it]  5%|▍         | 24/500 [07:08<2:28:45, 18.75s/it]  5%|▌         | 25/500 [07:27<2:29:05, 18.83s/it]  5%|▌         | 26/500 [07:46<2:28:21, 18.78s/it]  5%|▌         | 27/500 [08:05<2:27:49, 18.75s/it]  6%|▌         | 28/500 [08:24<2:27:43, 18.78s/it]  6%|▌         | 29/500 [08:38<2:16:37, 17.40s/it]  6%|▌         | 30/500 [08:56<2:19:18, 17.78s/it]  6%|▌         | 31/500 [09:10<2:09:58, 16.63s/it]  6%|▋         | 32/500 [09:29<2:15:00, 17.31s/it]  7%|▋         | 33/500 [09:43<2:07:21, 16.36s/it]  7%|▋         | 34/500 [09:58<2:02:20, 15.75s/it]  7%|▋         | 35/500 [10:12<1:58:40, 15.31s/it]  7%|▋         | 36/500 [10:31<2:06:32, 16.36s/it]  7%|▋         | 37/500 [10:45<2:01:05, 15.69s/it]  8%|▊         | 38/500 [10:59<1:57:29, 15.26s/it]  8%|▊         | 39/500 [11:14<1:55:11, 14.99s/it]  8%|▊         | 40/500 [11:28<1:54:30, 14.94s/it]  8%|▊         | 41/500 [11:48<2:04:37, 16.29s/it]  8%|▊         | 42/500 [12:06<2:09:43, 16.99s/it]  9%|▊         | 43/500 [12:25<2:12:36, 17.41s/it]  9%|▉         | 44/500 [12:39<2:05:09, 16.47s/it]  9%|▉         | 45/500 [12:58<2:11:15, 17.31s/it]  9%|▉         | 46/500 [13:13<2:04:05, 16.40s/it]  9%|▉         | 47/500 [13:31<2:09:00, 17.09s/it] 10%|▉         | 48/500 [13:46<2:02:28, 16.26s/it] 10%|▉         | 49/500 [14:00<1:57:34, 15.64s/it] 10%|█         | 50/500 [14:14<1:54:23, 15.25s/it] 10%|█         | 51/500 [14:29<1:52:29, 15.03s/it] 10%|█         | 52/500 [14:44<1:51:34, 14.94s/it] 11%|█         | 53/500 [14:58<1:50:29, 14.83s/it] 11%|█         | 54/500 [15:12<1:48:52, 14.65s/it] 11%|█         | 55/500 [15:27<1:48:10, 14.59s/it] 11%|█         | 56/500 [15:41<1:47:29, 14.53s/it] 11%|█▏        | 57/500 [15:55<1:46:38, 14.44s/it] 12%|█▏        | 58/500 [16:10<1:47:22, 14.57s/it] 12%|█▏        | 59/500 [16:29<1:57:24, 15.97s/it] 12%|█▏        | 60/500 [16:44<1:53:55, 15.53s/it] 12%|█▏        | 61/500 [16:59<1:52:41, 15.40s/it] 12%|█▏        | 62/500 [17:14<1:51:26, 15.27s/it] 13%|█▎        | 63/500 [17:28<1:48:43, 14.93s/it] 13%|█▎        | 64/500 [17:42<1:46:35, 14.67s/it] 13%|█▎        | 65/500 [17:57<1:45:50, 14.60s/it] 13%|█▎        | 66/500 [18:15<1:54:15, 15.80s/it] 13%|█▎        | 67/500 [18:34<2:00:02, 16.63s/it] 14%|█▎        | 68/500 [18:52<2:03:49, 17.20s/it] 14%|█▍        | 69/500 [19:11<2:06:42, 17.64s/it] 14%|█▍        | 69/500 [19:15<2:00:20, 16.75s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.030 MB uploadedwandb: \ 0.019 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▂▂▂▄▄▅▅▅▃▆▇▆▇▆▅▇▆▇▇▇▅▇▇▆▇▇▇▇▇▆▇▇▇▇███
wandb:     train_loss ▂▂▄▆▁▂▂▅▁▁▁▂▁▁▁▁▇▁▁▁▁▁▃▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▁▁▁▁▂▃▄▄▅▃▆▇▇██▅▆█▇▇█▇█████▇█▇▇▇█▇▇▇▇▇
wandb:       val_loss ▂▂▂▂▃▂▃▄▂▄▃▄▃▁▁▂▂▃▃▃█▁▃▁▁▁▃▁▃▂▃▆▁▃▂▃▂▁▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 68
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.99554
wandb:     train_loss 0.00011
wandb:   val_accuracy 0.54222
wandb:       val_loss 7.68197
wandb: 
wandb: 🚀 View run vague-meadow-53 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wxjpb6p1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_115506-wxjpb6p1/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_121501-g3kyh71k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-deluge-54
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/g3kyh71k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:11:30, 30.24s/it]  0%|          | 2/500 [00:45<2:56:19, 21.24s/it]  1%|          | 3/500 [01:08<3:04:01, 22.22s/it]  1%|          | 4/500 [01:27<2:53:36, 21.00s/it]  1%|          | 5/500 [01:42<2:34:34, 18.74s/it]  1%|          | 6/500 [02:01<2:35:01, 18.83s/it]  1%|▏         | 7/500 [02:20<2:35:16, 18.90s/it]  2%|▏         | 8/500 [02:34<2:23:23, 17.49s/it]  2%|▏         | 9/500 [02:49<2:15:53, 16.61s/it]  2%|▏         | 10/500 [03:08<2:22:01, 17.39s/it]  2%|▏         | 11/500 [03:27<2:25:23, 17.84s/it]  2%|▏         | 12/500 [03:47<2:28:59, 18.32s/it]  3%|▎         | 13/500 [04:01<2:19:17, 17.16s/it]  3%|▎         | 14/500 [04:20<2:23:49, 17.76s/it]  3%|▎         | 15/500 [04:34<2:14:54, 16.69s/it]  3%|▎         | 16/500 [04:49<2:08:36, 15.94s/it]  3%|▎         | 17/500 [05:03<2:05:08, 15.55s/it]  4%|▎         | 18/500 [05:19<2:05:58, 15.68s/it]  4%|▍         | 19/500 [05:38<2:14:12, 16.74s/it]  4%|▍         | 20/500 [05:53<2:08:33, 16.07s/it]  4%|▍         | 21/500 [06:08<2:05:06, 15.67s/it]  4%|▍         | 22/500 [06:22<2:02:32, 15.38s/it]  5%|▍         | 23/500 [06:41<2:10:44, 16.45s/it]  5%|▍         | 24/500 [06:56<2:05:46, 15.85s/it]  5%|▌         | 25/500 [07:15<2:12:39, 16.76s/it]  5%|▌         | 26/500 [07:29<2:06:13, 15.98s/it]  5%|▌         | 27/500 [07:47<2:12:11, 16.77s/it]  6%|▌         | 28/500 [08:02<2:06:33, 16.09s/it]  6%|▌         | 29/500 [08:16<2:02:34, 15.61s/it]  6%|▌         | 30/500 [08:32<2:01:38, 15.53s/it]  6%|▌         | 31/500 [08:51<2:09:05, 16.51s/it]  6%|▋         | 32/500 [09:10<2:14:41, 17.27s/it]  7%|▋         | 33/500 [09:28<2:17:57, 17.73s/it]  7%|▋         | 34/500 [09:47<2:19:56, 18.02s/it]  7%|▋         | 35/500 [10:06<2:21:14, 18.23s/it]  7%|▋         | 36/500 [10:20<2:12:24, 17.12s/it]  7%|▋         | 37/500 [10:35<2:06:35, 16.41s/it]  8%|▊         | 38/500 [10:50<2:02:29, 15.91s/it]  8%|▊         | 39/500 [11:05<1:59:34, 15.56s/it]  8%|▊         | 40/500 [11:21<2:01:30, 15.85s/it]  8%|▊         | 41/500 [11:40<2:07:53, 16.72s/it]  8%|▊         | 42/500 [11:58<2:11:57, 17.29s/it]  9%|▊         | 43/500 [12:17<2:14:56, 17.72s/it]  9%|▉         | 44/500 [12:31<2:06:45, 16.68s/it]  9%|▉         | 45/500 [12:46<2:01:29, 16.02s/it]  9%|▉         | 46/500 [13:02<2:02:03, 16.13s/it]  9%|▉         | 47/500 [13:17<1:59:06, 15.78s/it] 10%|▉         | 48/500 [13:32<1:55:48, 15.37s/it] 10%|▉         | 49/500 [13:51<2:03:50, 16.48s/it] 10%|█         | 50/500 [14:10<2:09:48, 17.31s/it] 10%|█         | 51/500 [14:29<2:13:22, 17.82s/it] 10%|█         | 52/500 [14:48<2:16:02, 18.22s/it] 11%|█         | 53/500 [15:07<2:17:57, 18.52s/it] 11%|█         | 54/500 [15:22<2:08:14, 17.25s/it] 11%|█         | 55/500 [15:41<2:12:02, 17.80s/it] 11%|█         | 56/500 [15:55<2:04:11, 16.78s/it] 11%|█▏        | 57/500 [16:14<2:09:08, 17.49s/it] 12%|█▏        | 58/500 [16:33<2:12:31, 17.99s/it] 12%|█▏        | 59/500 [16:52<2:14:28, 18.30s/it] 12%|█▏        | 60/500 [17:12<2:15:57, 18.54s/it] 12%|█▏        | 61/500 [17:30<2:16:31, 18.66s/it] 12%|█▏        | 62/500 [17:49<2:16:47, 18.74s/it] 13%|█▎        | 63/500 [18:04<2:06:53, 17.42s/it] 13%|█▎        | 64/500 [18:18<2:00:11, 16.54s/it] 13%|█▎        | 65/500 [18:33<1:55:04, 15.87s/it] 13%|█▎        | 66/500 [18:52<2:02:31, 16.94s/it] 13%|█▎        | 67/500 [19:06<1:56:47, 16.18s/it] 14%|█▎        | 68/500 [19:25<2:02:28, 17.01s/it] 14%|█▍        | 69/500 [19:44<2:06:09, 17.56s/it] 14%|█▍        | 70/500 [20:03<2:08:28, 17.93s/it] 14%|█▍        | 71/500 [20:22<2:10:15, 18.22s/it] 14%|█▍        | 72/500 [20:41<2:11:31, 18.44s/it] 15%|█▍        | 73/500 [21:00<2:12:03, 18.56s/it] 15%|█▍        | 73/500 [21:04<2:03:16, 17.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.317 MB uploadedwandb: - 0.019 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▆▆▆▆▆▇▇▇▇▇▇█▇▇▇▇▇██▇▇████████████▇███
wandb:     train_loss █▇█▆▇▆▆▄▅▃▆▆▅▆▅▄▅▄▅▄▄▆▄▆▁▄▆▃▁█▄▂▆▆▅▄▃▂▄▃
wandb:   val_accuracy ▁▁█▆▄▂▃▃▅▄▄▄▄▅▅▄▄▅▄▄▄▅▄▄▅▅▅▄▅▄▄▅▅▅▅▅▄▅▄▄
wandb:       val_loss ▅▅▄▅▅▇▃▆▆▆▃█▄▅▇▅▅▆▆▃▁▅▁▅▆▆▆▄▅▃▄▃▂▅▅▅▄▇██
wandb: 
wandb: Run summary:
wandb:          epoch 72
wandb:  learning_rate 0.0
wandb: train_accuracy 0.78752
wandb:     train_loss 0.51049
wandb:   val_accuracy 0.46889
wandb:       val_loss 1.43864
wandb: 
wandb: 🚀 View run volcanic-deluge-54 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/g3kyh71k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_121501-g3kyh71k/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_123654-a21281rl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-wave-55
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a21281rl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:03:56, 29.33s/it]  0%|          | 2/500 [00:45<2:58:09, 21.46s/it]  1%|          | 3/500 [01:05<2:53:37, 20.96s/it]  1%|          | 4/500 [01:21<2:36:46, 18.97s/it]  1%|          | 5/500 [01:42<2:41:00, 19.52s/it]  1%|          | 6/500 [01:57<2:28:38, 18.05s/it]  1%|▏         | 7/500 [02:16<2:32:50, 18.60s/it]  2%|▏         | 8/500 [02:36<2:35:42, 18.99s/it]  2%|▏         | 9/500 [02:51<2:25:02, 17.72s/it]  2%|▏         | 10/500 [03:06<2:17:21, 16.82s/it]  2%|▏         | 11/500 [03:21<2:12:00, 16.20s/it]  2%|▏         | 12/500 [03:36<2:08:19, 15.78s/it]  3%|▎         | 13/500 [03:50<2:05:30, 15.46s/it]  3%|▎         | 14/500 [04:10<2:15:26, 16.72s/it]  3%|▎         | 15/500 [04:25<2:11:09, 16.23s/it]  3%|▎         | 16/500 [04:44<2:18:23, 17.16s/it]  3%|▎         | 17/500 [05:04<2:24:12, 17.91s/it]  4%|▎         | 18/500 [05:24<2:27:53, 18.41s/it]  4%|▍         | 19/500 [05:39<2:19:19, 17.38s/it]  4%|▍         | 20/500 [05:54<2:13:25, 16.68s/it]  4%|▍         | 21/500 [06:13<2:19:54, 17.52s/it]  4%|▍         | 22/500 [06:28<2:13:26, 16.75s/it]  5%|▍         | 23/500 [06:43<2:09:27, 16.28s/it]  5%|▍         | 24/500 [06:58<2:05:39, 15.84s/it]  5%|▌         | 25/500 [07:13<2:03:36, 15.61s/it]  5%|▌         | 26/500 [07:28<2:02:35, 15.52s/it]  5%|▌         | 27/500 [07:44<2:01:10, 15.37s/it]  6%|▌         | 28/500 [07:58<1:59:39, 15.21s/it]  6%|▌         | 29/500 [08:18<2:09:44, 16.53s/it]  6%|▌         | 30/500 [08:42<2:26:55, 18.76s/it]  6%|▌         | 31/500 [08:57<2:17:26, 17.58s/it]  6%|▋         | 32/500 [09:16<2:21:15, 18.11s/it]  7%|▋         | 33/500 [09:31<2:12:55, 17.08s/it]  7%|▋         | 34/500 [09:50<2:17:48, 17.74s/it]  7%|▋         | 35/500 [10:05<2:10:29, 16.84s/it]  7%|▋         | 36/500 [10:24<2:15:48, 17.56s/it]  7%|▋         | 37/500 [10:43<2:19:34, 18.09s/it]  8%|▊         | 38/500 [11:03<2:22:19, 18.48s/it]  8%|▊         | 39/500 [11:22<2:24:32, 18.81s/it]  8%|▊         | 40/500 [11:37<2:14:34, 17.55s/it]  8%|▊         | 41/500 [11:56<2:18:23, 18.09s/it]  8%|▊         | 42/500 [12:16<2:21:20, 18.52s/it]  9%|▊         | 43/500 [12:31<2:12:30, 17.40s/it]  9%|▉         | 44/500 [12:50<2:16:57, 18.02s/it]  9%|▉         | 45/500 [13:09<2:19:50, 18.44s/it]  9%|▉         | 46/500 [13:29<2:21:54, 18.75s/it]  9%|▉         | 47/500 [13:48<2:23:07, 18.96s/it] 10%|▉         | 48/500 [14:08<2:23:13, 19.01s/it] 10%|▉         | 49/500 [14:27<2:23:19, 19.07s/it] 10%|█         | 50/500 [14:46<2:24:08, 19.22s/it] 10%|█         | 51/500 [15:01<2:13:39, 17.86s/it] 10%|█         | 52/500 [15:16<2:06:28, 16.94s/it] 11%|█         | 53/500 [15:36<2:13:16, 17.89s/it] 11%|█         | 54/500 [15:51<2:06:54, 17.07s/it] 11%|█         | 55/500 [16:06<2:01:43, 16.41s/it] 11%|█         | 56/500 [16:21<1:58:02, 15.95s/it] 11%|█▏        | 57/500 [16:36<1:55:16, 15.61s/it] 12%|█▏        | 58/500 [16:51<1:53:48, 15.45s/it] 12%|█▏        | 59/500 [17:09<2:00:46, 16.43s/it] 12%|█▏        | 60/500 [17:29<2:07:28, 17.38s/it] 12%|█▏        | 61/500 [17:44<2:01:42, 16.63s/it] 12%|█▏        | 62/500 [18:03<2:07:30, 17.47s/it] 13%|█▎        | 63/500 [18:23<2:11:32, 18.06s/it] 13%|█▎        | 64/500 [18:42<2:13:51, 18.42s/it] 13%|█▎        | 65/500 [18:57<2:05:51, 17.36s/it] 13%|█▎        | 66/500 [19:17<2:11:02, 18.12s/it] 13%|█▎        | 66/500 [19:17<2:06:50, 17.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.233 MB of 0.315 MB uploadedwandb: \ 0.233 MB of 0.315 MB uploadedwandb: | 0.233 MB of 0.315 MB uploadedwandb: / 0.233 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▄▄▅▅▅▅▅▆▆▅▆▆▇▆██▆▆█▇▇▇▇▇
wandb:     train_loss ██▁█▆▂▇▅▆▂▆▅▅▅▅▅▅▅▅▃▄▄▄▅▂▅▄▃▅▅▄▄▅▅▆▅▅▄▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▄▆▅▇▇▅▆▇▄▇▇▃█▂▂▅▄▇▆▂▄▄▂▅▁
wandb:       val_loss ▃▃▇▇█▆▃▅▂▄▃▃▂▄▃▃▁▁▅▃▃▃▃▄▄▃▃▅▅▃▃▃▃▄▄▃▂▃▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.43239
wandb:     train_loss 1.10632
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.05698
wandb: 
wandb: 🚀 View run fearless-wave-55 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/a21281rl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_123654-a21281rl/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_125701-o5kn2pxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-thunder-56
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o5kn2pxo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:11:31, 30.24s/it]  0%|          | 2/500 [00:50<3:22:53, 24.45s/it]  1%|          | 3/500 [01:04<2:43:38, 19.76s/it]  1%|          | 4/500 [01:23<2:40:18, 19.39s/it]  1%|          | 5/500 [01:42<2:38:39, 19.23s/it]  1%|          | 6/500 [01:56<2:24:00, 17.49s/it]  1%|▏         | 7/500 [02:11<2:15:32, 16.50s/it]  2%|▏         | 8/500 [02:25<2:10:12, 15.88s/it]  2%|▏         | 9/500 [02:44<2:17:51, 16.85s/it]  2%|▏         | 10/500 [03:03<2:22:32, 17.45s/it]  2%|▏         | 11/500 [03:17<2:14:14, 16.47s/it]  2%|▏         | 12/500 [03:37<2:21:31, 17.40s/it]  3%|▎         | 13/500 [03:51<2:13:43, 16.48s/it]  3%|▎         | 14/500 [04:10<2:18:46, 17.13s/it]  3%|▎         | 15/500 [04:28<2:22:04, 17.58s/it]  3%|▎         | 16/500 [04:47<2:24:28, 17.91s/it]  3%|▎         | 17/500 [05:01<2:14:40, 16.73s/it]  4%|▎         | 18/500 [05:15<2:07:35, 15.88s/it]  4%|▍         | 19/500 [05:29<2:03:14, 15.37s/it]  4%|▍         | 20/500 [05:48<2:10:26, 16.30s/it]  4%|▍         | 21/500 [06:02<2:05:17, 15.69s/it]  4%|▍         | 22/500 [06:16<2:01:29, 15.25s/it]  5%|▍         | 23/500 [06:31<2:00:45, 15.19s/it]  5%|▍         | 24/500 [06:50<2:08:50, 16.24s/it]  5%|▌         | 25/500 [07:04<2:03:12, 15.56s/it]  5%|▌         | 26/500 [07:18<1:59:35, 15.14s/it]  5%|▌         | 27/500 [07:37<2:07:53, 16.22s/it]  6%|▌         | 28/500 [07:51<2:02:24, 15.56s/it]  6%|▌         | 29/500 [08:09<2:09:18, 16.47s/it]  6%|▌         | 30/500 [08:28<2:14:01, 17.11s/it]  6%|▌         | 31/500 [08:42<2:06:55, 16.24s/it]  6%|▋         | 32/500 [08:56<2:02:12, 15.67s/it]  7%|▋         | 33/500 [09:11<1:59:34, 15.36s/it]  7%|▋         | 34/500 [09:30<2:07:01, 16.35s/it]  7%|▋         | 35/500 [09:48<2:11:55, 17.02s/it]  7%|▋         | 36/500 [10:03<2:05:12, 16.19s/it]  7%|▋         | 37/500 [10:22<2:11:25, 17.03s/it]  8%|▊         | 38/500 [10:36<2:04:27, 16.16s/it]  8%|▊         | 39/500 [10:50<1:59:28, 15.55s/it]  8%|▊         | 40/500 [11:05<1:58:16, 15.43s/it]  8%|▊         | 41/500 [11:24<2:05:11, 16.36s/it]  8%|▊         | 42/500 [11:38<2:01:02, 15.86s/it]  9%|▊         | 43/500 [11:57<2:08:31, 16.87s/it]  9%|▉         | 44/500 [12:12<2:02:26, 16.11s/it]  9%|▉         | 45/500 [12:26<1:58:22, 15.61s/it]  9%|▉         | 46/500 [12:45<2:05:28, 16.58s/it]  9%|▉         | 46/500 [12:45<2:05:56, 16.64s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▁▁▂▂▃▂▇▇█▇▅▇▆▇▆▇▇▇█▇██████████▇██████
wandb:     train_loss ▂▂▂▆▃▁▂█▆▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▁▁▁▁▂▂▂▇▅▆▇▃▄▆█▅█▅▆█▆███▇▇▇█▇▇▆▄▆▄▅▆▅▅
wandb:       val_loss ▃▃▅▅█▆▁▄▁▆▁▂▁▂▄▁▁▄▃▁▄▁▁█▁▅▇▂▁▆▃▂▆▄▁▅▄▁▄█
wandb: 
wandb: Run summary:
wandb:          epoch 45
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.99406
wandb:     train_loss 8e-05
wandb:   val_accuracy 0.52667
wandb:       val_loss 1.60166
wandb: 
wandb: 🚀 View run winter-thunder-56 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o5kn2pxo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_125701-o5kn2pxo/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_131028-3w9n3c0z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-fog-57
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3w9n3c0z
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:41, 17.52s/it]  0%|          | 2/500 [00:31<2:09:52, 15.65s/it]  1%|          | 3/500 [00:50<2:21:49, 17.12s/it]  1%|          | 4/500 [01:09<2:26:33, 17.73s/it]  1%|          | 5/500 [01:23<2:15:38, 16.44s/it]  1%|          | 6/500 [01:37<2:09:05, 15.68s/it]  1%|▏         | 7/500 [01:52<2:05:08, 15.23s/it]  2%|▏         | 8/500 [02:10<2:14:22, 16.39s/it]  2%|▏         | 9/500 [02:29<2:19:49, 17.09s/it]  2%|▏         | 10/500 [02:43<2:12:04, 16.17s/it]  2%|▏         | 11/500 [03:02<2:18:21, 16.98s/it]  2%|▏         | 12/500 [03:16<2:10:52, 16.09s/it]  3%|▎         | 13/500 [03:30<2:06:35, 15.60s/it]  3%|▎         | 14/500 [03:49<2:14:20, 16.59s/it]  3%|▎         | 15/500 [04:08<2:19:12, 17.22s/it]  3%|▎         | 16/500 [04:27<2:22:06, 17.62s/it]  3%|▎         | 17/500 [04:45<2:24:33, 17.96s/it]  4%|▎         | 18/500 [05:04<2:26:02, 18.18s/it]  4%|▍         | 19/500 [05:23<2:27:09, 18.36s/it]  4%|▍         | 20/500 [05:41<2:27:06, 18.39s/it]  4%|▍         | 21/500 [05:55<2:16:45, 17.13s/it]  4%|▍         | 22/500 [06:09<2:08:50, 16.17s/it]  5%|▍         | 23/500 [06:28<2:14:15, 16.89s/it]  5%|▍         | 24/500 [06:42<2:06:50, 15.99s/it]  5%|▌         | 25/500 [06:56<2:03:05, 15.55s/it]  5%|▌         | 26/500 [07:15<2:09:33, 16.40s/it]  5%|▌         | 27/500 [07:33<2:13:39, 16.96s/it]  6%|▌         | 28/500 [07:52<2:17:46, 17.51s/it]  6%|▌         | 29/500 [08:10<2:19:22, 17.75s/it]  6%|▌         | 30/500 [08:29<2:21:34, 18.07s/it]  6%|▌         | 31/500 [08:43<2:12:07, 16.90s/it]  6%|▋         | 32/500 [08:57<2:05:17, 16.06s/it]  7%|▋         | 33/500 [09:11<2:00:35, 15.49s/it]  7%|▋         | 34/500 [09:30<2:07:10, 16.38s/it]  7%|▋         | 35/500 [09:48<2:11:39, 16.99s/it]  7%|▋         | 36/500 [10:07<2:15:27, 17.52s/it]  7%|▋         | 37/500 [10:21<2:07:02, 16.46s/it]  8%|▊         | 38/500 [10:40<2:11:31, 17.08s/it]  8%|▊         | 39/500 [10:58<2:14:55, 17.56s/it]  8%|▊         | 40/500 [11:12<2:06:31, 16.50s/it]  8%|▊         | 41/500 [11:31<2:11:21, 17.17s/it]  8%|▊         | 42/500 [11:45<2:03:59, 16.24s/it]  9%|▊         | 43/500 [11:59<1:59:21, 15.67s/it]  9%|▉         | 44/500 [12:15<1:58:21, 15.57s/it]  9%|▉         | 45/500 [12:33<2:04:40, 16.44s/it]  9%|▉         | 46/500 [12:47<1:58:56, 15.72s/it]  9%|▉         | 47/500 [13:05<2:04:25, 16.48s/it] 10%|▉         | 48/500 [13:24<2:08:43, 17.09s/it] 10%|▉         | 49/500 [13:38<2:01:08, 16.12s/it] 10%|█         | 50/500 [13:57<2:06:39, 16.89s/it] 10%|█         | 51/500 [14:15<2:10:46, 17.48s/it] 10%|█         | 52/500 [14:34<2:12:59, 17.81s/it] 11%|█         | 53/500 [14:53<2:15:02, 18.13s/it] 11%|█         | 54/500 [15:11<2:15:39, 18.25s/it] 11%|█         | 55/500 [15:30<2:15:49, 18.31s/it] 11%|█         | 56/500 [15:48<2:15:53, 18.36s/it] 11%|█▏        | 57/500 [16:07<2:16:24, 18.47s/it] 12%|█▏        | 58/500 [16:26<2:16:53, 18.58s/it] 12%|█▏        | 59/500 [16:44<2:16:35, 18.58s/it] 12%|█▏        | 60/500 [16:59<2:06:27, 17.24s/it] 12%|█▏        | 61/500 [17:18<2:10:07, 17.79s/it] 12%|█▏        | 62/500 [17:32<2:02:24, 16.77s/it] 13%|█▎        | 63/500 [17:46<1:57:03, 16.07s/it] 13%|█▎        | 64/500 [18:01<1:53:20, 15.60s/it] 13%|█▎        | 65/500 [18:20<1:59:49, 16.53s/it] 13%|█▎        | 66/500 [18:34<1:54:19, 15.80s/it] 13%|█▎        | 66/500 [18:34<2:02:07, 16.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.317 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.233 MB of 0.317 MB uploadedwandb: / 0.233 MB of 0.317 MB uploadedwandb: - 0.233 MB of 0.317 MB uploadedwandb: \ 0.233 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▄▄▄▅▅▆▇▇▇▇▇▇█████▇▇██▇▇█▇█▇███▇█▇████
wandb:     train_loss ▅▅█▄▄▄▄▄▄▄▂▅▄▄▄▄▂▃▄▃▄▃▄▄▃▅▄▃▅▅▁▁▅▃▅▅▄▃▅▄
wandb:   val_accuracy ▂▂▁▃▃▄▄▃▄▄▆▇▆▇▇██▇▇▇▇█▇▇▇█▇▆█▇▇▇▇▇▇█▇▇█▇
wandb:       val_loss ▃▃▂▄▄▂▅▃▃▅▄▃▇▂▁▃▂▁▃▇▃▃▂█▄▃▃▅▃▃▂▃▄▃▃▅▆▅▅▂
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.71768
wandb:     train_loss 0.91857
wandb:   val_accuracy 0.44444
wandb:       val_loss 0.87065
wandb: 
wandb: 🚀 View run dainty-fog-57 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3w9n3c0z
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_131028-3w9n3c0z/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_132941-zi8cvujb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-night-58
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zi8cvujb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:48:23, 27.46s/it]  0%|          | 2/500 [00:42<2:45:51, 19.98s/it]  1%|          | 3/500 [01:06<3:00:21, 21.77s/it]  1%|          | 4/500 [01:20<2:36:59, 18.99s/it]  1%|          | 5/500 [01:40<2:37:12, 19.06s/it]  1%|          | 6/500 [01:54<2:24:44, 17.58s/it]  1%|▏         | 7/500 [02:14<2:29:01, 18.14s/it]  2%|▏         | 8/500 [02:33<2:32:11, 18.56s/it]  2%|▏         | 9/500 [02:52<2:33:47, 18.79s/it]  2%|▏         | 10/500 [03:07<2:23:07, 17.53s/it]  2%|▏         | 11/500 [03:26<2:26:43, 18.00s/it]  2%|▏         | 12/500 [03:45<2:29:15, 18.35s/it]  3%|▎         | 13/500 [04:04<2:31:07, 18.62s/it]  3%|▎         | 14/500 [04:24<2:33:14, 18.92s/it]  3%|▎         | 15/500 [04:39<2:22:26, 17.62s/it]  3%|▎         | 16/500 [04:53<2:15:12, 16.76s/it]  3%|▎         | 17/500 [05:08<2:09:28, 16.08s/it]  4%|▎         | 18/500 [05:27<2:17:29, 17.11s/it]  4%|▍         | 19/500 [05:46<2:21:48, 17.69s/it]  4%|▍         | 20/500 [06:01<2:13:51, 16.73s/it]  4%|▍         | 21/500 [06:20<2:19:14, 17.44s/it]  4%|▍         | 22/500 [06:35<2:13:03, 16.70s/it]  5%|▍         | 23/500 [06:55<2:20:52, 17.72s/it]  5%|▍         | 24/500 [07:11<2:15:13, 17.05s/it]  5%|▌         | 25/500 [07:30<2:20:34, 17.76s/it]  5%|▌         | 26/500 [07:45<2:13:40, 16.92s/it]  5%|▌         | 27/500 [08:09<2:29:09, 18.92s/it]  6%|▌         | 28/500 [08:23<2:18:36, 17.62s/it]  6%|▌         | 29/500 [08:38<2:11:43, 16.78s/it]  6%|▌         | 30/500 [09:02<2:28:19, 18.94s/it]  6%|▌         | 31/500 [09:21<2:28:27, 18.99s/it]  6%|▋         | 32/500 [09:36<2:18:19, 17.73s/it]  7%|▋         | 33/500 [09:55<2:21:25, 18.17s/it]  7%|▋         | 34/500 [10:10<2:12:35, 17.07s/it]  7%|▋         | 35/500 [10:24<2:06:11, 16.28s/it]  7%|▋         | 36/500 [10:39<2:02:52, 15.89s/it]  7%|▋         | 37/500 [10:58<2:10:58, 16.97s/it]  8%|▊         | 38/500 [11:13<2:05:27, 16.29s/it]  8%|▊         | 39/500 [11:28<2:02:22, 15.93s/it]  8%|▊         | 40/500 [11:44<2:01:31, 15.85s/it]  8%|▊         | 41/500 [12:03<2:08:46, 16.83s/it]  8%|▊         | 42/500 [12:22<2:13:45, 17.52s/it]  9%|▊         | 43/500 [12:42<2:18:02, 18.12s/it]  9%|▉         | 44/500 [13:01<2:20:56, 18.54s/it]  9%|▉         | 45/500 [13:21<2:22:40, 18.81s/it]  9%|▉         | 46/500 [13:35<2:12:53, 17.56s/it]  9%|▉         | 47/500 [13:55<2:16:31, 18.08s/it] 10%|▉         | 48/500 [14:10<2:08:59, 17.12s/it] 10%|▉         | 49/500 [14:24<2:03:12, 16.39s/it] 10%|█         | 50/500 [14:39<1:59:28, 15.93s/it] 10%|█         | 51/500 [14:59<2:07:39, 17.06s/it] 10%|█         | 52/500 [15:18<2:12:26, 17.74s/it] 11%|█         | 53/500 [15:37<2:15:18, 18.16s/it] 11%|█         | 54/500 [15:56<2:17:23, 18.48s/it] 11%|█         | 55/500 [16:16<2:19:26, 18.80s/it] 11%|█         | 56/500 [16:35<2:20:25, 18.98s/it] 11%|█▏        | 57/500 [16:55<2:20:42, 19.06s/it] 12%|█▏        | 58/500 [17:09<2:10:55, 17.77s/it] 12%|█▏        | 59/500 [17:29<2:14:32, 18.30s/it] 12%|█▏        | 60/500 [17:49<2:18:55, 18.95s/it] 12%|█▏        | 61/500 [18:10<2:21:32, 19.35s/it] 12%|█▏        | 62/500 [18:25<2:11:27, 18.01s/it] 13%|█▎        | 63/500 [18:39<2:04:11, 17.05s/it] 13%|█▎        | 64/500 [18:54<1:58:32, 16.31s/it] 13%|█▎        | 65/500 [19:13<2:04:30, 17.17s/it] 13%|█▎        | 66/500 [19:28<1:58:06, 16.33s/it] 13%|█▎        | 66/500 [19:28<2:08:00, 17.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▂█▃▃▃█▃▂▃▂▃▅▃▃▄█▆▆█▄▅▅▇▄
wandb:     train_loss ██▁▇▆▂▇▅▅▂▅▅▅▅▅▄▆▄▅▃▅▄▄▅▃▅▄▃▆▅▄▄▅▄▅▅▅▄▅▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂█▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁
wandb:       val_loss ▃▃█▇█▆▃▄▂▄▃▃▁▃▃▃▁▄▅▂▂▃▃▄▃▃▃▄▅▃▃▃▂▃▄▃▂▃▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 65
wandb:  learning_rate 0.0
wandb: train_accuracy 0.40862
wandb:     train_loss 1.11243
wandb:   val_accuracy 0.35111
wandb:       val_loss 1.052
wandb: 
wandb: 🚀 View run silver-night-58 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zi8cvujb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_132941-zi8cvujb/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_134955-lm0cln3y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-dew-59
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lm0cln3y
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:59:09, 28.76s/it]  0%|          | 2/500 [00:43<2:50:13, 20.51s/it]  1%|          | 3/500 [01:02<2:45:37, 19.99s/it]  1%|          | 4/500 [01:21<2:40:55, 19.47s/it]  1%|          | 5/500 [01:40<2:38:17, 19.19s/it]  1%|          | 6/500 [01:58<2:36:40, 19.03s/it]  1%|▏         | 7/500 [02:22<2:47:44, 20.42s/it]  2%|▏         | 8/500 [02:36<2:31:53, 18.52s/it]  2%|▏         | 9/500 [02:55<2:32:37, 18.65s/it]  2%|▏         | 10/500 [03:14<2:33:01, 18.74s/it]  2%|▏         | 11/500 [03:33<2:33:23, 18.82s/it]  2%|▏         | 12/500 [03:52<2:32:39, 18.77s/it]  3%|▎         | 13/500 [04:06<2:21:02, 17.38s/it]  3%|▎         | 14/500 [04:20<2:12:54, 16.41s/it]  3%|▎         | 15/500 [04:34<2:07:12, 15.74s/it]  3%|▎         | 16/500 [04:53<2:14:04, 16.62s/it]  3%|▎         | 17/500 [05:12<2:18:55, 17.26s/it]  4%|▎         | 18/500 [05:30<2:21:48, 17.65s/it]  4%|▍         | 19/500 [05:44<2:13:02, 16.60s/it]  4%|▍         | 20/500 [05:58<2:06:26, 15.81s/it]  4%|▍         | 21/500 [06:17<2:12:24, 16.59s/it]  4%|▍         | 22/500 [06:36<2:18:00, 17.32s/it]  5%|▍         | 23/500 [06:55<2:21:06, 17.75s/it]  5%|▍         | 24/500 [07:09<2:12:36, 16.72s/it]  5%|▌         | 25/500 [07:23<2:06:46, 16.01s/it]  5%|▌         | 26/500 [07:42<2:12:06, 16.72s/it]  5%|▌         | 27/500 [08:00<2:15:50, 17.23s/it]  6%|▌         | 28/500 [08:14<2:07:49, 16.25s/it]  6%|▌         | 29/500 [08:28<2:02:30, 15.61s/it]  6%|▌         | 30/500 [08:42<1:59:05, 15.20s/it]  6%|▌         | 31/500 [09:01<2:07:37, 16.33s/it]  6%|▋         | 32/500 [09:15<2:02:22, 15.69s/it]  7%|▋         | 33/500 [09:30<1:59:53, 15.40s/it]  7%|▋         | 34/500 [09:49<2:08:32, 16.55s/it]  7%|▋         | 35/500 [10:08<2:13:29, 17.22s/it]  7%|▋         | 36/500 [10:27<2:16:34, 17.66s/it]  7%|▋         | 37/500 [10:45<2:18:04, 17.89s/it]  8%|▊         | 38/500 [11:04<2:19:45, 18.15s/it]  8%|▊         | 39/500 [11:23<2:21:13, 18.38s/it]  8%|▊         | 40/500 [11:42<2:22:13, 18.55s/it]  8%|▊         | 41/500 [12:01<2:22:48, 18.67s/it]  8%|▊         | 42/500 [12:15<2:11:51, 17.27s/it]  9%|▊         | 43/500 [12:29<2:04:15, 16.31s/it]  9%|▊         | 43/500 [12:29<2:12:45, 17.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.303 MB of 0.316 MB uploadedwandb: - 0.303 MB of 0.316 MB uploadedwandb: \ 0.303 MB of 0.316 MB uploadedwandb: | 0.303 MB of 0.316 MB uploadedwandb: / 0.303 MB of 0.316 MB uploadedwandb: - 0.303 MB of 0.316 MB uploadedwandb: \ 0.303 MB of 0.316 MB uploadedwandb: | 0.303 MB of 0.316 MB uploadedwandb: / 0.303 MB of 0.316 MB uploadedwandb: - 0.303 MB of 0.316 MB uploadedwandb: \ 0.303 MB of 0.316 MB uploadedwandb: | 0.303 MB of 0.316 MB uploadedwandb: / 0.303 MB of 0.316 MB uploadedwandb: - 0.303 MB of 0.316 MB uploadedwandb: \ 0.303 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▂▁▃▁▁▁▂▂▆▄▅▅▆▅▇▆▇▇▇██▇██▇▇▇████▇█▇█▇▇▇
wandb:     train_loss ▂▂▂▅▁▁▄▃█▇▁▁▆▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁
wandb:   val_accuracy ▁▂▂▁▃▁▁▁▁▂█▄▇▂▅▄▆▇▅▅▇▇▇▅▇▆█▆▇▆▆▅▆▇▅▇▅▇▇▅
wandb:       val_loss ▃▃▄▃▃▄▁▆▄▁▄▁▅▂▅▄▆▁▄▆▂▁▃▁▁▄▁▇▇▂▁▄█▅▇▂▅▁▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.86627
wandb:     train_loss 0.11201
wandb:   val_accuracy 0.49778
wandb:       val_loss 0.13612
wandb: 
wandb: 🚀 View run earnest-dew-59 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lm0cln3y
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_134955-lm0cln3y/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_140318-q5p2nrqy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-feather-60
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/q5p2nrqy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<4:00:05, 28.87s/it]  0%|          | 2/500 [00:43<2:52:22, 20.77s/it]  1%|          | 3/500 [01:03<2:47:38, 20.24s/it]  1%|          | 4/500 [01:22<2:44:17, 19.87s/it]  1%|          | 5/500 [01:37<2:28:37, 18.01s/it]  1%|          | 6/500 [01:56<2:31:39, 18.42s/it]  1%|▏         | 7/500 [02:20<2:44:41, 20.04s/it]  2%|▏         | 8/500 [02:34<2:29:35, 18.24s/it]  2%|▏         | 9/500 [02:53<2:30:19, 18.37s/it]  2%|▏         | 10/500 [03:07<2:19:04, 17.03s/it]  2%|▏         | 11/500 [03:25<2:22:34, 17.49s/it]  2%|▏         | 12/500 [03:44<2:25:21, 17.87s/it]  3%|▎         | 13/500 [04:07<2:37:59, 19.46s/it]  3%|▎         | 14/500 [04:22<2:25:15, 17.93s/it]  3%|▎         | 15/500 [04:40<2:26:34, 18.13s/it]  3%|▎         | 16/500 [04:59<2:27:31, 18.29s/it]  3%|▎         | 17/500 [05:17<2:27:58, 18.38s/it]  4%|▎         | 18/500 [05:36<2:29:04, 18.56s/it]  4%|▍         | 19/500 [05:51<2:19:10, 17.36s/it]  4%|▍         | 20/500 [06:10<2:22:35, 17.82s/it]  4%|▍         | 21/500 [06:29<2:24:48, 18.14s/it]  4%|▍         | 22/500 [06:47<2:25:13, 18.23s/it]  5%|▍         | 23/500 [07:06<2:26:01, 18.37s/it]  5%|▍         | 24/500 [07:24<2:26:06, 18.42s/it]  5%|▌         | 25/500 [07:43<2:26:09, 18.46s/it]  5%|▌         | 26/500 [07:57<2:15:26, 17.15s/it]  5%|▌         | 27/500 [08:16<2:18:42, 17.60s/it]  6%|▌         | 28/500 [08:30<2:10:02, 16.53s/it]  6%|▌         | 29/500 [08:48<2:14:21, 17.12s/it]  6%|▌         | 30/500 [09:07<2:17:33, 17.56s/it]  6%|▌         | 31/500 [09:25<2:19:02, 17.79s/it]  6%|▋         | 32/500 [09:39<2:09:45, 16.64s/it]  7%|▋         | 33/500 [09:57<2:13:38, 17.17s/it]  7%|▋         | 34/500 [10:11<2:05:54, 16.21s/it]  7%|▋         | 35/500 [10:25<2:00:25, 15.54s/it]  7%|▋         | 36/500 [10:44<2:07:05, 16.43s/it]  7%|▋         | 37/500 [11:02<2:11:23, 17.03s/it]  8%|▊         | 38/500 [11:22<2:16:20, 17.71s/it]  8%|▊         | 39/500 [11:40<2:18:31, 18.03s/it]  8%|▊         | 40/500 [11:59<2:20:22, 18.31s/it]  8%|▊         | 41/500 [12:18<2:20:52, 18.42s/it]  8%|▊         | 42/500 [12:37<2:21:41, 18.56s/it]  9%|▊         | 43/500 [12:56<2:22:18, 18.68s/it]  9%|▉         | 44/500 [13:15<2:22:35, 18.76s/it]  9%|▉         | 45/500 [13:34<2:22:04, 18.74s/it]  9%|▉         | 46/500 [13:53<2:22:59, 18.90s/it]  9%|▉         | 47/500 [14:07<2:12:24, 17.54s/it] 10%|▉         | 48/500 [14:26<2:15:28, 17.98s/it] 10%|▉         | 49/500 [14:46<2:18:08, 18.38s/it] 10%|▉         | 49/500 [14:50<2:16:35, 18.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▃▃▃▃▁▃▂▄▃▃▃▃▄▃▂▃▃▃▇▃█▆▄▄▄█▄▇▄▄▃▄▄▄▄▇▄▄
wandb:     train_loss ▃▃▃▄█▃▂▆▃▃▁▁▇▁▂▃▃▃▃▇▂▂▁▂▃▂▃▂▄▂▂▂█▂▂▂▄▂▄▃
wandb:   val_accuracy ▃▃▃▃▃▃▁▃▂▃▃▃▃▃▃▂▁▃▃▃█▃█▅▃▄▄▇▄▆▄▄▃▄▄▄▅▇▄▄
wandb:       val_loss ▃▃▃▂▅▂▂▄▂▂▁▆▄▃▃▃▃▃▂▄▂▃▂▂▃▃▃▃▄▃▃▃█▃▂▃▃▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.45765
wandb:     train_loss 1.45573
wandb:   val_accuracy 0.37111
wandb:       val_loss 1.20297
wandb: 
wandb: 🚀 View run true-feather-60 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/q5p2nrqy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_140318-q5p2nrqy/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_141850-ia4v6rjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-dew-61
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ia4v6rjk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:01:42, 29.06s/it]  0%|          | 2/500 [00:44<2:52:56, 20.84s/it]  1%|          | 3/500 [01:07<3:03:51, 22.20s/it]  1%|          | 4/500 [01:23<2:41:32, 19.54s/it]  1%|          | 5/500 [01:42<2:41:08, 19.53s/it]  1%|          | 6/500 [02:02<2:41:32, 19.62s/it]  1%|▏         | 7/500 [02:21<2:40:06, 19.49s/it]  2%|▏         | 8/500 [02:41<2:38:55, 19.38s/it]  2%|▏         | 9/500 [03:00<2:37:28, 19.24s/it]  2%|▏         | 10/500 [03:14<2:25:11, 17.78s/it]  2%|▏         | 11/500 [03:29<2:16:56, 16.80s/it]  2%|▏         | 12/500 [03:43<2:10:35, 16.06s/it]  3%|▎         | 13/500 [04:02<2:17:43, 16.97s/it]  3%|▎         | 14/500 [04:21<2:22:15, 17.56s/it]  3%|▎         | 15/500 [04:40<2:25:33, 18.01s/it]  3%|▎         | 16/500 [04:59<2:27:30, 18.29s/it]  3%|▎         | 17/500 [05:14<2:19:26, 17.32s/it]  4%|▎         | 18/500 [05:33<2:23:52, 17.91s/it]  4%|▍         | 19/500 [05:48<2:15:02, 16.85s/it]  4%|▍         | 20/500 [06:07<2:19:57, 17.49s/it]  4%|▍         | 21/500 [06:26<2:22:55, 17.90s/it]  4%|▍         | 22/500 [06:40<2:14:14, 16.85s/it]  5%|▍         | 23/500 [06:59<2:18:43, 17.45s/it]  5%|▍         | 24/500 [07:13<2:11:03, 16.52s/it]  5%|▌         | 25/500 [07:28<2:06:04, 15.93s/it]  5%|▌         | 26/500 [07:43<2:03:40, 15.65s/it]  5%|▌         | 27/500 [08:07<2:23:10, 18.16s/it]  6%|▌         | 28/500 [08:21<2:14:10, 17.06s/it]  6%|▌         | 29/500 [08:40<2:18:08, 17.60s/it]  6%|▌         | 30/500 [08:59<2:21:25, 18.05s/it]  6%|▌         | 31/500 [09:14<2:12:34, 16.96s/it]  6%|▋         | 32/500 [09:28<2:06:22, 16.20s/it]  7%|▋         | 33/500 [09:43<2:02:46, 15.77s/it]  7%|▋         | 34/500 [10:02<2:11:10, 16.89s/it]  7%|▋         | 35/500 [10:21<2:16:03, 17.56s/it]  7%|▋         | 36/500 [10:40<2:19:11, 18.00s/it]  7%|▋         | 37/500 [11:00<2:21:57, 18.40s/it]  8%|▊         | 38/500 [11:14<2:13:09, 17.29s/it]  8%|▊         | 39/500 [11:34<2:17:45, 17.93s/it]  8%|▊         | 40/500 [11:53<2:20:05, 18.27s/it]  8%|▊         | 41/500 [12:12<2:21:40, 18.52s/it]  8%|▊         | 42/500 [12:31<2:22:53, 18.72s/it]  9%|▊         | 43/500 [12:51<2:23:51, 18.89s/it]  9%|▉         | 44/500 [13:10<2:24:35, 19.02s/it]  9%|▉         | 45/500 [13:29<2:24:32, 19.06s/it]  9%|▉         | 46/500 [13:44<2:14:55, 17.83s/it]  9%|▉         | 47/500 [14:03<2:17:35, 18.22s/it] 10%|▉         | 48/500 [14:18<2:08:46, 17.09s/it] 10%|▉         | 49/500 [14:37<2:12:51, 17.68s/it] 10%|█         | 50/500 [14:56<2:16:45, 18.23s/it] 10%|█         | 51/500 [15:11<2:08:10, 17.13s/it] 10%|█         | 52/500 [15:25<2:02:24, 16.39s/it] 11%|█         | 53/500 [15:40<1:57:46, 15.81s/it] 11%|█         | 54/500 [15:59<2:04:44, 16.78s/it] 11%|█         | 55/500 [16:14<1:59:49, 16.16s/it] 11%|█         | 56/500 [16:33<2:06:24, 17.08s/it] 11%|█▏        | 57/500 [16:52<2:10:54, 17.73s/it] 12%|█▏        | 58/500 [17:11<2:13:46, 18.16s/it] 12%|█▏        | 59/500 [17:31<2:16:11, 18.53s/it] 12%|█▏        | 60/500 [17:45<2:07:24, 17.37s/it] 12%|█▏        | 61/500 [18:00<2:01:14, 16.57s/it] 12%|█▏        | 62/500 [18:15<1:57:47, 16.14s/it] 13%|█▎        | 63/500 [18:35<2:05:39, 17.25s/it] 13%|█▎        | 63/500 [18:35<2:08:57, 17.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▅▆▁▁▁▁▁▁▁▁▁▁▂█▅▁▄▁▃▃▃▂▁▂▂▃▃▃▄
wandb:     train_loss ██▁▆▆▆▂▅▅▂▂▃▄▅▃▅▅▆▅▅▃▅▅▄▅▅▅▅▅▆▄▃▅▃▃▆▃▆▅▄
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▄▆▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁
wandb:       val_loss ▃▃█▃▇▅█▅▆▆▄▃▃▁▅▄▄▆▁▆▃▃▆▄▄▄▃▃▄▆▄▆▄▆▅▄▄▄▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 62
wandb:  learning_rate 0.0
wandb: train_accuracy 0.41159
wandb:     train_loss 1.01337
wandb:   val_accuracy 0.35111
wandb:       val_loss 1.10353
wandb: 
wandb: 🚀 View run crimson-dew-61 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ia4v6rjk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_141850-ia4v6rjk/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_143807-d3gxfkw7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-pine-62
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/d3gxfkw7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:13:03, 37.64s/it]  0%|          | 2/500 [01:07<4:36:32, 33.32s/it]  1%|          | 3/500 [01:31<3:59:51, 28.96s/it]  1%|          | 4/500 [02:10<4:30:23, 32.71s/it]  1%|          | 5/500 [02:33<4:02:17, 29.37s/it]  1%|          | 6/500 [03:05<4:07:48, 30.10s/it]  1%|▏         | 7/500 [03:34<4:06:10, 29.96s/it]  2%|▏         | 8/500 [04:04<4:05:15, 29.91s/it]  2%|▏         | 9/500 [04:39<4:16:45, 31.38s/it]  2%|▏         | 10/500 [05:13<4:23:05, 32.22s/it]  2%|▏         | 11/500 [05:45<4:21:19, 32.06s/it]  2%|▏         | 12/500 [06:16<4:19:52, 31.95s/it]  3%|▎         | 13/500 [06:48<4:18:56, 31.90s/it]  3%|▎         | 14/500 [07:28<4:37:55, 34.31s/it]  3%|▎         | 15/500 [07:51<4:11:17, 31.09s/it]  3%|▎         | 16/500 [08:23<4:12:23, 31.29s/it]  3%|▎         | 17/500 [08:58<4:19:36, 32.25s/it]  4%|▎         | 18/500 [09:32<4:23:55, 32.85s/it]  4%|▍         | 19/500 [10:03<4:19:46, 32.41s/it]  4%|▍         | 20/500 [10:35<4:18:02, 32.25s/it]  4%|▍         | 21/500 [11:04<4:09:46, 31.29s/it]  4%|▍         | 22/500 [11:38<4:15:30, 32.07s/it]  5%|▍         | 23/500 [12:11<4:17:02, 32.33s/it]  5%|▍         | 24/500 [12:44<4:16:53, 32.38s/it]  5%|▌         | 25/500 [13:15<4:15:07, 32.23s/it]  5%|▌         | 26/500 [13:47<4:13:25, 32.08s/it]  5%|▌         | 27/500 [14:19<4:11:31, 31.91s/it]  6%|▌         | 28/500 [14:51<4:10:54, 31.90s/it]  6%|▌         | 29/500 [15:22<4:10:03, 31.86s/it]  6%|▌         | 30/500 [15:54<4:09:34, 31.86s/it]  6%|▌         | 31/500 [16:23<4:02:41, 31.05s/it]  6%|▋         | 32/500 [16:56<4:04:50, 31.39s/it]  7%|▋         | 33/500 [17:27<4:03:41, 31.31s/it]  7%|▋         | 34/500 [17:56<3:57:24, 30.57s/it]  7%|▋         | 35/500 [18:26<3:55:40, 30.41s/it]  7%|▋         | 36/500 [19:00<4:04:21, 31.60s/it]  7%|▋         | 37/500 [19:31<4:02:59, 31.49s/it]  8%|▊         | 38/500 [20:02<4:01:41, 31.39s/it]  8%|▊         | 39/500 [20:31<3:55:30, 30.65s/it]  8%|▊         | 40/500 [21:02<3:55:35, 30.73s/it]  8%|▊         | 41/500 [21:30<3:48:24, 29.86s/it]  8%|▊         | 42/500 [21:54<3:33:23, 27.95s/it]  9%|▊         | 43/500 [22:22<3:34:52, 28.21s/it]  9%|▉         | 44/500 [22:48<3:29:37, 27.58s/it]  9%|▉         | 45/500 [23:15<3:26:54, 27.28s/it]  9%|▉         | 46/500 [23:43<3:27:37, 27.44s/it]  9%|▉         | 47/500 [24:06<3:17:22, 26.14s/it] 10%|▉         | 48/500 [24:33<3:19:31, 26.49s/it] 10%|▉         | 49/500 [25:06<3:34:22, 28.52s/it] 10%|█         | 50/500 [25:35<3:33:42, 28.50s/it] 10%|█         | 51/500 [26:06<3:38:11, 29.16s/it] 10%|█         | 52/500 [26:35<3:38:47, 29.30s/it] 11%|█         | 53/500 [27:07<3:42:56, 29.92s/it] 11%|█         | 54/500 [27:35<3:39:26, 29.52s/it] 11%|█         | 55/500 [28:09<3:48:14, 30.77s/it] 11%|█         | 56/500 [28:39<3:45:51, 30.52s/it] 11%|█▏        | 57/500 [29:12<3:50:56, 31.28s/it] 12%|█▏        | 58/500 [29:44<3:51:14, 31.39s/it] 12%|█▏        | 59/500 [30:15<3:51:17, 31.47s/it] 12%|█▏        | 60/500 [30:47<3:51:29, 31.57s/it] 12%|█▏        | 61/500 [31:19<3:51:48, 31.68s/it] 12%|█▏        | 62/500 [31:48<3:44:36, 30.77s/it] 13%|█▎        | 63/500 [32:21<3:50:22, 31.63s/it] 13%|█▎        | 64/500 [32:51<3:46:11, 31.13s/it] 13%|█▎        | 65/500 [33:21<3:42:35, 30.70s/it] 13%|█▎        | 66/500 [33:55<3:48:24, 31.58s/it] 13%|█▎        | 67/500 [34:23<3:41:57, 30.76s/it] 14%|█▎        | 68/500 [34:53<3:38:37, 30.36s/it] 14%|█▍        | 69/500 [35:27<3:45:34, 31.40s/it] 14%|█▍        | 70/500 [35:58<3:45:36, 31.48s/it] 14%|█▍        | 71/500 [36:30<3:45:14, 31.50s/it] 14%|█▍        | 72/500 [37:01<3:43:35, 31.35s/it] 15%|█▍        | 73/500 [37:30<3:38:37, 30.72s/it] 15%|█▍        | 74/500 [38:04<3:44:17, 31.59s/it] 15%|█▌        | 75/500 [38:32<3:37:04, 30.65s/it] 15%|█▌        | 76/500 [39:04<3:38:07, 30.87s/it] 15%|█▌        | 77/500 [39:35<3:39:04, 31.07s/it] 15%|█▌        | 77/500 [39:40<3:37:57, 30.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.019 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▂▃▁▁▁█▇▆███████████████████████████████
wandb:     train_loss ▃▂▄█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▅▄▁▁▁▇▅▃▅▄█▇▇█▇█▇▇█▇████████▇█████▇████
wandb:       val_loss ▂▂▁▇▄▅▃▁▂▂▂▁▆▁▂▆▁█▁▁▄▁▂▁▃▄▁▁▂▄▁▁▅▂▁▁▁▄▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 0.0
wandb:   val_accuracy 0.69111
wandb:       val_loss 3.74925
wandb: 
wandb: 🚀 View run clear-pine-62 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/d3gxfkw7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_143807-d3gxfkw7/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_151827-bf3suwtl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-jazz-63
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bf3suwtl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:12:49, 37.61s/it]  0%|          | 2/500 [01:08<4:39:30, 33.67s/it]  1%|          | 3/500 [01:36<4:18:59, 31.27s/it]  1%|          | 4/500 [02:06<4:14:15, 30.76s/it]  1%|          | 5/500 [02:29<3:50:06, 27.89s/it]  1%|          | 6/500 [03:00<3:56:43, 28.75s/it]  1%|▏         | 7/500 [03:32<4:05:33, 29.88s/it]  2%|▏         | 8/500 [04:12<4:31:35, 33.12s/it]  2%|▏         | 9/500 [04:35<4:04:56, 29.93s/it]  2%|▏         | 10/500 [05:12<4:23:49, 32.31s/it]  2%|▏         | 11/500 [05:36<4:00:13, 29.47s/it]  2%|▏         | 12/500 [06:07<4:04:10, 30.02s/it]  3%|▎         | 13/500 [06:45<4:23:44, 32.49s/it]  3%|▎         | 14/500 [07:08<3:59:42, 29.59s/it]  3%|▎         | 15/500 [07:39<4:02:38, 30.02s/it]  3%|▎         | 16/500 [08:10<4:04:14, 30.28s/it]  3%|▎         | 17/500 [08:39<4:02:08, 30.08s/it]  4%|▎         | 18/500 [09:10<4:02:55, 30.24s/it]  4%|▍         | 19/500 [09:40<4:02:08, 30.20s/it]  4%|▍         | 20/500 [10:18<4:19:23, 32.42s/it]  4%|▍         | 21/500 [10:44<4:04:38, 30.64s/it]  4%|▍         | 22/500 [11:17<4:08:41, 31.22s/it]  5%|▍         | 23/500 [11:46<4:04:01, 30.69s/it]  5%|▍         | 24/500 [12:19<4:09:12, 31.41s/it]  5%|▌         | 25/500 [12:49<4:04:09, 30.84s/it]  5%|▌         | 26/500 [13:20<4:03:24, 30.81s/it]  5%|▌         | 27/500 [13:50<4:02:52, 30.81s/it]  6%|▌         | 28/500 [14:20<3:59:23, 30.43s/it]  6%|▌         | 29/500 [14:50<3:57:50, 30.30s/it]  6%|▌         | 30/500 [15:24<4:05:25, 31.33s/it]  6%|▌         | 31/500 [15:57<4:10:35, 32.06s/it]  6%|▋         | 32/500 [16:26<4:02:01, 31.03s/it]  7%|▋         | 33/500 [17:03<4:14:24, 32.69s/it]  7%|▋         | 34/500 [17:25<3:50:43, 29.71s/it]  7%|▋         | 35/500 [17:57<3:53:58, 30.19s/it]  7%|▋         | 36/500 [18:32<4:04:31, 31.62s/it]  7%|▋         | 37/500 [18:54<3:43:46, 29.00s/it]  8%|▊         | 38/500 [19:27<3:51:04, 30.01s/it]  8%|▊         | 39/500 [19:59<3:54:50, 30.56s/it]  8%|▊         | 40/500 [20:25<3:44:16, 29.25s/it]  8%|▊         | 41/500 [20:51<3:37:20, 28.41s/it]  8%|▊         | 42/500 [21:18<3:31:56, 27.77s/it]  9%|▊         | 43/500 [21:48<3:37:39, 28.58s/it]  9%|▉         | 44/500 [22:17<3:37:15, 28.59s/it]  9%|▉         | 45/500 [22:47<3:40:21, 29.06s/it]  9%|▉         | 46/500 [23:17<3:42:45, 29.44s/it]  9%|▉         | 47/500 [23:48<3:44:59, 29.80s/it] 10%|▉         | 48/500 [24:20<3:50:19, 30.58s/it] 10%|▉         | 49/500 [24:43<3:31:25, 28.13s/it] 10%|█         | 50/500 [25:13<3:36:03, 28.81s/it] 10%|█         | 51/500 [25:46<3:45:26, 30.13s/it] 10%|█         | 52/500 [26:14<3:40:12, 29.49s/it] 11%|█         | 53/500 [26:44<3:39:28, 29.46s/it] 11%|█         | 54/500 [27:15<3:42:12, 29.89s/it] 11%|█         | 55/500 [27:47<3:47:37, 30.69s/it] 11%|█         | 56/500 [28:19<3:50:13, 31.11s/it] 11%|█▏        | 57/500 [28:48<3:45:37, 30.56s/it] 12%|█▏        | 58/500 [29:19<3:44:10, 30.43s/it] 12%|█▏        | 59/500 [29:49<3:43:32, 30.41s/it] 12%|█▏        | 60/500 [30:19<3:42:36, 30.36s/it] 12%|█▏        | 61/500 [30:52<3:48:24, 31.22s/it] 12%|█▏        | 62/500 [31:26<3:53:44, 32.02s/it] 13%|█▎        | 63/500 [31:54<3:43:06, 30.63s/it] 13%|█▎        | 64/500 [32:23<3:40:45, 30.38s/it] 13%|█▎        | 65/500 [32:52<3:36:45, 29.90s/it] 13%|█▎        | 66/500 [33:27<3:45:56, 31.24s/it] 13%|█▎        | 67/500 [33:59<3:48:03, 31.60s/it] 14%|█▎        | 68/500 [34:29<3:43:17, 31.01s/it] 14%|█▍        | 69/500 [34:59<3:40:17, 30.67s/it] 14%|█▍        | 70/500 [35:29<3:40:23, 30.75s/it] 14%|█▍        | 71/500 [36:02<3:42:46, 31.16s/it] 14%|█▍        | 72/500 [36:34<3:44:54, 31.53s/it] 15%|█▍        | 73/500 [37:03<3:39:00, 30.77s/it] 15%|█▍        | 74/500 [37:33<3:37:10, 30.59s/it] 15%|█▌        | 75/500 [38:03<3:34:36, 30.30s/it] 15%|█▌        | 76/500 [38:31<3:30:14, 29.75s/it] 15%|█▌        | 77/500 [39:04<3:35:54, 30.63s/it] 15%|█▌        | 77/500 [39:14<3:35:35, 30.58s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.019 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▅▅▅▆▆▆▇▆▇▇▆▇▇█▇▇▇██▇▇██████████▇██████
wandb:     train_loss ▅▅▆▅▂█▂▅▄▂▆▅▂▄▃▂▂▁▁█▄▂▄▃▂▅▂▄▄▁▃▃▂▅▆▄▄▂▁▁
wandb:   val_accuracy ▁▁▇█▆▃▄▄▄▄▄▄▅▅▅▄▄▄▅▄▄▅▅▄▄▄▅▅▄▅▄▄▄▄▄▄▄▄▅▄
wandb:       val_loss ▃▃▂▃▂▂▃▃▃▂▂▂▂▂▂▄▂▃▁▂▄▁▁▂▃█▂▃▁▃▃▁▄▄▂▂▄▅▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0
wandb: train_accuracy 0.82764
wandb:     train_loss 0.03924
wandb:   val_accuracy 0.50222
wandb:       val_loss 1.00987
wandb: 
wandb: 🚀 View run sandy-jazz-63 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bf3suwtl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_151827-bf3suwtl/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_155829-i21j8umm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-moon-64
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/i21j8umm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:08:50, 37.13s/it]  0%|          | 2/500 [01:00<3:59:26, 28.85s/it]  1%|          | 3/500 [01:29<3:59:35, 28.93s/it]  1%|          | 4/500 [01:52<3:39:43, 26.58s/it]  1%|          | 5/500 [02:20<3:45:20, 27.31s/it]  1%|          | 6/500 [02:51<3:54:19, 28.46s/it]  1%|▏         | 7/500 [03:25<4:08:05, 30.19s/it]  2%|▏         | 8/500 [03:57<4:13:42, 30.94s/it]  2%|▏         | 9/500 [04:25<4:04:15, 29.85s/it]  2%|▏         | 10/500 [04:48<3:48:21, 27.96s/it]  2%|▏         | 11/500 [05:15<3:43:23, 27.41s/it]  2%|▏         | 12/500 [05:39<3:34:43, 26.40s/it]  3%|▎         | 13/500 [06:01<3:24:16, 25.17s/it]  3%|▎         | 14/500 [06:29<3:31:52, 26.16s/it]  3%|▎         | 15/500 [07:00<3:42:09, 27.48s/it]  3%|▎         | 16/500 [07:30<3:48:09, 28.28s/it]  3%|▎         | 17/500 [08:01<3:54:31, 29.13s/it]  4%|▎         | 18/500 [08:35<4:04:17, 30.41s/it]  4%|▍         | 19/500 [09:03<3:58:13, 29.72s/it]  4%|▍         | 20/500 [09:31<3:54:08, 29.27s/it]  4%|▍         | 21/500 [10:02<3:57:32, 29.76s/it]  4%|▍         | 22/500 [10:34<4:02:20, 30.42s/it]  5%|▍         | 23/500 [11:02<3:57:16, 29.85s/it]  5%|▍         | 24/500 [11:32<3:55:04, 29.63s/it]  5%|▌         | 25/500 [12:02<3:57:24, 29.99s/it]  5%|▌         | 26/500 [12:30<3:52:10, 29.39s/it]  5%|▌         | 27/500 [12:56<3:43:12, 28.31s/it]  6%|▌         | 28/500 [13:28<3:50:41, 29.33s/it]  6%|▌         | 29/500 [13:58<3:52:49, 29.66s/it]  6%|▌         | 30/500 [14:28<3:52:48, 29.72s/it]  6%|▌         | 31/500 [14:58<3:52:25, 29.73s/it]  6%|▋         | 32/500 [15:22<3:38:10, 27.97s/it]  7%|▋         | 33/500 [15:46<3:29:36, 26.93s/it]  7%|▋         | 34/500 [16:16<3:35:42, 27.77s/it]  7%|▋         | 35/500 [16:46<3:40:03, 28.40s/it]  7%|▋         | 36/500 [17:15<3:42:06, 28.72s/it]  7%|▋         | 37/500 [17:46<3:45:48, 29.26s/it]  8%|▊         | 38/500 [18:17<3:48:58, 29.74s/it]  8%|▊         | 39/500 [18:47<3:50:25, 29.99s/it]  8%|▊         | 40/500 [19:17<3:49:43, 29.96s/it]  8%|▊         | 41/500 [19:51<3:58:12, 31.14s/it]  8%|▊         | 42/500 [20:24<4:00:53, 31.56s/it]  9%|▊         | 43/500 [20:53<3:56:13, 31.01s/it]  9%|▉         | 44/500 [21:24<3:55:04, 30.93s/it]  9%|▉         | 45/500 [21:52<3:48:13, 30.10s/it]  9%|▉         | 45/500 [21:57<3:42:05, 29.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▄▆▆▇▇███████▇██████████▇█
wandb:     train_loss ▁▇▁▁█▂▆▇▇▆▆▅▅▅▅▆▄▅▄▆▅▃▆▄▄▄▃▃▅▆▄▅▃▆▅▅▄▄▄▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▂▄▄▄▄▄▄▄▂▃▄▄▅▇▃▅▆▆▇█▆
wandb:       val_loss ▃█▂▇▂▂▂▆▄▄▁▅▆▄▃▅▃▃▂▂▄▃▄▃▃▄▄▅▃▃▃▄▄▂▃▅▁▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0
wandb: train_accuracy 0.61664
wandb:     train_loss 1.07359
wandb:   val_accuracy 0.43778
wandb:       val_loss 1.05611
wandb: 
wandb: 🚀 View run earnest-moon-64 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/i21j8umm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_155829-i21j8umm/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_162106-328u8rhx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sponge-65
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/328u8rhx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:10:12, 37.30s/it]  0%|          | 2/500 [01:08<4:37:31, 33.44s/it]  1%|          | 3/500 [01:31<3:57:32, 28.68s/it]  1%|          | 4/500 [02:01<4:03:00, 29.40s/it]  1%|          | 5/500 [02:31<4:05:12, 29.72s/it]  1%|          | 6/500 [03:04<4:11:37, 30.56s/it]  1%|▏         | 7/500 [03:33<4:07:21, 30.10s/it]  2%|▏         | 8/500 [04:01<4:02:10, 29.53s/it]  2%|▏         | 9/500 [04:33<4:08:40, 30.39s/it]  2%|▏         | 10/500 [05:12<4:30:10, 33.08s/it]  2%|▏         | 11/500 [05:36<4:06:28, 30.24s/it]  2%|▏         | 12/500 [06:08<4:10:38, 30.82s/it]  3%|▎         | 13/500 [06:40<4:12:08, 31.06s/it]  3%|▎         | 14/500 [07:20<4:34:23, 33.88s/it]  3%|▎         | 15/500 [07:43<4:07:34, 30.63s/it]  3%|▎         | 16/500 [08:14<4:07:13, 30.65s/it]  3%|▎         | 17/500 [08:45<4:06:31, 30.62s/it]  4%|▎         | 18/500 [09:19<4:13:49, 31.60s/it]  4%|▍         | 19/500 [09:49<4:10:49, 31.29s/it]  4%|▍         | 20/500 [10:26<4:23:31, 32.94s/it]  4%|▍         | 21/500 [10:49<3:59:56, 30.06s/it]  4%|▍         | 22/500 [11:25<4:12:36, 31.71s/it]  5%|▍         | 23/500 [11:51<3:58:02, 29.94s/it]  5%|▍         | 24/500 [12:23<4:04:23, 30.81s/it]  5%|▌         | 25/500 [12:52<3:58:34, 30.14s/it]  5%|▌         | 26/500 [13:23<4:01:13, 30.53s/it]  5%|▌         | 27/500 [13:53<3:57:17, 30.10s/it]  6%|▌         | 28/500 [14:18<3:45:08, 28.62s/it]  6%|▌         | 29/500 [14:50<3:52:45, 29.65s/it]  6%|▌         | 30/500 [15:20<3:53:40, 29.83s/it]  6%|▌         | 31/500 [15:50<3:54:30, 30.00s/it]  6%|▋         | 32/500 [16:21<3:56:15, 30.29s/it]  7%|▋         | 33/500 [16:58<4:10:06, 32.13s/it]  7%|▋         | 34/500 [17:22<3:51:55, 29.86s/it]  7%|▋         | 35/500 [17:54<3:56:24, 30.50s/it]  7%|▋         | 36/500 [18:32<4:13:28, 32.78s/it]  7%|▋         | 37/500 [18:56<3:51:20, 29.98s/it]  8%|▊         | 38/500 [19:27<3:54:13, 30.42s/it]  8%|▊         | 39/500 [19:57<3:51:07, 30.08s/it]  8%|▊         | 40/500 [20:28<3:52:25, 30.32s/it]  8%|▊         | 41/500 [21:04<4:06:31, 32.23s/it]  8%|▊         | 42/500 [21:27<3:45:11, 29.50s/it]  9%|▊         | 43/500 [21:58<3:47:11, 29.83s/it]  9%|▉         | 44/500 [22:30<3:51:48, 30.50s/it]  9%|▉         | 45/500 [22:59<3:46:42, 29.90s/it]  9%|▉         | 46/500 [23:33<3:55:58, 31.19s/it]  9%|▉         | 47/500 [24:06<4:00:35, 31.87s/it] 10%|▉         | 48/500 [24:37<3:57:24, 31.52s/it] 10%|▉         | 49/500 [25:05<3:49:24, 30.52s/it] 10%|█         | 50/500 [25:36<3:49:53, 30.65s/it] 10%|█         | 51/500 [26:07<3:50:01, 30.74s/it] 10%|█         | 52/500 [26:41<3:57:07, 31.76s/it] 11%|█         | 53/500 [27:14<3:58:05, 31.96s/it] 11%|█         | 54/500 [27:42<3:50:39, 31.03s/it] 11%|█         | 55/500 [28:14<3:50:22, 31.06s/it] 11%|█         | 56/500 [28:45<3:50:11, 31.11s/it] 11%|█▏        | 57/500 [29:16<3:50:33, 31.23s/it] 12%|█▏        | 58/500 [29:47<3:50:01, 31.23s/it] 12%|█▏        | 59/500 [30:18<3:49:04, 31.17s/it] 12%|█▏        | 60/500 [30:50<3:48:56, 31.22s/it] 12%|█▏        | 61/500 [31:21<3:48:58, 31.29s/it] 12%|█▏        | 62/500 [31:53<3:49:48, 31.48s/it] 13%|█▎        | 63/500 [32:24<3:48:14, 31.34s/it] 13%|█▎        | 64/500 [32:55<3:46:44, 31.20s/it] 13%|█▎        | 65/500 [33:26<3:45:04, 31.05s/it] 13%|█▎        | 66/500 [33:57<3:45:35, 31.19s/it] 13%|█▎        | 67/500 [34:29<3:45:36, 31.26s/it] 14%|█▎        | 68/500 [34:59<3:43:08, 30.99s/it] 14%|█▍        | 69/500 [35:32<3:45:42, 31.42s/it] 14%|█▍        | 70/500 [36:02<3:43:38, 31.20s/it] 14%|█▍        | 71/500 [36:33<3:41:20, 30.96s/it] 14%|█▍        | 72/500 [36:56<3:24:29, 28.67s/it] 15%|█▍        | 73/500 [37:28<3:31:51, 29.77s/it] 15%|█▍        | 74/500 [37:59<3:33:14, 30.03s/it] 15%|█▌        | 75/500 [38:27<3:28:33, 29.44s/it] 15%|█▌        | 76/500 [38:58<3:31:03, 29.87s/it] 15%|█▌        | 77/500 [39:28<3:32:12, 30.10s/it] 15%|█▌        | 77/500 [39:36<3:37:34, 30.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▆▁▁▂▇█▇▇██████████████████████████████
wandb:     train_loss ▃▂▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▇▃▁▁▂▆▆▅▇▇▆▇▆█▇▇▆▇▆▆▇▇▇▇▇▇█▇▆▇█▇██▇▇▇▇█
wandb:       val_loss ▂▂▂▇▅▁▃▁▁▃▁▂▄▃▃█▁▆▁▁▅▁▁▁▇▃▁▂▁▅▁▁▅▂▁▁▃▄▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 1e-05
wandb: train_accuracy 1.0
wandb:     train_loss 0.00014
wandb:   val_accuracy 0.68444
wandb:       val_loss 2.41493
wandb: 
wandb: 🚀 View run classic-sponge-65 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/328u8rhx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_162106-328u8rhx/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_170126-4vc5sqcc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-rain-66
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/4vc5sqcc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:03:44, 36.52s/it]  0%|          | 2/500 [01:05<4:24:53, 31.91s/it]  1%|          | 3/500 [01:33<4:12:09, 30.44s/it]  1%|          | 4/500 [02:03<4:10:18, 30.28s/it]  1%|          | 5/500 [02:26<3:48:19, 27.67s/it]  1%|          | 6/500 [02:57<3:54:46, 28.52s/it]  1%|▏         | 7/500 [03:34<4:18:43, 31.49s/it]  2%|▏         | 8/500 [04:05<4:15:51, 31.20s/it]  2%|▏         | 9/500 [04:34<4:11:16, 30.71s/it]  2%|▏         | 10/500 [05:03<4:04:42, 29.96s/it]  2%|▏         | 11/500 [05:30<3:57:31, 29.14s/it]  2%|▏         | 12/500 [06:03<4:06:21, 30.29s/it]  3%|▎         | 13/500 [06:38<4:17:32, 31.73s/it]  3%|▎         | 14/500 [07:04<4:02:43, 29.97s/it]  3%|▎         | 15/500 [07:33<3:59:25, 29.62s/it]  3%|▎         | 16/500 [08:03<4:01:34, 29.95s/it]  3%|▎         | 17/500 [08:37<4:09:03, 30.94s/it]  4%|▎         | 18/500 [09:10<4:13:18, 31.53s/it]  4%|▍         | 19/500 [09:39<4:08:04, 30.94s/it]  4%|▍         | 20/500 [10:09<4:05:50, 30.73s/it]  4%|▍         | 21/500 [10:38<4:00:37, 30.14s/it]  4%|▍         | 22/500 [11:16<4:18:35, 32.46s/it]  5%|▍         | 23/500 [11:43<4:04:37, 30.77s/it]  5%|▍         | 24/500 [12:06<3:45:39, 28.44s/it]  5%|▌         | 25/500 [12:36<3:50:22, 29.10s/it]  5%|▌         | 26/500 [13:10<4:01:17, 30.54s/it]  5%|▌         | 27/500 [13:36<3:48:15, 28.96s/it]  6%|▌         | 28/500 [14:08<3:56:48, 30.10s/it]  6%|▌         | 29/500 [14:37<3:52:45, 29.65s/it]  6%|▌         | 30/500 [15:03<3:42:57, 28.46s/it]  6%|▌         | 31/500 [15:29<3:37:32, 27.83s/it]  6%|▋         | 32/500 [15:53<3:28:03, 26.67s/it]  7%|▋         | 33/500 [16:22<3:32:02, 27.24s/it]  7%|▋         | 34/500 [16:52<3:39:00, 28.20s/it]  7%|▋         | 35/500 [17:17<3:29:56, 27.09s/it]  7%|▋         | 36/500 [17:49<3:41:25, 28.63s/it]  7%|▋         | 37/500 [18:15<3:35:52, 27.97s/it]  8%|▊         | 38/500 [18:42<3:32:40, 27.62s/it]  8%|▊         | 39/500 [19:10<3:32:42, 27.68s/it]  8%|▊         | 40/500 [19:37<3:30:32, 27.46s/it]  8%|▊         | 41/500 [20:02<3:25:09, 26.82s/it]  8%|▊         | 42/500 [20:28<3:22:48, 26.57s/it]  9%|▊         | 43/500 [21:02<3:38:26, 28.68s/it]  9%|▉         | 44/500 [21:26<3:28:48, 27.48s/it]  9%|▉         | 45/500 [21:58<3:37:41, 28.71s/it]  9%|▉         | 46/500 [22:23<3:30:00, 27.75s/it]  9%|▉         | 47/500 [22:51<3:30:04, 27.83s/it] 10%|▉         | 48/500 [23:19<3:28:47, 27.72s/it] 10%|▉         | 49/500 [23:44<3:22:02, 26.88s/it] 10%|█         | 50/500 [24:12<3:24:25, 27.26s/it] 10%|█         | 51/500 [24:41<3:27:20, 27.71s/it] 10%|█         | 52/500 [25:05<3:19:33, 26.73s/it] 11%|█         | 53/500 [25:30<3:15:33, 26.25s/it] 11%|█         | 54/500 [26:00<3:23:18, 27.35s/it] 11%|█         | 55/500 [26:24<3:14:50, 26.27s/it] 11%|█         | 56/500 [26:54<3:22:46, 27.40s/it] 11%|█▏        | 57/500 [27:22<3:23:58, 27.63s/it] 12%|█▏        | 58/500 [27:49<3:21:39, 27.37s/it] 12%|█▏        | 59/500 [28:14<3:16:36, 26.75s/it] 12%|█▏        | 60/500 [28:41<3:15:24, 26.65s/it] 12%|█▏        | 61/500 [29:10<3:21:55, 27.60s/it] 12%|█▏        | 62/500 [29:34<3:12:49, 26.41s/it] 13%|█▎        | 63/500 [30:03<3:17:52, 27.17s/it] 13%|█▎        | 64/500 [30:27<3:09:55, 26.14s/it] 13%|█▎        | 65/500 [30:55<3:13:43, 26.72s/it] 13%|█▎        | 66/500 [31:24<3:19:20, 27.56s/it] 13%|█▎        | 67/500 [31:50<3:15:23, 27.08s/it] 14%|█▎        | 68/500 [32:18<3:15:52, 27.21s/it] 14%|█▍        | 69/500 [32:43<3:10:35, 26.53s/it] 14%|█▍        | 70/500 [33:08<3:07:26, 26.15s/it] 14%|█▍        | 71/500 [33:35<3:08:06, 26.31s/it] 14%|█▍        | 72/500 [34:02<3:09:30, 26.57s/it] 15%|█▍        | 73/500 [34:26<3:03:32, 25.79s/it] 15%|█▍        | 74/500 [34:56<3:11:58, 27.04s/it] 15%|█▌        | 75/500 [35:19<3:03:39, 25.93s/it] 15%|█▌        | 76/500 [35:47<3:07:46, 26.57s/it] 15%|█▌        | 77/500 [36:14<3:08:03, 26.68s/it] 16%|█▌        | 78/500 [36:41<3:08:43, 26.83s/it] 16%|█▌        | 79/500 [37:05<3:02:01, 25.94s/it] 16%|█▌        | 80/500 [37:34<3:08:24, 26.91s/it] 16%|█▌        | 81/500 [37:59<3:03:17, 26.25s/it] 16%|█▋        | 82/500 [38:26<3:04:28, 26.48s/it] 17%|█▋        | 83/500 [38:52<3:02:05, 26.20s/it] 17%|█▋        | 84/500 [39:16<2:57:51, 25.65s/it] 17%|█▋        | 85/500 [39:43<2:59:13, 25.91s/it] 17%|█▋        | 86/500 [40:09<3:00:41, 26.19s/it] 17%|█▋        | 87/500 [40:33<2:54:32, 25.36s/it] 18%|█▊        | 88/500 [41:00<2:58:36, 26.01s/it] 18%|█▊        | 89/500 [41:27<3:00:04, 26.29s/it] 18%|█▊        | 90/500 [41:51<2:54:25, 25.53s/it] 18%|█▊        | 91/500 [42:18<2:56:57, 25.96s/it] 18%|█▊        | 92/500 [42:42<2:52:11, 25.32s/it] 19%|█▊        | 93/500 [43:08<2:53:33, 25.59s/it] 19%|█▉        | 94/500 [43:34<2:53:23, 25.62s/it] 19%|█▉        | 95/500 [43:59<2:51:40, 25.43s/it] 19%|█▉        | 96/500 [44:23<2:48:07, 24.97s/it] 19%|█▉        | 97/500 [44:53<2:57:53, 26.48s/it] 20%|█▉        | 98/500 [45:18<2:54:09, 25.99s/it] 20%|█▉        | 99/500 [45:41<2:49:32, 25.37s/it] 20%|██        | 100/500 [46:09<2:53:31, 26.03s/it] 20%|██        | 101/500 [46:34<2:50:24, 25.62s/it] 20%|██        | 102/500 [46:58<2:47:47, 25.30s/it] 21%|██        | 103/500 [47:24<2:47:59, 25.39s/it] 21%|██        | 104/500 [47:50<2:49:48, 25.73s/it] 21%|██        | 105/500 [48:20<2:56:53, 26.87s/it] 21%|██        | 106/500 [48:49<3:00:07, 27.43s/it] 21%|██▏       | 107/500 [49:12<2:52:11, 26.29s/it] 22%|██▏       | 108/500 [49:40<2:54:37, 26.73s/it] 22%|██▏       | 109/500 [50:07<2:54:07, 26.72s/it] 22%|██▏       | 110/500 [50:32<2:50:48, 26.28s/it] 22%|██▏       | 111/500 [50:59<2:52:04, 26.54s/it] 22%|██▏       | 112/500 [51:27<2:54:21, 26.96s/it] 22%|██▏       | 112/500 [51:27<2:58:16, 27.57s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.033 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.142 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆██▁▃▅▆▇▇▇▇▇▇█▇███▇███▇███████████▇███
wandb:     train_loss ▄▅▄▄▁█▄▅▃▃▄▄▄█▃▆▃▄▄▆▄▂▂▂▅▄▁▂▂▂▄▂▁▃▂▂▂▄▄▃
wandb:   val_accuracy ▂▆█▃▄▁▁▄▅▅▄▃▄▃▄▄▃▃▃▄▃▃▄▄▄▃▄▄▃▃▃▄▃▃▃▄▃▄▄▄
wandb:       val_loss ▃▃▃▂▃█▃▃▂▂▃▃▂▂▃▂▃▃▂▂▃▂▃▂▄▄▃▂▂▂▂▃▃▂▂▁▃▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 111
wandb:  learning_rate 0.0
wandb: train_accuracy 0.72363
wandb:     train_loss 0.57851
wandb:   val_accuracy 0.46222
wandb:       val_loss 1.61252
wandb: 
wandb: 🚀 View run vocal-rain-66 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/4vc5sqcc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_170126-4vc5sqcc/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_175331-ry10h5x3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sea-67
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ry10h5x3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:41, 26.42s/it]  0%|          | 2/500 [00:51<3:31:16, 25.46s/it]  1%|          | 3/500 [01:15<3:26:25, 24.92s/it]  1%|          | 4/500 [01:41<3:28:11, 25.18s/it]  1%|          | 5/500 [02:08<3:34:07, 25.95s/it]  1%|          | 6/500 [02:35<3:35:40, 26.20s/it]  1%|▏         | 7/500 [03:02<3:39:26, 26.71s/it]  2%|▏         | 8/500 [03:26<3:30:10, 25.63s/it]  2%|▏         | 9/500 [03:54<3:37:34, 26.59s/it]  2%|▏         | 10/500 [04:18<3:30:04, 25.72s/it]  2%|▏         | 11/500 [04:47<3:36:19, 26.54s/it]  2%|▏         | 12/500 [05:13<3:35:22, 26.48s/it]  3%|▎         | 13/500 [05:41<3:37:52, 26.84s/it]  3%|▎         | 14/500 [06:11<3:45:40, 27.86s/it]  3%|▎         | 15/500 [06:34<3:35:03, 26.60s/it]  3%|▎         | 16/500 [07:02<3:37:41, 26.99s/it]  3%|▎         | 17/500 [07:26<3:29:38, 26.04s/it]  4%|▎         | 18/500 [07:53<3:32:13, 26.42s/it]  4%|▍         | 19/500 [08:22<3:36:28, 27.00s/it]  4%|▍         | 20/500 [08:46<3:29:04, 26.13s/it]  4%|▍         | 21/500 [09:16<3:36:57, 27.18s/it]  4%|▍         | 22/500 [09:42<3:35:11, 27.01s/it]  5%|▍         | 23/500 [10:06<3:26:31, 25.98s/it]  5%|▍         | 24/500 [10:36<3:36:19, 27.27s/it]  5%|▌         | 25/500 [11:00<3:28:57, 26.40s/it]  5%|▌         | 25/500 [11:00<3:29:16, 26.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.132 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▄▆▆▇▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▆▅▅▅▅▆▂▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▂▂▃▂▃▃▄▃▃▄▄██▇
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▆▄▃▅▃▃▃▃▃▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.6003
wandb:     train_loss 1.14324
wandb:   val_accuracy 0.39333
wandb:       val_loss 1.10802
wandb: 
wandb: 🚀 View run restful-sea-67 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ry10h5x3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_175331-ry10h5x3/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_180512-0seet9wv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-resonance-68
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/0seet9wv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:42:34, 26.76s/it]  0%|          | 2/500 [00:51<3:32:51, 25.65s/it]  1%|          | 3/500 [01:16<3:29:16, 25.26s/it]  1%|          | 4/500 [01:42<3:29:53, 25.39s/it]  1%|          | 5/500 [02:11<3:42:47, 27.00s/it]  1%|          | 6/500 [02:38<3:40:05, 26.73s/it]  1%|▏         | 7/500 [03:03<3:37:25, 26.46s/it]  2%|▏         | 8/500 [03:29<3:34:56, 26.21s/it]  2%|▏         | 9/500 [03:58<3:40:18, 26.92s/it]  2%|▏         | 10/500 [04:22<3:33:20, 26.12s/it]  2%|▏         | 11/500 [04:56<3:52:37, 28.54s/it]  2%|▏         | 12/500 [05:22<3:45:40, 27.75s/it]  3%|▎         | 13/500 [05:52<3:50:06, 28.35s/it]  3%|▎         | 14/500 [06:19<3:47:33, 28.09s/it]  3%|▎         | 15/500 [06:46<3:44:02, 27.72s/it]  3%|▎         | 16/500 [07:16<3:48:04, 28.27s/it]  3%|▎         | 17/500 [07:43<3:45:04, 27.96s/it]  4%|▎         | 18/500 [08:14<3:51:40, 28.84s/it]  4%|▍         | 19/500 [08:39<3:42:25, 27.74s/it]  4%|▍         | 20/500 [09:08<3:45:32, 28.19s/it]  4%|▍         | 21/500 [09:34<3:40:14, 27.59s/it]  4%|▍         | 22/500 [10:04<3:45:45, 28.34s/it]  5%|▍         | 23/500 [10:30<3:39:41, 27.63s/it]  5%|▍         | 24/500 [11:02<3:48:38, 28.82s/it]  5%|▌         | 25/500 [11:26<3:37:46, 27.51s/it]  5%|▌         | 26/500 [12:01<3:53:33, 29.56s/it]  5%|▌         | 27/500 [12:27<3:44:44, 28.51s/it]  6%|▌         | 28/500 [13:00<3:56:01, 30.00s/it]  6%|▌         | 29/500 [13:26<3:46:08, 28.81s/it]  6%|▌         | 30/500 [13:54<3:42:18, 28.38s/it]  6%|▌         | 31/500 [14:23<3:42:54, 28.52s/it]  6%|▋         | 32/500 [14:51<3:41:08, 28.35s/it]  7%|▋         | 33/500 [15:16<3:35:03, 27.63s/it]  7%|▋         | 34/500 [15:43<3:32:18, 27.34s/it]  7%|▋         | 35/500 [16:09<3:29:32, 27.04s/it]  7%|▋         | 36/500 [16:37<3:29:05, 27.04s/it]  7%|▋         | 37/500 [17:06<3:35:08, 27.88s/it]  8%|▊         | 38/500 [17:33<3:31:57, 27.53s/it]  8%|▊         | 39/500 [18:06<3:44:39, 29.24s/it]  8%|▊         | 40/500 [18:32<3:35:09, 28.06s/it]  8%|▊         | 41/500 [19:03<3:41:37, 28.97s/it]  8%|▊         | 42/500 [19:30<3:37:10, 28.45s/it]  9%|▊         | 43/500 [19:59<3:37:58, 28.62s/it]  9%|▉         | 44/500 [20:28<3:37:45, 28.65s/it]  9%|▉         | 45/500 [20:56<3:35:35, 28.43s/it]  9%|▉         | 46/500 [21:24<3:34:04, 28.29s/it]  9%|▉         | 47/500 [21:49<3:26:38, 27.37s/it] 10%|▉         | 48/500 [22:22<3:39:04, 29.08s/it] 10%|▉         | 49/500 [22:46<3:28:12, 27.70s/it] 10%|█         | 50/500 [23:16<3:32:11, 28.29s/it] 10%|█         | 51/500 [23:47<3:38:02, 29.14s/it] 10%|█         | 52/500 [24:13<3:31:19, 28.30s/it] 11%|█         | 53/500 [24:39<3:24:24, 27.44s/it] 11%|█         | 54/500 [25:08<3:28:26, 28.04s/it] 11%|█         | 55/500 [25:34<3:23:46, 27.47s/it] 11%|█         | 56/500 [26:01<3:20:20, 27.07s/it] 11%|█▏        | 57/500 [26:29<3:21:56, 27.35s/it] 11%|█▏        | 57/500 [26:29<3:25:50, 27.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.139 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▂▂▂▁▇█▆▄█▆▅██████████████████████████
wandb:     train_loss █▅▅▅▆█▂▁▁▁▁▁▅▁▁▁▅▁▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁
wandb:   val_accuracy ▁▁▁▁▂▁▂▇▇▆▅█▅▅▇▇█▇█▇▇██▇▇█▇▇████▇▇██▇██▇
wandb:       val_loss ▃▃▃▃▃▂▄▄▃▃▅▃▃▄▃▃▄▇▃▃▃▂▁█▁▂▂▆▁▄▄▁▂▁▄▃▅▃▁▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.68945
wandb:     train_loss 0.00047
wandb:   val_accuracy 0.49333
wandb:       val_loss 3.40114
wandb: 
wandb: 🚀 View run wobbly-resonance-68 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/0seet9wv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_180512-0seet9wv/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_183228-v67ncw4t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-wind-69
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/v67ncw4t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:52, 26.44s/it]  0%|          | 2/500 [00:50<3:26:17, 24.85s/it]  1%|          | 3/500 [01:14<3:24:09, 24.65s/it]  1%|          | 4/500 [01:38<3:21:07, 24.33s/it]  1%|          | 5/500 [02:07<3:34:02, 25.94s/it]  1%|          | 6/500 [02:33<3:35:08, 26.13s/it]  1%|▏         | 7/500 [03:00<3:36:15, 26.32s/it]  2%|▏         | 8/500 [03:30<3:45:38, 27.52s/it]  2%|▏         | 9/500 [03:56<3:40:28, 26.94s/it]  2%|▏         | 10/500 [04:27<3:51:45, 28.38s/it]  2%|▏         | 11/500 [04:52<3:41:10, 27.14s/it]  2%|▏         | 12/500 [05:20<3:42:35, 27.37s/it]  3%|▎         | 13/500 [05:44<3:36:16, 26.65s/it]  3%|▎         | 14/500 [06:13<3:40:49, 27.26s/it]  3%|▎         | 15/500 [06:39<3:37:19, 26.89s/it]  3%|▎         | 16/500 [07:12<3:52:21, 28.80s/it]  3%|▎         | 17/500 [07:38<3:44:49, 27.93s/it]  4%|▎         | 18/500 [08:05<3:41:46, 27.61s/it]  4%|▍         | 19/500 [08:32<3:38:59, 27.32s/it]  4%|▍         | 20/500 [09:02<3:45:32, 28.19s/it]  4%|▍         | 21/500 [09:29<3:42:19, 27.85s/it]  4%|▍         | 22/500 [09:53<3:32:33, 26.68s/it]  5%|▍         | 23/500 [10:20<3:32:02, 26.67s/it]  5%|▍         | 24/500 [10:51<3:43:11, 28.13s/it]  5%|▌         | 25/500 [11:22<3:47:46, 28.77s/it]  5%|▌         | 26/500 [11:46<3:38:04, 27.60s/it]  5%|▌         | 27/500 [12:11<3:31:26, 26.82s/it]  6%|▌         | 28/500 [12:41<3:36:30, 27.52s/it]  6%|▌         | 29/500 [13:05<3:28:44, 26.59s/it]  6%|▌         | 30/500 [13:33<3:31:22, 26.98s/it]  6%|▌         | 31/500 [13:59<3:28:20, 26.65s/it]  6%|▋         | 32/500 [14:26<3:29:40, 26.88s/it]  7%|▋         | 33/500 [14:51<3:24:02, 26.22s/it]  7%|▋         | 34/500 [15:17<3:24:04, 26.27s/it]  7%|▋         | 35/500 [15:44<3:25:33, 26.52s/it]  7%|▋         | 36/500 [16:09<3:21:10, 26.01s/it]  7%|▋         | 37/500 [16:37<3:24:14, 26.47s/it]  8%|▊         | 38/500 [17:03<3:24:10, 26.52s/it]  8%|▊         | 39/500 [17:32<3:28:33, 27.14s/it]  8%|▊         | 40/500 [18:01<3:33:35, 27.86s/it]  8%|▊         | 41/500 [18:26<3:24:49, 26.77s/it]  8%|▊         | 42/500 [18:54<3:27:58, 27.25s/it]  9%|▊         | 43/500 [19:21<3:25:57, 27.04s/it]  9%|▉         | 44/500 [19:48<3:26:03, 27.11s/it]  9%|▉         | 45/500 [20:16<3:27:14, 27.33s/it]  9%|▉         | 45/500 [20:16<3:24:58, 27.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.232 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▆▅▆▆▆▇▇▇▇▇▇█▄▇█▇█▇▇███▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▆▆██▇▇▃▅▄▅▄▅▄▁▆▄▅▄▅▆▄▅▅▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃
wandb:       val_loss ▄▄▄▄▄▄▄▃▃▄▄▃▄▄▅█▃▂▃▃▃▃▄▃▃▁▇▅▃▄▄▅▄▃▅▅▅▅▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.45022
wandb:     train_loss 0.71079
wandb:   val_accuracy 0.44444
wandb:       val_loss 0.55368
wandb: 
wandb: 🚀 View run fallen-wind-69 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/v67ncw4t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_183228-v67ncw4t/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_185332-kuim8uhh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-silence-70
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kuim8uhh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:52, 25.36s/it]  0%|          | 2/500 [00:49<3:24:34, 24.65s/it]  1%|          | 3/500 [01:14<3:24:08, 24.64s/it]  1%|          | 4/500 [01:39<3:25:00, 24.80s/it]  1%|          | 5/500 [02:06<3:32:12, 25.72s/it]  1%|          | 6/500 [02:32<3:31:27, 25.68s/it]  1%|▏         | 7/500 [02:57<3:30:24, 25.61s/it]  2%|▏         | 8/500 [03:32<3:54:59, 28.66s/it]  2%|▏         | 9/500 [03:57<3:43:47, 27.35s/it]  2%|▏         | 10/500 [04:22<3:37:12, 26.60s/it]  2%|▏         | 11/500 [04:55<3:52:46, 28.56s/it]  2%|▏         | 12/500 [05:19<3:42:06, 27.31s/it]  3%|▎         | 13/500 [05:50<3:51:23, 28.51s/it]  3%|▎         | 14/500 [06:18<3:47:31, 28.09s/it]  3%|▎         | 15/500 [06:42<3:39:14, 27.12s/it]  3%|▎         | 16/500 [07:14<3:49:18, 28.43s/it]  3%|▎         | 17/500 [07:39<3:40:11, 27.35s/it]  4%|▎         | 18/500 [08:12<3:54:08, 29.15s/it]  4%|▍         | 19/500 [08:40<3:50:34, 28.76s/it]  4%|▍         | 20/500 [09:04<3:39:12, 27.40s/it]  4%|▍         | 21/500 [09:32<3:40:52, 27.67s/it]  4%|▍         | 22/500 [09:58<3:34:31, 26.93s/it]  5%|▍         | 23/500 [10:24<3:32:02, 26.67s/it]  5%|▍         | 24/500 [10:48<3:27:05, 26.10s/it]  5%|▌         | 25/500 [11:15<3:27:33, 26.22s/it]  5%|▌         | 25/500 [11:15<3:33:56, 27.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▄▆▆▆▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▅▅▅▅▆▂▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▃▃▃▂▃▄▄█▇▇
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▆▄▃▅▃▃▃▃▃▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.59584
wandb:     train_loss 1.14367
wandb:   val_accuracy 0.39333
wandb:       val_loss 1.10885
wandb: 
wandb: 🚀 View run upbeat-silence-70 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kuim8uhh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_185332-kuim8uhh/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_190533-hpyw71bi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-armadillo-71
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/hpyw71bi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:27:09, 24.91s/it]  0%|          | 2/500 [00:49<3:27:21, 24.98s/it]  1%|          | 3/500 [01:14<3:25:56, 24.86s/it]  1%|          | 4/500 [01:39<3:23:51, 24.66s/it]  1%|          | 5/500 [02:10<3:44:32, 27.22s/it]  1%|          | 6/500 [02:38<3:45:42, 27.41s/it]  1%|▏         | 7/500 [03:06<3:47:43, 27.71s/it]  2%|▏         | 8/500 [03:32<3:41:43, 27.04s/it]  2%|▏         | 9/500 [03:58<3:38:18, 26.68s/it]  2%|▏         | 10/500 [04:22<3:30:42, 25.80s/it]  2%|▏         | 11/500 [04:51<3:39:54, 26.98s/it]  2%|▏         | 12/500 [05:16<3:33:12, 26.21s/it]  3%|▎         | 13/500 [05:44<3:37:14, 26.77s/it]  3%|▎         | 14/500 [06:09<3:33:13, 26.32s/it]  3%|▎         | 15/500 [06:33<3:26:54, 25.60s/it]  3%|▎         | 16/500 [07:04<3:40:11, 27.30s/it]  3%|▎         | 17/500 [07:28<3:31:59, 26.33s/it]  4%|▎         | 18/500 [07:59<3:42:35, 27.71s/it]  4%|▍         | 19/500 [08:24<3:34:43, 26.78s/it]  4%|▍         | 20/500 [08:49<3:29:20, 26.17s/it]  4%|▍         | 21/500 [09:19<3:38:30, 27.37s/it]  4%|▍         | 22/500 [09:43<3:30:04, 26.37s/it]  5%|▍         | 23/500 [10:13<3:37:24, 27.35s/it]  5%|▍         | 24/500 [10:41<3:39:03, 27.61s/it]  5%|▍         | 24/500 [10:41<3:31:59, 26.72s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.027 MB uploadedwandb: / 0.010 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▁▄▃▃▆▂▃▆▇█▆██▇██████
wandb:     train_loss ▄▃▂▃▁▁▅▁▂▁▁▁▁▁▂▁▁█▁▁▁▁▁▁
wandb:   val_accuracy ▁█▂▃▁▄▃▇▆▃▅▅▇█▇██▇▇▆██▇▇
wandb:       val_loss ▂▂▂▂█▃▄▁▄▃▄▅▃▂█▂▃▄▁▄▂▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 23
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.97325
wandb:     train_loss 0.0
wandb:   val_accuracy 0.56667
wandb:       val_loss 1.57083
wandb: 
wandb: 🚀 View run apricot-armadillo-71 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/hpyw71bi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_190533-hpyw71bi/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_191703-w9zips71
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-snowflake-72
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w9zips71
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:00, 26.21s/it]  0%|          | 2/500 [00:50<3:25:44, 24.79s/it]  1%|          | 3/500 [01:14<3:23:10, 24.53s/it]  1%|          | 4/500 [01:38<3:22:24, 24.48s/it]  1%|          | 5/500 [02:05<3:30:05, 25.47s/it]  1%|          | 6/500 [02:31<3:30:49, 25.61s/it]  1%|▏         | 7/500 [03:00<3:38:19, 26.57s/it]  2%|▏         | 8/500 [03:24<3:31:17, 25.77s/it]  2%|▏         | 9/500 [03:52<3:36:56, 26.51s/it]  2%|▏         | 10/500 [04:20<3:39:42, 26.90s/it]  2%|▏         | 11/500 [04:44<3:31:46, 25.99s/it]  2%|▏         | 12/500 [05:13<3:40:44, 27.14s/it]  3%|▎         | 13/500 [05:37<3:32:28, 26.18s/it]  3%|▎         | 14/500 [06:08<3:43:22, 27.58s/it]  3%|▎         | 15/500 [06:32<3:34:22, 26.52s/it]  3%|▎         | 16/500 [07:01<3:39:40, 27.23s/it]  3%|▎         | 17/500 [07:28<3:39:21, 27.25s/it]  4%|▎         | 18/500 [07:53<3:33:26, 26.57s/it]  4%|▍         | 19/500 [08:23<3:41:19, 27.61s/it]  4%|▍         | 20/500 [08:51<3:41:51, 27.73s/it]  4%|▍         | 21/500 [09:19<3:40:12, 27.58s/it]  4%|▍         | 22/500 [09:43<3:31:38, 26.57s/it]  5%|▍         | 23/500 [10:12<3:36:10, 27.19s/it]  5%|▍         | 24/500 [10:37<3:32:18, 26.76s/it]  5%|▌         | 25/500 [11:02<3:25:54, 26.01s/it]  5%|▌         | 26/500 [11:31<3:33:25, 27.02s/it]  5%|▌         | 27/500 [11:55<3:27:04, 26.27s/it]  6%|▌         | 28/500 [12:25<3:33:55, 27.19s/it]  6%|▌         | 29/500 [12:49<3:26:59, 26.37s/it]  6%|▌         | 30/500 [13:21<3:40:01, 28.09s/it]  6%|▌         | 31/500 [13:45<3:29:34, 26.81s/it]  6%|▋         | 32/500 [14:18<3:42:12, 28.49s/it]  7%|▋         | 33/500 [14:42<3:31:32, 27.18s/it]  7%|▋         | 34/500 [15:11<3:35:46, 27.78s/it]  7%|▋         | 35/500 [15:38<3:32:36, 27.43s/it]  7%|▋         | 36/500 [16:05<3:31:35, 27.36s/it]  7%|▋         | 37/500 [16:32<3:31:18, 27.38s/it]  8%|▊         | 38/500 [17:01<3:34:33, 27.86s/it]  8%|▊         | 39/500 [17:33<3:42:13, 28.92s/it]  8%|▊         | 40/500 [17:57<3:31:14, 27.55s/it]  8%|▊         | 41/500 [18:26<3:33:44, 27.94s/it]  8%|▊         | 42/500 [18:55<3:36:19, 28.34s/it]  9%|▊         | 43/500 [19:19<3:25:18, 26.95s/it]  9%|▉         | 44/500 [19:48<3:30:19, 27.68s/it]  9%|▉         | 45/500 [20:15<3:29:15, 27.59s/it]  9%|▉         | 46/500 [20:43<3:28:10, 27.51s/it]  9%|▉         | 47/500 [21:07<3:20:03, 26.50s/it] 10%|▉         | 48/500 [21:32<3:17:25, 26.21s/it] 10%|▉         | 49/500 [22:01<3:22:20, 26.92s/it] 10%|█         | 50/500 [22:26<3:17:33, 26.34s/it] 10%|█         | 51/500 [22:54<3:20:07, 26.74s/it] 10%|█         | 52/500 [23:22<3:24:11, 27.35s/it] 11%|█         | 53/500 [23:46<3:15:33, 26.25s/it] 11%|█         | 54/500 [24:14<3:18:43, 26.73s/it] 11%|█         | 55/500 [24:38<3:12:33, 25.96s/it] 11%|█         | 56/500 [25:10<3:24:42, 27.66s/it] 11%|█▏        | 57/500 [25:34<3:16:16, 26.58s/it] 12%|█▏        | 58/500 [26:03<3:20:50, 27.26s/it] 12%|█▏        | 59/500 [26:30<3:20:36, 27.29s/it] 12%|█▏        | 60/500 [26:57<3:19:15, 27.17s/it] 12%|█▏        | 61/500 [27:29<3:30:20, 28.75s/it] 12%|█▏        | 62/500 [27:53<3:19:15, 27.30s/it] 13%|█▎        | 63/500 [28:22<3:22:03, 27.74s/it] 13%|█▎        | 64/500 [28:46<3:13:32, 26.63s/it] 13%|█▎        | 65/500 [29:16<3:20:23, 27.64s/it] 13%|█▎        | 65/500 [29:16<3:15:56, 27.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▃▄▄▅▆▅▆▇▇▇▇▇▇▇▇▇▇▇████████▇████▇███▇█
wandb:     train_loss ▄▄█▄▄▄▅▄▄▄▄▁▅▆▃█▅▃▁▂▆▃▂▇▃▁▄▄▄▆▄▁▄▅▄▇▃▄▂▅
wandb:   val_accuracy ▂▂▁▂▁▂▃▅▃▆▆▆▆▇▇▇▆▇▇▆▆▇▇▅▇██▇▆▆▇▇▇▇▇▇▆▇▇▇
wandb:       val_loss ▅▅▃▄▄▃▄▅██▆▄▂▃▄▅▂▄▄▃▂▆▄▂█▃▂▂▄▅▇▁▂█▅▄█▂▇▃
wandb: 
wandb: Run summary:
wandb:          epoch 64
wandb:  learning_rate 0.0
wandb: train_accuracy 0.72214
wandb:     train_loss 1.17923
wandb:   val_accuracy 0.48444
wandb:       val_loss 0.81653
wandb: 
wandb: 🚀 View run woven-snowflake-72 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w9zips71
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_191703-w9zips71/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_194712-8vbv7jhg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-snow-73
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8vbv7jhg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:21, 25.29s/it]  0%|          | 2/500 [00:51<3:35:40, 25.98s/it]  1%|          | 3/500 [01:16<3:29:55, 25.34s/it]  1%|          | 4/500 [01:40<3:27:01, 25.04s/it]  1%|          | 5/500 [02:09<3:38:01, 26.43s/it]  1%|          | 6/500 [02:38<3:43:20, 27.13s/it]  1%|▏         | 7/500 [03:10<3:56:52, 28.83s/it]  2%|▏         | 8/500 [03:34<3:44:12, 27.34s/it]  2%|▏         | 9/500 [04:02<3:44:56, 27.49s/it]  2%|▏         | 10/500 [04:30<3:45:32, 27.62s/it]  2%|▏         | 11/500 [04:57<3:42:49, 27.34s/it]  2%|▏         | 12/500 [05:23<3:38:33, 26.87s/it]  3%|▎         | 13/500 [05:48<3:33:35, 26.32s/it]  3%|▎         | 14/500 [06:16<3:39:35, 27.11s/it]  3%|▎         | 15/500 [06:42<3:35:32, 26.67s/it]  3%|▎         | 16/500 [07:15<3:50:16, 28.55s/it]  3%|▎         | 17/500 [07:39<3:39:48, 27.31s/it]  4%|▎         | 18/500 [08:11<3:50:45, 28.72s/it]  4%|▍         | 19/500 [08:43<3:56:43, 29.53s/it]  4%|▍         | 20/500 [09:07<3:43:52, 27.98s/it]  4%|▍         | 21/500 [09:33<3:37:56, 27.30s/it]  4%|▍         | 22/500 [10:03<3:43:16, 28.03s/it]  5%|▍         | 23/500 [10:27<3:34:43, 27.01s/it]  5%|▍         | 24/500 [10:54<3:32:59, 26.85s/it]  5%|▌         | 25/500 [11:22<3:34:56, 27.15s/it]  5%|▌         | 25/500 [11:22<3:36:01, 27.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.137 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▅▅▆▇▇▇██
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▅▅▅▅▆▃▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▂▁▁▃▃▄▂▅▂▄▇▇▇▇█
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▆▄▃▅▃▃▃▃▃▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.57801
wandb:     train_loss 1.14525
wandb:   val_accuracy 0.38222
wandb:       val_loss 1.11504
wandb: 
wandb: 🚀 View run peachy-snow-73 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8vbv7jhg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_194712-8vbv7jhg/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_195916-l8fklpjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-butterfly-74
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/l8fklpjt
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:11, 26.24s/it]  0%|          | 2/500 [00:50<3:28:41, 25.14s/it]  1%|          | 3/500 [01:14<3:22:14, 24.41s/it]  1%|          | 4/500 [01:37<3:19:21, 24.12s/it]  1%|          | 5/500 [02:06<3:32:10, 25.72s/it]  1%|          | 6/500 [02:32<3:33:46, 25.96s/it]  1%|▏         | 7/500 [03:00<3:38:11, 26.55s/it]  2%|▏         | 8/500 [03:24<3:30:14, 25.64s/it]  2%|▏         | 9/500 [03:53<3:38:31, 26.70s/it]  2%|▏         | 10/500 [04:17<3:31:12, 25.86s/it]  2%|▏         | 11/500 [04:47<3:42:46, 27.33s/it]  2%|▏         | 12/500 [05:11<3:32:38, 26.14s/it]  3%|▎         | 13/500 [05:40<3:39:35, 27.05s/it]  3%|▎         | 14/500 [06:04<3:31:07, 26.06s/it]  3%|▎         | 15/500 [06:32<3:36:12, 26.75s/it]  3%|▎         | 16/500 [06:57<3:30:41, 26.12s/it]  3%|▎         | 17/500 [07:25<3:34:58, 26.71s/it]  4%|▎         | 18/500 [07:52<3:36:14, 26.92s/it]  4%|▍         | 19/500 [08:19<3:35:02, 26.82s/it]  4%|▍         | 20/500 [08:44<3:29:23, 26.17s/it]  4%|▍         | 21/500 [09:12<3:33:18, 26.72s/it]  4%|▍         | 22/500 [09:37<3:30:40, 26.44s/it]  5%|▍         | 23/500 [10:04<3:30:12, 26.44s/it]  5%|▍         | 24/500 [10:39<3:51:00, 29.12s/it]  5%|▌         | 25/500 [11:04<3:39:35, 27.74s/it]  5%|▌         | 26/500 [11:36<3:49:56, 29.11s/it]  5%|▌         | 27/500 [12:00<3:37:58, 27.65s/it]  6%|▌         | 28/500 [12:28<3:38:29, 27.77s/it]  6%|▌         | 29/500 [12:56<3:36:55, 27.63s/it]  6%|▌         | 30/500 [13:20<3:29:44, 26.78s/it]  6%|▌         | 30/500 [13:20<3:29:07, 26.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▃▁▂▄▁▁▁▃▆▅▆▅▃▆▄▄▇▅▇█▅▆▇▅▇▆▆█
wandb:     train_loss ▂▂▂▅▁▅▇▁▄▁▁▁▁▇█▁▃▁▁▅▁▁▁▁▁▁▁▁█▁
wandb:   val_accuracy ▁█▁▁▁▅▁▁▂▃▇▆▇▅▁▇▇▅▇▆█▇▇▇▆▆▇▇█▇
wandb:       val_loss ▂▂▂▄▄▄▇▃█▃▂▃▃▁▇▁▁▃▂▃▃▁▃▃▅▄▂▃▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.96731
wandb:     train_loss 0.0
wandb:   val_accuracy 0.56667
wandb:       val_loss 3.68399
wandb: 
wandb: 🚀 View run swift-butterfly-74 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/l8fklpjt
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_195916-l8fklpjt/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_201316-uhxupgjm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-aardvark-75
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uhxupgjm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:35, 25.68s/it]  0%|          | 2/500 [00:49<3:23:52, 24.56s/it]  1%|          | 3/500 [01:13<3:21:16, 24.30s/it]  1%|          | 4/500 [01:37<3:20:15, 24.22s/it]  1%|          | 5/500 [02:10<3:45:54, 27.38s/it]  1%|          | 6/500 [02:34<3:36:14, 26.26s/it]  1%|▏         | 7/500 [03:05<3:47:47, 27.72s/it]  2%|▏         | 8/500 [03:32<3:46:03, 27.57s/it]  2%|▏         | 9/500 [03:58<3:41:51, 27.11s/it]  2%|▏         | 10/500 [04:25<3:41:15, 27.09s/it]  2%|▏         | 11/500 [04:55<3:48:06, 27.99s/it]  2%|▏         | 12/500 [05:20<3:38:55, 26.92s/it]  3%|▎         | 13/500 [05:49<3:43:04, 27.48s/it]  3%|▎         | 14/500 [06:13<3:34:14, 26.45s/it]  3%|▎         | 15/500 [06:46<3:49:46, 28.43s/it]  3%|▎         | 16/500 [07:12<3:44:35, 27.84s/it]  3%|▎         | 17/500 [07:36<3:35:44, 26.80s/it]  4%|▎         | 18/500 [08:14<4:02:26, 30.18s/it]  4%|▍         | 19/500 [08:38<3:46:57, 28.31s/it]  4%|▍         | 20/500 [09:03<3:36:40, 27.08s/it]  4%|▍         | 21/500 [09:35<3:49:21, 28.73s/it]  4%|▍         | 22/500 [10:04<3:48:12, 28.65s/it]  5%|▍         | 23/500 [10:32<3:46:59, 28.55s/it]  5%|▍         | 24/500 [11:00<3:46:10, 28.51s/it]  5%|▌         | 25/500 [11:26<3:37:55, 27.53s/it]  5%|▌         | 26/500 [11:55<3:42:04, 28.11s/it]  5%|▌         | 27/500 [12:19<3:30:51, 26.75s/it]  6%|▌         | 28/500 [12:46<3:30:53, 26.81s/it]  6%|▌         | 29/500 [13:15<3:35:47, 27.49s/it]  6%|▌         | 30/500 [13:42<3:34:33, 27.39s/it]  6%|▌         | 31/500 [14:10<3:35:35, 27.58s/it]  6%|▋         | 32/500 [14:34<3:26:44, 26.50s/it]  7%|▋         | 33/500 [15:02<3:29:42, 26.94s/it]  7%|▋         | 34/500 [15:27<3:23:55, 26.26s/it]  7%|▋         | 35/500 [15:57<3:33:12, 27.51s/it]  7%|▋         | 36/500 [16:21<3:23:37, 26.33s/it]  7%|▋         | 37/500 [16:48<3:25:14, 26.60s/it]  8%|▊         | 38/500 [17:12<3:18:10, 25.74s/it]  8%|▊         | 39/500 [17:40<3:23:19, 26.46s/it]  8%|▊         | 40/500 [18:05<3:19:26, 26.01s/it]  8%|▊         | 41/500 [18:30<3:18:38, 25.97s/it]  8%|▊         | 42/500 [18:57<3:19:46, 26.17s/it]  9%|▊         | 43/500 [19:21<3:14:00, 25.47s/it]  9%|▉         | 44/500 [19:53<3:29:33, 27.57s/it]  9%|▉         | 45/500 [20:17<3:19:10, 26.27s/it]  9%|▉         | 46/500 [20:47<3:28:22, 27.54s/it]  9%|▉         | 47/500 [21:11<3:19:35, 26.44s/it] 10%|▉         | 48/500 [21:42<3:29:13, 27.77s/it] 10%|▉         | 49/500 [22:05<3:19:15, 26.51s/it] 10%|█         | 50/500 [22:34<3:24:03, 27.21s/it] 10%|█         | 51/500 [23:01<3:23:15, 27.16s/it] 10%|█         | 52/500 [23:25<3:15:02, 26.12s/it] 11%|█         | 53/500 [23:58<3:29:57, 28.18s/it] 11%|█         | 54/500 [24:25<3:26:53, 27.83s/it] 11%|█         | 55/500 [24:49<3:18:48, 26.81s/it] 11%|█         | 56/500 [25:16<3:17:49, 26.73s/it] 11%|█▏        | 57/500 [25:41<3:14:06, 26.29s/it] 11%|█▏        | 57/500 [25:41<3:19:42, 27.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▆▅▆▅▅▆▆▆▃▇▆▇▄▇▇█▇▇▇▇▆▇▇▇▇▇██▇▇▇█████▇
wandb:     train_loss ▅▅▆▄▆▃▅▃▃▃▆▅▄▂▆▇▃▄▄▅▅▁▂▂▆▁▆▅▁▃▄▄▄▆█▂▃▄▆▂
wandb:   val_accuracy ▂▁▂▆▆▄▂▄▄▅▅▄▆▆▆▃▆▇▇▇▆▆▇▆██▆▆█▇▇▇▆▇█████▆
wandb:       val_loss ▅▅▄▄▅▅▄▄▄▅▄▆▃▃▄▅▄▆▃▃█▄▄▆▃▂▄▆▁▅▂▃▄▄█▃▃▃▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.75037
wandb:     train_loss 0.34612
wandb:   val_accuracy 0.46667
wandb:       val_loss 1.4712
wandb: 
wandb: 🚀 View run confused-aardvark-75 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uhxupgjm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_201316-uhxupgjm/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_203941-5ba2q7ex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-moon-76
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5ba2q7ex
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:52:10, 27.92s/it]  0%|          | 2/500 [00:51<3:31:23, 25.47s/it]  1%|          | 3/500 [01:17<3:30:41, 25.43s/it]  1%|          | 4/500 [01:41<3:25:58, 24.92s/it]  1%|          | 5/500 [02:08<3:32:42, 25.78s/it]  1%|          | 6/500 [02:33<3:29:39, 25.46s/it]  1%|▏         | 7/500 [02:57<3:25:31, 25.01s/it]  2%|▏         | 8/500 [03:22<3:24:58, 25.00s/it]  2%|▏         | 9/500 [03:50<3:32:24, 25.96s/it]  2%|▏         | 10/500 [04:18<3:38:08, 26.71s/it]  2%|▏         | 11/500 [04:43<3:31:37, 25.97s/it]  2%|▏         | 12/500 [05:13<3:41:29, 27.23s/it]  3%|▎         | 13/500 [05:37<3:34:13, 26.39s/it]  3%|▎         | 14/500 [06:02<3:30:19, 25.97s/it]  3%|▎         | 15/500 [06:26<3:24:41, 25.32s/it]  3%|▎         | 16/500 [06:51<3:22:52, 25.15s/it]  3%|▎         | 17/500 [07:15<3:20:40, 24.93s/it]  4%|▎         | 18/500 [07:39<3:18:19, 24.69s/it]  4%|▍         | 19/500 [08:08<3:27:06, 25.84s/it]  4%|▍         | 20/500 [08:31<3:19:30, 24.94s/it]  4%|▍         | 21/500 [08:57<3:22:52, 25.41s/it]  4%|▍         | 22/500 [09:22<3:21:30, 25.29s/it]  5%|▍         | 23/500 [09:46<3:18:24, 24.96s/it]  5%|▍         | 24/500 [10:11<3:17:37, 24.91s/it]  5%|▌         | 25/500 [10:38<3:22:03, 25.52s/it]  5%|▌         | 25/500 [10:38<3:22:16, 25.55s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▄▅▅▆▇▇█
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▆▅▅▅▆▃▅▅▅▆▄▄▆
wandb:   val_accuracy ▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▃▄▂▄▇▄▅█▇▆
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▇▄▄▆▃▄▄▃▂▄▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.47994
wandb:     train_loss 1.15792
wandb:   val_accuracy 0.35778
wandb:       val_loss 1.13658
wandb: 
wandb: 🚀 View run amber-moon-76 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5ba2q7ex
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_203941-5ba2q7ex/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205059-td9m7az5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-meadow-77
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/td9m7az5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.017 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: \ 0.028 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: 🚀 View run lilac-meadow-77 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/td9m7az5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205059-td9m7az5/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205134-o4euot4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-cherry-78
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o4euot4j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run young-cherry-78 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o4euot4j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205134-o4euot4j/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205207-60sux0um
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-breeze-79
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/60sux0um
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run revived-breeze-79 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/60sux0um
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205207-60sux0um/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205240-o4mguhf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-galaxy-80
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o4mguhf8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:00<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.021 MB of 0.031 MB uploadedwandb: \ 0.021 MB of 0.031 MB uploadedwandb: | 0.021 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.025 MB of 0.031 MB uploadedwandb: 🚀 View run olive-galaxy-80 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/o4mguhf8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205240-o4mguhf8/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205315-mqgbqle4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-bush-81
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mqgbqle4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.031 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fancy-bush-81 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mqgbqle4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205315-mqgbqle4/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205352-8e7p4axd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-dragon-82
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8e7p4axd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run ethereal-dragon-82 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8e7p4axd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205352-8e7p4axd/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205424-lagtm9cr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-feather-83
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lagtm9cr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run playful-feather-83 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lagtm9cr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205424-lagtm9cr/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205504-le5woqz5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-darkness-84
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/le5woqz5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.011 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run crisp-darkness-84 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/le5woqz5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205504-le5woqz5/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205536-ypdesvfe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-wildflower-85
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ypdesvfe
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.027 MB uploadedwandb: / 0.025 MB of 0.027 MB uploadedwandb: 🚀 View run vague-wildflower-85 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ypdesvfe
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205536-ypdesvfe/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205606-20ukkjxw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-microwave-86
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/20ukkjxw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run chocolate-microwave-86 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/20ukkjxw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205606-20ukkjxw/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205635-yfquljc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-hill-87
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yfquljc0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run breezy-hill-87 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yfquljc0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205635-yfquljc0/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205708-xbornmqo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-violet-88
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xbornmqo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run unique-violet-88 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xbornmqo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205708-xbornmqo/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205743-26ait58p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-meadow-89
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/26ait58p
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.019 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run scarlet-meadow-89 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/26ait58p
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205743-26ait58p/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205816-6sadrmpc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sea-90
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6sadrmpc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run expert-sea-90 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6sadrmpc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205816-6sadrmpc/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205849-01fl933h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-night-91
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/01fl933h
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run deep-night-91 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/01fl933h
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205849-01fl933h/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_205923-nvfr8fu6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-resonance-92
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nvfr8fu6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:51:42, 27.86s/it]  0%|          | 2/500 [00:51<3:32:24, 25.59s/it]  1%|          | 3/500 [01:15<3:24:35, 24.70s/it]  1%|          | 4/500 [01:39<3:20:54, 24.30s/it]  1%|          | 5/500 [02:02<3:18:46, 24.09s/it]  1%|          | 6/500 [02:27<3:20:11, 24.32s/it]  1%|▏         | 7/500 [03:00<3:41:42, 26.98s/it]  2%|▏         | 8/500 [03:28<3:45:00, 27.44s/it]  2%|▏         | 9/500 [03:55<3:43:47, 27.35s/it]  2%|▏         | 10/500 [04:24<3:46:14, 27.70s/it]  2%|▏         | 11/500 [04:48<3:37:30, 26.69s/it]  2%|▏         | 12/500 [05:20<3:48:45, 28.13s/it]  3%|▎         | 13/500 [05:44<3:39:17, 27.02s/it]  3%|▎         | 14/500 [06:16<3:51:25, 28.57s/it]  3%|▎         | 15/500 [06:41<3:41:00, 27.34s/it]  3%|▎         | 16/500 [07:10<3:44:35, 27.84s/it]  3%|▎         | 17/500 [07:33<3:33:04, 26.47s/it]  4%|▎         | 18/500 [08:05<3:46:51, 28.24s/it]  4%|▍         | 19/500 [08:31<3:39:10, 27.34s/it]  4%|▍         | 20/500 [08:59<3:41:12, 27.65s/it]  4%|▍         | 21/500 [09:26<3:40:04, 27.57s/it]  4%|▍         | 22/500 [09:51<3:32:13, 26.64s/it]  5%|▍         | 23/500 [10:19<3:36:13, 27.20s/it]  5%|▍         | 24/500 [10:46<3:35:08, 27.12s/it]  5%|▌         | 25/500 [11:14<3:35:17, 27.20s/it]  5%|▌         | 26/500 [11:41<3:36:24, 27.39s/it]  5%|▌         | 27/500 [12:08<3:33:09, 27.04s/it]  6%|▌         | 28/500 [12:31<3:25:15, 26.09s/it]  6%|▌         | 29/500 [13:00<3:29:37, 26.70s/it]  6%|▌         | 30/500 [13:27<3:29:51, 26.79s/it]  6%|▌         | 31/500 [13:53<3:29:33, 26.81s/it]  6%|▋         | 32/500 [14:18<3:24:47, 26.26s/it]  7%|▋         | 33/500 [14:48<3:31:02, 27.11s/it]  7%|▋         | 34/500 [15:12<3:23:27, 26.20s/it]  7%|▋         | 35/500 [15:44<3:38:35, 28.20s/it]  7%|▋         | 36/500 [16:08<3:28:04, 26.91s/it]  7%|▋         | 37/500 [16:41<3:39:54, 28.50s/it]  8%|▊         | 38/500 [17:08<3:37:22, 28.23s/it]  8%|▊         | 39/500 [17:37<3:37:38, 28.33s/it]  8%|▊         | 40/500 [18:01<3:26:55, 26.99s/it]  8%|▊         | 41/500 [18:31<3:33:30, 27.91s/it]  8%|▊         | 42/500 [18:55<3:25:14, 26.89s/it]  9%|▊         | 43/500 [19:29<3:39:43, 28.85s/it]  9%|▉         | 44/500 [20:03<3:51:49, 30.50s/it]  9%|▉         | 45/500 [20:32<3:47:55, 30.06s/it]  9%|▉         | 46/500 [20:56<3:34:43, 28.38s/it]  9%|▉         | 47/500 [21:25<3:33:59, 28.34s/it] 10%|▉         | 48/500 [21:56<3:39:14, 29.10s/it] 10%|▉         | 49/500 [22:19<3:26:44, 27.50s/it] 10%|█         | 50/500 [22:50<3:33:37, 28.48s/it] 10%|█         | 51/500 [23:14<3:23:08, 27.15s/it] 10%|█         | 52/500 [23:47<3:34:58, 28.79s/it] 11%|█         | 53/500 [24:10<3:22:54, 27.24s/it] 11%|█         | 54/500 [24:43<3:33:23, 28.71s/it] 11%|█         | 55/500 [25:06<3:21:52, 27.22s/it] 11%|█         | 56/500 [25:38<3:30:30, 28.45s/it] 11%|█▏        | 57/500 [26:01<3:19:07, 26.97s/it] 11%|█▏        | 57/500 [26:01<3:22:16, 27.40s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.139 MB of 0.311 MB uploadedwandb: / 0.139 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▂▃▂▁▁▅▆▇█▅▇▆█████▇█████████████████████
wandb:     train_loss ▂▂▄▁█▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy █▁▅▁▁▁▅▅▅▇▄▄▅▇▆▅▆▆▄▅▅▆▆▆▅▅▆▆▇▆▆▆█▇▆▆▇██▇
wandb:       val_loss ▂▂▂▅█▆▆▄▄▂▆▂▅▂▁▃▃▃▁▃▃▆▁▅▁▃▁▂▁▄▃▁▁▁▁▃▄▁▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 1.0
wandb:     train_loss 0.00106
wandb:   val_accuracy 0.66444
wandb:       val_loss 2.71701
wandb: 
wandb: 🚀 View run dutiful-resonance-92 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nvfr8fu6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_205923-nvfr8fu6/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_212604-6snfgtwj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-haze-93
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6snfgtwj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:36, 25.68s/it]  0%|          | 2/500 [00:49<3:24:34, 24.65s/it]  1%|          | 3/500 [01:13<3:21:58, 24.38s/it]  1%|          | 4/500 [01:38<3:22:24, 24.48s/it]  1%|          | 5/500 [02:10<3:46:16, 27.43s/it]  1%|          | 6/500 [02:34<3:35:57, 26.23s/it]  1%|▏         | 7/500 [02:59<3:32:10, 25.82s/it]  2%|▏         | 8/500 [03:29<3:42:49, 27.17s/it]  2%|▏         | 9/500 [03:54<3:34:47, 26.25s/it]  2%|▏         | 10/500 [04:22<3:39:14, 26.85s/it]  2%|▏         | 11/500 [04:46<3:31:45, 25.98s/it]  2%|▏         | 12/500 [05:15<3:39:58, 27.05s/it]  3%|▎         | 13/500 [05:40<3:33:41, 26.33s/it]  3%|▎         | 14/500 [06:11<3:45:41, 27.86s/it]  3%|▎         | 15/500 [06:36<3:37:56, 26.96s/it]  3%|▎         | 16/500 [07:06<3:43:27, 27.70s/it]  3%|▎         | 17/500 [07:33<3:41:53, 27.56s/it]  4%|▎         | 18/500 [07:57<3:34:05, 26.65s/it]  4%|▍         | 19/500 [08:26<3:38:37, 27.27s/it]  4%|▍         | 20/500 [08:54<3:39:31, 27.44s/it]  4%|▍         | 21/500 [09:20<3:36:38, 27.14s/it]  4%|▍         | 22/500 [09:50<3:42:28, 27.93s/it]  5%|▍         | 23/500 [10:14<3:32:49, 26.77s/it]  5%|▍         | 24/500 [10:40<3:30:48, 26.57s/it]  5%|▌         | 25/500 [11:06<3:27:13, 26.18s/it]  5%|▌         | 26/500 [11:32<3:26:57, 26.20s/it]  5%|▌         | 27/500 [11:59<3:29:28, 26.57s/it]  6%|▌         | 28/500 [12:28<3:34:03, 27.21s/it]  6%|▌         | 29/500 [12:56<3:36:04, 27.52s/it]  6%|▌         | 30/500 [13:26<3:41:22, 28.26s/it]  6%|▌         | 31/500 [13:51<3:31:35, 27.07s/it]  6%|▋         | 32/500 [14:22<3:40:23, 28.26s/it]  7%|▋         | 33/500 [14:50<3:39:23, 28.19s/it]  7%|▋         | 34/500 [15:19<3:41:24, 28.51s/it]  7%|▋         | 35/500 [15:43<3:30:54, 27.21s/it]  7%|▋         | 36/500 [16:10<3:30:56, 27.28s/it]  7%|▋         | 36/500 [16:10<3:28:35, 26.97s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▅▆▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▃▁▂▂▂▂▂▂▃▃▃▃▄
wandb:     train_loss ▄▄▄▄▃▄▄▂▄▅▂▁▂▄▄▄▄▃▁▄▄▂▂▁▆█▄▅▄▄▄▄▄▃▄▄
wandb:   val_accuracy ▄▄▇▇▇█▇▇▅▅▆▆▇▆▇▆▆▇▇▆▆▇▇▄▁▁▂▃▃▃▃▃▄▄▄▄
wandb:       val_loss ▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▂▃▁▁▁▁▁▁█▂▂▂▂▂▂▂▁▂▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 35
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.3789
wandb:     train_loss 1.02632
wandb:   val_accuracy 0.29778
wandb:       val_loss 1.06296
wandb: 
wandb: 🚀 View run daily-haze-93 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6snfgtwj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_212604-6snfgtwj/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_214305-7bmpgt7v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-yogurt-94
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7bmpgt7v
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:44, 25.22s/it]  0%|          | 2/500 [00:48<3:22:12, 24.36s/it]  1%|          | 3/500 [01:13<3:21:59, 24.38s/it]  1%|          | 4/500 [01:37<3:20:51, 24.30s/it]  1%|          | 5/500 [02:09<3:42:19, 26.95s/it]  1%|          | 6/500 [02:36<3:42:51, 27.07s/it]  1%|▏         | 7/500 [03:02<3:38:13, 26.56s/it]  2%|▏         | 8/500 [03:27<3:35:42, 26.31s/it]  2%|▏         | 9/500 [03:52<3:31:29, 25.84s/it]  2%|▏         | 10/500 [04:23<3:44:03, 27.43s/it]  2%|▏         | 11/500 [04:48<3:37:21, 26.67s/it]  2%|▏         | 12/500 [05:14<3:34:14, 26.34s/it]  3%|▎         | 13/500 [05:39<3:31:53, 26.11s/it]  3%|▎         | 14/500 [06:06<3:32:23, 26.22s/it]  3%|▎         | 15/500 [06:32<3:31:02, 26.11s/it]  3%|▎         | 16/500 [07:01<3:38:35, 27.10s/it]  3%|▎         | 17/500 [07:25<3:31:04, 26.22s/it]  4%|▎         | 18/500 [07:58<3:45:37, 28.09s/it]  4%|▍         | 19/500 [08:22<3:35:55, 26.93s/it]  4%|▍         | 20/500 [08:52<3:44:28, 28.06s/it]  4%|▍         | 21/500 [09:22<3:47:49, 28.54s/it]  4%|▍         | 22/500 [09:49<3:43:41, 28.08s/it]  5%|▍         | 23/500 [10:14<3:36:03, 27.18s/it]  5%|▍         | 24/500 [10:40<3:32:37, 26.80s/it]  5%|▌         | 25/500 [11:05<3:26:55, 26.14s/it]  5%|▌         | 25/500 [11:05<3:30:39, 26.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▅▅▆▆▇▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▆▅▅▅▅▆▃▃▅▄▆▅▃▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▂▁▂▂▃▂▄▃▄▂▃▄▄█▇█
wandb:       val_loss ▄█▂▇▃▃▃▆▁▅▄▂▅▆▅▃▅▃▃▃▃▂▄▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.61218
wandb:     train_loss 1.14232
wandb:   val_accuracy 0.40667
wandb:       val_loss 1.10398
wandb: 
wandb: 🚀 View run northern-yogurt-94 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7bmpgt7v
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_214305-7bmpgt7v/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_215453-xn51gzco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-cosmos-95
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xn51gzco
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:52, 25.35s/it]  0%|          | 2/500 [00:49<3:24:19, 24.62s/it]  1%|          | 3/500 [01:13<3:21:46, 24.36s/it]  1%|          | 4/500 [01:38<3:22:00, 24.44s/it]  1%|          | 5/500 [02:05<3:31:25, 25.63s/it]  1%|          | 6/500 [02:33<3:36:36, 26.31s/it]  1%|▏         | 7/500 [02:59<3:34:38, 26.12s/it]  2%|▏         | 8/500 [03:23<3:29:05, 25.50s/it]  2%|▏         | 9/500 [03:50<3:32:54, 26.02s/it]  2%|▏         | 10/500 [04:22<3:47:42, 27.88s/it]  2%|▏         | 11/500 [04:46<3:38:23, 26.80s/it]  2%|▏         | 12/500 [05:14<3:40:11, 27.07s/it]  3%|▎         | 13/500 [05:42<3:41:12, 27.25s/it]  3%|▎         | 14/500 [06:06<3:33:45, 26.39s/it]  3%|▎         | 15/500 [06:38<3:46:15, 27.99s/it]  3%|▎         | 16/500 [07:01<3:34:36, 26.60s/it]  3%|▎         | 17/500 [07:35<3:51:15, 28.73s/it]  4%|▎         | 18/500 [07:59<3:38:26, 27.19s/it]  4%|▍         | 19/500 [08:28<3:44:29, 28.00s/it]  4%|▍         | 20/500 [08:56<3:44:03, 28.01s/it]  4%|▍         | 21/500 [09:21<3:36:09, 27.08s/it]  4%|▍         | 22/500 [09:53<3:45:34, 28.31s/it]  5%|▍         | 23/500 [10:16<3:34:20, 26.96s/it]  5%|▍         | 24/500 [10:48<3:44:44, 28.33s/it]  5%|▌         | 25/500 [11:15<3:42:31, 28.11s/it]  5%|▌         | 26/500 [11:43<3:41:06, 27.99s/it]  5%|▌         | 27/500 [12:07<3:31:57, 26.89s/it]  6%|▌         | 28/500 [12:38<3:40:52, 28.08s/it]  6%|▌         | 29/500 [13:02<3:30:36, 26.83s/it]  6%|▌         | 30/500 [13:31<3:35:09, 27.47s/it]  6%|▌         | 30/500 [13:31<3:31:57, 27.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▂▃▃▃▁▁▁▁▁▆█▅▇▆▅▅▅▄█▅██▇█████▇█
wandb:     train_loss ▃▂▃▃▁▁▆▁▃▁▁▁▁▃█▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy █▇▅▄▁▁▁▁▁▅▅▂▅▆▂▃▆▁▅▄▇▇▇▇▇▇▆▇█▆
wandb:       val_loss ▂▂▂▁▄▇▇▄█▁▂▆▁▁█▃▁▆▄▃▃▁▂▅▆▁▂▃▁▆
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00013
wandb: train_accuracy 1.0
wandb:     train_loss 0.0
wandb:   val_accuracy 0.60444
wandb:       val_loss 5.86967
wandb: 
wandb: 🚀 View run ethereal-cosmos-95 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xn51gzco
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_215453-xn51gzco/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_220906-yapwyeyy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-valley-96
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yapwyeyy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:19, 25.41s/it]  0%|          | 2/500 [00:49<3:24:26, 24.63s/it]  1%|          | 3/500 [01:13<3:22:25, 24.44s/it]  1%|          | 4/500 [01:37<3:21:14, 24.34s/it]  1%|          | 5/500 [02:09<3:42:50, 27.01s/it]  1%|          | 6/500 [02:35<3:39:05, 26.61s/it]  1%|▏         | 7/500 [03:04<3:44:37, 27.34s/it]  2%|▏         | 8/500 [03:28<3:35:43, 26.31s/it]  2%|▏         | 9/500 [03:58<3:45:46, 27.59s/it]  2%|▏         | 10/500 [04:23<3:36:43, 26.54s/it]  2%|▏         | 11/500 [04:50<3:38:39, 26.83s/it]  2%|▏         | 12/500 [05:18<3:41:00, 27.17s/it]  3%|▎         | 13/500 [05:48<3:48:35, 28.16s/it]  3%|▎         | 14/500 [06:13<3:40:04, 27.17s/it]  3%|▎         | 15/500 [06:40<3:37:40, 26.93s/it]  3%|▎         | 16/500 [07:07<3:38:47, 27.12s/it]  3%|▎         | 17/500 [07:31<3:30:48, 26.19s/it]  4%|▎         | 18/500 [08:03<3:45:03, 28.02s/it]  4%|▍         | 19/500 [08:30<3:41:43, 27.66s/it]  4%|▍         | 20/500 [08:59<3:43:21, 27.92s/it]  4%|▍         | 21/500 [09:27<3:43:38, 28.01s/it]  4%|▍         | 22/500 [09:52<3:36:26, 27.17s/it]  5%|▍         | 23/500 [10:19<3:34:33, 26.99s/it]  5%|▍         | 24/500 [10:48<3:39:07, 27.62s/it]  5%|▌         | 25/500 [11:13<3:32:14, 26.81s/it]  5%|▌         | 26/500 [11:43<3:38:57, 27.72s/it]  5%|▌         | 27/500 [12:09<3:35:31, 27.34s/it]  6%|▌         | 28/500 [12:34<3:28:20, 26.49s/it]  6%|▌         | 29/500 [13:04<3:37:00, 27.64s/it]  6%|▌         | 30/500 [13:28<3:27:47, 26.53s/it]  6%|▌         | 31/500 [13:57<3:33:17, 27.29s/it]  6%|▋         | 32/500 [14:24<3:31:12, 27.08s/it]  7%|▋         | 33/500 [14:51<3:30:36, 27.06s/it]  7%|▋         | 34/500 [15:15<3:24:39, 26.35s/it]  7%|▋         | 35/500 [15:45<3:31:58, 27.35s/it]  7%|▋         | 36/500 [16:10<3:26:30, 26.70s/it]  7%|▋         | 37/500 [16:36<3:24:13, 26.46s/it]  8%|▊         | 38/500 [17:00<3:17:44, 25.68s/it]  8%|▊         | 39/500 [17:30<3:28:19, 27.11s/it]  8%|▊         | 40/500 [17:57<3:27:32, 27.07s/it]  8%|▊         | 41/500 [18:23<3:24:25, 26.72s/it]  8%|▊         | 42/500 [18:48<3:19:00, 26.07s/it]  9%|▊         | 43/500 [19:15<3:21:20, 26.43s/it]  9%|▉         | 44/500 [19:39<3:15:25, 25.71s/it]  9%|▉         | 45/500 [20:08<3:21:09, 26.53s/it]  9%|▉         | 46/500 [20:32<3:16:21, 25.95s/it]  9%|▉         | 47/500 [21:02<3:25:34, 27.23s/it] 10%|▉         | 48/500 [21:28<3:21:34, 26.76s/it] 10%|▉         | 49/500 [21:55<3:21:13, 26.77s/it] 10%|█         | 50/500 [22:20<3:17:54, 26.39s/it] 10%|█         | 51/500 [22:50<3:25:30, 27.46s/it] 10%|█         | 52/500 [23:16<3:20:07, 26.80s/it] 11%|█         | 53/500 [23:41<3:15:58, 26.30s/it] 11%|█         | 54/500 [24:06<3:13:19, 26.01s/it] 11%|█         | 55/500 [24:34<3:16:13, 26.46s/it] 11%|█         | 56/500 [24:59<3:12:55, 26.07s/it] 11%|█▏        | 57/500 [25:25<3:12:57, 26.14s/it] 12%|█▏        | 58/500 [25:51<3:12:47, 26.17s/it] 12%|█▏        | 59/500 [26:18<3:13:02, 26.26s/it] 12%|█▏        | 60/500 [26:46<3:17:49, 26.98s/it] 12%|█▏        | 61/500 [27:13<3:17:03, 26.93s/it] 12%|█▏        | 62/500 [27:37<3:09:47, 26.00s/it] 13%|█▎        | 63/500 [28:06<3:14:53, 26.76s/it] 13%|█▎        | 64/500 [28:34<3:18:34, 27.33s/it] 13%|█▎        | 65/500 [28:58<3:10:30, 26.28s/it] 13%|█▎        | 65/500 [28:58<3:13:55, 26.75s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.030 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▆▅▁▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████
wandb:     train_loss ▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▇█▁▃▃▅▄▅▃▄▃▄▅▄▄▅▄▄▄▅▆▄▅▅▅▅▄▄▆▅▅▅▆▅▅▄▅▅
wandb:       val_loss ▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 64
wandb:  learning_rate 0.0
wandb: train_accuracy 0.737
wandb:     train_loss 1.01393
wandb:   val_accuracy 0.53556
wandb:       val_loss 1.27726
wandb: 
wandb: 🚀 View run azure-valley-96 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yapwyeyy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_220906-yapwyeyy/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_223846-xmdzl6yf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-pyramid-97
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xmdzl6yf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:43, 25.94s/it]  0%|          | 2/500 [00:49<3:23:48, 24.56s/it]  1%|          | 3/500 [01:13<3:20:57, 24.26s/it]  1%|          | 4/500 [01:37<3:19:34, 24.14s/it]  1%|          | 5/500 [02:10<3:46:01, 27.40s/it]  1%|          | 6/500 [02:34<3:36:26, 26.29s/it]  1%|▏         | 7/500 [03:04<3:45:05, 27.39s/it]  2%|▏         | 8/500 [03:28<3:37:10, 26.48s/it]  2%|▏         | 9/500 [03:54<3:35:15, 26.30s/it]  2%|▏         | 10/500 [04:23<3:41:30, 27.12s/it]  2%|▏         | 11/500 [04:47<3:33:33, 26.20s/it]  2%|▏         | 12/500 [05:15<3:35:40, 26.52s/it]  3%|▎         | 13/500 [05:41<3:33:54, 26.35s/it]  3%|▎         | 14/500 [06:08<3:35:42, 26.63s/it]  3%|▎         | 15/500 [06:36<3:37:51, 26.95s/it]  3%|▎         | 16/500 [07:00<3:31:34, 26.23s/it]  3%|▎         | 17/500 [07:30<3:38:52, 27.19s/it]  4%|▎         | 18/500 [07:54<3:31:24, 26.32s/it]  4%|▍         | 19/500 [08:21<3:33:58, 26.69s/it]  4%|▍         | 20/500 [08:49<3:34:46, 26.85s/it]  4%|▍         | 21/500 [09:18<3:39:28, 27.49s/it]  4%|▍         | 22/500 [09:42<3:32:20, 26.65s/it]  5%|▍         | 23/500 [10:07<3:27:07, 26.05s/it]  5%|▍         | 24/500 [10:35<3:31:27, 26.65s/it]  5%|▌         | 25/500 [11:01<3:29:50, 26.51s/it]  5%|▌         | 25/500 [11:01<3:29:32, 26.47s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.230 MB of 0.315 MB uploadedwandb: - 0.230 MB of 0.315 MB uploadedwandb: \ 0.230 MB of 0.315 MB uploadedwandb: | 0.230 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▅▅▆▆▆▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▅▅▅▅▆▃▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▃▂▄▂▃▄▄███
wandb:       val_loss ▄█▂▇▃▃▃▆▁▅▄▁▅▆▄▃▅▃▃▃▃▂▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.61367
wandb:     train_loss 1.14233
wandb:   val_accuracy 0.4
wandb:       val_loss 1.10542
wandb: 
wandb: 🚀 View run hearty-pyramid-97 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xmdzl6yf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_223846-xmdzl6yf/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_225030-zeijyjzs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-water-98
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zeijyjzs
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:32:17, 25.53s/it]  0%|          | 2/500 [00:49<3:23:07, 24.47s/it]  1%|          | 3/500 [01:13<3:20:17, 24.18s/it]  1%|          | 4/500 [01:39<3:27:25, 25.09s/it]  1%|          | 5/500 [02:09<3:40:02, 26.67s/it]  1%|          | 6/500 [02:36<3:41:43, 26.93s/it]  1%|▏         | 7/500 [03:04<3:44:33, 27.33s/it]  2%|▏         | 8/500 [03:32<3:45:34, 27.51s/it]  2%|▏         | 9/500 [03:56<3:36:43, 26.48s/it]  2%|▏         | 10/500 [04:26<3:44:31, 27.49s/it]  2%|▏         | 11/500 [04:53<3:41:53, 27.23s/it]  2%|▏         | 12/500 [05:17<3:34:33, 26.38s/it]  3%|▎         | 13/500 [05:47<3:42:27, 27.41s/it]  3%|▎         | 14/500 [06:11<3:33:29, 26.36s/it]  3%|▎         | 15/500 [06:42<3:45:43, 27.92s/it]  3%|▎         | 16/500 [07:07<3:37:21, 26.95s/it]  3%|▎         | 17/500 [07:38<3:45:39, 28.03s/it]  4%|▎         | 18/500 [08:02<3:35:41, 26.85s/it]  4%|▍         | 19/500 [08:34<3:48:27, 28.50s/it]  4%|▍         | 20/500 [09:01<3:43:33, 27.94s/it]  4%|▍         | 21/500 [09:26<3:36:00, 27.06s/it]  4%|▍         | 22/500 [09:59<3:50:23, 28.92s/it]  5%|▍         | 23/500 [10:23<3:37:37, 27.37s/it]  5%|▍         | 24/500 [10:53<3:44:13, 28.26s/it]  5%|▌         | 25/500 [11:22<3:44:21, 28.34s/it]  5%|▌         | 26/500 [11:49<3:42:21, 28.15s/it]  5%|▌         | 27/500 [12:18<3:42:23, 28.21s/it]  6%|▌         | 28/500 [12:41<3:31:06, 26.84s/it]  6%|▌         | 29/500 [13:14<3:45:23, 28.71s/it]  6%|▌         | 30/500 [13:38<3:33:18, 27.23s/it]  6%|▌         | 30/500 [13:38<3:33:44, 27.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▁▄▄▃▃▃▃▁▇▅▇█▅▆▇▇███▇████████
wandb:     train_loss ▃▂▃▂▁▁▃▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▅▁▄▇▂▄▂▂▁▆▃▅▆▂▅▇█▆▆▇▆▇▆▆▇▆▆▆▆
wandb:       val_loss ▂▂▃▂▂▇▃▃▇▃▂▅▂▁█▂▃▁▂▂▃▁▁▂▄▁▁▃▅▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.98811
wandb:     train_loss 1e-05
wandb:   val_accuracy 0.61333
wandb:       val_loss 3.42738
wandb: 
wandb: 🚀 View run grateful-water-98 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zeijyjzs
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_225030-zeijyjzs/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_230451-j3i738m1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-durian-99
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j3i738m1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:53, 25.24s/it]  0%|          | 2/500 [00:49<3:22:39, 24.42s/it]  1%|          | 3/500 [01:13<3:21:21, 24.31s/it]  1%|          | 4/500 [01:38<3:25:24, 24.85s/it]  1%|          | 5/500 [02:07<3:37:12, 26.33s/it]  1%|          | 6/500 [02:32<3:32:48, 25.85s/it]  1%|▏         | 7/500 [03:02<3:42:22, 27.06s/it]  2%|▏         | 8/500 [03:27<3:35:49, 26.32s/it]  2%|▏         | 9/500 [03:54<3:37:10, 26.54s/it]  2%|▏         | 10/500 [04:21<3:39:02, 26.82s/it]  2%|▏         | 11/500 [04:46<3:32:46, 26.11s/it]  2%|▏         | 12/500 [05:14<3:39:12, 26.95s/it]  3%|▎         | 13/500 [05:39<3:32:28, 26.18s/it]  3%|▎         | 14/500 [06:09<3:42:24, 27.46s/it]  3%|▎         | 15/500 [06:36<3:41:16, 27.37s/it]  3%|▎         | 16/500 [07:07<3:47:35, 28.21s/it]  3%|▎         | 17/500 [07:31<3:38:19, 27.12s/it]  4%|▎         | 18/500 [08:02<3:46:51, 28.24s/it]  4%|▍         | 19/500 [08:27<3:38:23, 27.24s/it]  4%|▍         | 20/500 [08:52<3:33:09, 26.65s/it]  4%|▍         | 21/500 [09:18<3:30:50, 26.41s/it]  4%|▍         | 22/500 [09:44<3:30:19, 26.40s/it]  5%|▍         | 23/500 [10:10<3:28:07, 26.18s/it]  5%|▍         | 24/500 [10:39<3:33:59, 26.97s/it]  5%|▌         | 25/500 [11:03<3:27:25, 26.20s/it]  5%|▌         | 26/500 [11:32<3:33:55, 27.08s/it]  5%|▌         | 27/500 [12:00<3:34:02, 27.15s/it]  6%|▌         | 28/500 [12:24<3:26:24, 26.24s/it]  6%|▌         | 29/500 [12:53<3:33:20, 27.18s/it]  6%|▌         | 30/500 [13:20<3:31:51, 27.05s/it]  6%|▌         | 31/500 [13:48<3:33:18, 27.29s/it]  6%|▋         | 32/500 [14:12<3:26:14, 26.44s/it]  7%|▋         | 33/500 [14:41<3:30:21, 27.03s/it]  7%|▋         | 34/500 [15:05<3:23:24, 26.19s/it]  7%|▋         | 35/500 [15:31<3:21:53, 26.05s/it]  7%|▋         | 36/500 [16:00<3:28:08, 26.91s/it]  7%|▋         | 37/500 [16:24<3:21:29, 26.11s/it]  8%|▊         | 38/500 [16:56<3:35:02, 27.93s/it]  8%|▊         | 39/500 [17:20<3:25:50, 26.79s/it]  8%|▊         | 40/500 [17:48<3:28:52, 27.25s/it]  8%|▊         | 41/500 [18:16<3:29:47, 27.42s/it]  8%|▊         | 42/500 [18:43<3:28:44, 27.35s/it]  9%|▊         | 43/500 [19:08<3:21:23, 26.44s/it]  9%|▉         | 44/500 [19:39<3:32:03, 27.90s/it]  9%|▉         | 45/500 [20:05<3:26:17, 27.20s/it]  9%|▉         | 46/500 [20:31<3:23:21, 26.88s/it]  9%|▉         | 47/500 [20:56<3:19:38, 26.44s/it] 10%|▉         | 48/500 [21:20<3:13:52, 25.74s/it] 10%|▉         | 49/500 [21:51<3:25:14, 27.30s/it] 10%|█         | 50/500 [22:20<3:27:04, 27.61s/it] 10%|█         | 51/500 [22:47<3:25:32, 27.47s/it] 10%|█         | 52/500 [23:15<3:27:41, 27.82s/it] 11%|█         | 53/500 [23:40<3:19:51, 26.83s/it] 11%|█         | 54/500 [24:07<3:20:11, 26.93s/it] 11%|█         | 55/500 [24:32<3:15:04, 26.30s/it] 11%|█         | 56/500 [25:02<3:22:13, 27.33s/it] 11%|█▏        | 57/500 [25:30<3:23:34, 27.57s/it] 12%|█▏        | 58/500 [25:54<3:16:02, 26.61s/it] 12%|█▏        | 59/500 [26:23<3:21:21, 27.40s/it] 12%|█▏        | 60/500 [26:50<3:18:48, 27.11s/it] 12%|█▏        | 61/500 [27:19<3:22:41, 27.70s/it] 12%|█▏        | 62/500 [27:48<3:24:18, 27.99s/it] 13%|█▎        | 63/500 [28:16<3:24:17, 28.05s/it] 13%|█▎        | 64/500 [28:45<3:25:59, 28.35s/it] 13%|█▎        | 65/500 [29:09<3:17:02, 27.18s/it] 13%|█▎        | 65/500 [29:09<3:15:09, 26.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.233 MB of 0.315 MB uploadedwandb: - 0.233 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▃▅▆▁▃▄▅▆▆▆▇▇▇▇▇▇▇█████████████████████
wandb:     train_loss ▄▅▅▃▄▄▆█▄▄▄▄▄▇▅█▃▃▁▂▅▃▂▄▅▂▄▃▄▄▄▁▄▃▄▇▂▅▂▄
wandb:   val_accuracy ▃▃▃██▁▃▄▄▄▄▄▄▄▄▄▄▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       val_loss ▄▄▃▃▃█▄▄▆▇█▇▄▃▅▃▂▂▄▃▃▄▄▁▇▃▃▄▆▅▆▁▄█▅▂▅▂▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 64
wandb:  learning_rate 0.0
wandb: train_accuracy 0.74443
wandb:     train_loss 0.86523
wandb:   val_accuracy 0.46444
wandb:       val_loss 1.29616
wandb: 
wandb: 🚀 View run exalted-durian-99 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/j3i738m1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_230451-j3i738m1/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_233438-bv0srmvu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-leaf-100
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bv0srmvu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:13, 25.64s/it]  0%|          | 2/500 [00:49<3:23:43, 24.55s/it]  1%|          | 3/500 [01:13<3:21:12, 24.29s/it]  1%|          | 4/500 [01:41<3:34:21, 25.93s/it]  1%|          | 5/500 [02:07<3:33:45, 25.91s/it]  1%|          | 6/500 [02:37<3:45:21, 27.37s/it]  1%|▏         | 7/500 [03:02<3:36:27, 26.34s/it]  2%|▏         | 8/500 [03:31<3:43:15, 27.23s/it]  2%|▏         | 9/500 [03:55<3:36:15, 26.43s/it]  2%|▏         | 10/500 [04:23<3:38:15, 26.73s/it]  2%|▏         | 11/500 [04:51<3:40:53, 27.10s/it]  2%|▏         | 12/500 [05:14<3:31:51, 26.05s/it]  3%|▎         | 13/500 [05:44<3:40:49, 27.21s/it]  3%|▎         | 14/500 [06:09<3:34:16, 26.45s/it]  3%|▎         | 15/500 [06:38<3:40:25, 27.27s/it]  3%|▎         | 16/500 [07:03<3:33:22, 26.45s/it]  3%|▎         | 17/500 [07:35<3:48:09, 28.34s/it]  4%|▎         | 18/500 [08:00<3:38:18, 27.17s/it]  4%|▍         | 19/500 [08:30<3:44:56, 28.06s/it]  4%|▍         | 20/500 [08:55<3:35:58, 27.00s/it]  4%|▍         | 21/500 [09:26<3:46:29, 28.37s/it]  4%|▍         | 22/500 [09:51<3:37:34, 27.31s/it]  5%|▍         | 23/500 [10:20<3:40:05, 27.68s/it]  5%|▍         | 24/500 [10:44<3:31:39, 26.68s/it]  5%|▌         | 25/500 [11:16<3:43:26, 28.22s/it]  5%|▌         | 25/500 [11:16<3:34:09, 27.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.230 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▄▅▆▆▆▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▅▅▅▅▆▃▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▃▂▄▂▃▅▄███
wandb:       val_loss ▄█▂▇▃▃▃▆▁▅▄▁▅▆▄▃▅▃▃▃▃▂▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.6107
wandb:     train_loss 1.14237
wandb:   val_accuracy 0.4
wandb:       val_loss 1.10584
wandb: 
wandb: 🚀 View run proud-leaf-100 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bv0srmvu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_233438-bv0srmvu/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240917_234635-yb7f0zzl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-mountain-101
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yb7f0zzl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:12, 25.28s/it]  0%|          | 2/500 [00:49<3:23:09, 24.48s/it]  1%|          | 3/500 [01:13<3:20:46, 24.24s/it]  1%|          | 4/500 [01:37<3:19:31, 24.14s/it]  1%|          | 5/500 [02:06<3:33:23, 25.87s/it]  1%|          | 6/500 [02:33<3:36:23, 26.28s/it]  1%|▏         | 7/500 [02:57<3:30:25, 25.61s/it]  2%|▏         | 8/500 [03:25<3:37:01, 26.47s/it]  2%|▏         | 9/500 [03:49<3:30:51, 25.77s/it]  2%|▏         | 10/500 [04:21<3:44:31, 27.49s/it]  2%|▏         | 11/500 [04:45<3:35:39, 26.46s/it]  2%|▏         | 12/500 [05:11<3:35:07, 26.45s/it]  3%|▎         | 13/500 [05:41<3:41:44, 27.32s/it]  3%|▎         | 14/500 [06:10<3:45:07, 27.79s/it]  3%|▎         | 15/500 [06:34<3:36:19, 26.76s/it]  3%|▎         | 16/500 [06:59<3:31:15, 26.19s/it]  3%|▎         | 17/500 [07:29<3:40:06, 27.34s/it]  4%|▎         | 18/500 [07:57<3:42:59, 27.76s/it]  4%|▍         | 19/500 [08:22<3:34:33, 26.76s/it]  4%|▍         | 20/500 [08:49<3:34:53, 26.86s/it]  4%|▍         | 21/500 [09:14<3:30:14, 26.33s/it]  4%|▍         | 22/500 [09:40<3:29:39, 26.32s/it]  5%|▍         | 23/500 [10:05<3:24:25, 25.71s/it]  5%|▍         | 24/500 [10:33<3:30:11, 26.50s/it]  5%|▌         | 25/500 [10:59<3:29:21, 26.45s/it]  5%|▌         | 26/500 [11:27<3:32:14, 26.87s/it]  5%|▌         | 27/500 [11:57<3:37:40, 27.61s/it]  6%|▌         | 28/500 [12:23<3:35:00, 27.33s/it]  6%|▌         | 29/500 [12:50<3:33:59, 27.26s/it]  6%|▌         | 30/500 [13:15<3:26:42, 26.39s/it]  6%|▌         | 31/500 [13:42<3:29:12, 26.77s/it]  6%|▋         | 32/500 [14:06<3:22:04, 25.91s/it]  7%|▋         | 33/500 [14:38<3:34:34, 27.57s/it]  7%|▋         | 34/500 [15:05<3:32:58, 27.42s/it]  7%|▋         | 35/500 [15:29<3:25:57, 26.57s/it]  7%|▋         | 36/500 [15:57<3:29:05, 27.04s/it]  7%|▋         | 37/500 [16:22<3:23:43, 26.40s/it]  8%|▊         | 38/500 [16:46<3:17:36, 25.66s/it]  8%|▊         | 39/500 [17:16<3:25:26, 26.74s/it]  8%|▊         | 40/500 [17:40<3:18:48, 25.93s/it]  8%|▊         | 41/500 [18:08<3:24:39, 26.75s/it]  8%|▊         | 42/500 [18:36<3:26:49, 27.09s/it]  9%|▊         | 43/500 [19:00<3:19:08, 26.15s/it]  9%|▉         | 44/500 [19:27<3:20:17, 26.35s/it]  9%|▉         | 45/500 [19:53<3:19:34, 26.32s/it]  9%|▉         | 46/500 [20:19<3:18:21, 26.22s/it]  9%|▉         | 47/500 [20:44<3:15:25, 25.88s/it] 10%|▉         | 48/500 [21:09<3:11:35, 25.43s/it] 10%|▉         | 49/500 [21:37<3:18:13, 26.37s/it] 10%|█         | 50/500 [22:01<3:12:32, 25.67s/it] 10%|█         | 51/500 [22:26<3:11:06, 25.54s/it] 10%|█         | 52/500 [22:52<3:09:38, 25.40s/it] 11%|█         | 53/500 [23:21<3:17:56, 26.57s/it] 11%|█         | 54/500 [23:45<3:12:47, 25.94s/it] 11%|█         | 55/500 [24:15<3:21:40, 27.19s/it] 11%|█         | 56/500 [24:42<3:20:26, 27.09s/it] 11%|█▏        | 57/500 [25:10<3:20:25, 27.15s/it] 11%|█▏        | 57/500 [25:10<3:15:37, 26.49s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.019 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▂▁▁▃▃▇█▇██▇██▇████████████████████████
wandb:     train_loss ▆▃█▁▅▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▂▅▂▁▄▄▇▇▆▇▇▆▇▇▇▇▇█▇▇▇▇██▇█▇███▇▇▇▇████▇
wandb:       val_loss ▄▃▃▇▇▃▇▃▂▃▅▃▆▂▆▁▁█▁▂▂▁▆▅▁▁▁▄▁▄▂▅▃▁▇▂▂█▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.99406
wandb:     train_loss 0.00479
wandb:   val_accuracy 0.64444
wandb:       val_loss 1.17302
wandb: 
wandb: 🚀 View run desert-mountain-101 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yb7f0zzl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_234635-yb7f0zzl/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_001232-bocc6h2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-shadow-102
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bocc6h2k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:55, 25.72s/it]  0%|          | 2/500 [00:49<3:22:18, 24.38s/it]  1%|          | 3/500 [01:13<3:20:32, 24.21s/it]  1%|          | 4/500 [01:42<3:36:47, 26.22s/it]  1%|          | 5/500 [02:07<3:31:36, 25.65s/it]  1%|          | 6/500 [02:40<3:51:45, 28.15s/it]  1%|▏         | 7/500 [03:04<3:40:24, 26.82s/it]  2%|▏         | 8/500 [03:37<3:56:51, 28.89s/it]  2%|▏         | 9/500 [04:07<3:59:42, 29.29s/it]  2%|▏         | 10/500 [04:35<3:55:02, 28.78s/it]  2%|▏         | 11/500 [04:59<3:42:02, 27.24s/it]  2%|▏         | 12/500 [05:30<3:52:42, 28.61s/it]  3%|▎         | 13/500 [05:55<3:43:35, 27.55s/it]  3%|▎         | 14/500 [06:21<3:39:17, 27.07s/it]  3%|▎         | 15/500 [06:47<3:34:19, 26.51s/it]  3%|▎         | 16/500 [07:12<3:31:21, 26.20s/it]  3%|▎         | 17/500 [07:36<3:24:17, 25.38s/it]  4%|▎         | 18/500 [08:07<3:38:02, 27.14s/it]  4%|▍         | 19/500 [08:31<3:30:15, 26.23s/it]  4%|▍         | 20/500 [08:58<3:32:18, 26.54s/it]  4%|▍         | 21/500 [09:24<3:29:32, 26.25s/it]  4%|▍         | 22/500 [09:49<3:27:34, 26.05s/it]  5%|▍         | 23/500 [10:18<3:33:11, 26.82s/it]  5%|▍         | 24/500 [10:42<3:26:59, 26.09s/it]  5%|▌         | 25/500 [11:13<3:36:13, 27.31s/it]  5%|▌         | 26/500 [11:39<3:32:51, 26.94s/it]  5%|▌         | 27/500 [12:04<3:28:38, 26.47s/it]  6%|▌         | 28/500 [12:28<3:23:37, 25.88s/it]  6%|▌         | 29/500 [12:54<3:21:43, 25.70s/it]  6%|▌         | 30/500 [13:19<3:19:33, 25.48s/it]  6%|▌         | 31/500 [13:43<3:16:59, 25.20s/it]  6%|▋         | 32/500 [14:13<3:28:11, 26.69s/it]  7%|▋         | 33/500 [14:37<3:21:32, 25.89s/it]  7%|▋         | 34/500 [15:09<3:34:08, 27.57s/it]  7%|▋         | 35/500 [15:35<3:29:34, 27.04s/it]  7%|▋         | 36/500 [15:59<3:22:25, 26.17s/it]  7%|▋         | 37/500 [16:30<3:32:18, 27.51s/it]  8%|▊         | 38/500 [16:53<3:23:04, 26.37s/it]  8%|▊         | 39/500 [17:23<3:31:32, 27.53s/it]  8%|▊         | 40/500 [17:48<3:23:07, 26.49s/it]  8%|▊         | 41/500 [18:14<3:22:33, 26.48s/it]  8%|▊         | 42/500 [18:41<3:23:13, 26.62s/it]  9%|▊         | 43/500 [19:05<3:15:54, 25.72s/it]  9%|▉         | 44/500 [19:32<3:18:50, 26.16s/it]  9%|▉         | 45/500 [19:56<3:14:16, 25.62s/it]  9%|▉         | 46/500 [20:22<3:13:57, 25.63s/it]  9%|▉         | 47/500 [20:49<3:17:26, 26.15s/it] 10%|▉         | 48/500 [21:13<3:11:54, 25.48s/it] 10%|▉         | 49/500 [21:45<3:25:21, 27.32s/it] 10%|█         | 50/500 [22:09<3:17:41, 26.36s/it] 10%|█         | 51/500 [22:38<3:24:41, 27.35s/it] 10%|█         | 52/500 [23:03<3:18:00, 26.52s/it] 11%|█         | 53/500 [23:29<3:15:32, 26.25s/it] 11%|█         | 54/500 [23:54<3:13:19, 26.01s/it] 11%|█         | 55/500 [24:19<3:10:00, 25.62s/it] 11%|█         | 56/500 [24:44<3:09:05, 25.55s/it] 11%|█▏        | 57/500 [25:08<3:05:03, 25.06s/it] 12%|█▏        | 58/500 [25:37<3:12:56, 26.19s/it] 12%|█▏        | 59/500 [26:05<3:16:04, 26.68s/it] 12%|█▏        | 60/500 [26:33<3:18:22, 27.05s/it] 12%|█▏        | 61/500 [26:57<3:11:29, 26.17s/it] 12%|█▏        | 62/500 [27:29<3:24:20, 27.99s/it] 13%|█▎        | 63/500 [27:56<3:21:28, 27.66s/it] 13%|█▎        | 64/500 [28:20<3:12:54, 26.55s/it] 13%|█▎        | 65/500 [28:47<3:12:37, 26.57s/it] 13%|█▎        | 65/500 [28:47<3:12:37, 26.57s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▆▅▁▃▅▅▆▇▇▇▇▇████████▇▆▇▇▇▇▇▇██▇███▇███
wandb:     train_loss ▅▅▅▃▄▅▆▅▅▅▄▂▆█▅▅▇▄▁▂▇▃▅▅▅▃▇█▄▅▇▂▇▆▄▆▂▇▃▄
wandb:   val_accuracy ▃▃▆█▁▃▄▅▅▆▅▆▅▆▇▇▆▆▆▆▆▇▇▆▇▇█▆▆▆▆▆▆▆▇▆▆▆▆▆
wandb:       val_loss ▆▆▅▅▇█▅▇▇█▅▅▁▂▅▆▁▂▄▃▁▅▄▅▆▃▃▂▅▇▅▂▄▄▅▃▆▁▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 64
wandb:  learning_rate 0.0
wandb: train_accuracy 0.72214
wandb:     train_loss 0.72658
wandb:   val_accuracy 0.55333
wandb:       val_loss 0.90412
wandb: 
wandb: 🚀 View run effortless-shadow-102 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bocc6h2k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_001232-bocc6h2k/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_004204-fu274ayu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-snowball-103
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fu274ayu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:26, 26.38s/it]  0%|          | 2/500 [00:50<3:26:12, 24.84s/it]  1%|          | 3/500 [01:18<3:40:29, 26.62s/it]  1%|          | 4/500 [01:43<3:32:51, 25.75s/it]  1%|          | 5/500 [02:12<3:41:32, 26.85s/it]  1%|          | 6/500 [02:39<3:42:19, 27.00s/it]  1%|▏         | 7/500 [03:08<3:46:48, 27.60s/it]  2%|▏         | 8/500 [03:32<3:36:48, 26.44s/it]  2%|▏         | 9/500 [04:04<3:50:09, 28.12s/it]  2%|▏         | 10/500 [04:27<3:38:21, 26.74s/it]  2%|▏         | 11/500 [04:59<3:49:59, 28.22s/it]  2%|▏         | 12/500 [05:23<3:38:38, 26.88s/it]  3%|▎         | 13/500 [05:51<3:41:54, 27.34s/it]  3%|▎         | 14/500 [06:19<3:43:26, 27.59s/it]  3%|▎         | 15/500 [06:43<3:33:26, 26.41s/it]  3%|▎         | 16/500 [07:11<3:36:29, 26.84s/it]  3%|▎         | 17/500 [07:35<3:30:34, 26.16s/it]  4%|▎         | 18/500 [08:05<3:38:27, 27.19s/it]  4%|▍         | 19/500 [08:29<3:29:54, 26.18s/it]  4%|▍         | 20/500 [09:01<3:44:22, 28.05s/it]  4%|▍         | 21/500 [09:29<3:44:42, 28.15s/it]  4%|▍         | 22/500 [09:57<3:43:50, 28.10s/it]  5%|▍         | 23/500 [10:22<3:35:00, 27.05s/it]  5%|▍         | 24/500 [10:49<3:35:01, 27.10s/it]  5%|▌         | 25/500 [11:14<3:28:06, 26.29s/it]  5%|▌         | 25/500 [11:14<3:33:28, 26.96s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.019 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▆▅▆▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▅▅▅▅▆▃▄▅▄▆▅▄▆
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▃▂▅▃▄▇█▇▇█
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▆▄▃▅▃▃▄▃▂▄▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.59138
wandb:     train_loss 1.14541
wandb:   val_accuracy 0.38222
wandb:       val_loss 1.11218
wandb: 
wandb: 🚀 View run winter-snowball-103 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fu274ayu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_004204-fu274ayu/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_005405-4m1y7ql6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-terrain-104
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/4m1y7ql6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:30, 25.43s/it]  0%|          | 2/500 [00:49<3:25:38, 24.78s/it]  1%|          | 3/500 [01:14<3:25:55, 24.86s/it]  1%|          | 4/500 [01:39<3:24:24, 24.73s/it]  1%|          | 5/500 [02:07<3:35:22, 26.11s/it]  1%|          | 6/500 [02:33<3:32:37, 25.83s/it]  1%|▏         | 7/500 [03:05<3:49:43, 27.96s/it]  2%|▏         | 8/500 [03:31<3:43:52, 27.30s/it]  2%|▏         | 9/500 [03:56<3:39:06, 26.77s/it]  2%|▏         | 10/500 [04:24<3:40:54, 27.05s/it]  2%|▏         | 11/500 [04:50<3:37:27, 26.68s/it]  2%|▏         | 12/500 [05:16<3:36:04, 26.57s/it]  3%|▎         | 13/500 [05:43<3:35:16, 26.52s/it]  3%|▎         | 14/500 [06:11<3:38:27, 26.97s/it]  3%|▎         | 15/500 [06:37<3:35:19, 26.64s/it]  3%|▎         | 16/500 [07:06<3:42:02, 27.53s/it]  3%|▎         | 17/500 [07:31<3:35:48, 26.81s/it]  4%|▎         | 18/500 [08:03<3:47:18, 28.30s/it]  4%|▍         | 19/500 [08:28<3:39:23, 27.37s/it]  4%|▍         | 20/500 [09:00<3:49:10, 28.65s/it]  4%|▍         | 21/500 [09:25<3:39:48, 27.53s/it]  4%|▍         | 22/500 [09:53<3:41:59, 27.87s/it]  5%|▍         | 23/500 [10:21<3:41:32, 27.87s/it]  5%|▍         | 24/500 [10:46<3:34:31, 27.04s/it]  5%|▌         | 25/500 [11:12<3:30:36, 26.60s/it]  5%|▌         | 26/500 [11:43<3:39:45, 27.82s/it]  5%|▌         | 27/500 [12:08<3:32:46, 26.99s/it]  6%|▌         | 28/500 [12:37<3:38:14, 27.74s/it]  6%|▌         | 29/500 [13:08<3:45:31, 28.73s/it]  6%|▌         | 30/500 [13:33<3:35:16, 27.48s/it]  6%|▌         | 31/500 [14:06<3:47:55, 29.16s/it]  6%|▋         | 32/500 [14:31<3:36:48, 27.80s/it]  7%|▋         | 33/500 [15:00<3:39:27, 28.19s/it]  7%|▋         | 34/500 [15:24<3:30:39, 27.12s/it]  7%|▋         | 35/500 [15:53<3:33:57, 27.61s/it]  7%|▋         | 36/500 [16:23<3:38:34, 28.26s/it]  7%|▋         | 36/500 [16:23<3:31:14, 27.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.106 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▅▂▅▅▆▂▁▆▆▅▆▄▄▃▃▄▅▇▄██▇▂▄▄▄▅▄▄▆▄▅▅▆
wandb:     train_loss ▂▂▂▇▁▄▁▁▂▁▁▁▁▁▁▁▂█▁▁▁▁▂▁▇▂▅▁▁▆▄█▇▃▁▁
wandb:   val_accuracy ▄▄▅▄▆▆█▁▁▆▆▄▆▄▄▂▂▅▅▇▃▆█▆▄▃▃▅▅▅▅▅▆▆▅▆
wandb:       val_loss ▁▂▁▃▂▃▁▁▂▁▃▃▄█▄▂▁▂▃▁▁▁▁▂▃▂▁▁▃▂▃▁▂▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 35
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.77415
wandb:     train_loss 0.00708
wandb:   val_accuracy 0.49333
wandb:       val_loss 2.46875
wandb: 
wandb: 🚀 View run likely-terrain-104 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/4m1y7ql6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_005405-4m1y7ql6/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_011111-xmuk81a1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-puddle-105
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xmuk81a1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:36:06, 25.99s/it]  0%|          | 2/500 [00:50<3:26:10, 24.84s/it]  1%|          | 3/500 [01:13<3:19:34, 24.09s/it]  1%|          | 4/500 [01:37<3:18:08, 23.97s/it]  1%|          | 5/500 [02:05<3:31:24, 25.62s/it]  1%|          | 6/500 [02:29<3:26:58, 25.14s/it]  1%|▏         | 7/500 [02:54<3:26:18, 25.11s/it]  2%|▏         | 8/500 [03:19<3:24:21, 24.92s/it]  2%|▏         | 9/500 [03:49<3:37:31, 26.58s/it]  2%|▏         | 10/500 [04:12<3:27:55, 25.46s/it]  2%|▏         | 11/500 [04:40<3:34:54, 26.37s/it]  2%|▏         | 12/500 [05:13<3:50:17, 28.31s/it]  3%|▎         | 13/500 [05:38<3:40:53, 27.21s/it]  3%|▎         | 14/500 [06:06<3:42:11, 27.43s/it]  3%|▎         | 15/500 [06:29<3:31:02, 26.11s/it]  3%|▎         | 16/500 [07:01<3:44:10, 27.79s/it]  3%|▎         | 17/500 [07:27<3:40:48, 27.43s/it]  4%|▎         | 18/500 [07:52<3:33:54, 26.63s/it]  4%|▍         | 19/500 [08:19<3:34:56, 26.81s/it]  4%|▍         | 20/500 [08:43<3:27:44, 25.97s/it]  4%|▍         | 21/500 [09:16<3:43:48, 28.03s/it]  4%|▍         | 22/500 [09:39<3:31:44, 26.58s/it]  5%|▍         | 23/500 [10:08<3:36:29, 27.23s/it]  5%|▍         | 24/500 [10:37<3:40:29, 27.79s/it]  5%|▌         | 25/500 [11:08<3:47:10, 28.70s/it]  5%|▌         | 26/500 [11:32<3:36:16, 27.38s/it]  5%|▌         | 27/500 [12:00<3:36:07, 27.42s/it]  6%|▌         | 28/500 [12:28<3:38:34, 27.79s/it]  6%|▌         | 29/500 [12:52<3:27:39, 26.45s/it]  6%|▌         | 30/500 [13:22<3:36:33, 27.65s/it]  6%|▌         | 31/500 [13:46<3:27:47, 26.58s/it]  6%|▋         | 32/500 [14:16<3:34:12, 27.46s/it]  7%|▋         | 33/500 [14:40<3:25:29, 26.40s/it]  7%|▋         | 34/500 [15:05<3:23:23, 26.19s/it]  7%|▋         | 35/500 [15:29<3:17:13, 25.45s/it]  7%|▋         | 36/500 [15:55<3:19:09, 25.75s/it]  7%|▋         | 36/500 [15:56<3:25:24, 26.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▅▅▆▆▂▅▅▄▅▆▅▄▄▃▆▂█▃▄▃▅▆▆▅▅▅▃▄▄▂▄▅▅
wandb:     train_loss ▃▄▄▄▃▃▄▄▄▆▃▃▂▃▄▃▂▂▇▃▃▅▂▃▃▃▇▃▄▁█▂█▆▃▁
wandb:   val_accuracy ▁▁▂▂█▄▄▂▂▂▁▃▄▃▃▃▁▅▃▆▃▄▄▅▆▇▅▇▆▄▅▅▁▄▅▅
wandb:       val_loss ▃▄▃▃▃▃▃▆▂▄▃▂▄▆▃▄▇▃▅▁▂▂▃▂▅▂▂▂█▅▅▃▂▅▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 35
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.55572
wandb:     train_loss 0.37877
wandb:   val_accuracy 0.43778
wandb:       val_loss 0.62221
wandb: 
wandb: 🚀 View run prime-puddle-105 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xmuk81a1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_011111-xmuk81a1/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_012746-da7o47rb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sponge-106
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/da7o47rb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:59:52, 28.84s/it]  0%|          | 2/500 [00:57<3:56:27, 28.49s/it]  1%|          | 3/500 [01:20<3:38:26, 26.37s/it]  1%|          | 4/500 [01:44<3:28:45, 25.25s/it]  1%|          | 5/500 [02:13<3:40:50, 26.77s/it]  1%|          | 6/500 [02:37<3:32:46, 25.84s/it]  1%|▏         | 7/500 [03:06<3:39:15, 26.68s/it]  2%|▏         | 8/500 [03:35<3:44:31, 27.38s/it]  2%|▏         | 9/500 [04:04<3:49:41, 28.07s/it]  2%|▏         | 10/500 [04:33<3:51:03, 28.29s/it]  2%|▏         | 11/500 [04:57<3:40:42, 27.08s/it]  2%|▏         | 12/500 [05:28<3:49:55, 28.27s/it]  3%|▎         | 13/500 [05:53<3:39:48, 27.08s/it]  3%|▎         | 14/500 [06:21<3:40:54, 27.27s/it]  3%|▎         | 15/500 [06:44<3:32:13, 26.26s/it]  3%|▎         | 16/500 [07:11<3:32:49, 26.38s/it]  3%|▎         | 17/500 [07:36<3:28:42, 25.93s/it]  4%|▎         | 18/500 [08:00<3:23:52, 25.38s/it]  4%|▍         | 19/500 [08:29<3:31:43, 26.41s/it]  4%|▍         | 20/500 [08:59<3:40:36, 27.58s/it]  4%|▍         | 21/500 [09:28<3:42:36, 27.88s/it]  4%|▍         | 22/500 [09:52<3:32:56, 26.73s/it]  5%|▍         | 23/500 [10:25<3:46:55, 28.54s/it]  5%|▍         | 24/500 [10:51<3:42:29, 28.04s/it]  5%|▌         | 25/500 [11:21<3:46:01, 28.55s/it]  5%|▌         | 25/500 [11:21<3:35:52, 27.27s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.230 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▄▅▅▆▇██
wandb:     train_loss ▁▇▁▁█▂▆▇▆▇▆▇▆▅▅▅▆▃▅▅▅▆▄▄▆
wandb:   val_accuracy ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▄▄▂▅▅▂▄▅▇█
wandb:       val_loss ▄█▃▇▃▃▃▆▁▅▄▁▆▇▄▃▆▃▄▄▃▂▄▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.47697
wandb:     train_loss 1.16187
wandb:   val_accuracy 0.36667
wandb:       val_loss 1.13799
wandb: 
wandb: 🚀 View run usual-sponge-106 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/da7o47rb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_012746-da7o47rb/logs
Successfully processed 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_013959-83jgcdtz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-waterfall-107
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/83jgcdtz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fallen-waterfall-107 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/83jgcdtz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_013959-83jgcdtz/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014034-vs3tqhzc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-serenity-108
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vs3tqhzc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run rare-serenity-108 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vs3tqhzc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014034-vs3tqhzc/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014110-lgr19mh0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-jazz-109
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lgr19mh0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run pious-jazz-109 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/lgr19mh0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014110-lgr19mh0/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014141-b11eaxok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-microwave-110
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/b11eaxok
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run resilient-microwave-110 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/b11eaxok
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014141-b11eaxok/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014217-fhza1yps
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-thunder-111
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fhza1yps
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run blooming-thunder-111 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fhza1yps
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014217-fhza1yps/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014248-ijxwd0g9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-resonance-112
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ijxwd0g9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run sandy-resonance-112 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ijxwd0g9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014248-ijxwd0g9/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014321-3ne62rww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-yogurt-113
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3ne62rww
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.027 MB uploadedwandb: - 0.021 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb: 🚀 View run helpful-yogurt-113 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3ne62rww
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014321-3ne62rww/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014352-wuz1gko8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-thunder-114
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wuz1gko8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.021 MB of 0.031 MB uploadedwandb: 🚀 View run winter-thunder-114 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wuz1gko8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014352-wuz1gko8/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014422-7pjfjbue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-mountain-115
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7pjfjbue
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run generous-mountain-115 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7pjfjbue
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014422-7pjfjbue/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014458-1cii01yg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-waterfall-116
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1cii01yg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run sleek-waterfall-116 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1cii01yg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014458-1cii01yg/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014534-rqil9vua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-puddle-117
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rqil9vua
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run floral-puddle-117 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rqil9vua
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014534-rqil9vua/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014612-uj5kjt3t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-monkey-118
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uj5kjt3t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run sunny-monkey-118 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/uj5kjt3t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014612-uj5kjt3t/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014647-tw1zya6l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-shadow-119
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tw1zya6l
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.021 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run quiet-shadow-119 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tw1zya6l
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014647-tw1zya6l/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014722-hg0vd7m3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-star-120
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/hg0vd7m3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.019 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run honest-star-120 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/hg0vd7m3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014722-hg0vd7m3/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014753-eze20gfz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sun-121
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/eze20gfz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.026 MB uploadedwandb: / 0.019 MB of 0.026 MB uploadedwandb: - 0.026 MB of 0.026 MB uploadedwandb: 🚀 View run apricot-sun-121 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/eze20gfz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014753-eze20gfz/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.01 GiB is free. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 17.44 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014825-kfck4tvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-jazz-122
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kfck4tvl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.032 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run fresh-jazz-122 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kfck4tvl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014825-kfck4tvl/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014855-e4pqf1wg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-spaceship-123
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/e4pqf1wg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:00<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.028 MB of 0.032 MB uploadedwandb: 🚀 View run serene-spaceship-123 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/e4pqf1wg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014855-e4pqf1wg/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014926-5vxn6fmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-violet-124
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5vxn6fmn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run vibrant-violet-124 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5vxn6fmn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014926-5vxn6fmn/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_014958-iaxftcff
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-bush-125
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/iaxftcff
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run dainty-bush-125 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/iaxftcff
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_014958-iaxftcff/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015031-8xgx64au
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sound-126
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8xgx64au
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.011 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run crisp-sound-126 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8xgx64au
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015031-8xgx64au/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015105-f27d6ffj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-dream-127
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/f27d6ffj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run stilted-dream-127 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/f27d6ffj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015105-f27d6ffj/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015137-mvtbkzb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-serenity-128
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mvtbkzb8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run lunar-serenity-128 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mvtbkzb8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015137-mvtbkzb8/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015212-1av22cc5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-firefly-129
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1av22cc5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.028 MB of 0.032 MB uploadedwandb: 🚀 View run olive-firefly-129 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1av22cc5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015212-1av22cc5/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015241-n5h4gfw5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-firebrand-130
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/n5h4gfw5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.032 MB uploadedwandb: | 0.011 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run dainty-firebrand-130 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/n5h4gfw5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015241-n5h4gfw5/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015311-bobori97
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-planet-131
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bobori97
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.011 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run deep-planet-131 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bobori97
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015311-bobori97/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015351-mnlyoubz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-wind-132
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mnlyoubz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run ancient-wind-132 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mnlyoubz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015351-mnlyoubz/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015427-qmvk6ay3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-river-133
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qmvk6ay3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.019 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run sandy-river-133 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qmvk6ay3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015427-qmvk6ay3/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015502-ruid9pmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-shape-134
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ruid9pmn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.032 MB uploadedwandb: | 0.019 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run legendary-shape-134 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ruid9pmn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015502-ruid9pmn/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015533-5vnxcxg4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-aardvark-135
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5vnxcxg4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.032 MB uploadedwandb: / 0.011 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run polished-aardvark-135 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5vnxcxg4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015533-5vnxcxg4/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015606-k287qapw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-plasma-136
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/k287qapw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.032 MB uploadedwandb: - 0.019 MB of 0.032 MB uploadedwandb: \ 0.032 MB of 0.032 MB uploadedwandb: 🚀 View run ethereal-plasma-136 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/k287qapw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015606-k287qapw/logs
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_PE.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 76, in forward
    x_beta = self.GAT_beta(data['beta'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 538, in propagate
    coll_dict = self._collect(self._user_args, edge_index,
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 400, in _collect
    data = self._lift(data, edge_index, dim)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 360, in _lift
    return self._index_select(src, edge_index[dim])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 306, in _index_select
    return self._index_select_safe(src, index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 329, in _index_select_safe
    raise e
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 310, in _index_select_safe
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.45 GiB. GPU 1 has a total capacity of 23.69 GiB of which 6.15 GiB is free. Including non-PyTorch memory, this process has 17.53 GiB memory in use. Of the allocated memory 8.91 GiB is allocated by PyTorch, and 8.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 3_20140603
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_015637-cspua2lu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sea-137
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/cspua2lu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:50:38, 13.30s/it]  0%|          | 2/500 [00:24<1:42:33, 12.36s/it]  1%|          | 3/500 [00:36<1:39:31, 12.02s/it]  1%|          | 4/500 [00:48<1:39:25, 12.03s/it]  1%|          | 5/500 [01:00<1:38:43, 11.97s/it]  1%|          | 6/500 [01:12<1:37:42, 11.87s/it]  1%|▏         | 7/500 [01:24<1:37:54, 11.92s/it]  2%|▏         | 8/500 [01:35<1:36:41, 11.79s/it]  2%|▏         | 9/500 [01:47<1:37:14, 11.88s/it]  2%|▏         | 10/500 [02:00<1:37:59, 12.00s/it]  2%|▏         | 11/500 [02:12<1:38:18, 12.06s/it]  2%|▏         | 12/500 [02:24<1:38:00, 12.05s/it]  3%|▎         | 13/500 [02:36<1:37:23, 12.00s/it]  3%|▎         | 14/500 [02:48<1:37:39, 12.06s/it]  3%|▎         | 15/500 [03:00<1:38:08, 12.14s/it]  3%|▎         | 16/500 [03:12<1:37:12, 12.05s/it]  3%|▎         | 17/500 [03:24<1:37:04, 12.06s/it]  4%|▎         | 18/500 [03:36<1:36:32, 12.02s/it]  4%|▍         | 19/500 [03:48<1:36:49, 12.08s/it]  4%|▍         | 20/500 [04:00<1:36:15, 12.03s/it]  4%|▍         | 21/500 [04:13<1:37:49, 12.25s/it]  4%|▍         | 22/500 [04:26<1:40:07, 12.57s/it]  5%|▍         | 23/500 [04:38<1:38:50, 12.43s/it]  5%|▍         | 24/500 [04:50<1:36:33, 12.17s/it]  5%|▌         | 25/500 [05:02<1:35:58, 12.12s/it]  5%|▌         | 26/500 [05:14<1:35:21, 12.07s/it]  5%|▌         | 27/500 [05:26<1:35:13, 12.08s/it]  5%|▌         | 27/500 [05:26<1:35:19, 12.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▅▃▇▆▄▇▆█▆█▅▇▇██▆███▇██▇█
wandb:     train_loss ▃▄▂▂▂▆▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▂▂▄▂▇▆▁█▇▇▇▇▅▇█▇▆▆█▇▇▇████
wandb:       val_loss ▂▃▂▂▂▃▅▅▁▄▁▄▁▅▄▂▅▇▁█▄▄▄▄▄▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.97771
wandb:     train_loss 0.00387
wandb:   val_accuracy 0.56889
wandb:       val_loss 1.74458
wandb: 
wandb: 🚀 View run worthy-sea-137 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/cspua2lu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_015637-cspua2lu/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_020249-or4x3atp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-elevator-138
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/or4x3atp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:45:17, 19.87s/it]  0%|          | 2/500 [00:35<2:22:15, 17.14s/it]  1%|          | 3/500 [00:54<2:29:27, 18.04s/it]  1%|          | 4/500 [01:07<2:13:14, 16.12s/it]  1%|          | 5/500 [01:20<2:02:50, 14.89s/it]  1%|          | 6/500 [01:32<1:56:02, 14.09s/it]  1%|▏         | 7/500 [01:44<1:50:41, 13.47s/it]  2%|▏         | 8/500 [01:57<1:48:40, 13.25s/it]  2%|▏         | 9/500 [02:11<1:49:05, 13.33s/it]  2%|▏         | 10/500 [02:24<1:48:43, 13.31s/it]  2%|▏         | 11/500 [02:36<1:46:38, 13.08s/it]  2%|▏         | 12/500 [02:49<1:45:39, 12.99s/it]  3%|▎         | 13/500 [03:03<1:46:34, 13.13s/it]  3%|▎         | 14/500 [03:15<1:44:19, 12.88s/it]  3%|▎         | 15/500 [03:28<1:43:17, 12.78s/it]  3%|▎         | 16/500 [03:40<1:42:14, 12.67s/it]  3%|▎         | 17/500 [03:52<1:40:20, 12.46s/it]  4%|▎         | 18/500 [04:04<1:38:45, 12.29s/it]  4%|▍         | 19/500 [04:16<1:37:50, 12.20s/it]  4%|▍         | 19/500 [04:16<1:48:14, 13.50s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.230 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▄▅▆▃▃▃▃▄▄▄▃▄▄▃▃▁█
wandb:     train_loss ▂▃▂▂▂▃▁▁█▁▂█▆▇▁▁▇▂▂
wandb:   val_accuracy ██▅▇▇▄▄▄▄▄▄▄▄▄▄▄▄▁▇
wandb:       val_loss ▁▁▁▁▁▁▁▃▅▄▁▂▁▅▁█▃▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.4844
wandb:     train_loss 1.04402
wandb:   val_accuracy 0.34222
wandb:       val_loss 1.09643
wandb: 
wandb: 🚀 View run fearless-elevator-138 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/or4x3atp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_020249-or4x3atp/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_020754-ya51uwbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-dawn-139
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ya51uwbm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:00:45, 14.52s/it]  0%|          | 2/500 [00:26<1:46:10, 12.79s/it]  1%|          | 3/500 [00:37<1:41:05, 12.20s/it]  1%|          | 4/500 [00:49<1:40:30, 12.16s/it]  1%|          | 5/500 [01:01<1:38:35, 11.95s/it]  1%|          | 6/500 [01:13<1:38:07, 11.92s/it]  1%|▏         | 7/500 [01:25<1:38:02, 11.93s/it]  2%|▏         | 8/500 [01:36<1:36:18, 11.75s/it]  2%|▏         | 9/500 [01:47<1:34:37, 11.56s/it]  2%|▏         | 10/500 [01:59<1:34:42, 11.60s/it]  2%|▏         | 11/500 [02:10<1:33:10, 11.43s/it]  2%|▏         | 12/500 [02:22<1:34:46, 11.65s/it]  3%|▎         | 13/500 [02:34<1:34:58, 11.70s/it]  3%|▎         | 14/500 [02:46<1:36:12, 11.88s/it]  3%|▎         | 15/500 [02:58<1:35:29, 11.81s/it]  3%|▎         | 16/500 [03:09<1:34:46, 11.75s/it]  3%|▎         | 17/500 [03:21<1:33:36, 11.63s/it]  4%|▎         | 18/500 [03:32<1:32:53, 11.56s/it]  4%|▍         | 19/500 [03:43<1:31:56, 11.47s/it]  4%|▍         | 20/500 [03:55<1:31:36, 11.45s/it]  4%|▍         | 21/500 [04:07<1:32:09, 11.54s/it]  4%|▍         | 22/500 [04:18<1:31:43, 11.51s/it]  5%|▍         | 23/500 [04:30<1:32:12, 11.60s/it]  5%|▍         | 24/500 [04:42<1:32:31, 11.66s/it]  5%|▌         | 25/500 [04:53<1:31:59, 11.62s/it]  5%|▌         | 26/500 [05:04<1:30:56, 11.51s/it]  5%|▌         | 27/500 [05:16<1:30:30, 11.48s/it]  6%|▌         | 28/500 [05:27<1:30:13, 11.47s/it]  6%|▌         | 29/500 [05:39<1:30:50, 11.57s/it]  6%|▌         | 30/500 [05:51<1:31:45, 11.71s/it]  6%|▌         | 30/500 [05:51<1:31:48, 11.72s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.021 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁█▁▁▁▁▁▁▁▁▁▆▁
wandb:     train_loss ▆█▆█▇▁▆▇▁▇▅▁▂▂▇▄▅▆▄▄▅▆▅▆▅▇▅▂▃▆
wandb:   val_accuracy ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▂▇▇▁█▇▇▇▇▇▇▇▇▂█
wandb:       val_loss █▄█▃▃▃▆▃▆▃▅▇▆▁▄▄▆▄▄▅▂▃▄▄▃▄▄▄▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31204
wandb:     train_loss 1.19831
wandb:   val_accuracy 0.35111
wandb:       val_loss 1.07923
wandb: 
wandb: 🚀 View run pleasant-dawn-139 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ya51uwbm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_020754-ya51uwbm/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_021425-mlmg3kno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-puddle-140
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mlmg3kno
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:29, 18.46s/it]  0%|          | 2/500 [00:30<2:00:51, 14.56s/it]  1%|          | 3/500 [00:41<1:49:40, 13.24s/it]  1%|          | 4/500 [00:54<1:48:14, 13.09s/it]  1%|          | 5/500 [01:06<1:44:08, 12.62s/it]  1%|          | 6/500 [01:19<1:45:31, 12.82s/it]  1%|▏         | 7/500 [01:31<1:42:49, 12.51s/it]  2%|▏         | 8/500 [01:44<1:43:36, 12.64s/it]  2%|▏         | 9/500 [01:56<1:41:08, 12.36s/it]  2%|▏         | 10/500 [02:08<1:40:20, 12.29s/it]  2%|▏         | 11/500 [02:20<1:39:45, 12.24s/it]  2%|▏         | 12/500 [02:32<1:38:51, 12.16s/it]  3%|▎         | 13/500 [02:45<1:41:32, 12.51s/it]  3%|▎         | 14/500 [02:57<1:40:02, 12.35s/it]  3%|▎         | 15/500 [03:10<1:40:12, 12.40s/it]  3%|▎         | 16/500 [03:22<1:38:32, 12.22s/it]  3%|▎         | 17/500 [03:38<1:49:06, 13.55s/it]  4%|▎         | 18/500 [03:50<1:44:58, 13.07s/it]  4%|▍         | 19/500 [04:03<1:42:44, 12.82s/it]  4%|▍         | 20/500 [04:15<1:41:06, 12.64s/it]  4%|▍         | 21/500 [04:27<1:40:28, 12.59s/it]  4%|▍         | 22/500 [04:39<1:37:39, 12.26s/it]  5%|▍         | 23/500 [04:51<1:37:08, 12.22s/it]  5%|▍         | 24/500 [05:03<1:36:13, 12.13s/it]  5%|▌         | 25/500 [05:15<1:35:12, 12.03s/it]  5%|▌         | 26/500 [05:26<1:34:29, 11.96s/it]  5%|▌         | 27/500 [05:38<1:33:41, 11.88s/it]  6%|▌         | 28/500 [05:50<1:34:00, 11.95s/it]  6%|▌         | 29/500 [06:02<1:33:54, 11.96s/it]  6%|▌         | 29/500 [06:02<1:38:10, 12.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.184 MB of 0.311 MB uploadedwandb: \ 0.184 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▃▆▇▅▆▅█▇█▇██▇▇▇▇▇█▇██▇▇▇█
wandb:     train_loss ▃▅▃▂▂▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▄▁
wandb:   val_accuracy ▂▃▃▆▃▁▄▄▆▅▇▇▇▇▅█▇▆▇▆▇▇▇▆▇▇▆▇▇
wandb:       val_loss ▂▂▁▂▂▄▅▄▃▅▁▆▁▄▄▁▅▅▁▇▅▄▁▁▄▁▂▃█
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.99703
wandb:     train_loss 1e-05
wandb:   val_accuracy 0.52222
wandb:       val_loss 5.57091
wandb: 
wandb: 🚀 View run firm-puddle-140 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mlmg3kno
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_021425-mlmg3kno/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_022111-bj27xt5w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-star-141
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bj27xt5w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:52, 13.81s/it]  0%|          | 2/500 [00:25<1:45:17, 12.69s/it]  1%|          | 3/500 [00:37<1:41:54, 12.30s/it]  1%|          | 4/500 [00:49<1:40:12, 12.12s/it]  1%|          | 5/500 [01:00<1:38:20, 11.92s/it]  1%|          | 6/500 [01:12<1:37:20, 11.82s/it]  1%|▏         | 7/500 [01:24<1:37:17, 11.84s/it]  2%|▏         | 8/500 [01:36<1:38:23, 12.00s/it]  2%|▏         | 9/500 [01:48<1:37:30, 11.92s/it]  2%|▏         | 10/500 [02:00<1:37:38, 11.96s/it]  2%|▏         | 11/500 [02:12<1:37:15, 11.93s/it]  2%|▏         | 12/500 [02:24<1:36:05, 11.81s/it]  3%|▎         | 13/500 [02:35<1:36:18, 11.87s/it]  3%|▎         | 14/500 [02:47<1:34:58, 11.72s/it]  3%|▎         | 15/500 [02:59<1:34:59, 11.75s/it]  3%|▎         | 16/500 [03:11<1:35:15, 11.81s/it]  3%|▎         | 17/500 [03:22<1:34:39, 11.76s/it]  4%|▎         | 18/500 [03:34<1:34:43, 11.79s/it]  4%|▍         | 19/500 [03:46<1:35:28, 11.91s/it]  4%|▍         | 20/500 [03:58<1:35:19, 11.91s/it]  4%|▍         | 21/500 [04:11<1:36:02, 12.03s/it]  4%|▍         | 22/500 [04:22<1:35:30, 11.99s/it]  5%|▍         | 23/500 [04:34<1:34:47, 11.92s/it]  5%|▍         | 24/500 [04:46<1:34:12, 11.88s/it]  5%|▌         | 25/500 [04:57<1:33:04, 11.76s/it]  5%|▌         | 26/500 [05:09<1:31:27, 11.58s/it]  5%|▌         | 26/500 [05:09<1:33:55, 11.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.230 MB of 0.316 MB uploadedwandb: - 0.230 MB of 0.316 MB uploadedwandb: \ 0.230 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▄▂▅▂▂▂▂█▂▂▁▂▂▂▂▂▂▃▂▂▁▂▂█
wandb:     train_loss ▂▂▂▂▂▂▁▁▅▁▂▄▂█▁▁▃▂▅▂▅▁▂▁▂▁
wandb:   val_accuracy ▃▃▂▂▄▂▂▂▂█▂▂▁▂▂▂▂▃▂▃▂▂▂▃▂▅
wandb:       val_loss ▁▁▁▂▁▁▂▅▅▁▁▂▁█▃▄▂▁▄▁▅▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.72065
wandb:     train_loss 0.49288
wandb:   val_accuracy 0.41556
wandb:       val_loss 0.99085
wandb: 
wandb: 🚀 View run playful-star-141 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bj27xt5w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_022111-bj27xt5w/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_022658-2v5qp2gk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-lion-142
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2v5qp2gk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:35, 13.78s/it]  0%|          | 2/500 [00:25<1:45:02, 12.66s/it]  1%|          | 3/500 [00:37<1:40:38, 12.15s/it]  1%|          | 4/500 [00:49<1:39:50, 12.08s/it]  1%|          | 5/500 [01:00<1:38:08, 11.90s/it]  1%|          | 6/500 [01:12<1:37:00, 11.78s/it]  1%|▏         | 7/500 [01:23<1:35:24, 11.61s/it]  2%|▏         | 8/500 [01:35<1:36:04, 11.72s/it]  2%|▏         | 9/500 [01:47<1:35:27, 11.67s/it]  2%|▏         | 10/500 [01:58<1:35:29, 11.69s/it]  2%|▏         | 11/500 [02:10<1:35:57, 11.77s/it]  2%|▏         | 12/500 [02:22<1:35:04, 11.69s/it]  3%|▎         | 13/500 [02:34<1:35:04, 11.71s/it]  3%|▎         | 14/500 [02:45<1:34:38, 11.68s/it]  3%|▎         | 15/500 [02:57<1:34:00, 11.63s/it]  3%|▎         | 16/500 [03:08<1:33:16, 11.56s/it]  3%|▎         | 17/500 [03:20<1:33:12, 11.58s/it]  4%|▎         | 18/500 [03:31<1:32:39, 11.53s/it]  4%|▍         | 19/500 [03:43<1:34:16, 11.76s/it]  4%|▍         | 20/500 [03:55<1:33:47, 11.72s/it]  4%|▍         | 21/500 [04:08<1:36:03, 12.03s/it]  4%|▍         | 22/500 [04:19<1:34:41, 11.89s/it]  5%|▍         | 23/500 [04:31<1:34:02, 11.83s/it]  5%|▍         | 24/500 [04:42<1:32:26, 11.65s/it]  5%|▌         | 25/500 [04:54<1:31:39, 11.58s/it]  5%|▌         | 26/500 [05:05<1:30:24, 11.44s/it]  5%|▌         | 27/500 [05:17<1:32:29, 11.73s/it]  6%|▌         | 28/500 [05:29<1:32:01, 11.70s/it]  6%|▌         | 29/500 [05:40<1:31:46, 11.69s/it]  6%|▌         | 30/500 [05:52<1:30:33, 11.56s/it]  6%|▌         | 30/500 [05:52<1:31:59, 11.74s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.021 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁█▄█▅
wandb:     train_loss ▆█▆▇▇▁▇▇▂▇▆▂▃▂▇▆▃▆▅▅▅▆▅▇▅▆▄▄▄▆
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▃▂█▃
wandb:       val_loss █▄█▃▃▃▇▃▆▃▆▆▆▁▄▅▃▄▅▄▁▄▃▄▂▄▅▅▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.0
wandb: train_accuracy 0.38187
wandb:     train_loss 1.16597
wandb:   val_accuracy 0.34
wandb:       val_loss 1.08523
wandb: 
wandb: 🚀 View run fancy-lion-142 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2v5qp2gk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_022658-2v5qp2gk/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_023335-klq6mnd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-salad-143
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/klq6mnd1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:15:55, 16.34s/it]  0%|          | 2/500 [00:30<2:03:00, 14.82s/it]  1%|          | 3/500 [00:42<1:54:32, 13.83s/it]  1%|          | 4/500 [00:56<1:53:56, 13.78s/it]  1%|          | 5/500 [01:07<1:46:30, 12.91s/it]  1%|          | 6/500 [01:19<1:41:32, 12.33s/it]  1%|▏         | 7/500 [01:30<1:38:55, 12.04s/it]  2%|▏         | 8/500 [01:40<1:33:30, 11.40s/it]  2%|▏         | 9/500 [01:51<1:31:58, 11.24s/it]  2%|▏         | 10/500 [02:02<1:31:41, 11.23s/it]  2%|▏         | 11/500 [02:19<1:45:14, 12.91s/it]  2%|▏         | 12/500 [02:30<1:41:46, 12.51s/it]  3%|▎         | 13/500 [02:41<1:37:41, 12.04s/it]  3%|▎         | 14/500 [02:52<1:35:09, 11.75s/it]  3%|▎         | 15/500 [03:04<1:33:55, 11.62s/it]  3%|▎         | 16/500 [03:15<1:32:24, 11.46s/it]  3%|▎         | 17/500 [03:26<1:31:29, 11.36s/it]  4%|▎         | 18/500 [03:37<1:29:52, 11.19s/it]  4%|▍         | 19/500 [03:48<1:30:15, 11.26s/it]  4%|▍         | 20/500 [03:59<1:29:44, 11.22s/it]  4%|▍         | 21/500 [04:11<1:29:42, 11.24s/it]  4%|▍         | 22/500 [04:22<1:29:48, 11.27s/it]  5%|▍         | 23/500 [04:33<1:29:00, 11.20s/it]  5%|▍         | 24/500 [04:44<1:28:27, 11.15s/it]  5%|▌         | 25/500 [04:55<1:27:43, 11.08s/it]  5%|▌         | 26/500 [05:06<1:26:52, 11.00s/it]  5%|▌         | 27/500 [05:16<1:25:41, 10.87s/it]  5%|▌         | 27/500 [05:16<1:32:30, 11.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.021 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▅▆▅▇▄▆▇██████▆▇▇▆█████▇█
wandb:     train_loss ▆█▅▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▃▁██▂▄▁██▇▇▇▇▆▇█▆███▅▇▆▇█▇
wandb:       val_loss ▂▂▂▂▂▄▆▄▁▁▁▆▁▄▃▁▅▅▁█▄▂▁▁▃▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.95691
wandb:     train_loss 0.00514
wandb:   val_accuracy 0.52667
wandb:       val_loss 0.1173
wandb: 
wandb: 🚀 View run twilight-salad-143 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/klq6mnd1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_023335-klq6mnd1/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_023936-yzw3dg49
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-cosmos-144
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yzw3dg49
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:37:57, 18.99s/it]  0%|          | 2/500 [00:31<2:03:36, 14.89s/it]  1%|          | 3/500 [00:42<1:51:08, 13.42s/it]  1%|          | 4/500 [00:54<1:45:29, 12.76s/it]  1%|          | 5/500 [01:06<1:42:48, 12.46s/it]  1%|          | 6/500 [01:18<1:40:44, 12.24s/it]  1%|▏         | 7/500 [01:30<1:40:49, 12.27s/it]  2%|▏         | 8/500 [01:46<1:51:20, 13.58s/it]  2%|▏         | 9/500 [01:58<1:46:55, 13.07s/it]  2%|▏         | 10/500 [02:10<1:43:18, 12.65s/it]  2%|▏         | 11/500 [02:21<1:40:06, 12.28s/it]  2%|▏         | 12/500 [02:33<1:38:00, 12.05s/it]  3%|▎         | 13/500 [02:45<1:36:33, 11.90s/it]  3%|▎         | 14/500 [02:56<1:36:27, 11.91s/it]  3%|▎         | 15/500 [03:08<1:36:10, 11.90s/it]  3%|▎         | 16/500 [03:20<1:34:46, 11.75s/it]  3%|▎         | 17/500 [03:31<1:34:14, 11.71s/it]  4%|▎         | 18/500 [03:44<1:35:05, 11.84s/it]  4%|▍         | 19/500 [03:55<1:34:04, 11.74s/it]  4%|▍         | 20/500 [04:06<1:32:49, 11.60s/it]  4%|▍         | 21/500 [04:18<1:33:40, 11.73s/it]  4%|▍         | 22/500 [04:31<1:34:33, 11.87s/it]  5%|▍         | 23/500 [04:42<1:33:22, 11.75s/it]  5%|▍         | 24/500 [04:54<1:34:11, 11.87s/it]  5%|▌         | 25/500 [05:06<1:33:30, 11.81s/it]  5%|▌         | 26/500 [05:18<1:33:16, 11.81s/it]  5%|▌         | 27/500 [05:29<1:32:28, 11.73s/it]  6%|▌         | 28/500 [05:41<1:33:33, 11.89s/it]  6%|▌         | 29/500 [05:54<1:33:55, 11.96s/it]  6%|▌         | 30/500 [06:05<1:33:14, 11.90s/it]  6%|▌         | 31/500 [06:17<1:32:14, 11.80s/it]  6%|▋         | 32/500 [06:29<1:31:35, 11.74s/it]  7%|▋         | 33/500 [06:40<1:31:34, 11.76s/it]  7%|▋         | 34/500 [06:52<1:32:09, 11.87s/it]  7%|▋         | 35/500 [07:09<1:42:11, 13.19s/it]  7%|▋         | 36/500 [07:20<1:38:27, 12.73s/it]  7%|▋         | 37/500 [07:32<1:35:41, 12.40s/it]  8%|▊         | 38/500 [07:44<1:33:28, 12.14s/it]  8%|▊         | 39/500 [07:55<1:31:07, 11.86s/it]  8%|▊         | 40/500 [08:06<1:30:29, 11.80s/it]  8%|▊         | 41/500 [08:18<1:29:25, 11.69s/it]  8%|▊         | 42/500 [08:30<1:30:08, 11.81s/it]  9%|▊         | 43/500 [08:41<1:29:18, 11.72s/it]  9%|▉         | 44/500 [08:53<1:29:34, 11.79s/it]  9%|▉         | 45/500 [09:05<1:28:59, 11.74s/it]  9%|▉         | 46/500 [09:17<1:28:32, 11.70s/it]  9%|▉         | 47/500 [09:28<1:27:35, 11.60s/it] 10%|▉         | 48/500 [09:40<1:27:19, 11.59s/it] 10%|▉         | 49/500 [09:52<1:28:17, 11.75s/it] 10%|█         | 50/500 [10:03<1:27:45, 11.70s/it] 10%|█         | 51/500 [10:15<1:27:19, 11.67s/it] 10%|█         | 52/500 [10:26<1:26:58, 11.65s/it] 11%|█         | 53/500 [10:38<1:27:11, 11.70s/it] 11%|█         | 54/500 [10:50<1:27:25, 11.76s/it] 11%|█         | 55/500 [11:02<1:26:19, 11.64s/it] 11%|█         | 56/500 [11:13<1:26:30, 11.69s/it] 11%|█▏        | 57/500 [11:25<1:27:15, 11.82s/it] 12%|█▏        | 58/500 [11:37<1:26:29, 11.74s/it] 12%|█▏        | 59/500 [11:49<1:26:06, 11.72s/it] 12%|█▏        | 60/500 [12:00<1:25:05, 11.60s/it] 12%|█▏        | 61/500 [12:11<1:24:06, 11.50s/it] 12%|█▏        | 62/500 [12:22<1:23:16, 11.41s/it] 13%|█▎        | 63/500 [12:34<1:23:32, 11.47s/it] 13%|█▎        | 64/500 [12:46<1:24:00, 11.56s/it] 13%|█▎        | 65/500 [12:59<1:26:13, 11.89s/it] 13%|█▎        | 66/500 [13:11<1:26:36, 11.97s/it] 13%|█▎        | 67/500 [13:23<1:26:43, 12.02s/it] 14%|█▎        | 68/500 [13:36<1:28:17, 12.26s/it] 14%|█▍        | 69/500 [13:48<1:27:24, 12.17s/it] 14%|█▍        | 70/500 [14:00<1:27:46, 12.25s/it] 14%|█▍        | 71/500 [14:12<1:26:35, 12.11s/it] 14%|█▍        | 72/500 [14:24<1:26:31, 12.13s/it] 15%|█▍        | 73/500 [14:36<1:25:03, 11.95s/it] 15%|█▍        | 74/500 [14:48<1:25:21, 12.02s/it] 15%|█▍        | 74/500 [14:48<1:25:13, 12.00s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.135 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▄▃▃▂▃▃▃▃▃▃█▁▃▆▅▃▃█▃▂▇▄█▃▇▇▅▅█▆█▇█▆▇▇██
wandb:     train_loss ▂▂▂▂▁▂▆█▁▄▃▆▂▂▅▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁
wandb:   val_accuracy ▄▄▃▃▃▃▃▃▃▃▃▃▇▁▃▅▅▃▃▇▃▂▅▄█▃▅▅▅▅█▅█▇▇▅▆▅▆▇
wandb:       val_loss ▂▂▂▂▆▂▃█▃▂▃▆▁▂▃▂▂▃▁▁▄▂▁▂▂▂▁▂▁▂▁▂▂▁▁▂▁▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 73
wandb:  learning_rate 0.0
wandb: train_accuracy 0.80089
wandb:     train_loss 0.20133
wandb:   val_accuracy 0.48222
wandb:       val_loss 1.11767
wandb: 
wandb: 🚀 View run iconic-cosmos-144 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/yzw3dg49
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_023936-yzw3dg49/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_025508-xa7e9dwf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-fog-145
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xa7e9dwf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:12:28, 15.93s/it]  0%|          | 2/500 [00:29<1:59:23, 14.38s/it]  1%|          | 3/500 [00:43<1:57:52, 14.23s/it]  1%|          | 4/500 [00:56<1:53:36, 13.74s/it]  1%|          | 5/500 [01:11<1:56:52, 14.17s/it]  1%|          | 6/500 [01:23<1:52:39, 13.68s/it]  1%|▏         | 7/500 [01:37<1:53:15, 13.78s/it]  2%|▏         | 8/500 [01:50<1:49:37, 13.37s/it]  2%|▏         | 9/500 [02:02<1:47:15, 13.11s/it]  2%|▏         | 10/500 [02:17<1:49:59, 13.47s/it]  2%|▏         | 11/500 [02:31<1:51:47, 13.72s/it]  2%|▏         | 12/500 [02:44<1:50:47, 13.62s/it]  3%|▎         | 13/500 [02:59<1:53:54, 14.03s/it]  3%|▎         | 14/500 [03:13<1:53:19, 13.99s/it]  3%|▎         | 15/500 [03:26<1:51:09, 13.75s/it]  3%|▎         | 16/500 [03:40<1:50:46, 13.73s/it]  3%|▎         | 17/500 [03:55<1:52:08, 13.93s/it]  4%|▎         | 18/500 [04:08<1:49:53, 13.68s/it]  4%|▍         | 19/500 [04:21<1:48:35, 13.55s/it]  4%|▍         | 20/500 [04:33<1:44:52, 13.11s/it]  4%|▍         | 21/500 [04:46<1:44:09, 13.05s/it]  4%|▍         | 22/500 [04:59<1:45:10, 13.20s/it]  5%|▍         | 23/500 [05:14<1:48:35, 13.66s/it]  5%|▍         | 24/500 [05:27<1:46:52, 13.47s/it]  5%|▌         | 25/500 [05:41<1:46:21, 13.44s/it]  5%|▌         | 26/500 [05:53<1:44:16, 13.20s/it]  5%|▌         | 27/500 [06:06<1:42:48, 13.04s/it]  6%|▌         | 28/500 [06:18<1:40:13, 12.74s/it]  6%|▌         | 29/500 [06:30<1:37:51, 12.47s/it]  6%|▌         | 30/500 [06:42<1:36:05, 12.27s/it]  6%|▌         | 31/500 [06:54<1:36:12, 12.31s/it]  6%|▋         | 32/500 [07:06<1:34:53, 12.17s/it]  7%|▋         | 33/500 [07:18<1:35:09, 12.23s/it]  7%|▋         | 34/500 [07:30<1:34:31, 12.17s/it]  7%|▋         | 35/500 [07:42<1:33:16, 12.04s/it]  7%|▋         | 36/500 [07:55<1:34:53, 12.27s/it]  7%|▋         | 37/500 [08:08<1:35:56, 12.43s/it]  7%|▋         | 37/500 [08:08<1:41:47, 13.19s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▂▁▂▁▁▁▁██▄▆▁▁▅▁▁▁▁
wandb:     train_loss ▆█▆██▁█▇▁▇▆▂▂▃▆▇▅▆▄▄▅▆▅▇▅▆▄▄▄▆▆▆▄▅▅▅▃
wandb:   val_accuracy ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▂▄▄▄▄▂▅█▁▄▄▆▄▄▄▄
wandb:       val_loss █▄█▃▃▃▇▃▆▃▆▆▆▃▅▆▅▄▅▅▁▄▃▃▃▃▅▅▄▄▇▅▅▆▃▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31055
wandb:     train_loss 0.98329
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.09036
wandb: 
wandb: 🚀 View run bright-fog-145 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/xa7e9dwf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_025508-xa7e9dwf/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_030356-9h4mbkh4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-snowflake-146
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9h4mbkh4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:11:11, 15.77s/it]  0%|          | 2/500 [00:29<2:02:01, 14.70s/it]  1%|          | 3/500 [00:42<1:54:34, 13.83s/it]  1%|          | 4/500 [00:54<1:49:38, 13.26s/it]  1%|          | 5/500 [01:07<1:47:49, 13.07s/it]  1%|          | 6/500 [01:19<1:45:29, 12.81s/it]  1%|▏         | 7/500 [01:32<1:44:54, 12.77s/it]  2%|▏         | 8/500 [01:45<1:44:26, 12.74s/it]  2%|▏         | 9/500 [02:02<1:54:50, 14.03s/it]  2%|▏         | 10/500 [02:15<1:52:30, 13.78s/it]  2%|▏         | 11/500 [02:27<1:49:18, 13.41s/it]  2%|▏         | 12/500 [02:40<1:46:53, 13.14s/it]  3%|▎         | 13/500 [02:53<1:45:11, 12.96s/it]  3%|▎         | 14/500 [03:06<1:45:20, 13.00s/it]  3%|▎         | 15/500 [03:19<1:45:45, 13.08s/it]  3%|▎         | 16/500 [03:32<1:44:57, 13.01s/it]  3%|▎         | 17/500 [03:44<1:44:03, 12.93s/it]  4%|▎         | 18/500 [03:57<1:43:52, 12.93s/it]  4%|▍         | 19/500 [04:10<1:42:52, 12.83s/it]  4%|▍         | 20/500 [04:22<1:41:40, 12.71s/it]  4%|▍         | 21/500 [04:35<1:42:02, 12.78s/it]  4%|▍         | 22/500 [04:47<1:39:47, 12.53s/it]  5%|▍         | 23/500 [04:59<1:38:24, 12.38s/it]  5%|▍         | 24/500 [05:12<1:38:49, 12.46s/it]  5%|▌         | 25/500 [05:24<1:37:37, 12.33s/it]  5%|▌         | 26/500 [05:36<1:36:39, 12.24s/it]  5%|▌         | 27/500 [05:48<1:35:35, 12.13s/it]  6%|▌         | 28/500 [06:00<1:35:57, 12.20s/it]  6%|▌         | 29/500 [06:13<1:36:31, 12.30s/it]  6%|▌         | 29/500 [06:13<1:41:03, 12.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.137 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▂▄▆▃██▇▅▆▅█▆▇▇▄▆▇▆▆▆▆▆▇▆▆
wandb:     train_loss ▂▂▂▁▃▆▁▁▁▁▁▇▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:   val_accuracy ▁▂▃▆▁▅▆▁▅▄▆█▆▅▅▇▆▅▄▇▆▆▆▆▆▆▆▆▆
wandb:       val_loss ▂▂▂▂▂▃▃▆▁▄▁▁▁▆▄▁▄▆▁▇█▄▄▅▇▄▄▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.77266
wandb:     train_loss 0.00545
wandb:   val_accuracy 0.53333
wandb:       val_loss 3.93129
wandb: 
wandb: 🚀 View run sparkling-snowflake-146 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9h4mbkh4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_030356-9h4mbkh4/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_031049-9e2qex36
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-snowflake-147
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9e2qex36
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:05:49, 15.13s/it]  0%|          | 2/500 [00:27<1:54:29, 13.79s/it]  1%|          | 3/500 [00:40<1:49:52, 13.26s/it]  1%|          | 4/500 [00:53<1:47:33, 13.01s/it]  1%|          | 5/500 [01:06<1:47:34, 13.04s/it]  1%|          | 6/500 [01:19<1:46:55, 12.99s/it]  1%|▏         | 7/500 [01:32<1:48:01, 13.15s/it]  2%|▏         | 8/500 [01:45<1:47:18, 13.09s/it]  2%|▏         | 9/500 [01:58<1:46:41, 13.04s/it]  2%|▏         | 10/500 [02:11<1:45:26, 12.91s/it]  2%|▏         | 11/500 [02:24<1:45:34, 12.95s/it]  2%|▏         | 12/500 [02:37<1:44:50, 12.89s/it]  3%|▎         | 13/500 [02:50<1:46:24, 13.11s/it]  3%|▎         | 14/500 [03:03<1:45:34, 13.03s/it]  3%|▎         | 15/500 [03:16<1:45:15, 13.02s/it]  3%|▎         | 16/500 [03:30<1:46:40, 13.22s/it]  3%|▎         | 17/500 [03:43<1:45:50, 13.15s/it]  4%|▎         | 18/500 [03:55<1:44:46, 13.04s/it]  4%|▍         | 19/500 [04:08<1:42:26, 12.78s/it]  4%|▍         | 20/500 [04:19<1:40:01, 12.50s/it]  4%|▍         | 21/500 [04:32<1:38:43, 12.37s/it]  4%|▍         | 22/500 [04:44<1:39:04, 12.44s/it]  5%|▍         | 23/500 [04:57<1:39:42, 12.54s/it]  5%|▍         | 24/500 [05:09<1:39:05, 12.49s/it]  5%|▌         | 25/500 [05:22<1:38:47, 12.48s/it]  5%|▌         | 26/500 [05:34<1:38:58, 12.53s/it]  5%|▌         | 27/500 [05:47<1:38:16, 12.47s/it]  5%|▌         | 27/500 [05:47<1:41:23, 12.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.021 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▃▂▁▁▂▁▁▁▂▁▁▁▁▁▂▂▁▁▂▅█▂▁▁
wandb:     train_loss ▂▂▂▂▂▃▁▂█▁▂▃▄█▁▁▆▂▃▄▆▁▂▁▂▁▅
wandb:   val_accuracy ▅▅▄▄▃▄▄▄▄▄▁▃▄▄▄▄▄▄▃▄▄▃▅█▅▄▄
wandb:       val_loss ▁▁▁▁▁▁▁▁▅▅▁▁▁▅▂█▂▁▁▁▄▂▁▁▁▅▂
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 4.27836
wandb:   val_accuracy 0.32222
wandb:       val_loss 2.30402
wandb: 
wandb: 🚀 View run worldly-snowflake-147 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9e2qex36
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_031049-9e2qex36/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_031721-rqd2ss42
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-forest-148
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rqd2ss42
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:02:07, 14.68s/it]  0%|          | 2/500 [00:27<1:51:19, 13.41s/it]  1%|          | 3/500 [00:40<1:51:18, 13.44s/it]  1%|          | 4/500 [00:53<1:50:38, 13.38s/it]  1%|          | 5/500 [01:06<1:48:17, 13.13s/it]  1%|          | 6/500 [01:20<1:49:05, 13.25s/it]  1%|▏         | 7/500 [01:34<1:52:26, 13.68s/it]  2%|▏         | 8/500 [01:46<1:48:11, 13.19s/it]  2%|▏         | 9/500 [02:00<1:48:49, 13.30s/it]  2%|▏         | 10/500 [02:13<1:46:56, 13.09s/it]  2%|▏         | 11/500 [02:26<1:47:04, 13.14s/it]  2%|▏         | 12/500 [02:40<1:48:37, 13.36s/it]  3%|▎         | 13/500 [02:53<1:47:34, 13.25s/it]  3%|▎         | 14/500 [03:04<1:43:43, 12.81s/it]  3%|▎         | 15/500 [03:17<1:43:51, 12.85s/it]  3%|▎         | 16/500 [03:29<1:41:43, 12.61s/it]  3%|▎         | 17/500 [03:43<1:43:35, 12.87s/it]  4%|▎         | 18/500 [03:57<1:46:26, 13.25s/it]  4%|▍         | 19/500 [04:11<1:47:47, 13.45s/it]  4%|▍         | 20/500 [04:25<1:47:56, 13.49s/it]  4%|▍         | 21/500 [04:38<1:48:50, 13.63s/it]  4%|▍         | 22/500 [04:52<1:48:08, 13.57s/it]  5%|▍         | 23/500 [05:04<1:44:36, 13.16s/it]  5%|▍         | 24/500 [05:18<1:47:14, 13.52s/it]  5%|▌         | 25/500 [05:32<1:47:30, 13.58s/it]  5%|▌         | 26/500 [05:45<1:45:45, 13.39s/it]  5%|▌         | 27/500 [05:59<1:46:08, 13.46s/it]  6%|▌         | 28/500 [06:13<1:48:28, 13.79s/it]  6%|▌         | 29/500 [06:30<1:55:58, 14.77s/it]  6%|▌         | 30/500 [06:44<1:52:25, 14.35s/it]  6%|▌         | 30/500 [06:44<1:45:33, 13.48s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.021 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁█▁
wandb:     train_loss ▆█▆█▇▁▆▇▂▆▄▁▂▁▆▆▅▆▅▄▅▇▅▆▅▇▄▂▄▆
wandb:   val_accuracy ▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▃▆█▆
wandb:       val_loss █▄█▃▃▃▆▃▅▄▄▇▆▁▅▅▆▄▅▅▂▄▄▄▃▄▅▅▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31055
wandb:     train_loss 1.20433
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.07261
wandb: 
wandb: 🚀 View run worthy-forest-148 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/rqd2ss42
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_031721-rqd2ss42/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_032454-cockpy8q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-cherry-149
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/cockpy8q
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:21:51, 17.06s/it]  0%|          | 2/500 [00:30<2:05:46, 15.15s/it]  1%|          | 3/500 [00:42<1:53:49, 13.74s/it]  1%|          | 4/500 [00:54<1:46:59, 12.94s/it]  1%|          | 5/500 [01:06<1:42:52, 12.47s/it]  1%|          | 6/500 [01:17<1:39:42, 12.11s/it]  1%|▏         | 7/500 [01:28<1:36:12, 11.71s/it]  2%|▏         | 8/500 [01:40<1:35:49, 11.69s/it]  2%|▏         | 9/500 [01:51<1:34:54, 11.60s/it]  2%|▏         | 10/500 [02:02<1:33:52, 11.50s/it]  2%|▏         | 11/500 [02:14<1:33:00, 11.41s/it]  2%|▏         | 12/500 [02:25<1:33:35, 11.51s/it]  3%|▎         | 13/500 [02:37<1:33:50, 11.56s/it]  3%|▎         | 14/500 [02:48<1:33:04, 11.49s/it]  3%|▎         | 15/500 [02:59<1:31:02, 11.26s/it]  3%|▎         | 16/500 [03:10<1:30:39, 11.24s/it]  3%|▎         | 17/500 [03:22<1:30:52, 11.29s/it]  4%|▎         | 18/500 [03:33<1:31:42, 11.42s/it]  4%|▍         | 19/500 [03:45<1:31:02, 11.36s/it]  4%|▍         | 20/500 [03:56<1:30:36, 11.33s/it]  4%|▍         | 21/500 [04:07<1:30:53, 11.39s/it]  4%|▍         | 22/500 [04:19<1:31:04, 11.43s/it]  5%|▍         | 23/500 [04:31<1:31:52, 11.56s/it]  5%|▍         | 24/500 [04:43<1:33:10, 11.74s/it]  5%|▌         | 25/500 [04:54<1:31:34, 11.57s/it]  5%|▌         | 26/500 [05:06<1:31:03, 11.53s/it]  5%|▌         | 27/500 [05:17<1:31:16, 11.58s/it]  6%|▌         | 28/500 [05:30<1:32:47, 11.79s/it]  6%|▌         | 29/500 [05:41<1:31:22, 11.64s/it]  6%|▌         | 30/500 [05:52<1:30:09, 11.51s/it]  6%|▌         | 31/500 [06:03<1:29:49, 11.49s/it]  6%|▋         | 32/500 [06:15<1:29:15, 11.44s/it]  7%|▋         | 33/500 [06:26<1:28:46, 11.41s/it]  7%|▋         | 34/500 [06:37<1:27:57, 11.33s/it]  7%|▋         | 35/500 [06:49<1:28:15, 11.39s/it]  7%|▋         | 36/500 [07:00<1:27:39, 11.33s/it]  7%|▋         | 37/500 [07:11<1:27:06, 11.29s/it]  8%|▊         | 38/500 [07:22<1:26:34, 11.24s/it]  8%|▊         | 39/500 [07:34<1:26:43, 11.29s/it]  8%|▊         | 40/500 [07:45<1:27:06, 11.36s/it]  8%|▊         | 41/500 [07:57<1:27:55, 11.49s/it]  8%|▊         | 42/500 [08:08<1:27:00, 11.40s/it]  9%|▊         | 43/500 [08:20<1:26:52, 11.41s/it]  9%|▉         | 44/500 [08:31<1:26:54, 11.44s/it]  9%|▉         | 45/500 [08:42<1:26:00, 11.34s/it]  9%|▉         | 46/500 [08:53<1:25:13, 11.26s/it]  9%|▉         | 47/500 [09:05<1:25:30, 11.33s/it] 10%|▉         | 48/500 [09:17<1:26:30, 11.48s/it] 10%|▉         | 49/500 [09:28<1:25:47, 11.41s/it] 10%|█         | 50/500 [09:41<1:28:14, 11.77s/it] 10%|█         | 51/500 [09:53<1:29:12, 11.92s/it] 10%|█         | 52/500 [10:05<1:30:30, 12.12s/it] 11%|█         | 53/500 [10:18<1:30:33, 12.16s/it] 11%|█         | 54/500 [10:29<1:28:08, 11.86s/it] 11%|█         | 55/500 [10:40<1:27:03, 11.74s/it] 11%|█         | 56/500 [10:52<1:26:29, 11.69s/it] 11%|█         | 56/500 [10:52<1:26:11, 11.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.232 MB of 0.316 MB uploadedwandb: | 0.232 MB of 0.316 MB uploadedwandb: / 0.232 MB of 0.316 MB uploadedwandb: - 0.232 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▇▅▄▇▆▆▆▆▅▇▇▇▆▆▇▆▅▇▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇▇█
wandb:     train_loss ▂▂▂▁▃▁▂▁▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▁▁▁▂▁▁▁▂▁▁▃▁
wandb:   val_accuracy ▁▁▆▃▄▆▃▇▇▆▆▆▆▇▆▁▅▄▂▇▅▇▇▅█▃▄██▄▆▄▆▆▇▅▆▆▇▆
wandb:       val_loss ▂▂▁▁▂▂▁▂▁▁▂▃▁▁▂▄▅▅▇▄▄▃▁▃▁▅▃▃▁█▅▃▁▃▂▃▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.80981
wandb:     train_loss 0.0
wandb:   val_accuracy 0.55111
wandb:       val_loss 2.70036
wandb: 
wandb: 🚀 View run devout-cherry-149 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/cockpy8q
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_032454-cockpy8q/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_033631-zlhpjvx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-cosmos-150
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zlhpjvx2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:40:16, 19.27s/it]  0%|          | 2/500 [00:32<2:10:47, 15.76s/it]  1%|          | 3/500 [00:45<2:01:24, 14.66s/it]  1%|          | 4/500 [00:58<1:54:07, 13.80s/it]  1%|          | 5/500 [01:11<1:51:54, 13.57s/it]  1%|          | 6/500 [01:24<1:48:37, 13.19s/it]  1%|▏         | 7/500 [01:36<1:45:09, 12.80s/it]  2%|▏         | 8/500 [01:47<1:41:40, 12.40s/it]  2%|▏         | 9/500 [01:59<1:41:23, 12.39s/it]  2%|▏         | 10/500 [02:11<1:39:21, 12.17s/it]  2%|▏         | 11/500 [02:23<1:38:03, 12.03s/it]  2%|▏         | 12/500 [02:35<1:37:13, 11.95s/it]  3%|▎         | 13/500 [02:46<1:35:56, 11.82s/it]  3%|▎         | 14/500 [02:58<1:35:54, 11.84s/it]  3%|▎         | 15/500 [03:10<1:37:14, 12.03s/it]  3%|▎         | 16/500 [03:22<1:35:48, 11.88s/it]  3%|▎         | 17/500 [03:35<1:37:45, 12.14s/it]  4%|▎         | 18/500 [03:48<1:39:07, 12.34s/it]  4%|▍         | 19/500 [04:00<1:38:41, 12.31s/it]  4%|▍         | 20/500 [04:11<1:36:15, 12.03s/it]  4%|▍         | 21/500 [04:23<1:35:41, 11.99s/it]  4%|▍         | 22/500 [04:35<1:34:32, 11.87s/it]  5%|▍         | 23/500 [04:47<1:34:36, 11.90s/it]  5%|▍         | 24/500 [04:58<1:34:09, 11.87s/it]  5%|▌         | 25/500 [05:10<1:33:59, 11.87s/it]  5%|▌         | 26/500 [05:22<1:33:05, 11.78s/it]  5%|▌         | 27/500 [05:34<1:34:15, 11.96s/it]  6%|▌         | 28/500 [05:46<1:34:32, 12.02s/it]  6%|▌         | 29/500 [05:59<1:35:15, 12.14s/it]  6%|▌         | 30/500 [06:11<1:34:27, 12.06s/it]  6%|▌         | 31/500 [06:23<1:35:51, 12.26s/it]  6%|▋         | 32/500 [06:36<1:35:28, 12.24s/it]  7%|▋         | 33/500 [06:48<1:34:57, 12.20s/it]  7%|▋         | 34/500 [06:59<1:33:26, 12.03s/it]  7%|▋         | 35/500 [07:11<1:32:15, 11.90s/it]  7%|▋         | 36/500 [07:23<1:32:13, 11.93s/it]  7%|▋         | 37/500 [07:34<1:30:39, 11.75s/it]  8%|▊         | 38/500 [07:47<1:31:36, 11.90s/it]  8%|▊         | 39/500 [07:59<1:33:11, 12.13s/it]  8%|▊         | 40/500 [08:12<1:34:35, 12.34s/it]  8%|▊         | 41/500 [08:25<1:34:46, 12.39s/it]  8%|▊         | 42/500 [08:36<1:32:29, 12.12s/it]  9%|▊         | 43/500 [08:49<1:33:20, 12.25s/it]  9%|▉         | 44/500 [09:00<1:30:49, 11.95s/it]  9%|▉         | 45/500 [09:11<1:29:43, 11.83s/it]  9%|▉         | 46/500 [09:23<1:29:37, 11.85s/it]  9%|▉         | 47/500 [09:35<1:29:11, 11.81s/it] 10%|▉         | 48/500 [09:47<1:29:36, 11.89s/it] 10%|▉         | 49/500 [09:59<1:29:24, 11.90s/it] 10%|█         | 50/500 [10:10<1:27:42, 11.69s/it] 10%|█         | 51/500 [10:22<1:28:30, 11.83s/it] 10%|█         | 52/500 [10:33<1:26:47, 11.62s/it] 11%|█         | 53/500 [10:45<1:27:10, 11.70s/it] 11%|█         | 54/500 [10:58<1:28:39, 11.93s/it] 11%|█         | 55/500 [11:09<1:27:34, 11.81s/it] 11%|█         | 56/500 [11:21<1:26:28, 11.69s/it] 11%|█         | 56/500 [11:21<1:30:01, 12.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.019 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▄▄▄▅▄▃▄▄▄▄▃▄▄▄▄▂▄▁▁█▄▄▃▆▄▅▅▂▅▆█▇█▅▅▄▅▆▆▅
wandb:     train_loss ▃▃▂▃▇▂▃▁▃▄▂▂▃▃▄▃▄▃▃▂█▃▃▂▅▂▂▂▃▂▂▂▂▁▃▅▁▂▂▁
wandb:   val_accuracy ▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▂▄▁▂█▄▅▄▅▄▄▃▂▄▅▆▆▆▄▄▄▅▆▅▄
wandb:       val_loss ▂▂▂▂▅▂▂█▂▂▂▂▂▂▂▂▃▂▂▁▆▂▂▂▂▂▂▂▁▂▂▂▁▂▂▃▂▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.0
wandb: train_accuracy 0.44131
wandb:     train_loss 0.34206
wandb:   val_accuracy 0.33111
wandb:       val_loss 1.20104
wandb: 
wandb: 🚀 View run ethereal-cosmos-150 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zlhpjvx2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_033631-zlhpjvx2/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_034838-nycw9h39
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-wildflower-151
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nycw9h39
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:01:54, 14.66s/it]  0%|          | 2/500 [00:27<1:54:08, 13.75s/it]  1%|          | 3/500 [00:40<1:49:43, 13.25s/it]  1%|          | 4/500 [00:52<1:47:08, 12.96s/it]  1%|          | 5/500 [01:05<1:45:53, 12.84s/it]  1%|          | 6/500 [01:17<1:42:31, 12.45s/it]  1%|▏         | 7/500 [01:29<1:42:04, 12.42s/it]  2%|▏         | 8/500 [01:40<1:38:57, 12.07s/it]  2%|▏         | 9/500 [01:52<1:37:47, 11.95s/it]  2%|▏         | 10/500 [02:04<1:36:42, 11.84s/it]  2%|▏         | 11/500 [02:16<1:37:23, 11.95s/it]  2%|▏         | 12/500 [02:28<1:36:48, 11.90s/it]  3%|▎         | 13/500 [02:39<1:35:58, 11.82s/it]  3%|▎         | 14/500 [02:52<1:36:54, 11.96s/it]  3%|▎         | 15/500 [03:04<1:36:36, 11.95s/it]  3%|▎         | 16/500 [03:15<1:36:00, 11.90s/it]  3%|▎         | 17/500 [03:27<1:35:17, 11.84s/it]  4%|▎         | 18/500 [03:39<1:35:03, 11.83s/it]  4%|▍         | 19/500 [03:51<1:35:14, 11.88s/it]  4%|▍         | 20/500 [04:03<1:35:14, 11.90s/it]  4%|▍         | 21/500 [04:15<1:35:58, 12.02s/it]  4%|▍         | 22/500 [04:28<1:36:59, 12.17s/it]  5%|▍         | 23/500 [04:39<1:35:37, 12.03s/it]  5%|▍         | 24/500 [04:51<1:35:43, 12.07s/it]  5%|▌         | 25/500 [05:04<1:36:18, 12.17s/it]  5%|▌         | 26/500 [05:16<1:36:19, 12.19s/it]  5%|▌         | 27/500 [05:28<1:35:16, 12.09s/it]  6%|▌         | 28/500 [05:41<1:36:29, 12.27s/it]  6%|▌         | 29/500 [05:52<1:34:54, 12.09s/it]  6%|▌         | 30/500 [06:05<1:35:45, 12.22s/it]  6%|▌         | 30/500 [06:05<1:35:24, 12.18s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.318 MB uploadedwandb: \ 0.021 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▅▁
wandb:     train_loss ▆█▆▇█▁▆▇▂▇▅▂▂▂▆▅▅▆▅▅▅▇▅▆▅▇▅▂▄▆
wandb:   val_accuracy ███████████████▁█▇█▅████████▃█
wandb:       val_loss █▄█▃▃▃▆▃▆▃▆▆▆▁▄▄▇▄▅▅▂▃▃▄▂▃▄▄▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.0
wandb: train_accuracy 0.31055
wandb:     train_loss 1.17613
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.06961
wandb: 
wandb: 🚀 View run elated-wildflower-151 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nycw9h39
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_034838-nycw9h39/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_035545-1s5bnu6f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-valley-152
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1s5bnu6f
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:13, 18.06s/it]  0%|          | 2/500 [00:33<2:19:11, 16.77s/it]  1%|          | 3/500 [00:49<2:15:20, 16.34s/it]  1%|          | 4/500 [01:05<2:14:40, 16.29s/it]  1%|          | 5/500 [01:22<2:13:37, 16.20s/it]  1%|          | 6/500 [01:37<2:12:03, 16.04s/it]  1%|▏         | 7/500 [01:53<2:11:35, 16.02s/it]  2%|▏         | 8/500 [02:09<2:10:40, 15.94s/it]  2%|▏         | 9/500 [02:25<2:10:26, 15.94s/it]  2%|▏         | 10/500 [02:41<2:10:14, 15.95s/it]  2%|▏         | 11/500 [02:57<2:09:12, 15.85s/it]  2%|▏         | 12/500 [03:13<2:09:42, 15.95s/it]  3%|▎         | 13/500 [03:28<2:08:54, 15.88s/it]  3%|▎         | 14/500 [03:44<2:08:49, 15.91s/it]  3%|▎         | 15/500 [04:00<2:08:33, 15.90s/it]  3%|▎         | 16/500 [04:16<2:07:47, 15.84s/it]  3%|▎         | 17/500 [04:32<2:07:26, 15.83s/it]  4%|▎         | 18/500 [04:49<2:09:20, 16.10s/it]  4%|▍         | 19/500 [05:04<2:07:51, 15.95s/it]  4%|▍         | 20/500 [05:20<2:07:21, 15.92s/it]  4%|▍         | 21/500 [05:36<2:07:56, 16.03s/it]  4%|▍         | 22/500 [05:52<2:07:00, 15.94s/it]  5%|▍         | 23/500 [06:08<2:05:45, 15.82s/it]  5%|▍         | 24/500 [06:24<2:06:43, 15.97s/it]  5%|▌         | 25/500 [06:40<2:06:17, 15.95s/it]  5%|▌         | 26/500 [06:55<2:05:02, 15.83s/it]  5%|▌         | 27/500 [07:11<2:04:35, 15.80s/it]  6%|▌         | 28/500 [07:27<2:03:58, 15.76s/it]  6%|▌         | 28/500 [07:27<2:05:42, 15.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.309 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▁▇▃█▅▅▆▇█████████████████
wandb:     train_loss ▄▃▅█▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▃▁█▃▇▅▄▆▇█▇█▇▇█▆▇█▇█▇█▇▇▇▇
wandb:       val_loss ▂▃▃▂▁▂▄▄▅▃▃▁▃▃▄▅▂▂▅█▁▂▁▅▁▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.99108
wandb:     train_loss 0.00169
wandb:   val_accuracy 0.58444
wandb:       val_loss 1.52519
wandb: 
wandb: 🚀 View run super-valley-152 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1s5bnu6f
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_035545-1s5bnu6f/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_040359-aylxvmcm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-salad-153
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/aylxvmcm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:38:05, 19.01s/it]  0%|          | 2/500 [00:38<2:40:12, 19.30s/it]  1%|          | 3/500 [00:54<2:28:54, 17.98s/it]  1%|          | 4/500 [01:10<2:21:43, 17.14s/it]  1%|          | 5/500 [01:26<2:17:02, 16.61s/it]  1%|          | 6/500 [01:46<2:27:09, 17.87s/it]  1%|▏         | 7/500 [02:02<2:21:35, 17.23s/it]  2%|▏         | 8/500 [02:18<2:17:10, 16.73s/it]  2%|▏         | 9/500 [02:34<2:14:28, 16.43s/it]  2%|▏         | 10/500 [02:50<2:12:58, 16.28s/it]  2%|▏         | 11/500 [03:06<2:11:52, 16.18s/it]  2%|▏         | 12/500 [03:21<2:10:03, 15.99s/it]  3%|▎         | 13/500 [03:37<2:09:32, 15.96s/it]  3%|▎         | 14/500 [03:53<2:08:47, 15.90s/it]  3%|▎         | 15/500 [04:11<2:14:05, 16.59s/it]  3%|▎         | 16/500 [04:26<2:11:14, 16.27s/it]  3%|▎         | 17/500 [04:42<2:09:41, 16.11s/it]  4%|▎         | 18/500 [04:58<2:08:46, 16.03s/it]  4%|▍         | 19/500 [05:14<2:08:36, 16.04s/it]  4%|▍         | 20/500 [05:31<2:09:15, 16.16s/it]  4%|▍         | 21/500 [05:47<2:08:52, 16.14s/it]  4%|▍         | 22/500 [06:03<2:08:44, 16.16s/it]  5%|▍         | 23/500 [06:19<2:07:45, 16.07s/it]  5%|▍         | 24/500 [06:35<2:07:05, 16.02s/it]  5%|▌         | 25/500 [06:50<2:06:32, 15.98s/it]  5%|▌         | 26/500 [07:06<2:05:33, 15.89s/it]  5%|▌         | 27/500 [07:22<2:04:50, 15.84s/it]  6%|▌         | 28/500 [07:38<2:05:02, 15.89s/it]  6%|▌         | 29/500 [07:54<2:04:12, 15.82s/it]  6%|▌         | 30/500 [08:10<2:04:37, 15.91s/it]  6%|▌         | 31/500 [08:25<2:03:56, 15.86s/it]  6%|▋         | 32/500 [08:42<2:06:14, 16.18s/it]  7%|▋         | 33/500 [08:58<2:04:54, 16.05s/it]  7%|▋         | 34/500 [09:14<2:03:30, 15.90s/it]  7%|▋         | 35/500 [09:29<2:01:54, 15.73s/it]  7%|▋         | 36/500 [09:44<2:00:22, 15.57s/it]  7%|▋         | 37/500 [10:00<2:00:52, 15.66s/it]  8%|▊         | 38/500 [10:21<2:12:06, 17.16s/it]  8%|▊         | 39/500 [10:37<2:09:03, 16.80s/it]  8%|▊         | 40/500 [10:57<2:17:20, 17.91s/it]  8%|▊         | 41/500 [11:13<2:12:07, 17.27s/it]  8%|▊         | 42/500 [11:29<2:08:56, 16.89s/it]  9%|▊         | 43/500 [11:45<2:06:25, 16.60s/it]  9%|▉         | 44/500 [12:01<2:04:00, 16.32s/it]  9%|▉         | 45/500 [12:20<2:11:07, 17.29s/it]  9%|▉         | 46/500 [12:36<2:07:24, 16.84s/it]  9%|▉         | 46/500 [12:40<2:05:08, 16.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.019 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▄▄▆▆▇▇▇█▇▇▇█▇█████████▇██████████████
wandb:     train_loss █▇▆▇▆▅▆▆▆▄▆▃▁▇▃▇▆█▇▆▅▆▁▁▂▆▁▆▆▅▁▅▃▆▆▅▄▅▃▁
wandb:   val_accuracy ▁▁▁▄▅▆▆▇▇▇████▇████▇▇██▇▇▇▇█▇▇▇▇█▇█▇██▇▇
wandb:       val_loss ▃▃▃▃▃▂▃▃▂▃▂▁▃▃▃▄▂▄█▁▆▂▄▃▁▃▁▂▂▁▅▂▁▄▂▁▆▃▅▂
wandb: 
wandb: Run summary:
wandb:          epoch 45
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.82169
wandb:     train_loss 0.79433
wandb:   val_accuracy 0.52
wandb:       val_loss 1.46815
wandb: 
wandb: 🚀 View run misty-salad-153 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/aylxvmcm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_040359-aylxvmcm/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_041727-7mqbnacf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-bush-154
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7mqbnacf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:21<2:59:45, 21.62s/it]  0%|          | 2/500 [00:38<2:34:24, 18.60s/it]  1%|          | 3/500 [00:59<2:45:47, 20.01s/it]  1%|          | 4/500 [01:16<2:34:22, 18.67s/it]  1%|          | 5/500 [01:32<2:27:49, 17.92s/it]  1%|          | 6/500 [01:53<2:35:33, 18.89s/it]  1%|▏         | 7/500 [02:10<2:28:11, 18.04s/it]  2%|▏         | 8/500 [02:26<2:23:00, 17.44s/it]  2%|▏         | 9/500 [02:46<2:31:04, 18.46s/it]  2%|▏         | 10/500 [03:07<2:37:07, 19.24s/it]  2%|▏         | 11/500 [03:24<2:29:18, 18.32s/it]  2%|▏         | 12/500 [03:40<2:25:03, 17.83s/it]  3%|▎         | 13/500 [03:57<2:21:03, 17.38s/it]  3%|▎         | 14/500 [04:13<2:18:19, 17.08s/it]  3%|▎         | 15/500 [04:30<2:16:38, 16.90s/it]  3%|▎         | 16/500 [04:46<2:15:35, 16.81s/it]  3%|▎         | 17/500 [05:08<2:26:50, 18.24s/it]  4%|▎         | 18/500 [05:28<2:31:07, 18.81s/it]  4%|▍         | 19/500 [05:50<2:38:01, 19.71s/it]  4%|▍         | 19/500 [05:50<2:27:45, 18.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.136 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▃▁▁▁
wandb:     train_loss ██▆▁▆▇▅▅▂▂▆▅▅▆▄▄▆▂▅
wandb:   val_accuracy ▄▄▄▄▄▄▄▄▄▄▄▄█▄▁▁▄▄▂
wandb:       val_loss ▃▃▁█▃▆▇▅▇█▄▄▄▂▅▄▂▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.32095
wandb:     train_loss 1.10155
wandb:   val_accuracy 0.33111
wandb:       val_loss 1.10773
wandb: 
wandb: 🚀 View run stellar-bush-154 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/7mqbnacf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_041727-7mqbnacf/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_042402-klrswmwn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-glade-155
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/klrswmwn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:00, 18.16s/it]  0%|          | 2/500 [00:36<2:30:12, 18.10s/it]  1%|          | 3/500 [00:53<2:27:30, 17.81s/it]  1%|          | 4/500 [01:15<2:38:59, 19.23s/it]  1%|          | 5/500 [01:30<2:26:58, 17.82s/it]  1%|          | 6/500 [01:52<2:37:20, 19.11s/it]  1%|▏         | 7/500 [02:08<2:29:08, 18.15s/it]  2%|▏         | 8/500 [02:24<2:23:58, 17.56s/it]  2%|▏         | 9/500 [02:40<2:20:39, 17.19s/it]  2%|▏         | 10/500 [02:57<2:18:50, 17.00s/it]  2%|▏         | 11/500 [03:13<2:16:54, 16.80s/it]  2%|▏         | 12/500 [03:29<2:14:39, 16.56s/it]  3%|▎         | 13/500 [03:45<2:13:28, 16.44s/it]  3%|▎         | 14/500 [04:02<2:12:43, 16.39s/it]  3%|▎         | 15/500 [04:18<2:12:09, 16.35s/it]  3%|▎         | 16/500 [04:36<2:15:22, 16.78s/it]  3%|▎         | 17/500 [04:52<2:12:50, 16.50s/it]  4%|▎         | 18/500 [05:07<2:10:46, 16.28s/it]  4%|▍         | 19/500 [05:23<2:09:39, 16.17s/it]  4%|▍         | 20/500 [05:39<2:09:03, 16.13s/it]  4%|▍         | 21/500 [06:00<2:19:15, 17.44s/it]  4%|▍         | 22/500 [06:16<2:15:24, 17.00s/it]  5%|▍         | 23/500 [06:35<2:20:22, 17.66s/it]  5%|▍         | 24/500 [06:52<2:19:24, 17.57s/it]  5%|▌         | 25/500 [07:09<2:16:26, 17.23s/it]  5%|▌         | 26/500 [07:25<2:12:41, 16.80s/it]  5%|▌         | 27/500 [07:40<2:10:05, 16.50s/it]  6%|▌         | 28/500 [07:57<2:11:09, 16.67s/it]  6%|▌         | 29/500 [08:14<2:09:56, 16.55s/it]  6%|▌         | 30/500 [08:30<2:08:17, 16.38s/it]  6%|▌         | 31/500 [08:46<2:07:30, 16.31s/it]  6%|▋         | 32/500 [09:02<2:06:27, 16.21s/it]  7%|▋         | 33/500 [09:17<2:04:37, 16.01s/it]  7%|▋         | 34/500 [09:33<2:04:29, 16.03s/it]  7%|▋         | 35/500 [09:50<2:05:37, 16.21s/it]  7%|▋         | 36/500 [10:06<2:05:08, 16.18s/it]  7%|▋         | 37/500 [10:22<2:04:02, 16.07s/it]  8%|▊         | 38/500 [10:38<2:03:29, 16.04s/it]  8%|▊         | 39/500 [10:54<2:02:53, 15.99s/it]  8%|▊         | 40/500 [11:11<2:04:38, 16.26s/it]  8%|▊         | 41/500 [11:31<2:14:36, 17.59s/it]  8%|▊         | 41/500 [11:32<2:09:07, 16.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.240 MB uploadedwandb: / 0.010 MB of 0.240 MB uploadedwandb: - 0.138 MB of 0.240 MB uploadedwandb: \ 0.138 MB of 0.240 MB uploadedwandb: | 0.138 MB of 0.240 MB uploadedwandb: / 0.138 MB of 0.240 MB uploadedwandb: - 0.240 MB of 0.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▅█▁▃▇▅▇▇█▇███████████████████████████
wandb:     train_loss ▄▄▅▅▂▁▂▃▅▄▁▁▁▁▄█▁▅▁▄▁▁▄▁▁▁▁▁▁▁▁▄▁▁▁▄▁▄▁▄
wandb:   val_accuracy ▁▁▃▅▅▁▂█▃▇█▆▇██▄███▇▆▆▇▇▆▇▇▆▇▅▆▆▆▇▇▇▆▅█▅
wandb:       val_loss ▃▃▃▂▃▂▂▂▃▂▃▁▂▂▂▆▂█▆▆▆▁▁▅▁▃▇▂▅▆▁▄▂█▃▃▅▃▆▇
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.68945
wandb:     train_loss 1.09861
wandb:   val_accuracy 0.48444
wandb:       val_loss 4.28308
wandb: 
wandb: 🚀 View run lively-glade-155 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/klrswmwn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_042402-klrswmwn/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_043614-k8sqkcd7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-cosmos-156
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/k8sqkcd7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:50, 17.66s/it]  0%|          | 2/500 [00:33<2:17:45, 16.60s/it]  1%|          | 3/500 [00:49<2:14:57, 16.29s/it]  1%|          | 4/500 [01:05<2:12:31, 16.03s/it]  1%|          | 5/500 [01:20<2:11:03, 15.89s/it]  1%|          | 6/500 [01:36<2:11:18, 15.95s/it]  1%|▏         | 7/500 [01:52<2:09:55, 15.81s/it]  2%|▏         | 8/500 [02:12<2:21:09, 17.21s/it]  2%|▏         | 9/500 [02:28<2:16:51, 16.72s/it]  2%|▏         | 10/500 [02:44<2:15:42, 16.62s/it]  2%|▏         | 11/500 [03:00<2:14:08, 16.46s/it]  2%|▏         | 12/500 [03:16<2:12:40, 16.31s/it]  3%|▎         | 13/500 [03:33<2:12:45, 16.36s/it]  3%|▎         | 14/500 [03:49<2:13:32, 16.49s/it]  3%|▎         | 15/500 [04:05<2:11:27, 16.26s/it]  3%|▎         | 16/500 [04:21<2:09:48, 16.09s/it]  3%|▎         | 17/500 [04:37<2:10:27, 16.21s/it]  4%|▎         | 18/500 [04:54<2:12:24, 16.48s/it]  4%|▍         | 19/500 [05:10<2:10:36, 16.29s/it]  4%|▍         | 20/500 [05:27<2:12:01, 16.50s/it]  4%|▍         | 21/500 [05:44<2:13:11, 16.68s/it]  4%|▍         | 22/500 [06:01<2:12:52, 16.68s/it]  5%|▍         | 23/500 [06:19<2:14:51, 16.96s/it]  5%|▍         | 24/500 [06:36<2:14:52, 17.00s/it]  5%|▍         | 24/500 [06:40<2:12:28, 16.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.021 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▅▆▆▂▇▇▇▇█▇█▇████████
wandb:     train_loss █▇▆▇▇▅▆▂▆▆▄▇▃▁▇▇▂█▇▇▆▅▄▅
wandb:   val_accuracy ▂▂▁▆▆▆▆▁▇▇▇▇█▇█▇██▇█████
wandb:       val_loss ▃▃▃▃▃▂▃▁▃▃▃▃▂▃▃█▄▄▂▄▇▂▃▇
wandb: 
wandb: Run summary:
wandb:          epoch 23
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.81129
wandb:     train_loss 0.8368
wandb:   val_accuracy 0.51111
wandb:       val_loss 2.78687
wandb: 
wandb: 🚀 View run autumn-cosmos-156 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/k8sqkcd7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_043614-k8sqkcd7/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_044335-wrlhr7zs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-frost-157
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wrlhr7zs
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:32, 18.10s/it]  0%|          | 2/500 [00:33<2:19:25, 16.80s/it]  1%|          | 3/500 [00:49<2:16:00, 16.42s/it]  1%|          | 4/500 [01:06<2:15:00, 16.33s/it]  1%|          | 5/500 [01:22<2:13:19, 16.16s/it]  1%|          | 6/500 [01:37<2:12:21, 16.08s/it]  1%|▏         | 7/500 [01:53<2:11:24, 15.99s/it]  2%|▏         | 8/500 [02:09<2:10:41, 15.94s/it]  2%|▏         | 9/500 [02:25<2:09:36, 15.84s/it]  2%|▏         | 10/500 [02:40<2:08:45, 15.77s/it]  2%|▏         | 11/500 [02:56<2:08:13, 15.73s/it]  2%|▏         | 12/500 [03:12<2:07:53, 15.72s/it]  3%|▎         | 13/500 [03:31<2:16:04, 16.77s/it]  3%|▎         | 14/500 [03:47<2:14:29, 16.60s/it]  3%|▎         | 15/500 [04:03<2:11:43, 16.30s/it]  3%|▎         | 16/500 [04:18<2:10:03, 16.12s/it]  3%|▎         | 17/500 [04:34<2:09:38, 16.10s/it]  4%|▎         | 18/500 [04:52<2:12:38, 16.51s/it]  4%|▍         | 19/500 [05:07<2:09:59, 16.22s/it]  4%|▍         | 19/500 [05:07<2:09:54, 16.20s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▄█▁▁▁▂▁▁
wandb:     train_loss ██▆▁▆▇▆▆▂▂▆▄▅▆▃▂▂▃▅
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▃█▅▅▅▁▅▄
wandb:       val_loss ▃▃▁█▃▇█▅▆▇▄▂▅▂▆▃▇▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.31501
wandb:     train_loss 1.1147
wandb:   val_accuracy 0.34222
wandb:       val_loss 1.1038
wandb: 
wandb: 🚀 View run magic-frost-157 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/wrlhr7zs
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_044335-wrlhr7zs/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_044934-km0mmfz2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-plant-158
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/km0mmfz2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:38, 17.87s/it]  0%|          | 2/500 [00:34<2:20:31, 16.93s/it]  1%|          | 3/500 [00:49<2:15:27, 16.35s/it]  1%|          | 4/500 [01:05<2:13:41, 16.17s/it]  1%|          | 5/500 [01:21<2:12:01, 16.00s/it]  1%|          | 6/500 [01:37<2:11:47, 16.01s/it]  1%|▏         | 7/500 [01:53<2:10:59, 15.94s/it]  2%|▏         | 8/500 [02:09<2:11:30, 16.04s/it]  2%|▏         | 9/500 [02:25<2:12:15, 16.16s/it]  2%|▏         | 10/500 [02:42<2:12:45, 16.26s/it]  2%|▏         | 11/500 [02:59<2:13:45, 16.41s/it]  2%|▏         | 12/500 [03:15<2:12:13, 16.26s/it]  3%|▎         | 13/500 [03:31<2:11:52, 16.25s/it]  3%|▎         | 14/500 [03:47<2:11:34, 16.24s/it]  3%|▎         | 15/500 [04:08<2:22:30, 17.63s/it]  3%|▎         | 16/500 [04:24<2:18:33, 17.18s/it]  3%|▎         | 17/500 [04:40<2:16:13, 16.92s/it]  4%|▎         | 18/500 [04:57<2:14:22, 16.73s/it]  4%|▍         | 19/500 [05:12<2:12:03, 16.47s/it]  4%|▍         | 20/500 [05:28<2:10:39, 16.33s/it]  4%|▍         | 21/500 [05:45<2:10:48, 16.39s/it]  4%|▍         | 22/500 [06:01<2:09:49, 16.30s/it]  5%|▍         | 23/500 [06:19<2:12:38, 16.68s/it]  5%|▍         | 24/500 [06:34<2:10:16, 16.42s/it]  5%|▌         | 25/500 [06:51<2:09:26, 16.35s/it]  5%|▌         | 26/500 [07:07<2:08:04, 16.21s/it]  5%|▌         | 27/500 [07:26<2:15:17, 17.16s/it]  6%|▌         | 28/500 [07:42<2:12:13, 16.81s/it]  6%|▌         | 29/500 [07:58<2:09:51, 16.54s/it]  6%|▌         | 30/500 [08:14<2:07:52, 16.33s/it]  6%|▌         | 31/500 [08:30<2:06:58, 16.24s/it]  6%|▋         | 32/500 [08:46<2:05:43, 16.12s/it]  7%|▋         | 33/500 [09:01<2:04:43, 16.02s/it]  7%|▋         | 34/500 [09:17<2:04:02, 15.97s/it]  7%|▋         | 35/500 [09:33<2:03:19, 15.91s/it]  7%|▋         | 36/500 [09:49<2:02:46, 15.88s/it]  7%|▋         | 37/500 [10:04<2:01:54, 15.80s/it]  8%|▊         | 38/500 [10:20<2:01:31, 15.78s/it]  8%|▊         | 39/500 [10:37<2:04:07, 16.16s/it]  8%|▊         | 40/500 [10:53<2:02:16, 15.95s/it]  8%|▊         | 41/500 [11:09<2:02:19, 15.99s/it]  8%|▊         | 42/500 [11:25<2:03:29, 16.18s/it]  9%|▊         | 43/500 [11:41<2:02:52, 16.13s/it]  9%|▉         | 44/500 [11:58<2:03:21, 16.23s/it]  9%|▉         | 45/500 [12:14<2:02:05, 16.10s/it]  9%|▉         | 46/500 [12:30<2:01:36, 16.07s/it]  9%|▉         | 47/500 [12:45<2:00:37, 15.98s/it] 10%|▉         | 48/500 [13:02<2:01:05, 16.07s/it] 10%|▉         | 49/500 [13:18<2:00:54, 16.09s/it] 10%|▉         | 49/500 [13:18<2:02:27, 16.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.232 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▁▄▃▅▅▅▇█▇█▇██▇█████████████████████████
wandb:     train_loss ▆▇▄█▂▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb:   val_accuracy ▂▁▄▃█▆▇▇▇▆▇▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       val_loss ▂▂▂▂▃▁▁▅▅▁▁▃▄▄▄▄▇▁▁▆▁▃▃▃▄▄▁▇▄▃▄█▁▆▁▁▃▅▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.99851
wandb:     train_loss 9e-05
wandb:   val_accuracy 0.55111
wandb:       val_loss 5.50466
wandb: 
wandb: 🚀 View run young-plant-158 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/km0mmfz2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_044934-km0mmfz2/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_050333-05u4n9uh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-frog-159
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/05u4n9uh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:26, 18.45s/it]  0%|          | 2/500 [00:34<2:22:58, 17.23s/it]  1%|          | 3/500 [00:52<2:25:11, 17.53s/it]  1%|          | 4/500 [01:09<2:21:41, 17.14s/it]  1%|          | 5/500 [01:25<2:20:06, 16.98s/it]  1%|          | 6/500 [01:42<2:18:14, 16.79s/it]  1%|▏         | 7/500 [01:59<2:17:43, 16.76s/it]  2%|▏         | 8/500 [02:15<2:17:32, 16.77s/it]  2%|▏         | 9/500 [02:32<2:15:47, 16.59s/it]  2%|▏         | 10/500 [02:48<2:15:10, 16.55s/it]  2%|▏         | 11/500 [03:05<2:14:45, 16.54s/it]  2%|▏         | 12/500 [03:21<2:15:05, 16.61s/it]  3%|▎         | 13/500 [03:37<2:13:35, 16.46s/it]  3%|▎         | 14/500 [03:53<2:12:13, 16.32s/it]  3%|▎         | 15/500 [04:15<2:23:47, 17.79s/it]  3%|▎         | 16/500 [04:31<2:19:47, 17.33s/it]  3%|▎         | 17/500 [04:47<2:17:22, 17.07s/it]  4%|▎         | 18/500 [05:05<2:18:19, 17.22s/it]  4%|▍         | 19/500 [05:21<2:16:08, 16.98s/it]  4%|▍         | 20/500 [05:38<2:15:32, 16.94s/it]  4%|▍         | 21/500 [05:55<2:15:01, 16.91s/it]  4%|▍         | 22/500 [06:11<2:13:17, 16.73s/it]  5%|▍         | 23/500 [06:27<2:11:38, 16.56s/it]  5%|▍         | 23/500 [06:28<2:14:07, 16.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.136 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁
wandb: train_accuracy ▄▄▄▆▄▄▄▄▄▄▃▄▅▁▄▄▄█▆▄▄▄▂
wandb:     train_loss ▂▁▁▂▁▁▆▁▃▄▁▂▁▁█▄▁▁▂▂▆▃▁
wandb:   val_accuracy ▅▅▄█▄▄▄▄▄▄▃▄▄▁▄▄▄▅▄▄▄▄▃
wandb:       val_loss ▂▂▂▁▂▂▁▂▂▁▂▂▁▂▆▃▂▂▂▂█▄▂
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.20357
wandb:     train_loss 1.12404
wandb:   val_accuracy 0.3
wandb:       val_loss 1.33388
wandb: 
wandb: 🚀 View run vocal-frog-159 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/05u4n9uh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_050333-05u4n9uh/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_051045-eta97atk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sunset-160
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/eta97atk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:05, 17.57s/it]  0%|          | 2/500 [00:33<2:18:17, 16.66s/it]  1%|          | 3/500 [00:49<2:16:51, 16.52s/it]  1%|          | 4/500 [01:06<2:16:38, 16.53s/it]  1%|          | 5/500 [01:22<2:15:18, 16.40s/it]  1%|          | 6/500 [01:38<2:13:07, 16.17s/it]  1%|▏         | 7/500 [01:54<2:12:51, 16.17s/it]  2%|▏         | 8/500 [02:10<2:12:07, 16.11s/it]  2%|▏         | 9/500 [02:26<2:11:26, 16.06s/it]  2%|▏         | 10/500 [02:42<2:10:50, 16.02s/it]  2%|▏         | 11/500 [02:58<2:09:35, 15.90s/it]  2%|▏         | 12/500 [03:14<2:09:46, 15.96s/it]  3%|▎         | 13/500 [03:30<2:09:44, 15.99s/it]  3%|▎         | 14/500 [03:46<2:09:46, 16.02s/it]  3%|▎         | 15/500 [04:02<2:09:02, 15.96s/it]  3%|▎         | 16/500 [04:18<2:09:16, 16.03s/it]  3%|▎         | 17/500 [04:34<2:09:23, 16.07s/it]  4%|▎         | 18/500 [04:50<2:09:26, 16.11s/it]  4%|▍         | 19/500 [05:06<2:08:19, 16.01s/it]  4%|▍         | 19/500 [05:06<2:09:17, 16.13s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄█
wandb:     train_loss ██▆▁▆█▆▆▂▂▇▅▇▆▃▂▆▃▅
wandb:   val_accuracy ██████████████████▁
wandb:       val_loss ▃▃▁▇▃▇█▅▆▇▃▄▇▂▅▃▂▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.32244
wandb:     train_loss 1.11101
wandb:   val_accuracy 0.34444
wandb:       val_loss 1.09775
wandb: 
wandb: 🚀 View run celestial-sunset-160 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/eta97atk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_051045-eta97atk/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_051636-djtbrqb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-night-161
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/djtbrqb3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:20<2:49:10, 20.34s/it]  0%|          | 2/500 [00:36<2:26:01, 17.59s/it]  1%|          | 3/500 [00:52<2:20:50, 17.00s/it]  1%|          | 4/500 [01:08<2:18:21, 16.74s/it]  1%|          | 5/500 [01:24<2:15:15, 16.40s/it]  1%|          | 6/500 [01:40<2:14:22, 16.32s/it]  1%|▏         | 7/500 [01:56<2:12:59, 16.19s/it]  2%|▏         | 8/500 [02:12<2:11:49, 16.08s/it]  2%|▏         | 9/500 [02:27<2:09:06, 15.78s/it]  2%|▏         | 10/500 [02:42<2:06:57, 15.54s/it]  2%|▏         | 11/500 [02:58<2:06:56, 15.58s/it]  2%|▏         | 12/500 [03:14<2:08:08, 15.75s/it]  3%|▎         | 13/500 [03:30<2:08:44, 15.86s/it]  3%|▎         | 14/500 [03:47<2:10:15, 16.08s/it]  3%|▎         | 15/500 [04:02<2:09:39, 16.04s/it]  3%|▎         | 16/500 [04:19<2:09:36, 16.07s/it]  3%|▎         | 17/500 [04:34<2:08:58, 16.02s/it]  4%|▎         | 18/500 [04:50<2:08:01, 15.94s/it]  4%|▍         | 19/500 [05:06<2:08:19, 16.01s/it]  4%|▍         | 20/500 [05:24<2:12:08, 16.52s/it]  4%|▍         | 21/500 [05:40<2:10:36, 16.36s/it]  4%|▍         | 22/500 [05:56<2:09:37, 16.27s/it]  5%|▍         | 23/500 [06:12<2:08:49, 16.20s/it]  5%|▍         | 23/500 [06:12<2:08:51, 16.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.230 MB of 0.312 MB uploadedwandb: - 0.230 MB of 0.312 MB uploadedwandb: \ 0.230 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁
wandb: train_accuracy ▁▂▄▂▃▂▃▃▄▅▅▅█▅▅▃▆▅▇▇▆▆▇
wandb:     train_loss ▂▁▂▅▄▁▁▁█▁▁▁▁▁█▄▁▅▁▁▁▁▁
wandb:   val_accuracy ▁▂▆▁▂▁▂▃█▂█▇▇▇▆▃▆▇▇▆▆▆▇
wandb:       val_loss ▂▂▂▂▃▂▁▁▂█▃▁▂▃▁▃▄▄▃▄▃▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.90639
wandb:     train_loss 0.00295
wandb:   val_accuracy 0.55778
wandb:       val_loss 0.02145
wandb: 
wandb: 🚀 View run warm-night-161 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/djtbrqb3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_051636-djtbrqb3/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_052331-tlaqoj53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-firebrand-162
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tlaqoj53
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:38:17, 19.03s/it]  0%|          | 2/500 [00:34<2:22:00, 17.11s/it]  1%|          | 3/500 [00:50<2:15:34, 16.37s/it]  1%|          | 4/500 [01:05<2:11:23, 15.89s/it]  1%|          | 5/500 [01:20<2:09:42, 15.72s/it]  1%|          | 6/500 [01:36<2:08:31, 15.61s/it]  1%|▏         | 7/500 [01:51<2:06:41, 15.42s/it]  2%|▏         | 8/500 [02:06<2:05:39, 15.32s/it]  2%|▏         | 9/500 [02:21<2:04:22, 15.20s/it]  2%|▏         | 10/500 [02:36<2:03:37, 15.14s/it]  2%|▏         | 11/500 [02:51<2:03:10, 15.11s/it]  2%|▏         | 12/500 [03:07<2:06:34, 15.56s/it]  3%|▎         | 13/500 [03:22<2:04:48, 15.38s/it]  3%|▎         | 14/500 [03:38<2:04:23, 15.36s/it]  3%|▎         | 15/500 [03:53<2:04:00, 15.34s/it]  3%|▎         | 16/500 [04:08<2:03:36, 15.32s/it]  3%|▎         | 17/500 [04:24<2:03:49, 15.38s/it]  4%|▎         | 18/500 [04:39<2:03:21, 15.36s/it]  4%|▍         | 19/500 [04:55<2:04:44, 15.56s/it]  4%|▍         | 20/500 [05:10<2:03:07, 15.39s/it]  4%|▍         | 21/500 [05:26<2:03:38, 15.49s/it]  4%|▍         | 22/500 [05:42<2:05:19, 15.73s/it]  5%|▍         | 23/500 [05:57<2:03:50, 15.58s/it]  5%|▍         | 24/500 [06:12<2:01:29, 15.31s/it]  5%|▌         | 25/500 [06:27<1:59:42, 15.12s/it]  5%|▌         | 26/500 [06:42<2:00:42, 15.28s/it]  5%|▌         | 27/500 [06:59<2:03:03, 15.61s/it]  6%|▌         | 28/500 [07:14<2:01:45, 15.48s/it]  6%|▌         | 29/500 [07:29<2:01:20, 15.46s/it]  6%|▌         | 30/500 [07:45<2:00:59, 15.44s/it]  6%|▌         | 31/500 [08:00<1:59:38, 15.31s/it]  6%|▋         | 32/500 [08:15<1:59:42, 15.35s/it]  7%|▋         | 33/500 [08:30<1:59:12, 15.32s/it]  7%|▋         | 34/500 [08:45<1:58:05, 15.21s/it]  7%|▋         | 35/500 [09:01<1:58:55, 15.34s/it]  7%|▋         | 36/500 [09:16<1:58:44, 15.35s/it]  7%|▋         | 37/500 [09:32<1:59:04, 15.43s/it]  8%|▊         | 38/500 [09:52<2:09:08, 16.77s/it]  8%|▊         | 39/500 [10:07<2:04:56, 16.26s/it]  8%|▊         | 40/500 [10:22<2:02:27, 15.97s/it]  8%|▊         | 41/500 [10:38<2:00:32, 15.76s/it]  8%|▊         | 42/500 [10:53<1:59:40, 15.68s/it]  9%|▊         | 43/500 [11:08<1:58:21, 15.54s/it]  9%|▉         | 44/500 [11:23<1:56:52, 15.38s/it]  9%|▉         | 44/500 [11:23<1:58:08, 15.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.019 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▅▁▂▂▃▃▄▅▅▅▆▅▇▆▆▇▇▇█▇▇█▆█▆█████▇██████
wandb:     train_loss ▇▆▅▆█▆▃▅▅▅▄▆▅▆█▅▆▅▄▃▃▂▃▁▄▁▃▂▁▄▁▁▅▂▂▆▄▄▂▁
wandb:   val_accuracy ▂▂▂▅▁▁▁▁▃▂▃▄▄▄▃▅▅▅▅▅▆▅▆▅▆█▆▅▆▆▆▇▇▆▆▇▆▆▆▆
wandb:       val_loss ▄▅▄▅▇▄▅▄▅▄▄▅▄▅▇▄▆▅▆▇▂█▂▇▆▁▇▄▃▂▄█▇▅▅▄▂▆█▆
wandb: 
wandb: Run summary:
wandb:          epoch 43
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.83952
wandb:     train_loss 0.20398
wandb:   val_accuracy 0.47556
wandb:       val_loss 1.35398
wandb: 
wandb: 🚀 View run ethereal-firebrand-162 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tlaqoj53
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_052331-tlaqoj53/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_053540-88s9ng3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-flower-163
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/88s9ng3q
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:40:17, 19.27s/it]  0%|          | 2/500 [00:35<2:24:35, 17.42s/it]  1%|          | 3/500 [00:51<2:20:37, 16.98s/it]  1%|          | 4/500 [01:07<2:16:04, 16.46s/it]  1%|          | 5/500 [01:23<2:13:15, 16.15s/it]  1%|          | 6/500 [01:38<2:11:54, 16.02s/it]  1%|▏         | 7/500 [01:54<2:11:11, 15.97s/it]  2%|▏         | 8/500 [02:10<2:09:48, 15.83s/it]  2%|▏         | 9/500 [02:26<2:09:26, 15.82s/it]  2%|▏         | 10/500 [02:41<2:08:46, 15.77s/it]  2%|▏         | 11/500 [02:57<2:08:53, 15.82s/it]  2%|▏         | 12/500 [03:13<2:08:11, 15.76s/it]  3%|▎         | 13/500 [03:28<2:07:19, 15.69s/it]  3%|▎         | 14/500 [03:44<2:06:43, 15.65s/it]  3%|▎         | 15/500 [04:00<2:06:46, 15.68s/it]  3%|▎         | 16/500 [04:15<2:06:05, 15.63s/it]  3%|▎         | 17/500 [04:31<2:05:47, 15.63s/it]  4%|▎         | 18/500 [04:46<2:05:29, 15.62s/it]  4%|▍         | 19/500 [05:02<2:05:08, 15.61s/it]  4%|▍         | 19/500 [05:02<2:07:37, 15.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▂▄▃▁
wandb:     train_loss ██▆▁▆▇▆▄▂▂▆▅▇▇▄▃▄▅▅
wandb:   val_accuracy ▄▄▄▄▄▄▄█▄▄▄▄▄▄▃▁▁▁▄
wandb:       val_loss ▃▃▁█▃▇█▃▇▇▃▅▇▂▅▄▄▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.31798
wandb:     train_loss 1.12009
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.09077
wandb: 
wandb: 🚀 View run quiet-flower-163 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/88s9ng3q
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_053540-88s9ng3q/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_054131-vjpg22nd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-snowball-164
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vjpg22nd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:14, 17.58s/it]  0%|          | 2/500 [00:32<2:12:40, 15.99s/it]  1%|          | 3/500 [00:46<2:06:32, 15.28s/it]  1%|          | 4/500 [01:01<2:05:22, 15.17s/it]  1%|          | 5/500 [01:17<2:05:38, 15.23s/it]  1%|          | 6/500 [01:36<2:17:29, 16.70s/it]  1%|▏         | 7/500 [01:52<2:13:21, 16.23s/it]  2%|▏         | 8/500 [02:07<2:11:30, 16.04s/it]  2%|▏         | 9/500 [02:23<2:09:50, 15.87s/it]  2%|▏         | 10/500 [02:43<2:20:03, 17.15s/it]  2%|▏         | 11/500 [02:58<2:15:30, 16.63s/it]  2%|▏         | 12/500 [03:13<2:12:05, 16.24s/it]  3%|▎         | 13/500 [03:29<2:09:28, 15.95s/it]  3%|▎         | 14/500 [03:44<2:08:37, 15.88s/it]  3%|▎         | 15/500 [04:02<2:12:35, 16.40s/it]  3%|▎         | 16/500 [04:18<2:11:27, 16.30s/it]  3%|▎         | 17/500 [04:34<2:09:00, 16.03s/it]  4%|▎         | 18/500 [04:49<2:07:43, 15.90s/it]  4%|▍         | 19/500 [05:05<2:06:27, 15.77s/it]  4%|▍         | 20/500 [05:20<2:05:28, 15.68s/it]  4%|▍         | 21/500 [05:37<2:09:04, 16.17s/it]  4%|▍         | 22/500 [05:58<2:18:56, 17.44s/it]  5%|▍         | 23/500 [06:14<2:15:22, 17.03s/it]  5%|▍         | 24/500 [06:31<2:14:20, 16.93s/it]  5%|▌         | 25/500 [06:53<2:26:39, 18.53s/it]  5%|▌         | 26/500 [07:09<2:21:56, 17.97s/it]  5%|▌         | 27/500 [07:26<2:18:03, 17.51s/it]  6%|▌         | 28/500 [07:42<2:15:27, 17.22s/it]  6%|▌         | 29/500 [08:00<2:15:49, 17.30s/it]  6%|▌         | 30/500 [08:17<2:14:12, 17.13s/it]  6%|▌         | 31/500 [08:38<2:23:37, 18.37s/it]  6%|▋         | 32/500 [08:55<2:19:29, 17.88s/it]  7%|▋         | 33/500 [09:12<2:17:05, 17.61s/it]  7%|▋         | 34/500 [09:29<2:16:06, 17.53s/it]  7%|▋         | 35/500 [09:46<2:14:13, 17.32s/it]  7%|▋         | 36/500 [10:03<2:13:58, 17.32s/it]  7%|▋         | 37/500 [10:25<2:24:59, 18.79s/it]  8%|▊         | 38/500 [10:42<2:19:45, 18.15s/it]  8%|▊         | 39/500 [10:59<2:17:31, 17.90s/it]  8%|▊         | 40/500 [11:16<2:14:43, 17.57s/it]  8%|▊         | 41/500 [11:33<2:12:50, 17.37s/it]  8%|▊         | 42/500 [11:51<2:13:54, 17.54s/it]  9%|▊         | 43/500 [12:08<2:12:20, 17.38s/it]  9%|▉         | 44/500 [12:29<2:20:34, 18.50s/it]  9%|▉         | 45/500 [12:46<2:16:54, 18.05s/it]  9%|▉         | 46/500 [13:03<2:13:18, 17.62s/it]  9%|▉         | 47/500 [13:24<2:21:21, 18.72s/it] 10%|▉         | 48/500 [13:41<2:17:31, 18.26s/it] 10%|▉         | 49/500 [13:59<2:16:14, 18.13s/it] 10%|█         | 50/500 [14:16<2:12:52, 17.72s/it] 10%|█         | 51/500 [14:33<2:10:31, 17.44s/it] 10%|█         | 52/500 [14:49<2:08:41, 17.23s/it] 11%|█         | 53/500 [15:06<2:08:10, 17.21s/it] 11%|█         | 54/500 [15:23<2:05:46, 16.92s/it] 11%|█         | 55/500 [15:40<2:05:48, 16.96s/it] 11%|█         | 56/500 [16:02<2:16:40, 18.47s/it] 11%|█▏        | 57/500 [16:18<2:11:35, 17.82s/it] 12%|█▏        | 58/500 [16:35<2:09:31, 17.58s/it] 12%|█▏        | 59/500 [16:52<2:08:03, 17.42s/it] 12%|█▏        | 60/500 [17:09<2:05:38, 17.13s/it] 12%|█▏        | 61/500 [17:25<2:03:53, 16.93s/it] 12%|█▏        | 62/500 [17:41<2:01:43, 16.67s/it] 13%|█▎        | 63/500 [17:57<1:59:32, 16.41s/it] 13%|█▎        | 64/500 [18:12<1:56:32, 16.04s/it] 13%|█▎        | 65/500 [18:28<1:54:54, 15.85s/it] 13%|█▎        | 66/500 [18:43<1:54:20, 15.81s/it] 13%|█▎        | 67/500 [18:59<1:54:44, 15.90s/it] 14%|█▎        | 68/500 [19:15<1:54:21, 15.88s/it] 14%|█▍        | 69/500 [19:32<1:55:30, 16.08s/it] 14%|█▍        | 70/500 [19:48<1:54:59, 16.04s/it] 14%|█▍        | 71/500 [20:04<1:55:45, 16.19s/it] 14%|█▍        | 72/500 [20:25<2:05:11, 17.55s/it] 14%|█▍        | 72/500 [20:25<2:01:24, 17.02s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▅▄▁▇▆▇█▇▇▇▇▇▇▇▇▇▇▇▆▆▆▇▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▇
wandb:     train_loss ▁▂▂▁▂▁▂▃▁▃▁▁▁▁▁▃█▁▃▁▁▁▁▁▁▂▁▁▂▂▁▃▃▁▁▁▁▁▂▁
wandb:   val_accuracy ▅▄▁█▁▄▆▅▅▅▄▅▄▄▅▅▅▅▅▅▄▅▅▅▅▄▄▄▅▅▅▅▄▅▅▄▄▅▄▄
wandb:       val_loss ▁▁▁▁▁▂▂▃▂▂▃▁▇▂▂▂▂▂▂▄▂▃▃▁▂▂▂▂▂▃▂▂█▃▂▃▁▂▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 71
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.50669
wandb:     train_loss 0.8808
wandb:   val_accuracy 0.33556
wandb:       val_loss 6.18044
wandb: 
wandb: 🚀 View run fresh-snowball-164 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/vjpg22nd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_054131-vjpg22nd/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_060237-98izabbv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-wildflower-165
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/98izabbv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:19, 18.31s/it]  0%|          | 2/500 [00:34<2:21:16, 17.02s/it]  1%|          | 3/500 [00:55<2:35:48, 18.81s/it]  1%|          | 4/500 [01:10<2:24:32, 17.48s/it]  1%|          | 5/500 [01:26<2:18:45, 16.82s/it]  1%|          | 6/500 [01:42<2:16:09, 16.54s/it]  1%|▏         | 7/500 [01:58<2:14:00, 16.31s/it]  2%|▏         | 8/500 [02:14<2:12:56, 16.21s/it]  2%|▏         | 9/500 [02:30<2:11:43, 16.10s/it]  2%|▏         | 10/500 [02:50<2:22:20, 17.43s/it]  2%|▏         | 11/500 [03:06<2:17:38, 16.89s/it]  2%|▏         | 12/500 [03:22<2:17:01, 16.85s/it]  3%|▎         | 13/500 [03:39<2:16:23, 16.80s/it]  3%|▎         | 14/500 [03:55<2:13:13, 16.45s/it]  3%|▎         | 15/500 [04:11<2:12:06, 16.34s/it]  3%|▎         | 16/500 [04:27<2:11:02, 16.25s/it]  3%|▎         | 17/500 [04:43<2:09:34, 16.10s/it]  4%|▎         | 18/500 [04:59<2:09:25, 16.11s/it]  4%|▍         | 19/500 [05:16<2:11:04, 16.35s/it]  4%|▍         | 20/500 [05:33<2:11:58, 16.50s/it]  4%|▍         | 21/500 [05:50<2:12:50, 16.64s/it]  4%|▍         | 22/500 [06:06<2:12:17, 16.60s/it]  5%|▍         | 23/500 [06:22<2:11:00, 16.48s/it]  5%|▍         | 24/500 [06:43<2:19:53, 17.63s/it]  5%|▌         | 25/500 [06:59<2:16:44, 17.27s/it]  5%|▌         | 26/500 [07:15<2:13:44, 16.93s/it]  5%|▌         | 27/500 [07:31<2:11:20, 16.66s/it]  6%|▌         | 28/500 [07:47<2:09:41, 16.49s/it]  6%|▌         | 29/500 [08:03<2:08:25, 16.36s/it]  6%|▌         | 30/500 [08:19<2:07:02, 16.22s/it]  6%|▌         | 31/500 [08:36<2:07:39, 16.33s/it]  6%|▋         | 32/500 [08:52<2:06:51, 16.26s/it]  7%|▋         | 33/500 [09:08<2:05:51, 16.17s/it]  7%|▋         | 34/500 [09:25<2:07:14, 16.38s/it]  7%|▋         | 35/500 [09:41<2:07:22, 16.43s/it]  7%|▋         | 36/500 [09:57<2:06:19, 16.34s/it]  7%|▋         | 37/500 [10:14<2:07:32, 16.53s/it]  8%|▊         | 38/500 [10:30<2:05:44, 16.33s/it]  8%|▊         | 39/500 [10:51<2:15:40, 17.66s/it]  8%|▊         | 40/500 [11:07<2:12:13, 17.25s/it]  8%|▊         | 41/500 [11:23<2:09:25, 16.92s/it]  8%|▊         | 42/500 [11:44<2:18:36, 18.16s/it]  9%|▊         | 43/500 [12:01<2:13:38, 17.55s/it]  9%|▉         | 44/500 [12:16<2:09:27, 17.03s/it]  9%|▉         | 45/500 [12:32<2:06:44, 16.71s/it]  9%|▉         | 46/500 [12:53<2:15:01, 17.84s/it]  9%|▉         | 47/500 [13:09<2:10:20, 17.26s/it] 10%|▉         | 48/500 [13:25<2:06:38, 16.81s/it] 10%|▉         | 49/500 [13:41<2:05:13, 16.66s/it] 10%|█         | 50/500 [13:58<2:05:46, 16.77s/it] 10%|█         | 51/500 [14:20<2:16:38, 18.26s/it] 10%|█         | 52/500 [14:41<2:24:20, 19.33s/it] 11%|█         | 53/500 [14:59<2:19:08, 18.68s/it] 11%|█         | 53/500 [14:59<2:06:22, 16.96s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.019 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▅▆▆▇▆▇▇▇█▇██▇▇▇▇▇██▇▇█▇███▇████▇████▇
wandb:     train_loss ▇▇▆▆▆▆▆▆▅▄▂▆▄▅▆▆▆▆▆▁▁▆▁▄█▆▁▄▅▆▅▅▅▁▅▃▄▅▃▆
wandb:   val_accuracy ▁▁▁▁▂▁▃▂▂▂▃▄▂▆▇▇▆▇▅▇▇▇▅▆█▇█▇█▇▇▇▇█▇██▇▇▅
wandb:       val_loss ▄▄▄▆▄▅▅▅▅▅▄▄▅▆▃█▃▄▄▅▄▄▃▆▂▃█▁▁▆▂▆▄▂▆▆▆▂▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 52
wandb:  learning_rate 0.0
wandb: train_accuracy 0.77266
wandb:     train_loss 1.01611
wandb:   val_accuracy 0.44
wandb:       val_loss 1.16078
wandb: 
wandb: 🚀 View run helpful-wildflower-165 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/98izabbv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_060237-98izabbv/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_061823-w2vlyk5d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-microwave-166
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w2vlyk5d
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:23<3:16:38, 23.64s/it]  0%|          | 2/500 [00:40<2:43:22, 19.68s/it]  1%|          | 3/500 [00:56<2:29:16, 18.02s/it]  1%|          | 4/500 [01:16<2:35:16, 18.78s/it]  1%|          | 5/500 [01:32<2:26:22, 17.74s/it]  1%|          | 6/500 [01:48<2:21:26, 17.18s/it]  1%|▏         | 7/500 [02:08<2:29:35, 18.21s/it]  2%|▏         | 8/500 [02:25<2:24:16, 17.59s/it]  2%|▏         | 9/500 [02:41<2:21:55, 17.34s/it]  2%|▏         | 10/500 [02:58<2:18:50, 17.00s/it]  2%|▏         | 11/500 [03:14<2:17:57, 16.93s/it]  2%|▏         | 12/500 [03:31<2:16:30, 16.78s/it]  3%|▎         | 13/500 [03:47<2:14:19, 16.55s/it]  3%|▎         | 14/500 [04:04<2:14:25, 16.60s/it]  3%|▎         | 15/500 [04:19<2:12:07, 16.35s/it]  3%|▎         | 16/500 [04:36<2:12:58, 16.48s/it]  3%|▎         | 17/500 [04:52<2:11:14, 16.30s/it]  4%|▎         | 18/500 [05:08<2:10:32, 16.25s/it]  4%|▍         | 19/500 [05:25<2:11:19, 16.38s/it]  4%|▍         | 19/500 [05:25<2:17:16, 17.12s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.136 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██
wandb:     train_loss ██▆▁▆█▆▆▂▂▇▅▇▆▃▂▆▂▅
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss ▃▃▁▇▃▇█▅▆▇▃▄▇▂▆▃▂▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.31204
wandb:     train_loss 1.1191
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.09691
wandb: 
wandb: 🚀 View run absurd-microwave-166 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/w2vlyk5d
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_061823-w2vlyk5d/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_062433-3e06atu1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-galaxy-167
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3e06atu1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:46, 25.70s/it]  0%|          | 2/500 [00:49<3:26:24, 24.87s/it]  1%|          | 3/500 [01:17<3:36:13, 26.10s/it]  1%|          | 4/500 [01:45<3:42:10, 26.88s/it]  1%|          | 5/500 [02:15<3:51:43, 28.09s/it]  1%|          | 6/500 [02:43<3:51:04, 28.07s/it]  1%|▏         | 7/500 [03:11<3:50:28, 28.05s/it]  2%|▏         | 8/500 [03:37<3:42:45, 27.17s/it]  2%|▏         | 9/500 [04:10<3:57:21, 29.00s/it]  2%|▏         | 10/500 [04:36<3:50:59, 28.29s/it]  2%|▏         | 11/500 [05:03<3:46:25, 27.78s/it]  2%|▏         | 12/500 [05:30<3:43:53, 27.53s/it]  3%|▎         | 13/500 [05:55<3:37:19, 26.77s/it]  3%|▎         | 14/500 [06:22<3:36:53, 26.78s/it]  3%|▎         | 15/500 [06:56<3:54:48, 29.05s/it]  3%|▎         | 16/500 [07:21<3:44:49, 27.87s/it]  3%|▎         | 17/500 [07:51<3:49:49, 28.55s/it]  4%|▎         | 18/500 [08:18<3:45:37, 28.09s/it]  4%|▍         | 19/500 [08:45<3:42:20, 27.73s/it]  4%|▍         | 20/500 [09:16<3:49:00, 28.63s/it]  4%|▍         | 21/500 [09:44<3:46:40, 28.39s/it]  4%|▍         | 22/500 [10:13<3:48:37, 28.70s/it]  5%|▍         | 23/500 [10:46<3:58:32, 30.01s/it]  5%|▍         | 24/500 [11:11<3:46:20, 28.53s/it]  5%|▌         | 25/500 [11:41<3:48:04, 28.81s/it]  5%|▌         | 26/500 [12:10<3:48:14, 28.89s/it]  5%|▌         | 27/500 [12:42<3:55:44, 29.90s/it]  6%|▌         | 28/500 [13:14<3:58:31, 30.32s/it]  6%|▌         | 28/500 [13:14<3:43:04, 28.36s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.137 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▄▃▄▂▄▅▆▇▆▇▆█▆▆█▅▇██████▇█
wandb:     train_loss ▂▂▁▁▁▁▅▁▃▁▁▁▁▁▄▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▃▄▄▄▂▄▆▇▇▆▇▅▆▇▆▇▆▇▆▇▆▇█▇▅█
wandb:       val_loss ▃▃▂▅▂▃▅▅▁▃▂▁▇▃▁▆█▅▁▂▅▆▆▁▂▅▇▇
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.97474
wandb:     train_loss 0.0102
wandb:   val_accuracy 0.60444
wandb:       val_loss 3.22664
wandb: 
wandb: 🚀 View run grateful-galaxy-167 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/3e06atu1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_062433-3e06atu1/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_063835-km894trc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-shadow-168
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/km894trc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:15:39, 30.74s/it]  0%|          | 2/500 [01:00<4:11:22, 30.29s/it]  1%|          | 3/500 [01:24<3:48:00, 27.53s/it]  1%|          | 4/500 [01:57<4:03:58, 29.51s/it]  1%|          | 5/500 [02:21<3:46:57, 27.51s/it]  1%|          | 6/500 [02:56<4:06:18, 29.92s/it]  1%|▏         | 7/500 [03:20<3:51:39, 28.19s/it]  2%|▏         | 8/500 [03:51<3:58:45, 29.12s/it]  2%|▏         | 9/500 [04:15<3:43:50, 27.35s/it]  2%|▏         | 10/500 [04:43<3:45:58, 27.67s/it]  2%|▏         | 11/500 [05:08<3:38:18, 26.79s/it]  2%|▏         | 12/500 [05:33<3:32:50, 26.17s/it]  3%|▎         | 13/500 [05:58<3:29:30, 25.81s/it]  3%|▎         | 14/500 [06:29<3:42:57, 27.53s/it]  3%|▎         | 15/500 [06:52<3:31:44, 26.19s/it]  3%|▎         | 16/500 [07:28<3:54:09, 29.03s/it]  3%|▎         | 17/500 [07:51<3:40:13, 27.36s/it]  4%|▎         | 18/500 [08:20<3:43:12, 27.79s/it]  4%|▍         | 19/500 [08:46<3:37:47, 27.17s/it]  4%|▍         | 20/500 [09:13<3:36:30, 27.06s/it]  4%|▍         | 21/500 [09:38<3:32:41, 26.64s/it]  4%|▍         | 22/500 [10:06<3:34:00, 26.86s/it]  5%|▍         | 23/500 [10:32<3:33:08, 26.81s/it]  5%|▍         | 24/500 [10:59<3:31:11, 26.62s/it]  5%|▌         | 25/500 [11:24<3:28:03, 26.28s/it]  5%|▌         | 26/500 [11:52<3:32:17, 26.87s/it]  5%|▌         | 27/500 [12:21<3:36:41, 27.49s/it]  6%|▌         | 28/500 [12:53<3:45:33, 28.67s/it]  6%|▌         | 29/500 [13:23<3:48:29, 29.11s/it]  6%|▌         | 30/500 [13:52<3:47:57, 29.10s/it]  6%|▌         | 31/500 [14:16<3:36:49, 27.74s/it]  6%|▋         | 32/500 [14:47<3:43:39, 28.67s/it]  7%|▋         | 33/500 [15:16<3:42:40, 28.61s/it]  7%|▋         | 34/500 [15:40<3:31:36, 27.25s/it]  7%|▋         | 35/500 [16:11<3:40:14, 28.42s/it]  7%|▋         | 35/500 [16:11<3:35:07, 27.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▃▃▄▄▅▆▆▅▆▅▆▆▇▂▆▆▇▇▆▇▇▇▇▇▇██▇████
wandb:     train_loss ▄▄▆█▃▅▅▂▅▄▃▁▁▅▄▄▃▆▂▄▄▃▇▂▃▃▃▃▄▂▁▁▃▁▄
wandb:   val_accuracy ▃▂▁▁▃▅▄▄▄▆▅▄▆▆▆▅▂▆▆▅▅▄▆▆▅▆█▆▇▆█▅▆▆▅
wandb:       val_loss ▃▃▃▂▃▃▃▃▃▂▂▂▃▂▂▄█▄▁▂▄▂▄▂▁▄▂▃▆▄▁▃▁▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 34
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.83804
wandb:     train_loss 0.77108
wandb:   val_accuracy 0.46667
wandb:       val_loss 0.61872
wandb: 
wandb: 🚀 View run fine-shadow-168 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/km894trc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_063835-km894trc/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_065528-jboxjijd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-dawn-169
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/jboxjijd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:16:57, 30.90s/it]  0%|          | 2/500 [00:55<3:46:47, 27.33s/it]  1%|          | 3/500 [01:26<3:59:51, 28.96s/it]  1%|          | 4/500 [01:51<3:44:49, 27.20s/it]  1%|          | 5/500 [02:19<3:46:59, 27.51s/it]  1%|          | 6/500 [02:50<3:55:45, 28.63s/it]  1%|▏         | 7/500 [03:21<4:01:39, 29.41s/it]  2%|▏         | 8/500 [03:45<3:48:38, 27.88s/it]  2%|▏         | 9/500 [04:16<3:56:20, 28.88s/it]  2%|▏         | 10/500 [04:44<3:53:04, 28.54s/it]  2%|▏         | 11/500 [05:14<3:55:16, 28.87s/it]  2%|▏         | 12/500 [05:39<3:45:01, 27.67s/it]  3%|▎         | 13/500 [06:09<3:50:34, 28.41s/it]  3%|▎         | 14/500 [06:38<3:51:44, 28.61s/it]  3%|▎         | 15/500 [07:06<3:49:35, 28.40s/it]  3%|▎         | 16/500 [07:31<3:40:56, 27.39s/it]  3%|▎         | 17/500 [07:55<3:33:35, 26.53s/it]  4%|▎         | 18/500 [08:34<4:01:33, 30.07s/it]  4%|▍         | 19/500 [08:58<3:47:21, 28.36s/it]  4%|▍         | 20/500 [09:26<3:46:53, 28.36s/it]  4%|▍         | 21/500 [09:51<3:38:56, 27.43s/it]  4%|▍         | 22/500 [10:18<3:35:41, 27.07s/it]  5%|▍         | 23/500 [10:44<3:34:10, 26.94s/it]  5%|▍         | 24/500 [11:13<3:38:31, 27.54s/it]  5%|▌         | 25/500 [11:38<3:31:10, 26.68s/it]  5%|▌         | 25/500 [11:38<3:41:12, 27.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▅▅▅▇▇▇▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▄▅▆▅▅▅
wandb:   val_accuracy ▄▄▄▄▄▄▄▄▄▃▅▅▄▄█▄▄▇█▇▇▇▄▃▁
wandb:       val_loss ▃█▂▇▂▂▂▆▁▅▂▂▅▅▂▃▅▃▃▄▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.50074
wandb:     train_loss 1.12412
wandb:   val_accuracy 0.33333
wandb:       val_loss 1.10645
wandb: 
wandb: 🚀 View run volcanic-dawn-169 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/jboxjijd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_065528-jboxjijd/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_070743-sme7s99x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-cosmos-170
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/sme7s99x
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:10:02, 30.06s/it]  0%|          | 2/500 [00:54<3:40:41, 26.59s/it]  1%|          | 3/500 [01:18<3:32:22, 25.64s/it]  1%|          | 4/500 [01:47<3:42:46, 26.95s/it]  1%|          | 5/500 [02:16<3:48:37, 27.71s/it]  1%|          | 6/500 [02:44<3:49:38, 27.89s/it]  1%|▏         | 7/500 [03:13<3:50:40, 28.07s/it]  2%|▏         | 8/500 [03:38<3:41:53, 27.06s/it]  2%|▏         | 9/500 [04:13<4:01:27, 29.51s/it]  2%|▏         | 10/500 [04:36<3:46:29, 27.73s/it]  2%|▏         | 11/500 [05:14<4:09:27, 30.61s/it]  2%|▏         | 12/500 [05:38<3:54:05, 28.78s/it]  3%|▎         | 13/500 [06:06<3:50:53, 28.45s/it]  3%|▎         | 14/500 [06:35<3:52:42, 28.73s/it]  3%|▎         | 15/500 [07:00<3:41:42, 27.43s/it]  3%|▎         | 16/500 [07:28<3:42:39, 27.60s/it]  3%|▎         | 17/500 [07:54<3:37:53, 27.07s/it]  4%|▎         | 18/500 [08:19<3:34:45, 26.73s/it]  4%|▍         | 19/500 [08:45<3:31:53, 26.43s/it]  4%|▍         | 20/500 [09:15<3:40:38, 27.58s/it]  4%|▍         | 21/500 [09:53<4:04:29, 30.63s/it]  4%|▍         | 22/500 [10:18<3:48:57, 28.74s/it]  5%|▍         | 23/500 [10:46<3:48:54, 28.79s/it]  5%|▍         | 24/500 [11:16<3:49:25, 28.92s/it]  5%|▌         | 25/500 [11:45<3:50:06, 29.07s/it]  5%|▌         | 26/500 [12:10<3:40:48, 27.95s/it]  5%|▌         | 27/500 [12:43<3:51:21, 29.35s/it]  6%|▌         | 28/500 [13:08<3:40:03, 27.97s/it]  6%|▌         | 29/500 [13:40<3:48:36, 29.12s/it]  6%|▌         | 30/500 [14:08<3:47:11, 29.00s/it]  6%|▌         | 31/500 [14:41<3:54:48, 30.04s/it]  6%|▋         | 32/500 [15:08<3:46:47, 29.08s/it]  7%|▋         | 33/500 [15:33<3:37:01, 27.88s/it]  7%|▋         | 34/500 [15:59<3:33:53, 27.54s/it]  7%|▋         | 35/500 [16:27<3:32:58, 27.48s/it]  7%|▋         | 36/500 [16:55<3:35:02, 27.81s/it]  7%|▋         | 37/500 [17:24<3:36:13, 28.02s/it]  8%|▊         | 38/500 [17:49<3:28:02, 27.02s/it]  8%|▊         | 39/500 [18:20<3:37:19, 28.29s/it]  8%|▊         | 40/500 [18:44<3:27:15, 27.03s/it]  8%|▊         | 40/500 [18:49<3:36:31, 28.24s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.019 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▅▄▄▇▇██▆██████████████████████████████
wandb:     train_loss ▅▃▂▂▁▂▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▇▄▃▅▇██▆▇▇▇▆█▅▆▇▅▆▆▆▆▆▆▇▆▆▆▅▆▇▆▆▆▇▇▅▅▆
wandb:       val_loss ▂▄▂▄▂▂▁▃▁▃▁▁█▆▁▄▆▄▄▁▃▃▄▁▃▃▂▅▆▃▃▂▅▄▃█▄▄▆█
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 6e-05
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.44889
wandb:       val_loss 10.03379
wandb: 
wandb: 🚀 View run expert-cosmos-170 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/sme7s99x
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_070743-sme7s99x/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_072711-dpdo0hod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-wave-171
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dpdo0hod
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:59:26, 28.79s/it]  0%|          | 2/500 [00:52<3:35:27, 25.96s/it]  1%|          | 3/500 [01:27<4:07:49, 29.92s/it]  1%|          | 4/500 [01:56<4:04:33, 29.58s/it]  1%|          | 5/500 [02:26<4:04:02, 29.58s/it]  1%|          | 6/500 [02:57<4:10:03, 30.37s/it]  1%|▏         | 7/500 [03:27<4:06:37, 30.02s/it]  2%|▏         | 8/500 [03:56<4:03:47, 29.73s/it]  2%|▏         | 9/500 [04:25<4:00:35, 29.40s/it]  2%|▏         | 10/500 [04:55<4:01:35, 29.58s/it]  2%|▏         | 11/500 [05:19<3:49:25, 28.15s/it]  2%|▏         | 12/500 [05:53<4:02:29, 29.81s/it]  3%|▎         | 13/500 [06:18<3:49:28, 28.27s/it]  3%|▎         | 14/500 [06:51<4:01:38, 29.83s/it]  3%|▎         | 15/500 [07:20<3:58:45, 29.54s/it]  3%|▎         | 16/500 [07:51<4:00:48, 29.85s/it]  3%|▎         | 17/500 [08:17<3:52:52, 28.93s/it]  4%|▎         | 18/500 [08:51<4:02:39, 30.21s/it]  4%|▍         | 19/500 [09:16<3:50:01, 28.69s/it]  4%|▍         | 20/500 [09:49<4:01:25, 30.18s/it]  4%|▍         | 21/500 [10:17<3:54:45, 29.41s/it]  4%|▍         | 22/500 [10:47<3:55:31, 29.56s/it]  5%|▍         | 23/500 [11:16<3:53:35, 29.38s/it]  5%|▍         | 24/500 [11:45<3:52:32, 29.31s/it]  5%|▌         | 25/500 [12:15<3:53:17, 29.47s/it]  5%|▌         | 26/500 [12:41<3:44:26, 28.41s/it]  5%|▌         | 27/500 [13:19<4:08:05, 31.47s/it]  6%|▌         | 28/500 [13:45<3:53:20, 29.66s/it]  6%|▌         | 29/500 [14:14<3:52:17, 29.59s/it]  6%|▌         | 30/500 [14:46<3:57:10, 30.28s/it]  6%|▌         | 31/500 [15:14<3:50:28, 29.49s/it]  6%|▋         | 32/500 [15:42<3:47:37, 29.18s/it]  7%|▋         | 33/500 [16:10<3:44:02, 28.79s/it]  7%|▋         | 34/500 [16:36<3:37:50, 28.05s/it]  7%|▋         | 35/500 [17:08<3:45:01, 29.04s/it]  7%|▋         | 36/500 [17:44<4:01:16, 31.20s/it]  7%|▋         | 37/500 [18:14<3:57:06, 30.73s/it]  8%|▊         | 38/500 [18:44<3:56:41, 30.74s/it]  8%|▊         | 39/500 [19:12<3:48:41, 29.76s/it]  8%|▊         | 40/500 [19:41<3:45:47, 29.45s/it]  8%|▊         | 41/500 [20:10<3:43:55, 29.27s/it]  8%|▊         | 42/500 [20:38<3:40:50, 28.93s/it]  9%|▊         | 43/500 [21:08<3:44:36, 29.49s/it]  9%|▉         | 44/500 [21:39<3:46:22, 29.79s/it]  9%|▉         | 45/500 [22:08<3:43:41, 29.50s/it]  9%|▉         | 46/500 [22:33<3:34:07, 28.30s/it]  9%|▉         | 47/500 [23:04<3:38:19, 28.92s/it] 10%|▉         | 48/500 [23:33<3:38:57, 29.06s/it] 10%|▉         | 49/500 [24:04<3:42:17, 29.57s/it] 10%|█         | 50/500 [24:36<3:46:37, 30.22s/it] 10%|█         | 51/500 [25:06<3:46:41, 30.29s/it] 10%|█         | 52/500 [25:32<3:35:56, 28.92s/it] 11%|█         | 53/500 [26:04<3:43:21, 29.98s/it] 11%|█         | 54/500 [26:39<3:54:22, 31.53s/it] 11%|█         | 55/500 [27:08<3:47:22, 30.66s/it] 11%|█         | 56/500 [27:39<3:46:58, 30.67s/it] 11%|█▏        | 57/500 [28:09<3:45:06, 30.49s/it] 11%|█▏        | 57/500 [28:09<3:38:48, 29.64s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.317 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.107 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▄▆▇▆▇▆▇▇▇▇▇▆▆▆▇▇▇▇▇█▇█▇██████▇█████▇
wandb:     train_loss ▅▅▆▃▅▂▅▃▁▂▅▅▆▃▄▄█▂▃▃▄▁▁▁▄▃▃▃▂▆▂▄▅▄▂▂▂▃▄▁
wandb:   val_accuracy ▂▂▁▃▄▄▆▄▅▅▅▆▅▆▆▆▅▆▆▆▆█▇▇▇▇██▇▇▇▇██▇▇▆▆▇█
wandb:       val_loss ▄▄▄▄▄▅▄▃▄▄▄▅▄▂▄▆▄▂▄▃█▃▄▅▂▂▃▆▁▅▂▃▁▄▃▂▄▄▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.76969
wandb:     train_loss 0.08978
wandb:   val_accuracy 0.57778
wandb:       val_loss 1.45411
wandb: 
wandb: 🚀 View run distinctive-wave-171 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dpdo0hod
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_072711-dpdo0hod/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_075601-gzwve32a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-donkey-172
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gzwve32a
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:20:29, 31.32s/it]  0%|          | 2/500 [00:59<4:06:49, 29.74s/it]  1%|          | 3/500 [01:24<3:48:31, 27.59s/it]  1%|          | 4/500 [01:53<3:49:32, 27.77s/it]  1%|          | 5/500 [02:19<3:44:17, 27.19s/it]  1%|          | 6/500 [02:46<3:43:15, 27.12s/it]  1%|▏         | 7/500 [03:14<3:47:04, 27.64s/it]  2%|▏         | 8/500 [03:43<3:50:10, 28.07s/it]  2%|▏         | 9/500 [04:22<4:16:19, 31.32s/it]  2%|▏         | 10/500 [04:49<4:04:51, 29.98s/it]  2%|▏         | 11/500 [05:15<3:54:18, 28.75s/it]  2%|▏         | 12/500 [05:42<3:49:37, 28.23s/it]  3%|▎         | 13/500 [06:06<3:39:23, 27.03s/it]  3%|▎         | 14/500 [06:33<3:39:40, 27.12s/it]  3%|▎         | 15/500 [06:59<3:34:57, 26.59s/it]  3%|▎         | 16/500 [07:24<3:32:15, 26.31s/it]  3%|▎         | 17/500 [07:50<3:29:09, 25.98s/it]  4%|▎         | 18/500 [08:14<3:25:55, 25.63s/it]  4%|▍         | 19/500 [08:42<3:28:53, 26.06s/it]  4%|▍         | 20/500 [09:06<3:24:13, 25.53s/it]  4%|▍         | 21/500 [09:33<3:26:56, 25.92s/it]  4%|▍         | 22/500 [09:58<3:25:38, 25.81s/it]  5%|▍         | 23/500 [10:27<3:31:07, 26.56s/it]  5%|▍         | 24/500 [10:51<3:25:43, 25.93s/it]  5%|▌         | 25/500 [11:21<3:34:12, 27.06s/it]  5%|▌         | 25/500 [11:21<3:35:42, 27.25s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.137 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▅▅▅▇▇▇▇▇█▇█
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▄▅▆▅▅▅
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▄▄█▇▆▇▆▅▅▁
wandb:       val_loss ▃█▂▇▂▂▂▅▁▅▂▂▅▅▂▃▅▃▃▄▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.49331
wandb:     train_loss 1.12559
wandb:   val_accuracy 0.32667
wandb:       val_loss 1.10993
wandb: 
wandb: 🚀 View run unique-donkey-172 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/gzwve32a
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_075601-gzwve32a/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_080806-8oiazs98
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-hill-173
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8oiazs98
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:03:17, 29.25s/it]  0%|          | 2/500 [00:53<3:40:05, 26.52s/it]  1%|          | 3/500 [01:22<3:46:59, 27.40s/it]  1%|          | 4/500 [01:46<3:36:48, 26.23s/it]  1%|          | 5/500 [02:21<4:00:57, 29.21s/it]  1%|          | 6/500 [02:46<3:50:14, 27.96s/it]  1%|▏         | 7/500 [03:15<3:51:32, 28.18s/it]  2%|▏         | 8/500 [03:40<3:43:34, 27.26s/it]  2%|▏         | 9/500 [04:10<3:48:27, 27.92s/it]  2%|▏         | 10/500 [04:39<3:50:51, 28.27s/it]  2%|▏         | 11/500 [05:03<3:41:12, 27.14s/it]  2%|▏         | 12/500 [05:34<3:50:05, 28.29s/it]  3%|▎         | 13/500 [06:00<3:44:44, 27.69s/it]  3%|▎         | 14/500 [06:29<3:46:17, 27.94s/it]  3%|▎         | 15/500 [06:54<3:37:58, 26.97s/it]  3%|▎         | 16/500 [07:28<3:55:18, 29.17s/it]  3%|▎         | 17/500 [07:52<3:42:48, 27.68s/it]  4%|▎         | 18/500 [08:25<3:55:23, 29.30s/it]  4%|▎         | 18/500 [08:37<3:50:45, 28.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.021 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▂▂▄▅▅▅▅█▇█▇█▅▇█
wandb:     train_loss ▂█▁▂▃▁▁▁▁▂▁▁▁▂▁▃▁▁
wandb:   val_accuracy ▁▁▁▁▁▃▅▅▅▅█▆█▄█▅▅▆
wandb:       val_loss ▂▁▂▄▄▂▂▃▁▅▁▁▄█▄▃▃▇
wandb: 
wandb: Run summary:
wandb:          epoch 17
wandb:  learning_rate 0.0005
wandb: train_accuracy 0.99554
wandb:     train_loss 0.01905
wandb:   val_accuracy 0.46444
wandb:       val_loss 5.19831
wandb: 
wandb: 🚀 View run super-hill-173 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8oiazs98
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_080806-8oiazs98/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_081723-h9zk8wt2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-blaze-174
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/h9zk8wt2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:22, 26.38s/it]  0%|          | 2/500 [00:51<3:31:18, 25.46s/it]  1%|          | 3/500 [01:16<3:29:36, 25.31s/it]  1%|          | 4/500 [01:46<3:44:53, 27.20s/it]  1%|          | 5/500 [02:13<3:45:10, 27.29s/it]  1%|          | 6/500 [02:44<3:52:54, 28.29s/it]  1%|▏         | 7/500 [03:13<3:54:07, 28.49s/it]  2%|▏         | 8/500 [03:40<3:51:28, 28.23s/it]  2%|▏         | 9/500 [04:05<3:42:43, 27.22s/it]  2%|▏         | 10/500 [04:35<3:47:58, 27.91s/it]  2%|▏         | 11/500 [05:05<3:53:35, 28.66s/it]  2%|▏         | 12/500 [05:35<3:57:25, 29.19s/it]  3%|▎         | 13/500 [06:07<4:02:41, 29.90s/it]  3%|▎         | 14/500 [06:31<3:49:07, 28.29s/it]  3%|▎         | 15/500 [07:01<3:52:06, 28.71s/it]  3%|▎         | 16/500 [07:27<3:44:24, 27.82s/it]  3%|▎         | 17/500 [07:57<3:49:06, 28.46s/it]  4%|▎         | 18/500 [08:22<3:39:56, 27.38s/it]  4%|▍         | 19/500 [08:57<3:59:21, 29.86s/it]  4%|▍         | 20/500 [09:26<3:55:21, 29.42s/it]  4%|▍         | 21/500 [09:53<3:50:31, 28.88s/it]  4%|▍         | 22/500 [10:22<3:48:42, 28.71s/it]  5%|▍         | 23/500 [10:50<3:47:52, 28.66s/it]  5%|▍         | 24/500 [11:18<3:45:31, 28.43s/it]  5%|▌         | 25/500 [11:48<3:47:30, 28.74s/it]  5%|▌         | 26/500 [12:18<3:50:58, 29.24s/it]  5%|▌         | 27/500 [12:44<3:42:17, 28.20s/it]  6%|▌         | 28/500 [13:15<3:49:20, 29.15s/it]  6%|▌         | 29/500 [13:45<3:50:38, 29.38s/it]  6%|▌         | 30/500 [14:17<3:56:02, 30.13s/it]  6%|▌         | 31/500 [14:42<3:43:53, 28.64s/it]  6%|▋         | 32/500 [15:15<3:53:39, 29.96s/it]  7%|▋         | 33/500 [15:46<3:55:40, 30.28s/it]  7%|▋         | 34/500 [16:11<3:43:19, 28.75s/it]  7%|▋         | 35/500 [16:42<3:48:15, 29.45s/it]  7%|▋         | 36/500 [17:17<3:58:59, 30.90s/it]  7%|▋         | 37/500 [17:45<3:53:11, 30.22s/it]  8%|▊         | 38/500 [18:16<3:52:30, 30.20s/it]  8%|▊         | 39/500 [18:43<3:45:02, 29.29s/it]  8%|▊         | 40/500 [19:14<3:49:29, 29.93s/it]  8%|▊         | 41/500 [19:49<4:00:22, 31.42s/it]  8%|▊         | 42/500 [20:14<3:44:35, 29.42s/it]  9%|▊         | 43/500 [20:45<3:48:45, 30.03s/it]  9%|▉         | 44/500 [21:10<3:37:02, 28.56s/it]  9%|▉         | 45/500 [21:41<3:40:30, 29.08s/it]  9%|▉         | 46/500 [22:15<3:52:37, 30.74s/it]  9%|▉         | 47/500 [22:44<3:48:10, 30.22s/it] 10%|▉         | 48/500 [23:14<3:46:51, 30.11s/it] 10%|▉         | 49/500 [23:42<3:40:45, 29.37s/it] 10%|█         | 50/500 [24:12<3:41:08, 29.49s/it] 10%|█         | 51/500 [24:37<3:31:01, 28.20s/it] 10%|█         | 52/500 [25:05<3:31:22, 28.31s/it] 11%|█         | 53/500 [25:33<3:29:30, 28.12s/it] 11%|█         | 54/500 [26:00<3:26:43, 27.81s/it] 11%|█         | 55/500 [26:30<3:31:16, 28.49s/it] 11%|█         | 56/500 [26:58<3:29:52, 28.36s/it] 11%|█▏        | 57/500 [27:23<3:22:12, 27.39s/it] 11%|█▏        | 57/500 [27:23<3:32:55, 28.84s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.232 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▄▄▄▄▆▆▇▇▆▇▇▇▇▇▇▇██▇████▇██████▇█████▇
wandb:     train_loss ▅▅▇▄▅▂▅▄▁▂▅▅▄▃▅▂▅▃▄▄▅▁▁▁▄▅▃▄▁█▂▄▃▃▂▂▂▃▄▁
wandb:   val_accuracy ▁▁▁▅▅▄▃▇▇▇▆▇▇▇▇█▇██▇█▇███▇██▇█▇▇██▇▇▆▆▇█
wandb:       val_loss ▄▄▄▄▄▄▄▃▄▄▃▆▄▂▃▂▃▂▃▄█▄▄▆▂▂▃▆▁▅▂▃▃▃▁▃▄▃▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81575
wandb:     train_loss 0.04313
wandb:   val_accuracy 0.56
wandb:       val_loss 1.56166
wandb: 
wandb: 🚀 View run elated-blaze-174 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/h9zk8wt2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_081723-h9zk8wt2/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_084532-ie8du5re
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-forest-175
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ie8du5re
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:50:30, 27.72s/it]  0%|          | 2/500 [00:52<3:36:31, 26.09s/it]  1%|          | 3/500 [01:18<3:33:51, 25.82s/it]  1%|          | 4/500 [01:42<3:29:09, 25.30s/it]  1%|          | 5/500 [02:12<3:43:23, 27.08s/it]  1%|          | 6/500 [02:37<3:36:20, 26.28s/it]  1%|▏         | 7/500 [03:11<3:55:49, 28.70s/it]  2%|▏         | 8/500 [03:35<3:44:40, 27.40s/it]  2%|▏         | 9/500 [04:16<4:17:15, 31.44s/it]  2%|▏         | 10/500 [04:43<4:07:24, 30.30s/it]  2%|▏         | 11/500 [05:11<4:00:50, 29.55s/it]  2%|▏         | 12/500 [05:41<3:59:56, 29.50s/it]  3%|▎         | 13/500 [06:09<3:57:23, 29.25s/it]  3%|▎         | 14/500 [06:41<4:03:49, 30.10s/it]  3%|▎         | 15/500 [07:10<3:58:34, 29.51s/it]  3%|▎         | 16/500 [07:35<3:47:48, 28.24s/it]  3%|▎         | 17/500 [08:02<3:44:33, 27.90s/it]  4%|▎         | 18/500 [08:36<3:59:13, 29.78s/it]  4%|▍         | 19/500 [09:04<3:54:07, 29.20s/it]  4%|▍         | 20/500 [09:32<3:49:53, 28.74s/it]  4%|▍         | 21/500 [10:01<3:50:19, 28.85s/it]  4%|▍         | 22/500 [10:26<3:40:50, 27.72s/it]  5%|▍         | 23/500 [10:51<3:35:20, 27.09s/it]  5%|▍         | 24/500 [11:21<3:40:54, 27.85s/it]  5%|▌         | 25/500 [11:46<3:33:04, 26.92s/it]  5%|▌         | 25/500 [11:46<3:43:40, 28.25s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.226 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▆▅▅▇▇▇▇▇█▇█
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▄▅▆▅▅▅
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▄▅▅▅▅▇▄▅█▇▆▇▆▅▅▁
wandb:       val_loss ▃█▂▇▂▂▂▆▁▅▂▂▅▅▃▃▅▃▃▄▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.48886
wandb:     train_loss 1.1247
wandb:   val_accuracy 0.32667
wandb:       val_loss 1.10983
wandb: 
wandb: 🚀 View run radiant-forest-175 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ie8du5re
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_084532-ie8du5re/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_085803-5ykoly7j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-elevator-176
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5ykoly7j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:43:28, 26.87s/it]  0%|          | 2/500 [00:51<3:30:13, 25.33s/it]  1%|          | 3/500 [01:16<3:28:30, 25.17s/it]  1%|          | 4/500 [01:45<3:41:42, 26.82s/it]  1%|          | 5/500 [02:09<3:34:19, 25.98s/it]  1%|          | 6/500 [02:44<3:56:50, 28.77s/it]  1%|▏         | 7/500 [03:18<4:12:21, 30.71s/it]  2%|▏         | 8/500 [03:47<4:06:42, 30.09s/it]  2%|▏         | 9/500 [04:17<4:04:38, 29.89s/it]  2%|▏         | 10/500 [04:44<3:58:59, 29.26s/it]  2%|▏         | 11/500 [05:12<3:54:15, 28.74s/it]  2%|▏         | 12/500 [05:40<3:52:12, 28.55s/it]  3%|▎         | 13/500 [06:05<3:42:07, 27.37s/it]  3%|▎         | 14/500 [06:36<3:50:40, 28.48s/it]  3%|▎         | 15/500 [07:00<3:40:01, 27.22s/it]  3%|▎         | 16/500 [07:35<3:57:50, 29.48s/it]  3%|▎         | 17/500 [08:05<3:58:19, 29.61s/it]  4%|▎         | 18/500 [08:34<3:57:00, 29.50s/it]  4%|▍         | 19/500 [09:05<4:00:29, 30.00s/it]  4%|▍         | 20/500 [09:30<3:46:36, 28.33s/it]  4%|▍         | 21/500 [10:00<3:50:55, 28.93s/it]  4%|▍         | 22/500 [10:33<4:01:19, 30.29s/it]  5%|▍         | 23/500 [10:58<3:47:32, 28.62s/it]  5%|▍         | 24/500 [11:28<3:49:31, 28.93s/it]  5%|▌         | 25/500 [11:53<3:39:17, 27.70s/it]  5%|▌         | 26/500 [12:27<3:54:13, 29.65s/it]  5%|▌         | 27/500 [12:53<3:45:47, 28.64s/it]  6%|▌         | 28/500 [13:20<3:40:21, 28.01s/it]  6%|▌         | 29/500 [13:46<3:36:09, 27.54s/it]  6%|▌         | 30/500 [14:16<3:40:58, 28.21s/it]  6%|▌         | 31/500 [14:51<3:55:56, 30.18s/it]  6%|▋         | 32/500 [15:17<3:46:45, 29.07s/it]  7%|▋         | 33/500 [15:45<3:42:54, 28.64s/it]  7%|▋         | 34/500 [16:12<3:39:39, 28.28s/it]  7%|▋         | 35/500 [16:41<3:39:50, 28.37s/it]  7%|▋         | 35/500 [16:41<3:41:42, 28.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.019 MB of 0.310 MB uploadedwandb: - 0.028 MB of 0.310 MB uploadedwandb: \ 0.137 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▆▄▁▂▇▆▅▇▇▇██▆██▇██▇███▇█▇████▇█▇
wandb:     train_loss ▂▂▃▂▁▃█▁▄▁▁▁▁▃▁▅▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃
wandb:   val_accuracy ▂▂█▇▇▁▃▇▄▄▆▆▆▅▆▄▅▅▅▆▆▅▆▆▆▆▄▅▆▅▄▅▅▅▆
wandb:       val_loss ▂▂▂▂▅▆▄▂▁▄▁▁▃▆▂▂█▃▁▁▃▂▂▁▅▁▅▄▆▆█▃▁▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 34
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.92868
wandb:     train_loss 1.4129
wandb:   val_accuracy 0.49333
wandb:       val_loss 5.61691
wandb: 
wandb: 🚀 View run comic-elevator-176 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/5ykoly7j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_085803-5ykoly7j/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_091536-2kdzatto
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-dream-177
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2kdzatto
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:17:48, 31.00s/it]  0%|          | 2/500 [00:59<4:04:26, 29.45s/it]  1%|          | 3/500 [01:23<3:44:10, 27.06s/it]  1%|          | 4/500 [01:51<3:47:23, 27.51s/it]  1%|          | 5/500 [02:19<3:48:57, 27.75s/it]  1%|          | 6/500 [02:48<3:51:01, 28.06s/it]  1%|▏         | 7/500 [03:17<3:52:16, 28.27s/it]  2%|▏         | 8/500 [03:41<3:41:35, 27.02s/it]  2%|▏         | 9/500 [04:15<3:59:01, 29.21s/it]  2%|▏         | 10/500 [04:40<3:47:18, 27.83s/it]  2%|▏         | 11/500 [05:10<3:53:13, 28.62s/it]  2%|▏         | 12/500 [05:38<3:50:23, 28.33s/it]  3%|▎         | 13/500 [06:03<3:41:06, 27.24s/it]  3%|▎         | 14/500 [06:36<3:54:23, 28.94s/it]  3%|▎         | 15/500 [07:01<3:44:05, 27.72s/it]  3%|▎         | 16/500 [07:26<3:38:40, 27.11s/it]  3%|▎         | 17/500 [07:55<3:42:06, 27.59s/it]  4%|▎         | 18/500 [08:19<3:33:34, 26.59s/it]  4%|▍         | 19/500 [08:51<3:45:03, 28.07s/it]  4%|▍         | 20/500 [09:15<3:36:33, 27.07s/it]  4%|▍         | 21/500 [09:49<3:52:25, 29.11s/it]  4%|▍         | 22/500 [10:14<3:42:25, 27.92s/it]  5%|▍         | 23/500 [10:43<3:42:30, 27.99s/it]  5%|▍         | 24/500 [11:13<3:46:40, 28.57s/it]  5%|▌         | 25/500 [11:37<3:36:15, 27.32s/it]  5%|▌         | 26/500 [12:10<3:49:38, 29.07s/it]  5%|▌         | 27/500 [12:41<3:52:46, 29.53s/it]  6%|▌         | 28/500 [13:05<3:40:23, 28.01s/it]  6%|▌         | 29/500 [13:40<3:56:17, 30.10s/it]  6%|▌         | 30/500 [14:05<3:44:11, 28.62s/it]  6%|▌         | 31/500 [14:36<3:49:04, 29.31s/it]  6%|▋         | 32/500 [15:05<3:46:34, 29.05s/it]  7%|▋         | 33/500 [15:29<3:35:30, 27.69s/it]  7%|▋         | 34/500 [15:58<3:37:58, 28.06s/it]  7%|▋         | 35/500 [16:26<3:37:53, 28.11s/it]  7%|▋         | 36/500 [16:51<3:30:33, 27.23s/it]  7%|▋         | 37/500 [17:21<3:35:19, 27.90s/it]  8%|▊         | 38/500 [17:46<3:28:48, 27.12s/it]  8%|▊         | 39/500 [18:15<3:31:56, 27.58s/it]  8%|▊         | 40/500 [18:40<3:26:41, 26.96s/it]  8%|▊         | 41/500 [19:10<3:31:46, 27.68s/it]  8%|▊         | 42/500 [19:39<3:35:37, 28.25s/it]  9%|▊         | 43/500 [20:11<3:43:24, 29.33s/it]  9%|▉         | 44/500 [20:37<3:35:43, 28.39s/it]  9%|▉         | 45/500 [21:02<3:27:18, 27.34s/it]  9%|▉         | 46/500 [21:30<3:28:22, 27.54s/it]  9%|▉         | 46/500 [21:30<3:32:19, 28.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.138 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▄▄▅▆▇▇█▃▁▃▄▄▄▄▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▇▆▆▇
wandb:     train_loss ▅▆█▇▄▅▆▆▁▆▇▇▆▆▅▆▆▄▆▇▆▆▆▆▅▅▆▅▆▅▆▆▅▅▆▆▆▄▆▆
wandb:   val_accuracy ▄▄▄▆▆▆▆█▃▁▄▄▄▄▄▄▄▅▅▆▆▅▆▅▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:       val_loss ▂▂▂▁▁▂▁▁█▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▁▂▁▁▁▂▂▂▁▂▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 45
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.62407
wandb:     train_loss 1.03697
wandb:   val_accuracy 0.37778
wandb:       val_loss 1.00886
wandb: 
wandb: 🚀 View run usual-dream-177 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/2kdzatto
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_091536-2kdzatto/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_093756-1xrtxt3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-plant-178
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1xrtxt3u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:47:54, 27.40s/it]  0%|          | 2/500 [00:52<3:38:12, 26.29s/it]  1%|          | 3/500 [01:19<3:37:06, 26.21s/it]  1%|          | 4/500 [01:45<3:36:20, 26.17s/it]  1%|          | 5/500 [02:14<3:44:51, 27.26s/it]  1%|          | 6/500 [02:40<3:40:24, 26.77s/it]  1%|▏         | 7/500 [03:08<3:44:57, 27.38s/it]  2%|▏         | 8/500 [03:34<3:39:36, 26.78s/it]  2%|▏         | 9/500 [04:02<3:41:49, 27.11s/it]  2%|▏         | 10/500 [04:28<3:38:50, 26.80s/it]  2%|▏         | 11/500 [04:56<3:41:09, 27.14s/it]  2%|▏         | 12/500 [05:23<3:41:31, 27.24s/it]  3%|▎         | 13/500 [05:52<3:44:25, 27.65s/it]  3%|▎         | 14/500 [06:20<3:45:47, 27.88s/it]  3%|▎         | 15/500 [06:48<3:45:21, 27.88s/it]  3%|▎         | 16/500 [07:16<3:45:39, 27.97s/it]  3%|▎         | 17/500 [07:41<3:38:08, 27.10s/it]  4%|▎         | 18/500 [08:10<3:41:31, 27.57s/it]  4%|▍         | 19/500 [08:38<3:42:18, 27.73s/it]  4%|▍         | 20/500 [09:05<3:41:02, 27.63s/it]  4%|▍         | 21/500 [09:29<3:31:48, 26.53s/it]  4%|▍         | 22/500 [10:03<3:49:21, 28.79s/it]  5%|▍         | 23/500 [10:32<3:47:32, 28.62s/it]  5%|▍         | 24/500 [10:59<3:44:21, 28.28s/it]  5%|▌         | 25/500 [11:30<3:49:11, 28.95s/it]  5%|▌         | 25/500 [11:30<3:38:32, 27.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.019 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▂▂▅▄▄▆▇▇▇▆███
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▄▅▆▅▅▅
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▃▆█▆▇▃▁▆▆
wandb:       val_loss ▃█▂▇▂▂▂▆▁▅▂▂▅▅▂▃▅▃▃▄▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.47845
wandb:     train_loss 1.12892
wandb:   val_accuracy 0.35333
wandb:       val_loss 1.12065
wandb: 
wandb: 🚀 View run stellar-plant-178 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/1xrtxt3u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_093756-1xrtxt3u/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_095021-8j4tj6p6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-fog-179
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8j4tj6p6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:50:16, 27.69s/it]  0%|          | 2/500 [00:52<3:35:13, 25.93s/it]  1%|          | 3/500 [01:17<3:33:33, 25.78s/it]  1%|          | 4/500 [01:43<3:31:41, 25.61s/it]  1%|          | 5/500 [02:15<3:51:01, 28.00s/it]  1%|          | 6/500 [02:44<3:52:40, 28.26s/it]  1%|▏         | 7/500 [03:09<3:43:06, 27.15s/it]  2%|▏         | 8/500 [03:42<3:58:30, 29.09s/it]  2%|▏         | 9/500 [04:10<3:56:28, 28.90s/it]  2%|▏         | 10/500 [04:40<3:56:42, 28.99s/it]  2%|▏         | 11/500 [05:09<3:56:21, 29.00s/it]  2%|▏         | 12/500 [05:43<4:08:33, 30.56s/it]  3%|▎         | 13/500 [06:07<3:53:12, 28.73s/it]  3%|▎         | 14/500 [06:41<4:04:45, 30.22s/it]  3%|▎         | 15/500 [07:09<3:59:37, 29.64s/it]  3%|▎         | 16/500 [07:34<3:46:46, 28.11s/it]  3%|▎         | 17/500 [08:07<3:59:42, 29.78s/it]  4%|▎         | 18/500 [08:33<3:48:45, 28.48s/it]  4%|▍         | 19/500 [09:04<3:55:23, 29.36s/it]  4%|▍         | 20/500 [09:29<3:43:08, 27.89s/it]  4%|▍         | 21/500 [09:58<3:46:14, 28.34s/it]  4%|▍         | 22/500 [10:33<4:00:12, 30.15s/it]  5%|▍         | 23/500 [10:58<3:47:46, 28.65s/it]  5%|▍         | 24/500 [11:33<4:03:53, 30.74s/it]  5%|▌         | 25/500 [11:58<3:49:52, 29.04s/it]  5%|▌         | 26/500 [12:28<3:50:52, 29.23s/it]  5%|▌         | 27/500 [13:03<4:03:50, 30.93s/it]  6%|▌         | 28/500 [13:33<4:02:16, 30.80s/it]  6%|▌         | 29/500 [14:02<3:56:32, 30.13s/it]  6%|▌         | 30/500 [14:31<3:53:51, 29.85s/it]  6%|▌         | 31/500 [14:56<3:42:20, 28.45s/it]  6%|▋         | 32/500 [15:27<3:47:22, 29.15s/it]  7%|▋         | 33/500 [15:53<3:39:26, 28.19s/it]  7%|▋         | 34/500 [16:22<3:39:34, 28.27s/it]  7%|▋         | 35/500 [16:49<3:38:05, 28.14s/it]  7%|▋         | 36/500 [17:18<3:38:23, 28.24s/it]  7%|▋         | 37/500 [17:47<3:39:07, 28.40s/it]  8%|▊         | 38/500 [18:19<3:47:34, 29.55s/it]  8%|▊         | 39/500 [18:43<3:34:53, 27.97s/it]  8%|▊         | 40/500 [19:14<3:40:30, 28.76s/it]  8%|▊         | 40/500 [19:14<3:41:15, 28.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.019 MB of 0.314 MB uploadedwandb: - 0.027 MB of 0.314 MB uploadedwandb: \ 0.027 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▁▁▁▂▅▄▆▁▄▆▅▇▇▄▅█▇▇██▇█▇▆█▇▇██▇▇▇▇▇▇▇██▇
wandb:     train_loss ▃▁▅▅▁▄▁▁█▂▁▁▁▁▇▇▁▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▆▁▁▁▁▁
wandb:   val_accuracy ▃▂▂▂▂▅▄▅▁▄▅▅▇▅▄▄▆▇▆▇▅▅▆▄▆▇▄▅█▆▆▇▅▄▅▅▆▆▆▅
wandb:       val_loss ▂▄▃▄▃▂▂▅▂▅▃▁▄█▄▅▂▅▁▁▃▂▄▁▅▅▆▄▆▂▅▁▃▃▁▂▅▆▆▆
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.93165
wandb:     train_loss 0.00042
wandb:   val_accuracy 0.45111
wandb:       val_loss 3.78362
wandb: 
wandb: 🚀 View run desert-fog-179 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8j4tj6p6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_095021-8j4tj6p6/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_101120-bm58ee8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-serenity-180
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bm58ee8p
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:41:09, 26.59s/it]  0%|          | 2/500 [00:51<3:30:19, 25.34s/it]  1%|          | 3/500 [01:15<3:28:03, 25.12s/it]  1%|          | 4/500 [01:43<3:35:15, 26.04s/it]  1%|          | 5/500 [02:07<3:30:23, 25.50s/it]  1%|          | 6/500 [02:33<3:29:56, 25.50s/it]  1%|▏         | 7/500 [03:11<4:02:06, 29.46s/it]  2%|▏         | 8/500 [03:35<3:47:16, 27.72s/it]  2%|▏         | 9/500 [04:06<3:56:31, 28.90s/it]  2%|▏         | 10/500 [04:32<3:47:47, 27.89s/it]  2%|▏         | 11/500 [04:57<3:41:04, 27.13s/it]  2%|▏         | 12/500 [05:26<3:45:58, 27.78s/it]  3%|▎         | 13/500 [05:59<3:58:27, 29.38s/it]  3%|▎         | 14/500 [06:29<3:58:42, 29.47s/it]  3%|▎         | 15/500 [07:03<4:09:04, 30.81s/it]  3%|▎         | 16/500 [07:28<3:53:16, 28.92s/it]  3%|▎         | 17/500 [08:02<4:06:45, 30.65s/it]  4%|▎         | 18/500 [08:27<3:52:00, 28.88s/it]  4%|▍         | 19/500 [09:07<4:19:29, 32.37s/it]  4%|▍         | 20/500 [09:38<4:13:44, 31.72s/it]  4%|▍         | 21/500 [10:02<3:56:02, 29.57s/it]  4%|▍         | 22/500 [10:36<4:06:51, 30.99s/it]  5%|▍         | 23/500 [11:13<4:20:08, 32.72s/it]  5%|▍         | 24/500 [11:47<4:22:20, 33.07s/it]  5%|▌         | 25/500 [12:15<4:08:42, 31.42s/it]  5%|▌         | 26/500 [12:41<3:55:33, 29.82s/it]  5%|▌         | 27/500 [13:12<3:59:15, 30.35s/it]  6%|▌         | 28/500 [13:40<3:51:34, 29.44s/it]  6%|▌         | 29/500 [14:07<3:46:26, 28.85s/it]  6%|▌         | 30/500 [14:34<3:42:24, 28.39s/it]  6%|▌         | 31/500 [14:59<3:33:50, 27.36s/it]  6%|▋         | 32/500 [15:29<3:38:01, 27.95s/it]  7%|▋         | 33/500 [15:57<3:38:52, 28.12s/it]  7%|▋         | 34/500 [16:22<3:30:15, 27.07s/it]  7%|▋         | 35/500 [16:52<3:37:42, 28.09s/it]  7%|▋         | 36/500 [17:18<3:31:04, 27.29s/it]  7%|▋         | 37/500 [17:44<3:27:09, 26.85s/it]  8%|▊         | 38/500 [18:13<3:33:09, 27.68s/it]  8%|▊         | 39/500 [18:40<3:30:30, 27.40s/it]  8%|▊         | 40/500 [19:11<3:38:25, 28.49s/it]  8%|▊         | 41/500 [19:39<3:36:26, 28.29s/it]  8%|▊         | 42/500 [20:10<3:43:10, 29.24s/it]  9%|▊         | 43/500 [20:39<3:41:39, 29.10s/it]  9%|▉         | 44/500 [21:04<3:31:22, 27.81s/it]  9%|▉         | 45/500 [21:39<3:47:33, 30.01s/it]  9%|▉         | 46/500 [22:05<3:37:16, 28.71s/it]  9%|▉         | 47/500 [22:37<3:44:49, 29.78s/it] 10%|▉         | 48/500 [23:03<3:35:03, 28.55s/it] 10%|▉         | 49/500 [23:32<3:36:45, 28.84s/it] 10%|█         | 50/500 [24:00<3:33:00, 28.40s/it] 10%|█         | 51/500 [24:30<3:36:19, 28.91s/it] 10%|█         | 52/500 [24:55<3:27:47, 27.83s/it] 11%|█         | 53/500 [25:26<3:33:32, 28.66s/it] 11%|█         | 54/500 [25:50<3:24:19, 27.49s/it] 11%|█         | 55/500 [26:22<3:34:00, 28.85s/it] 11%|█         | 56/500 [26:51<3:33:38, 28.87s/it] 11%|█▏        | 57/500 [27:22<3:36:52, 29.37s/it] 11%|█▏        | 57/500 [27:22<3:32:43, 28.81s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▅▅▇▅████▇█▇██▇█▇▇▇▆▇▇▇▇▆▇▇▇█▇▇▆█▇███▆
wandb:     train_loss ▄▅▅▃▅▁▅▃▁▁▄▄▅▆▅▃▅▃▄▄▄▁▁▁▄▇▄▄▁█▂▃▃▅▂▃▁▄▃▁
wandb:   val_accuracy ▂▂▁▃▂▄▁▇▇▇█▄▆█▇▇█████▄█▇██▄▅█▇███▄█████▄
wandb:       val_loss ▄▅▄▄▄▅▄▄▄▄▄▆▄▃▃▄▄▃▄▄█▇▃▆▃▂▃▇▁▅▂▃▃▃▂▅▄▃▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.63893
wandb:     train_loss 0.06177
wandb:   val_accuracy 0.40667
wandb:       val_loss 1.57347
wandb: 
wandb: 🚀 View run driven-serenity-180 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/bm58ee8p
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_101120-bm58ee8p/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_103934-575tqk1i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-hill-181
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/575tqk1i
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:36:08, 25.99s/it]  0%|          | 2/500 [00:50<3:28:00, 25.06s/it]  1%|          | 3/500 [01:15<3:26:20, 24.91s/it]  1%|          | 4/500 [01:42<3:33:51, 25.87s/it]  1%|          | 5/500 [02:11<3:41:45, 26.88s/it]  1%|          | 6/500 [02:35<3:34:27, 26.05s/it]  1%|▏         | 7/500 [03:08<3:51:13, 28.14s/it]  2%|▏         | 8/500 [03:32<3:40:53, 26.94s/it]  2%|▏         | 9/500 [04:12<4:12:56, 30.91s/it]  2%|▏         | 10/500 [04:36<3:55:47, 28.87s/it]  2%|▏         | 11/500 [05:01<3:45:50, 27.71s/it]  2%|▏         | 12/500 [05:31<3:52:07, 28.54s/it]  3%|▎         | 13/500 [05:56<3:41:52, 27.34s/it]  3%|▎         | 14/500 [06:26<3:47:50, 28.13s/it]  3%|▎         | 15/500 [06:51<3:41:11, 27.36s/it]  3%|▎         | 16/500 [07:17<3:37:20, 26.94s/it]  3%|▎         | 17/500 [07:48<3:45:31, 28.02s/it]  4%|▎         | 18/500 [08:12<3:36:11, 26.91s/it]  4%|▍         | 19/500 [08:42<3:41:33, 27.64s/it]  4%|▍         | 20/500 [09:06<3:33:43, 26.71s/it]  4%|▍         | 21/500 [09:37<3:43:24, 27.98s/it]  4%|▍         | 22/500 [10:02<3:36:31, 27.18s/it]  5%|▍         | 23/500 [10:31<3:40:34, 27.74s/it]  5%|▍         | 24/500 [10:59<3:38:25, 27.53s/it]  5%|▌         | 25/500 [11:25<3:34:50, 27.14s/it]  5%|▌         | 25/500 [11:25<3:36:59, 27.41s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.230 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▂▄▅▅▅▄█▆▇
wandb:     train_loss ▁▇▁▁█▂▆▇▆▆▆▆▆▆▅▅▅▅▅▄▅▇▅▅▆
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▅▅▅█▆▅▅▆▄▆▅▆▅▇▁▅
wandb:       val_loss ▃█▂▇▂▂▂▆▁▅▂▂▆▅▃▄▅▃▄▄▃▂▃▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.40862
wandb:     train_loss 1.13709
wandb:   val_accuracy 0.34444
wandb:       val_loss 1.14726
wandb: 
wandb: 🚀 View run autumn-hill-181 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/575tqk1i
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_103934-575tqk1i/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_105142-6gzebzlo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sea-182
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6gzebzlo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:33, 17.74s/it]  0%|          | 2/500 [00:34<2:21:01, 16.99s/it]  1%|          | 3/500 [00:50<2:17:40, 16.62s/it]  1%|          | 4/500 [01:06<2:15:25, 16.38s/it]  1%|          | 5/500 [01:22<2:15:37, 16.44s/it]  1%|          | 6/500 [01:40<2:17:09, 16.66s/it]  1%|▏         | 7/500 [01:56<2:16:21, 16.59s/it]  2%|▏         | 8/500 [02:13<2:16:11, 16.61s/it]  2%|▏         | 9/500 [02:28<2:14:00, 16.38s/it]  2%|▏         | 10/500 [02:44<2:12:27, 16.22s/it]  2%|▏         | 11/500 [03:01<2:13:20, 16.36s/it]  2%|▏         | 12/500 [03:18<2:15:11, 16.62s/it]  3%|▎         | 13/500 [03:34<2:13:42, 16.47s/it]  3%|▎         | 14/500 [03:52<2:16:19, 16.83s/it]  3%|▎         | 15/500 [04:16<2:32:59, 18.93s/it]  3%|▎         | 16/500 [04:33<2:28:12, 18.37s/it]  3%|▎         | 17/500 [04:50<2:25:53, 18.12s/it]  4%|▎         | 18/500 [05:08<2:23:12, 17.83s/it]  4%|▍         | 19/500 [05:26<2:23:26, 17.89s/it]  4%|▍         | 20/500 [05:43<2:20:55, 17.62s/it]  4%|▍         | 21/500 [05:59<2:17:39, 17.24s/it]  4%|▍         | 22/500 [06:16<2:17:52, 17.31s/it]  5%|▍         | 23/500 [06:33<2:16:16, 17.14s/it]  5%|▍         | 24/500 [06:50<2:14:49, 16.99s/it]  5%|▍         | 24/500 [06:50<2:15:43, 17.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.021 MB of 0.308 MB uploadedwandb: - 0.303 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁
wandb: train_accuracy ▁▂▂▂▄▇▇▄█▇███████▇██████
wandb:     train_loss ▆█▂▇▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▂▂▂▇█▃▇▆▇█▇▇▇▆▆▇▇▆▇█▇▇
wandb:       val_loss ▂▂▂▃▄▁▄▁▃▇▄▁▃▃▂▅▂▆▆█▂▁▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 23
wandb:  learning_rate 0.00025
wandb: train_accuracy 1.0
wandb:     train_loss 0.00101
wandb:   val_accuracy 0.50667
wandb:       val_loss 3.10158
wandb: 
wandb: 🚀 View run stoic-sea-182 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6gzebzlo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_105142-6gzebzlo/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_105916-t4v5bef9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-frog-183
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/t4v5bef9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:35, 18.47s/it]  0%|          | 2/500 [00:34<2:20:04, 16.88s/it]  1%|          | 3/500 [00:55<2:34:38, 18.67s/it]  1%|          | 4/500 [01:11<2:25:44, 17.63s/it]  1%|          | 5/500 [01:27<2:22:08, 17.23s/it]  1%|          | 6/500 [01:43<2:18:09, 16.78s/it]  1%|▏         | 7/500 [01:59<2:15:14, 16.46s/it]  2%|▏         | 8/500 [02:15<2:13:29, 16.28s/it]  2%|▏         | 9/500 [02:31<2:12:31, 16.19s/it]  2%|▏         | 10/500 [02:47<2:12:08, 16.18s/it]  2%|▏         | 11/500 [03:03<2:12:09, 16.22s/it]  2%|▏         | 12/500 [03:19<2:11:57, 16.22s/it]  3%|▎         | 13/500 [03:36<2:11:32, 16.21s/it]  3%|▎         | 14/500 [03:51<2:10:03, 16.06s/it]  3%|▎         | 15/500 [04:07<2:08:24, 15.89s/it]  3%|▎         | 16/500 [04:22<2:05:30, 15.56s/it]  3%|▎         | 17/500 [04:42<2:17:44, 17.11s/it]  4%|▎         | 18/500 [04:59<2:16:40, 17.01s/it]  4%|▍         | 19/500 [05:16<2:16:54, 17.08s/it]  4%|▍         | 20/500 [05:33<2:15:06, 16.89s/it]  4%|▍         | 21/500 [05:49<2:13:43, 16.75s/it]  4%|▍         | 22/500 [06:05<2:11:44, 16.54s/it]  5%|▍         | 23/500 [06:21<2:10:51, 16.46s/it]  5%|▍         | 24/500 [06:40<2:14:31, 16.96s/it]  5%|▌         | 25/500 [06:57<2:14:40, 17.01s/it]  5%|▌         | 26/500 [07:13<2:12:54, 16.82s/it]  5%|▌         | 26/500 [07:13<2:11:45, 16.68s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.021 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▃▄▃▃▆▅▂▂▁█▃▃▂▂▂▃▂▃▂▃▃▃▂▃
wandb:     train_loss ▁▁▁▁▁▁▁▁▂█▁▁▁▁▃▃▁▂▃▁▃▂▁▁▂▁
wandb:   val_accuracy ▃▃▅▁▃▃▅▅▃▃▂█▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:       val_loss ▂▂▂▂▂▂▂▂▃▁▂▂▂▂▄▄▅▃▄▂█▄▂▂▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.34027
wandb:     train_loss 1.00887
wandb:   val_accuracy 0.32444
wandb:       val_loss 1.84852
wandb: 
wandb: 🚀 View run lilac-frog-183 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/t4v5bef9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_105916-t4v5bef9/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_110718-9ga7kank
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sun-184
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9ga7kank
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:38:09, 19.02s/it]  0%|          | 2/500 [00:34<2:22:00, 17.11s/it]  1%|          | 3/500 [00:51<2:18:26, 16.71s/it]  1%|          | 4/500 [01:07<2:15:54, 16.44s/it]  1%|          | 5/500 [01:23<2:16:37, 16.56s/it]  1%|          | 6/500 [01:39<2:14:28, 16.33s/it]  1%|▏         | 7/500 [01:55<2:13:04, 16.20s/it]  2%|▏         | 8/500 [02:11<2:13:00, 16.22s/it]  2%|▏         | 9/500 [02:29<2:16:53, 16.73s/it]  2%|▏         | 10/500 [02:47<2:18:24, 16.95s/it]  2%|▏         | 11/500 [03:02<2:15:13, 16.59s/it]  2%|▏         | 12/500 [03:18<2:12:51, 16.33s/it]  3%|▎         | 13/500 [03:35<2:14:09, 16.53s/it]  3%|▎         | 14/500 [03:52<2:14:03, 16.55s/it]  3%|▎         | 15/500 [04:09<2:15:58, 16.82s/it]  3%|▎         | 16/500 [04:26<2:15:11, 16.76s/it]  3%|▎         | 17/500 [04:42<2:13:51, 16.63s/it]  4%|▎         | 18/500 [04:59<2:13:05, 16.57s/it]  4%|▍         | 19/500 [05:15<2:11:28, 16.40s/it]  4%|▍         | 19/500 [05:15<2:12:57, 16.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.021 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁█▁▁▁▁▁▃▂▁▂▃▇
wandb:     train_loss ██▆▁▆▇▆▄▂▂▆▅▆▅▃▃▆▃▄
wandb:   val_accuracy ███████▄█████▇█▇▇▆▁
wandb:       val_loss ▃▃▁▇▃▆█▂▆▇▃▄▆▂▅▄▂▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.40267
wandb:     train_loss 1.06509
wandb:   val_accuracy 0.32444
wandb:       val_loss 1.12086
wandb: 
wandb: 🚀 View run vital-sun-184 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/9ga7kank
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_110718-9ga7kank/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_111322-qoa74291
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-blaze-185
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qoa74291
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:38:41, 19.08s/it]  0%|          | 2/500 [00:35<2:24:17, 17.38s/it]  1%|          | 3/500 [00:52<2:22:09, 17.16s/it]  1%|          | 4/500 [01:08<2:18:25, 16.74s/it]  1%|          | 5/500 [01:24<2:16:20, 16.53s/it]  1%|          | 6/500 [01:41<2:18:45, 16.85s/it]  1%|▏         | 7/500 [01:58<2:17:33, 16.74s/it]  2%|▏         | 8/500 [02:14<2:15:11, 16.49s/it]  2%|▏         | 9/500 [02:30<2:14:39, 16.46s/it]  2%|▏         | 10/500 [02:47<2:14:39, 16.49s/it]  2%|▏         | 11/500 [03:03<2:13:40, 16.40s/it]  2%|▏         | 12/500 [03:20<2:14:22, 16.52s/it]  3%|▎         | 13/500 [03:36<2:13:06, 16.40s/it]  3%|▎         | 14/500 [03:53<2:13:26, 16.47s/it]  3%|▎         | 15/500 [04:09<2:12:17, 16.37s/it]  3%|▎         | 16/500 [04:25<2:11:39, 16.32s/it]  3%|▎         | 17/500 [04:42<2:12:16, 16.43s/it]  4%|▎         | 18/500 [04:59<2:14:42, 16.77s/it]  4%|▍         | 19/500 [05:16<2:13:41, 16.68s/it]  4%|▍         | 20/500 [05:32<2:12:28, 16.56s/it]  4%|▍         | 21/500 [05:49<2:12:55, 16.65s/it]  4%|▍         | 22/500 [06:05<2:11:58, 16.57s/it]  5%|▍         | 23/500 [06:22<2:12:29, 16.67s/it]  5%|▍         | 24/500 [06:39<2:13:45, 16.86s/it]  5%|▌         | 25/500 [06:57<2:14:28, 16.99s/it]  5%|▌         | 26/500 [07:13<2:13:44, 16.93s/it]  5%|▌         | 27/500 [07:31<2:14:28, 17.06s/it]  6%|▌         | 28/500 [07:47<2:12:31, 16.85s/it]  6%|▌         | 29/500 [08:04<2:12:03, 16.82s/it]  6%|▌         | 30/500 [08:22<2:14:42, 17.20s/it]  6%|▌         | 31/500 [08:39<2:15:04, 17.28s/it]  6%|▋         | 32/500 [08:57<2:16:17, 17.47s/it]  7%|▋         | 33/500 [09:16<2:18:10, 17.75s/it]  7%|▋         | 34/500 [09:34<2:18:57, 17.89s/it]  7%|▋         | 35/500 [09:52<2:18:55, 17.93s/it]  7%|▋         | 36/500 [10:10<2:19:54, 18.09s/it]  7%|▋         | 37/500 [10:28<2:18:36, 17.96s/it]  8%|▊         | 38/500 [10:47<2:19:50, 18.16s/it]  8%|▊         | 39/500 [11:04<2:17:45, 17.93s/it]  8%|▊         | 40/500 [11:20<2:13:12, 17.38s/it]  8%|▊         | 41/500 [11:36<2:10:07, 17.01s/it]  8%|▊         | 42/500 [11:53<2:07:53, 16.75s/it]  9%|▊         | 43/500 [12:10<2:10:12, 17.10s/it]  9%|▉         | 44/500 [12:28<2:10:46, 17.21s/it]  9%|▉         | 45/500 [12:44<2:09:04, 17.02s/it]  9%|▉         | 46/500 [13:01<2:06:39, 16.74s/it]  9%|▉         | 47/500 [13:17<2:06:26, 16.75s/it] 10%|▉         | 48/500 [13:34<2:05:37, 16.68s/it] 10%|▉         | 49/500 [13:50<2:05:14, 16.66s/it] 10%|█         | 50/500 [14:07<2:04:16, 16.57s/it] 10%|█         | 51/500 [14:24<2:05:09, 16.73s/it] 10%|█         | 52/500 [14:41<2:06:17, 16.91s/it] 11%|█         | 53/500 [14:59<2:07:50, 17.16s/it] 11%|█         | 54/500 [15:17<2:10:04, 17.50s/it] 11%|█         | 55/500 [15:33<2:06:27, 17.05s/it] 11%|█         | 56/500 [15:49<2:03:31, 16.69s/it] 11%|█▏        | 57/500 [16:06<2:02:49, 16.64s/it] 12%|█▏        | 58/500 [16:22<2:02:20, 16.61s/it] 12%|█▏        | 59/500 [16:39<2:02:53, 16.72s/it] 12%|█▏        | 60/500 [16:56<2:02:43, 16.74s/it] 12%|█▏        | 60/500 [16:56<2:04:13, 16.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.264 MB of 0.316 MB uploadedwandb: - 0.264 MB of 0.316 MB uploadedwandb: \ 0.264 MB of 0.316 MB uploadedwandb: | 0.264 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▂▃▅▅█▇▅▃▆▇█▇▇██▇▇█▇█▇████▇██▇██▇███▇█
wandb:     train_loss ▂▁▇█▃▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:   val_accuracy ▂▂▅▂▁█▇▇▇▆▄▇▇▇█▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       val_loss ▂▂▁▆▇▁▁▅▃▃▇▅▃█▁▁▁▄▁▄▅▁█▅▃▁▁▆▆▁▆▄▆▄▁▅▅▁▇▅
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.97028
wandb:     train_loss 0.00316
wandb:   val_accuracy 0.55111
wandb:       val_loss 3.21996
wandb: 
wandb: 🚀 View run hardy-blaze-185 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/qoa74291
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_111322-qoa74291/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_113105-i7ciukhy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-rain-186
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/i7ciukhy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:22, 18.08s/it]  0%|          | 2/500 [00:35<2:25:45, 17.56s/it]  1%|          | 3/500 [00:53<2:26:47, 17.72s/it]  1%|          | 4/500 [01:10<2:23:58, 17.42s/it]  1%|          | 5/500 [01:26<2:20:56, 17.08s/it]  1%|          | 6/500 [01:42<2:18:33, 16.83s/it]  1%|▏         | 7/500 [01:59<2:17:05, 16.68s/it]  2%|▏         | 8/500 [02:16<2:17:59, 16.83s/it]  2%|▏         | 9/500 [02:32<2:16:06, 16.63s/it]  2%|▏         | 10/500 [02:49<2:15:50, 16.63s/it]  2%|▏         | 11/500 [03:05<2:15:35, 16.64s/it]  2%|▏         | 12/500 [03:22<2:14:35, 16.55s/it]  3%|▎         | 13/500 [03:39<2:14:47, 16.61s/it]  3%|▎         | 14/500 [03:55<2:13:49, 16.52s/it]  3%|▎         | 15/500 [04:11<2:13:30, 16.52s/it]  3%|▎         | 16/500 [04:28<2:12:57, 16.48s/it]  3%|▎         | 17/500 [04:45<2:13:14, 16.55s/it]  4%|▎         | 18/500 [05:02<2:15:17, 16.84s/it]  4%|▍         | 19/500 [05:19<2:14:56, 16.83s/it]  4%|▍         | 20/500 [05:36<2:14:45, 16.85s/it]  4%|▍         | 21/500 [05:53<2:14:40, 16.87s/it]  4%|▍         | 22/500 [06:09<2:13:37, 16.77s/it]  5%|▍         | 23/500 [06:25<2:12:10, 16.63s/it]  5%|▍         | 24/500 [06:42<2:11:24, 16.56s/it]  5%|▌         | 25/500 [06:59<2:11:18, 16.59s/it]  5%|▌         | 26/500 [07:15<2:11:25, 16.64s/it]  5%|▌         | 27/500 [07:32<2:12:26, 16.80s/it]  6%|▌         | 28/500 [07:48<2:09:34, 16.47s/it]  6%|▌         | 29/500 [08:03<2:06:15, 16.08s/it]  6%|▌         | 30/500 [08:24<2:17:35, 17.56s/it]  6%|▌         | 31/500 [08:41<2:14:44, 17.24s/it]  6%|▋         | 32/500 [09:01<2:22:00, 18.21s/it]  7%|▋         | 33/500 [09:17<2:15:48, 17.45s/it]  7%|▋         | 34/500 [09:33<2:13:18, 17.16s/it]  7%|▋         | 35/500 [09:50<2:10:32, 16.84s/it]  7%|▋         | 36/500 [10:05<2:07:39, 16.51s/it]  7%|▋         | 37/500 [10:22<2:06:57, 16.45s/it]  8%|▊         | 38/500 [10:37<2:04:58, 16.23s/it]  8%|▊         | 39/500 [10:53<2:03:57, 16.13s/it]  8%|▊         | 40/500 [11:09<2:03:13, 16.07s/it]  8%|▊         | 41/500 [11:25<2:02:44, 16.04s/it]  8%|▊         | 42/500 [11:41<2:02:20, 16.03s/it]  9%|▊         | 43/500 [11:58<2:03:26, 16.21s/it]  9%|▉         | 44/500 [12:14<2:02:50, 16.16s/it]  9%|▉         | 45/500 [12:29<2:01:20, 16.00s/it]  9%|▉         | 46/500 [12:45<2:00:44, 15.96s/it]  9%|▉         | 47/500 [13:01<1:59:52, 15.88s/it] 10%|▉         | 48/500 [13:18<2:01:10, 16.08s/it] 10%|▉         | 49/500 [13:33<2:00:04, 15.98s/it] 10%|█         | 50/500 [13:49<1:59:39, 15.95s/it] 10%|█         | 50/500 [13:49<2:04:27, 16.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.019 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▄▅▅▄▆▇▇▇▇▆▇▇▇██▇█████████████████████
wandb:     train_loss ▇▇▆▇▅▆▆▆▅▆▄▁█▃█▆▆▅▅▅▁▁▂▅▄▅▇▅▅▃▄▆▄▄▅▃▅▂▄▄
wandb:   val_accuracy ▁▁▁▅▆▃▂▆▆▇▇▇▃▇█▇▇▇███▇▇▇█▇▇▇▇██▇██▇▇███▇
wandb:       val_loss ▃▃▃▃▂▃▂▃▃▂▂▂▆▃▃▂▇▂▂▆▃▂▁▂▃▂▂▁▂▁▁▄▁▅▂█▃▃▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 49
wandb:  learning_rate 0.0
wandb: train_accuracy 0.85438
wandb:     train_loss 0.59396
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.53197
wandb: 
wandb: 🚀 View run glad-rain-186 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/i7ciukhy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_113105-i7ciukhy/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_114541-6i89uhs5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-haze-187
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6i89uhs5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:03, 17.80s/it]  0%|          | 2/500 [00:33<2:16:05, 16.40s/it]  1%|          | 3/500 [00:49<2:14:33, 16.24s/it]  1%|          | 4/500 [01:04<2:12:17, 16.00s/it]  1%|          | 5/500 [01:20<2:09:42, 15.72s/it]  1%|          | 6/500 [01:35<2:09:46, 15.76s/it]  1%|▏         | 7/500 [01:56<2:23:05, 17.42s/it]  2%|▏         | 8/500 [02:15<2:25:39, 17.76s/it]  2%|▏         | 9/500 [02:31<2:22:29, 17.41s/it]  2%|▏         | 10/500 [02:49<2:21:33, 17.33s/it]  2%|▏         | 11/500 [03:05<2:19:44, 17.15s/it]  2%|▏         | 12/500 [03:22<2:18:05, 16.98s/it]  3%|▎         | 13/500 [03:40<2:19:50, 17.23s/it]  3%|▎         | 14/500 [03:57<2:19:33, 17.23s/it]  3%|▎         | 15/500 [04:14<2:19:16, 17.23s/it]  3%|▎         | 16/500 [04:31<2:17:33, 17.05s/it]  3%|▎         | 17/500 [04:48<2:16:41, 16.98s/it]  4%|▎         | 18/500 [05:04<2:15:15, 16.84s/it]  4%|▍         | 19/500 [05:21<2:14:01, 16.72s/it]  4%|▍         | 19/500 [05:21<2:15:28, 16.90s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▃▃▁▂
wandb:     train_loss ▆▆▅▁▅▆▄▅▁▂▅▄▅▄▃█▂▂▄
wandb:   val_accuracy ▆▆▆▆▆▆▆▆▆▆▆▆▆█▅▁▁▆▅
wandb:       val_loss ▃▃▁█▃▇▇▆▇▇▄▅▇▃▄█▇▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.34324
wandb:     train_loss 1.0936
wandb:   val_accuracy 0.33778
wandb:       val_loss 1.10981
wandb: 
wandb: 🚀 View run curious-haze-187 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/6i89uhs5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_114541-6i89uhs5/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_115144-aj3wpxk8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-wave-188
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/aj3wpxk8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:46, 17.41s/it]  0%|          | 2/500 [00:33<2:19:09, 16.77s/it]  1%|          | 3/500 [00:50<2:18:00, 16.66s/it]  1%|          | 4/500 [01:06<2:16:31, 16.52s/it]  1%|          | 5/500 [01:23<2:17:00, 16.61s/it]  1%|          | 6/500 [01:39<2:14:38, 16.35s/it]  1%|▏         | 7/500 [01:56<2:17:00, 16.67s/it]  2%|▏         | 8/500 [02:12<2:14:46, 16.44s/it]  2%|▏         | 9/500 [02:28<2:12:31, 16.20s/it]  2%|▏         | 10/500 [02:43<2:11:21, 16.08s/it]  2%|▏         | 11/500 [03:00<2:12:55, 16.31s/it]  2%|▏         | 12/500 [03:16<2:12:05, 16.24s/it]  3%|▎         | 13/500 [03:33<2:13:09, 16.41s/it]  3%|▎         | 14/500 [03:49<2:12:45, 16.39s/it]  3%|▎         | 15/500 [04:06<2:13:56, 16.57s/it]  3%|▎         | 16/500 [04:23<2:12:26, 16.42s/it]  3%|▎         | 17/500 [04:39<2:12:09, 16.42s/it]  4%|▎         | 18/500 [04:55<2:11:54, 16.42s/it]  4%|▍         | 19/500 [05:12<2:11:28, 16.40s/it]  4%|▍         | 20/500 [05:28<2:11:16, 16.41s/it]  4%|▍         | 21/500 [05:45<2:12:17, 16.57s/it]  4%|▍         | 22/500 [06:01<2:10:15, 16.35s/it]  5%|▍         | 23/500 [06:18<2:10:33, 16.42s/it]  5%|▍         | 23/500 [06:18<2:10:41, 16.44s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.021 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁
wandb: train_accuracy ▃▃▅█▅▅▆▃▁▂▄▄▂▆▂▃▄▂▃▂▄▄▄
wandb:     train_loss ▁▁▂▃▃▁▁█▂▁▂▁▂▁▂▁▂▁▁▂▁▁▁
wandb:   val_accuracy ▂▂▂█▂▁▂▂▁▂▂▂▁▃▂▂▄▅▅▄▄▅▅
wandb:       val_loss ▁▁▂▁▃▂▁█▂▂▂▁▂▁▂▁▁▁▁▂▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.37593
wandb:     train_loss 1.1873
wandb:   val_accuracy 0.45111
wandb:       val_loss 1.25088
wandb: 
wandb: 🚀 View run morning-wave-188 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/aj3wpxk8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_115144-aj3wpxk8/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_115850-8cnu28r4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-serenity-189
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8cnu28r4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:15, 18.19s/it]  0%|          | 2/500 [00:36<2:32:16, 18.35s/it]  1%|          | 3/500 [00:53<2:25:59, 17.63s/it]  1%|          | 4/500 [01:09<2:21:18, 17.09s/it]  1%|          | 5/500 [01:26<2:19:46, 16.94s/it]  1%|          | 6/500 [01:43<2:20:02, 17.01s/it]  1%|▏         | 7/500 [01:59<2:16:27, 16.61s/it]  2%|▏         | 8/500 [02:16<2:19:02, 16.96s/it]  2%|▏         | 9/500 [02:33<2:17:46, 16.84s/it]  2%|▏         | 10/500 [02:49<2:16:21, 16.70s/it]  2%|▏         | 11/500 [03:07<2:17:37, 16.89s/it]  2%|▏         | 12/500 [03:24<2:18:11, 16.99s/it]  3%|▎         | 13/500 [03:42<2:20:13, 17.28s/it]  3%|▎         | 14/500 [04:00<2:21:22, 17.45s/it]  3%|▎         | 15/500 [04:18<2:21:57, 17.56s/it]  3%|▎         | 16/500 [04:35<2:21:52, 17.59s/it]  3%|▎         | 17/500 [04:54<2:24:22, 17.93s/it]  4%|▎         | 18/500 [05:12<2:24:16, 17.96s/it]  4%|▍         | 19/500 [05:30<2:23:39, 17.92s/it]  4%|▍         | 20/500 [05:47<2:21:04, 17.64s/it]  4%|▍         | 21/500 [06:03<2:18:25, 17.34s/it]  4%|▍         | 22/500 [06:22<2:20:26, 17.63s/it]  5%|▍         | 23/500 [06:40<2:22:12, 17.89s/it]  5%|▍         | 24/500 [06:58<2:20:47, 17.75s/it]  5%|▌         | 25/500 [07:15<2:20:40, 17.77s/it]  5%|▌         | 26/500 [07:34<2:21:33, 17.92s/it]  5%|▌         | 27/500 [07:51<2:18:42, 17.59s/it]  6%|▌         | 28/500 [08:07<2:16:14, 17.32s/it]  6%|▌         | 29/500 [08:25<2:16:24, 17.38s/it]  6%|▌         | 30/500 [08:42<2:16:15, 17.40s/it]  6%|▌         | 31/500 [08:59<2:14:52, 17.25s/it]  6%|▋         | 32/500 [09:16<2:14:37, 17.26s/it]  7%|▋         | 33/500 [09:35<2:17:58, 17.73s/it]  7%|▋         | 34/500 [09:52<2:15:57, 17.51s/it]  7%|▋         | 35/500 [10:10<2:15:44, 17.51s/it]  7%|▋         | 36/500 [10:27<2:15:30, 17.52s/it]  7%|▋         | 37/500 [10:46<2:18:05, 17.89s/it]  8%|▊         | 38/500 [11:04<2:17:07, 17.81s/it]  8%|▊         | 39/500 [11:21<2:14:35, 17.52s/it]  8%|▊         | 40/500 [11:36<2:09:12, 16.85s/it]  8%|▊         | 41/500 [11:52<2:06:19, 16.51s/it]  8%|▊         | 42/500 [12:13<2:17:53, 18.06s/it]  9%|▊         | 43/500 [12:31<2:16:52, 17.97s/it]  9%|▉         | 44/500 [12:48<2:14:27, 17.69s/it]  9%|▉         | 45/500 [13:05<2:12:20, 17.45s/it]  9%|▉         | 46/500 [13:22<2:11:38, 17.40s/it]  9%|▉         | 47/500 [13:41<2:14:10, 17.77s/it] 10%|▉         | 48/500 [13:59<2:14:07, 17.80s/it] 10%|▉         | 49/500 [14:16<2:12:07, 17.58s/it] 10%|█         | 50/500 [14:33<2:11:33, 17.54s/it] 10%|█         | 51/500 [14:51<2:12:49, 17.75s/it] 10%|█         | 52/500 [15:08<2:09:57, 17.41s/it] 11%|█         | 53/500 [15:26<2:09:47, 17.42s/it] 11%|█         | 54/500 [15:44<2:11:40, 17.71s/it] 11%|█         | 55/500 [16:01<2:09:13, 17.42s/it] 11%|█         | 56/500 [16:18<2:08:18, 17.34s/it] 11%|█▏        | 57/500 [16:35<2:07:03, 17.21s/it] 12%|█▏        | 58/500 [16:54<2:10:45, 17.75s/it] 12%|█▏        | 59/500 [17:12<2:11:26, 17.88s/it] 12%|█▏        | 60/500 [17:30<2:11:05, 17.88s/it] 12%|█▏        | 61/500 [17:48<2:11:43, 18.00s/it] 12%|█▏        | 61/500 [17:48<2:08:10, 17.52s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.030 MB uploadedwandb: | 0.019 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▅▅▅▆▇▇▇▆▇▇███▇█▇█████████████████████
wandb:     train_loss █▇█▇▆▇▆▆▄▁▆▃▇▆▅▅▆▁▂▁▄█▅▅▃▇▅▅▄▁▂▄▄▂▆▅▃▃▂▆
wandb:   val_accuracy ▂▁▃▅▄▄▄▇▇▇▅▇▇▇▇██████▇██████▇▇█▇█▇██▇███
wandb:       val_loss ▃▃▃▃▃▂▃▃▂▂▆▃▂▆▂▆▂▃▁▁▃▂▁▁▁▄▁▃█▁▃▂▁▄▃▂▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81426
wandb:     train_loss 0.98866
wandb:   val_accuracy 0.52222
wandb:       val_loss 0.67281
wandb: 
wandb: 🚀 View run deep-serenity-189 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/8cnu28r4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_115850-8cnu28r4/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_121723-ket1pyag
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-surf-190
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ket1pyag
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:03, 17.80s/it]  0%|          | 2/500 [00:33<2:18:59, 16.75s/it]  1%|          | 3/500 [00:48<2:12:32, 16.00s/it]  1%|          | 4/500 [01:04<2:09:20, 15.65s/it]  1%|          | 5/500 [01:19<2:09:40, 15.72s/it]  1%|          | 6/500 [01:35<2:08:12, 15.57s/it]  1%|▏         | 7/500 [01:50<2:06:51, 15.44s/it]  2%|▏         | 8/500 [02:05<2:05:39, 15.32s/it]  2%|▏         | 9/500 [02:20<2:05:38, 15.35s/it]  2%|▏         | 10/500 [02:36<2:06:02, 15.43s/it]  2%|▏         | 11/500 [02:51<2:04:02, 15.22s/it]  2%|▏         | 12/500 [03:05<2:02:15, 15.03s/it]  3%|▎         | 13/500 [03:23<2:08:03, 15.78s/it]  3%|▎         | 14/500 [03:38<2:06:51, 15.66s/it]  3%|▎         | 15/500 [03:54<2:06:31, 15.65s/it]  3%|▎         | 16/500 [04:10<2:06:37, 15.70s/it]  3%|▎         | 17/500 [04:26<2:09:15, 16.06s/it]  4%|▎         | 18/500 [04:42<2:07:24, 15.86s/it]  4%|▍         | 19/500 [04:57<2:06:29, 15.78s/it]  4%|▍         | 19/500 [04:58<2:05:50, 15.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▃▇█▂
wandb:     train_loss ██▆▁▆▇▆▅▂▂▇▅▆▆▄▆▅▄▅
wandb:   val_accuracy ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▇█▃
wandb:       val_loss ▂▃▁█▃▆█▄▆▇▃▅▆▂▄▅▃▄▃
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.32541
wandb:     train_loss 1.1189
wandb:   val_accuracy 0.34444
wandb:       val_loss 1.09133
wandb: 
wandb: 🚀 View run devoted-surf-190 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ket1pyag
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_121723-ket1pyag/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_122315-tvdxh168
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-breeze-191
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tvdxh168
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:38:04, 19.01s/it]  0%|          | 2/500 [00:34<2:20:48, 16.96s/it]  1%|          | 3/500 [00:49<2:13:57, 16.17s/it]  1%|          | 4/500 [01:05<2:11:20, 15.89s/it]  1%|          | 5/500 [01:20<2:09:38, 15.71s/it]  1%|          | 6/500 [01:36<2:08:59, 15.67s/it]  1%|▏         | 7/500 [01:51<2:08:03, 15.59s/it]  2%|▏         | 8/500 [02:06<2:07:14, 15.52s/it]  2%|▏         | 9/500 [02:22<2:06:25, 15.45s/it]  2%|▏         | 10/500 [02:37<2:05:19, 15.35s/it]  2%|▏         | 11/500 [02:52<2:04:48, 15.31s/it]  2%|▏         | 12/500 [03:08<2:04:51, 15.35s/it]  3%|▎         | 13/500 [03:23<2:04:08, 15.30s/it]  3%|▎         | 14/500 [03:38<2:04:21, 15.35s/it]  3%|▎         | 15/500 [03:54<2:04:23, 15.39s/it]  3%|▎         | 16/500 [04:09<2:02:47, 15.22s/it]  3%|▎         | 17/500 [04:24<2:03:01, 15.28s/it]  4%|▎         | 18/500 [04:39<2:02:17, 15.22s/it]  4%|▍         | 19/500 [04:56<2:05:06, 15.61s/it]  4%|▍         | 20/500 [05:12<2:06:00, 15.75s/it]  4%|▍         | 21/500 [05:27<2:05:06, 15.67s/it]  4%|▍         | 22/500 [05:42<2:03:19, 15.48s/it]  5%|▍         | 23/500 [05:57<2:02:12, 15.37s/it]  5%|▍         | 24/500 [06:12<2:00:51, 15.23s/it]  5%|▌         | 25/500 [06:28<2:01:03, 15.29s/it]  5%|▌         | 26/500 [06:43<2:01:23, 15.36s/it]  5%|▌         | 26/500 [06:43<2:02:38, 15.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.021 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▃▂▅█▅▃█▆█▅▅▄▇▆▅▅▇██▇▇█
wandb:     train_loss ▂▁▃▆▂▁▁▁▇█▁▁▁▁▇▄▁▃▁▄▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▁▁▄▁█▇▇▂▆▇▇▇█▄█▇█▆█▇███▇
wandb:       val_loss ▂▂▄▃▄▂▁▁▅▁▅▁▂▄▁▆▆▄▄▃▁▁▁█▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.95394
wandb:     train_loss 0.00037
wandb:   val_accuracy 0.54889
wandb:       val_loss 0.01901
wandb: 
wandb: 🚀 View run vague-breeze-191 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/tvdxh168
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_122315-tvdxh168/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_123044-f757eamh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-grass-192
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/f757eamh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:01, 18.04s/it]  0%|          | 2/500 [00:33<2:18:05, 16.64s/it]  1%|          | 3/500 [00:49<2:14:44, 16.27s/it]  1%|          | 4/500 [01:05<2:12:40, 16.05s/it]  1%|          | 5/500 [01:21<2:11:40, 15.96s/it]  1%|          | 6/500 [01:37<2:11:44, 16.00s/it]  1%|▏         | 7/500 [01:52<2:10:38, 15.90s/it]  2%|▏         | 8/500 [02:08<2:09:47, 15.83s/it]  2%|▏         | 9/500 [02:24<2:08:47, 15.74s/it]  2%|▏         | 10/500 [02:39<2:08:28, 15.73s/it]  2%|▏         | 11/500 [02:54<2:06:42, 15.55s/it]  2%|▏         | 12/500 [03:10<2:06:16, 15.53s/it]  3%|▎         | 13/500 [03:25<2:05:16, 15.43s/it]  3%|▎         | 14/500 [03:41<2:06:24, 15.61s/it]  3%|▎         | 15/500 [03:57<2:07:15, 15.74s/it]  3%|▎         | 16/500 [04:13<2:08:10, 15.89s/it]  3%|▎         | 17/500 [04:29<2:08:09, 15.92s/it]  4%|▎         | 18/500 [04:46<2:09:34, 16.13s/it]  4%|▍         | 19/500 [05:02<2:09:15, 16.12s/it]  4%|▍         | 20/500 [05:18<2:07:46, 15.97s/it]  4%|▍         | 21/500 [05:34<2:07:33, 15.98s/it]  4%|▍         | 22/500 [05:50<2:07:08, 15.96s/it]  5%|▍         | 23/500 [06:06<2:07:38, 16.05s/it]  5%|▍         | 24/500 [06:22<2:07:31, 16.07s/it]  5%|▌         | 25/500 [06:38<2:06:50, 16.02s/it]  5%|▌         | 26/500 [06:54<2:06:04, 15.96s/it]  5%|▌         | 27/500 [07:10<2:06:27, 16.04s/it]  6%|▌         | 28/500 [07:26<2:05:24, 15.94s/it]  6%|▌         | 29/500 [07:41<2:03:47, 15.77s/it]  6%|▌         | 30/500 [07:57<2:03:22, 15.75s/it]  6%|▌         | 31/500 [08:14<2:05:50, 16.10s/it]  6%|▋         | 32/500 [08:29<2:03:43, 15.86s/it]  7%|▋         | 33/500 [08:44<2:02:25, 15.73s/it]  7%|▋         | 34/500 [09:00<2:01:48, 15.68s/it]  7%|▋         | 35/500 [09:16<2:01:50, 15.72s/it]  7%|▋         | 36/500 [09:32<2:01:38, 15.73s/it]  7%|▋         | 37/500 [09:50<2:08:50, 16.70s/it]  8%|▊         | 38/500 [10:06<2:06:35, 16.44s/it]  8%|▊         | 39/500 [10:22<2:03:56, 16.13s/it]  8%|▊         | 40/500 [10:37<2:02:30, 15.98s/it]  8%|▊         | 41/500 [10:53<2:01:07, 15.83s/it]  8%|▊         | 42/500 [11:08<2:00:08, 15.74s/it]  9%|▊         | 43/500 [11:24<1:59:45, 15.72s/it]  9%|▉         | 44/500 [11:40<1:59:59, 15.79s/it]  9%|▉         | 45/500 [11:56<1:59:50, 15.80s/it]  9%|▉         | 46/500 [12:11<1:58:55, 15.72s/it]  9%|▉         | 47/500 [12:27<1:58:44, 15.73s/it] 10%|▉         | 48/500 [12:43<1:58:08, 15.68s/it] 10%|▉         | 49/500 [12:59<1:58:53, 15.82s/it] 10%|█         | 50/500 [13:15<1:58:44, 15.83s/it] 10%|█         | 51/500 [13:31<1:58:46, 15.87s/it] 10%|█         | 52/500 [13:46<1:58:01, 15.81s/it] 11%|█         | 53/500 [14:02<1:57:50, 15.82s/it] 11%|█         | 54/500 [14:18<1:57:01, 15.74s/it] 11%|█         | 55/500 [14:33<1:56:48, 15.75s/it] 11%|█         | 56/500 [14:49<1:55:40, 15.63s/it] 11%|█▏        | 57/500 [15:04<1:54:26, 15.50s/it] 12%|█▏        | 58/500 [15:20<1:54:40, 15.57s/it] 12%|█▏        | 59/500 [15:36<1:55:56, 15.77s/it] 12%|█▏        | 60/500 [15:52<1:56:11, 15.84s/it] 12%|█▏        | 61/500 [16:08<1:55:37, 15.80s/it] 12%|█▏        | 62/500 [16:24<1:55:56, 15.88s/it] 13%|█▎        | 63/500 [16:39<1:55:05, 15.80s/it] 13%|█▎        | 64/500 [16:55<1:54:47, 15.80s/it] 13%|█▎        | 65/500 [17:11<1:54:59, 15.86s/it] 13%|█▎        | 66/500 [17:29<1:58:49, 16.43s/it] 13%|█▎        | 67/500 [17:45<1:56:42, 16.17s/it] 14%|█▎        | 68/500 [18:01<1:56:17, 16.15s/it] 14%|█▍        | 69/500 [18:16<1:55:07, 16.03s/it] 14%|█▍        | 70/500 [18:34<1:58:47, 16.58s/it] 14%|█▍        | 71/500 [18:50<1:56:43, 16.33s/it] 14%|█▍        | 72/500 [19:06<1:55:10, 16.15s/it] 15%|█▍        | 73/500 [19:21<1:53:17, 15.92s/it] 15%|█▍        | 74/500 [19:37<1:52:14, 15.81s/it] 15%|█▌        | 75/500 [19:52<1:51:53, 15.80s/it] 15%|█▌        | 76/500 [20:08<1:52:01, 15.85s/it] 15%|█▌        | 76/500 [20:08<1:52:24, 15.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.156 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▅▄▅▇▇▆▇▇▇██████████████▇██▇███▇██▇▇██
wandb:     train_loss ▇▇▆▅▆▆▆▁▆▇▆▅▅▁▁▅▄█▁▃▆▄▅▁▂▃▂▃▅▃▄▄▃▄▆▂▃▄▂▂
wandb:   val_accuracy ▁▁▃▅▁▄▇▇▄▇▇▇▇█▇▇▇▇▇█▇▇▇▇▇██▇▇▇▇████▇█▇▇█
wandb:       val_loss ▃▄▄▃▃▄▃▃█▅▄▂█▅▃▃▅▂▆▁▅▂▃▁▄▅▅▂▃▃▅▁▆▄▃▅▂▃▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 75
wandb:  learning_rate 0.0
wandb: train_accuracy 0.81129
wandb:     train_loss 0.19517
wandb:   val_accuracy 0.52222
wandb:       val_loss 0.77087
wandb: 
wandb: 🚀 View run floral-grass-192 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/f757eamh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_123044-f757eamh/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_125140-dlv3j03j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-violet-193
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dlv3j03j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:05, 17.81s/it]  0%|          | 2/500 [00:33<2:16:45, 16.48s/it]  1%|          | 3/500 [00:49<2:14:48, 16.27s/it]  1%|          | 4/500 [01:04<2:12:11, 15.99s/it]  1%|          | 5/500 [01:20<2:10:18, 15.79s/it]  1%|          | 6/500 [01:35<2:08:54, 15.66s/it]  1%|▏         | 7/500 [01:51<2:07:45, 15.55s/it]  2%|▏         | 8/500 [02:06<2:07:07, 15.50s/it]  2%|▏         | 9/500 [02:21<2:06:29, 15.46s/it]  2%|▏         | 10/500 [02:37<2:06:34, 15.50s/it]  2%|▏         | 11/500 [02:52<2:05:33, 15.41s/it]  2%|▏         | 12/500 [03:07<2:04:28, 15.30s/it]  3%|▎         | 13/500 [03:22<2:03:17, 15.19s/it]  3%|▎         | 14/500 [03:40<2:09:27, 15.98s/it]  3%|▎         | 15/500 [03:56<2:08:38, 15.91s/it]  3%|▎         | 16/500 [04:12<2:08:58, 15.99s/it]  3%|▎         | 17/500 [04:28<2:08:13, 15.93s/it]  4%|▎         | 18/500 [04:43<2:07:36, 15.89s/it]  4%|▍         | 19/500 [04:59<2:06:56, 15.84s/it]  4%|▍         | 19/500 [04:59<2:06:29, 15.78s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.025 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▄██▁
wandb:     train_loss ██▆▁▆█▆▅▂▂▇▅▅▆▂▄▄▄▅
wandb:   val_accuracy ▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆█▂▂▁
wandb:       val_loss ▃▃▁█▃▇█▅▆█▄▅▄▂▆▄▃▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 18
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.32392
wandb:     train_loss 1.10405
wandb:   val_accuracy 0.33333
wandb:       val_loss 1.10641
wandb: 
wandb: 🚀 View run proud-violet-193 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dlv3j03j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_125140-dlv3j03j/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_125728-fal8yq2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-glade-194
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fal8yq2c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:41:25, 19.41s/it]  0%|          | 2/500 [00:36<2:31:00, 18.19s/it]  1%|          | 3/500 [00:53<2:23:29, 17.32s/it]  1%|          | 4/500 [01:09<2:21:03, 17.06s/it]  1%|          | 5/500 [01:26<2:18:37, 16.80s/it]  1%|          | 6/500 [01:44<2:21:46, 17.22s/it]  1%|▏         | 7/500 [02:00<2:19:29, 16.98s/it]  2%|▏         | 8/500 [02:17<2:18:34, 16.90s/it]  2%|▏         | 9/500 [02:33<2:17:00, 16.74s/it]  2%|▏         | 10/500 [02:50<2:16:56, 16.77s/it]  2%|▏         | 11/500 [03:07<2:16:54, 16.80s/it]  2%|▏         | 12/500 [03:24<2:18:23, 17.01s/it]  3%|▎         | 13/500 [03:41<2:18:03, 17.01s/it]  3%|▎         | 14/500 [03:58<2:17:16, 16.95s/it]  3%|▎         | 15/500 [04:15<2:17:03, 16.96s/it]  3%|▎         | 16/500 [04:32<2:16:57, 16.98s/it]  3%|▎         | 17/500 [04:49<2:16:01, 16.90s/it]  4%|▎         | 18/500 [05:06<2:15:32, 16.87s/it]  4%|▍         | 19/500 [05:23<2:16:22, 17.01s/it]  4%|▍         | 20/500 [05:40<2:15:06, 16.89s/it]  4%|▍         | 21/500 [05:57<2:15:10, 16.93s/it]  4%|▍         | 22/500 [06:14<2:16:30, 17.14s/it]  5%|▍         | 23/500 [06:32<2:17:22, 17.28s/it]  5%|▍         | 24/500 [06:50<2:19:02, 17.53s/it]  5%|▍         | 24/500 [06:50<2:15:42, 17.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.105 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁
wandb: train_accuracy ▃▁▃▂▄▄▃▃▄▃▅▆▇▆▆▅▄█▇▇████
wandb:     train_loss ▂▂▂▂▂▂▁▁▂▃▆▁▁▁▁▅█▃▁▁▁▁▁▁
wandb:   val_accuracy ▅▁▅▄▅▅▄▅▅▅▆▇█▆▇▆▆▇▇▇▇██▇
wandb:       val_loss ▂▂▂▂▂▁▂▁▂▃▂▁▅▂▂█▃▅▃▆▃▂▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 23
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.91828
wandb:     train_loss 0.02118
wandb:   val_accuracy 0.44444
wandb:       val_loss 6.23189
wandb: 
wandb: 🚀 View run true-glade-194 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/fal8yq2c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_125728-fal8yq2c/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_130504-radkcdck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sponge-195
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/radkcdck
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:39, 18.48s/it]  0%|          | 2/500 [00:33<2:16:49, 16.49s/it]  1%|          | 3/500 [00:48<2:09:39, 15.65s/it]  1%|          | 4/500 [01:02<2:05:36, 15.20s/it]  1%|          | 5/500 [01:17<2:03:32, 14.97s/it]  1%|          | 6/500 [01:32<2:04:01, 15.06s/it]  1%|▏         | 7/500 [01:47<2:03:34, 15.04s/it]  2%|▏         | 8/500 [02:02<2:04:09, 15.14s/it]  2%|▏         | 9/500 [02:18<2:03:59, 15.15s/it]  2%|▏         | 10/500 [02:33<2:03:58, 15.18s/it]  2%|▏         | 11/500 [02:48<2:03:20, 15.13s/it]  2%|▏         | 12/500 [03:03<2:03:19, 15.16s/it]  3%|▎         | 13/500 [03:18<2:03:10, 15.18s/it]  3%|▎         | 14/500 [03:34<2:03:58, 15.31s/it]  3%|▎         | 15/500 [03:51<2:08:26, 15.89s/it]  3%|▎         | 16/500 [04:07<2:07:40, 15.83s/it]  3%|▎         | 17/500 [04:22<2:05:49, 15.63s/it]  4%|▎         | 18/500 [04:37<2:04:28, 15.49s/it]  4%|▍         | 19/500 [04:52<2:03:19, 15.38s/it]  4%|▍         | 20/500 [05:07<2:01:43, 15.22s/it]  4%|▍         | 21/500 [05:22<2:01:45, 15.25s/it]  4%|▍         | 22/500 [05:38<2:01:10, 15.21s/it]  5%|▍         | 23/500 [05:53<2:00:59, 15.22s/it]  5%|▍         | 24/500 [06:08<2:00:10, 15.15s/it]  5%|▌         | 25/500 [06:23<2:00:52, 15.27s/it]  5%|▌         | 26/500 [06:39<2:00:46, 15.29s/it]  5%|▌         | 27/500 [06:54<2:00:31, 15.29s/it]  6%|▌         | 28/500 [07:09<1:59:36, 15.20s/it]  6%|▌         | 29/500 [07:24<1:59:49, 15.26s/it]  6%|▌         | 30/500 [07:39<1:58:50, 15.17s/it]  6%|▌         | 31/500 [07:54<1:56:52, 14.95s/it]  6%|▋         | 32/500 [08:08<1:55:14, 14.78s/it]  7%|▋         | 33/500 [08:23<1:55:18, 14.82s/it]  7%|▋         | 34/500 [08:42<2:05:10, 16.12s/it]  7%|▋         | 35/500 [08:57<2:02:32, 15.81s/it]  7%|▋         | 36/500 [09:12<2:00:39, 15.60s/it]  7%|▋         | 37/500 [09:28<1:59:49, 15.53s/it]  8%|▊         | 38/500 [09:43<1:58:28, 15.39s/it]  8%|▊         | 39/500 [09:58<1:57:21, 15.27s/it]  8%|▊         | 40/500 [10:13<1:56:05, 15.14s/it]  8%|▊         | 41/500 [10:28<1:55:27, 15.09s/it]  8%|▊         | 41/500 [10:28<1:57:11, 15.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.231 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▃▃▃▃▅▆▆▇▇▇▇▆█▇▇▆▇▆▆▅▆▆▆▅▅▅▆▅▅▆▄▆▅▆▅▅▆
wandb:     train_loss ▆▃▅▆▅▆▅▅▅▅▅▄▆▅▅▅▆▄▆▄▂▆▄▃▅▁▅▅▄▄▂▄▅▂▂▄▂▄█▅
wandb:   val_accuracy ▂▁▂▁▁▁▂▃▄▆▄▇▅█▇███▇█▆█▅▇█▇▅▅▆▆▆▅▅▄▅▄▆▅▅▆
wandb:       val_loss ▄▆▄▄▅▄▄▄▄▄▅▂▄▃▄█▃▅▃██▄▄▇▁▅▃▃▃▃▃▅▄▃▂▄▂▃▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 1e-05
wandb: train_accuracy 0.64042
wandb:     train_loss 0.92523
wandb:   val_accuracy 0.48667
wandb:       val_loss 0.69312
wandb: 
wandb: 🚀 View run radiant-sponge-195 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/radkcdck
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_130504-radkcdck/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_131622-pifp1yvh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-capybara-196
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/pifp1yvh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:43:28, 19.66s/it]  0%|          | 2/500 [00:36<2:28:24, 17.88s/it]  1%|          | 3/500 [00:53<2:25:23, 17.55s/it]  1%|          | 4/500 [01:10<2:22:21, 17.22s/it]  1%|          | 5/500 [01:27<2:21:08, 17.11s/it]  1%|          | 6/500 [01:44<2:21:49, 17.23s/it]  1%|▏         | 7/500 [02:01<2:20:15, 17.07s/it]  2%|▏         | 8/500 [02:18<2:21:24, 17.24s/it]  2%|▏         | 9/500 [02:36<2:21:56, 17.35s/it]  2%|▏         | 10/500 [02:53<2:21:29, 17.33s/it]  2%|▏         | 11/500 [03:10<2:20:25, 17.23s/it]  2%|▏         | 12/500 [03:28<2:20:33, 17.28s/it]  3%|▎         | 13/500 [03:45<2:21:16, 17.41s/it]  3%|▎         | 14/500 [04:03<2:21:22, 17.45s/it]  3%|▎         | 15/500 [04:22<2:24:59, 17.94s/it]  3%|▎         | 16/500 [04:41<2:27:16, 18.26s/it]  3%|▎         | 17/500 [04:57<2:20:41, 17.48s/it]  4%|▎         | 18/500 [05:12<2:15:42, 16.89s/it]  4%|▍         | 19/500 [05:29<2:16:00, 16.97s/it]  4%|▍         | 20/500 [05:51<2:27:15, 18.41s/it]  4%|▍         | 21/500 [06:09<2:25:03, 18.17s/it]  4%|▍         | 22/500 [06:26<2:22:52, 17.93s/it]  5%|▍         | 23/500 [06:43<2:21:01, 17.74s/it]  5%|▍         | 24/500 [07:00<2:19:04, 17.53s/it]  5%|▌         | 25/500 [07:17<2:16:34, 17.25s/it]  5%|▌         | 26/500 [07:34<2:16:08, 17.23s/it]  5%|▌         | 27/500 [07:52<2:17:03, 17.39s/it]  6%|▌         | 28/500 [08:09<2:15:03, 17.17s/it]  6%|▌         | 29/500 [08:26<2:14:52, 17.18s/it]  6%|▌         | 30/500 [08:43<2:15:41, 17.32s/it]  6%|▌         | 31/500 [09:01<2:14:47, 17.24s/it]  6%|▋         | 32/500 [09:22<2:24:03, 18.47s/it]  7%|▋         | 33/500 [09:41<2:24:49, 18.61s/it]  7%|▋         | 34/500 [09:58<2:21:52, 18.27s/it]  7%|▋         | 35/500 [10:16<2:20:03, 18.07s/it]  7%|▋         | 36/500 [10:33<2:18:39, 17.93s/it]  7%|▋         | 37/500 [10:50<2:15:59, 17.62s/it]  7%|▋         | 37/500 [10:50<2:15:44, 17.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.019 MB of 0.313 MB uploadedwandb: - 0.019 MB of 0.313 MB uploadedwandb: \ 0.024 MB of 0.313 MB uploadedwandb: | 0.026 MB of 0.313 MB uploadedwandb: / 0.026 MB of 0.313 MB uploadedwandb: - 0.026 MB of 0.313 MB uploadedwandb: \ 0.026 MB of 0.313 MB uploadedwandb: | 0.026 MB of 0.313 MB uploadedwandb: / 0.026 MB of 0.313 MB uploadedwandb: - 0.026 MB of 0.313 MB uploadedwandb: \ 0.026 MB of 0.313 MB uploadedwandb: | 0.028 MB of 0.313 MB uploadedwandb: / 0.028 MB of 0.313 MB uploadedwandb: - 0.294 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▃▁▂▃▁█▂▃▃▁▄█▇▃▂▄▃▇▆▇
wandb:     train_loss ██▆▁▆▇▆▅▂▂▇▅▃▇▄▃▆▃▅▅▅▅▄▅▅▄▆▄▅▄▅▃▅▅▂▄▅
wandb:   val_accuracy ▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄▄▁▄▄█▄▄▄▄▄▆█▄▅▄▅▆▃▆
wandb:       val_loss ▃▃▁▇▃▇█▄▆▇▃▅▁▂▄▃▃▃▃▃▁▄▄▃▄▄▁▄▂▂▅▂▅▃▅▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.0
wandb: train_accuracy 0.50817
wandb:     train_loss 1.10996
wandb:   val_accuracy 0.35556
wandb:       val_loss 1.0663
wandb: 
wandb: 🚀 View run polished-capybara-196 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/pifp1yvh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_131622-pifp1yvh/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_132829-mec0zwbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-water-197
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mec0zwbc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:53, 25.96s/it]  0%|          | 2/500 [00:51<3:31:05, 25.43s/it]  1%|          | 3/500 [01:15<3:28:56, 25.22s/it]  1%|          | 4/500 [01:41<3:28:41, 25.24s/it]  1%|          | 5/500 [02:09<3:36:30, 26.24s/it]  1%|          | 6/500 [02:34<3:33:22, 25.91s/it]  1%|▏         | 7/500 [03:07<3:51:06, 28.13s/it]  2%|▏         | 8/500 [03:32<3:42:42, 27.16s/it]  2%|▏         | 9/500 [04:05<3:58:40, 29.17s/it]  2%|▏         | 10/500 [04:30<3:47:11, 27.82s/it]  2%|▏         | 11/500 [05:04<4:00:58, 29.57s/it]  2%|▏         | 12/500 [05:31<3:55:18, 28.93s/it]  3%|▎         | 13/500 [06:00<3:53:26, 28.76s/it]  3%|▎         | 14/500 [06:24<3:42:39, 27.49s/it]  3%|▎         | 15/500 [06:53<3:45:10, 27.86s/it]  3%|▎         | 16/500 [07:22<3:47:49, 28.24s/it]  3%|▎         | 17/500 [07:47<3:39:00, 27.21s/it]  4%|▎         | 18/500 [08:21<3:54:43, 29.22s/it]  4%|▍         | 19/500 [08:48<3:49:03, 28.57s/it]  4%|▍         | 20/500 [09:13<3:41:45, 27.72s/it]  4%|▍         | 21/500 [09:43<3:44:23, 28.11s/it]  4%|▍         | 22/500 [10:08<3:36:57, 27.23s/it]  5%|▍         | 23/500 [10:41<3:49:51, 28.91s/it]  5%|▍         | 24/500 [11:08<3:46:48, 28.59s/it]  5%|▌         | 25/500 [11:36<3:44:48, 28.40s/it]  5%|▌         | 26/500 [12:01<3:35:35, 27.29s/it]  5%|▌         | 27/500 [12:35<3:51:35, 29.38s/it]  6%|▌         | 28/500 [13:03<3:46:56, 28.85s/it]  6%|▌         | 28/500 [13:03<3:40:05, 27.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.019 MB of 0.313 MB uploadedwandb: - 0.231 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▃▂▃▅▂▁▆▆▅▆▅▆▄▇█▆▇█▇█▇█▇██
wandb:     train_loss ▂▂▁▁█▁▂▁▇▁▁▁▁▆▅▃▁▁▂▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▁▃▅▃▃▆▃▃▄▇▆▇▅▆▅▇█▆█▆▇██▇▆▆▆
wandb:       val_loss ▃▂▃▄▃▃▄▇▁▁▂▁▆▆▂▅▇▅▁▁▃▅▅▁▂▄▆█
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00025
wandb: train_accuracy 0.99703
wandb:     train_loss 0.00361
wandb:   val_accuracy 0.51556
wandb:       val_loss 4.63909
wandb: 
wandb: 🚀 View run deft-water-197 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/mec0zwbc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_132829-mec0zwbc/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_134218-nrry313l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-firebrand-198
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nrry313l
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:46:29, 27.23s/it]  0%|          | 2/500 [00:51<3:31:53, 25.53s/it]  1%|          | 3/500 [01:16<3:30:32, 25.42s/it]  1%|          | 4/500 [01:41<3:27:03, 25.05s/it]  1%|          | 5/500 [02:08<3:34:06, 25.95s/it]  1%|          | 6/500 [02:38<3:43:15, 27.12s/it]  1%|▏         | 7/500 [03:06<3:45:00, 27.38s/it]  2%|▏         | 8/500 [03:33<3:45:13, 27.47s/it]  2%|▏         | 9/500 [03:59<3:40:07, 26.90s/it]  2%|▏         | 10/500 [04:28<3:44:01, 27.43s/it]  2%|▏         | 11/500 [04:57<3:48:06, 27.99s/it]  2%|▏         | 12/500 [05:23<3:42:58, 27.41s/it]  3%|▎         | 13/500 [05:49<3:38:58, 26.98s/it]  3%|▎         | 14/500 [06:18<3:42:32, 27.47s/it]  3%|▎         | 15/500 [06:48<3:48:04, 28.21s/it]  3%|▎         | 16/500 [07:13<3:40:03, 27.28s/it]  3%|▎         | 17/500 [07:46<3:54:16, 29.10s/it]  4%|▎         | 18/500 [08:13<3:48:30, 28.45s/it]  4%|▍         | 19/500 [08:41<3:48:25, 28.49s/it]  4%|▍         | 20/500 [09:12<3:51:52, 28.98s/it]  4%|▍         | 21/500 [09:44<3:59:01, 29.94s/it]  4%|▍         | 22/500 [10:09<3:47:31, 28.56s/it]  5%|▍         | 23/500 [10:41<3:55:16, 29.59s/it]  5%|▍         | 24/500 [11:09<3:51:22, 29.17s/it]  5%|▌         | 25/500 [11:36<3:45:18, 28.46s/it]  5%|▌         | 26/500 [12:07<3:49:50, 29.09s/it]  5%|▌         | 27/500 [12:32<3:40:45, 28.00s/it]  6%|▌         | 28/500 [13:00<3:39:45, 27.94s/it]  6%|▌         | 29/500 [13:29<3:40:53, 28.14s/it]  6%|▌         | 30/500 [13:55<3:37:35, 27.78s/it]  6%|▌         | 31/500 [14:21<3:30:43, 26.96s/it]  6%|▋         | 32/500 [14:48<3:30:31, 26.99s/it]  7%|▋         | 33/500 [15:19<3:39:16, 28.17s/it]  7%|▋         | 34/500 [15:46<3:36:19, 27.85s/it]  7%|▋         | 35/500 [16:17<3:44:19, 28.95s/it]  7%|▋         | 36/500 [16:45<3:41:42, 28.67s/it]  7%|▋         | 37/500 [17:14<3:41:18, 28.68s/it]  8%|▊         | 38/500 [17:45<3:47:28, 29.54s/it]  8%|▊         | 39/500 [18:14<3:44:35, 29.23s/it]  8%|▊         | 40/500 [18:42<3:40:35, 28.77s/it]  8%|▊         | 41/500 [19:10<3:40:25, 28.81s/it]  8%|▊         | 42/500 [19:38<3:37:46, 28.53s/it]  9%|▊         | 43/500 [20:05<3:33:30, 28.03s/it]  9%|▉         | 44/500 [20:32<3:29:46, 27.60s/it]  9%|▉         | 45/500 [20:56<3:22:28, 26.70s/it]  9%|▉         | 46/500 [21:25<3:25:11, 27.12s/it]  9%|▉         | 47/500 [21:55<3:33:14, 28.24s/it] 10%|▉         | 48/500 [22:20<3:23:54, 27.07s/it] 10%|▉         | 49/500 [22:48<3:26:41, 27.50s/it] 10%|█         | 50/500 [23:14<3:22:43, 27.03s/it] 10%|█         | 51/500 [23:46<3:32:03, 28.34s/it] 10%|█         | 52/500 [24:11<3:24:03, 27.33s/it] 11%|█         | 53/500 [24:42<3:32:31, 28.53s/it] 11%|█         | 54/500 [25:11<3:33:44, 28.75s/it] 11%|█         | 55/500 [25:35<3:23:24, 27.43s/it] 11%|█         | 56/500 [26:05<3:27:50, 28.09s/it] 11%|█▏        | 57/500 [26:30<3:20:11, 27.11s/it] 11%|█▏        | 57/500 [26:30<3:26:00, 27.90s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.233 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▄▄▃▆▆▇▇▅▇▇▇▇▇▇▇▇█▇▇█▇█▇▇█████▇████▇▇
wandb:     train_loss ▅▆▆▄▆▂▆▅▁▂▆▅▄▃▅▄▅▄▄▄▅▁▁▁▄▄▄▄▂█▂▄▄▅▃▃▂▂▄▂
wandb:   val_accuracy ▁▁▃▃▄▄▄▄▅▄▅▆▅▆▆▆▄▆▅▆▅▇▆▆▅▅█▇▅▅▅▅▆█▅▆▅▅▄▇
wandb:       val_loss ▄▄▄▄▄▅▄▃▄▅▄▅▄▂▄▂▅▂▄▄█▃▄▅▂▃▃▆▁▆▁▃▃▃▃▂▄▄▃▆
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.0
wandb: train_accuracy 0.789
wandb:     train_loss 0.15936
wandb:   val_accuracy 0.58444
wandb:       val_loss 1.57043
wandb: 
wandb: 🚀 View run fresh-firebrand-198 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/nrry313l
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_134218-nrry313l/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_140937-dqgioxdb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-bee-199
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dqgioxdb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:48:00, 27.42s/it]  0%|          | 2/500 [00:51<3:32:26, 25.60s/it]  1%|          | 3/500 [01:16<3:27:04, 25.00s/it]  1%|          | 4/500 [01:40<3:23:51, 24.66s/it]  1%|          | 5/500 [02:11<3:43:41, 27.11s/it]  1%|          | 6/500 [02:38<3:42:48, 27.06s/it]  1%|▏         | 7/500 [03:05<3:40:42, 26.86s/it]  2%|▏         | 8/500 [03:30<3:37:25, 26.51s/it]  2%|▏         | 9/500 [04:00<3:44:42, 27.46s/it]  2%|▏         | 10/500 [04:26<3:40:19, 26.98s/it]  2%|▏         | 11/500 [04:56<3:47:28, 27.91s/it]  2%|▏         | 12/500 [05:25<3:50:40, 28.36s/it]  3%|▎         | 13/500 [05:53<3:49:02, 28.22s/it]  3%|▎         | 14/500 [06:21<3:47:46, 28.12s/it]  3%|▎         | 15/500 [06:52<3:55:05, 29.08s/it]  3%|▎         | 16/500 [07:17<3:44:51, 27.88s/it]  3%|▎         | 17/500 [07:47<3:48:17, 28.36s/it]  4%|▎         | 18/500 [08:15<3:47:21, 28.30s/it]  4%|▍         | 19/500 [08:42<3:44:37, 28.02s/it]  4%|▍         | 20/500 [09:08<3:38:29, 27.31s/it]  4%|▍         | 21/500 [09:36<3:39:23, 27.48s/it]  4%|▍         | 22/500 [10:02<3:35:47, 27.09s/it]  5%|▍         | 23/500 [10:28<3:32:54, 26.78s/it]  5%|▍         | 24/500 [10:53<3:28:13, 26.25s/it]  5%|▌         | 25/500 [11:23<3:35:57, 27.28s/it]  5%|▌         | 25/500 [11:23<3:36:23, 27.33s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.168 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▆▆▅▇▇▇▇▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▃▅▆▅▅▅
wandb:   val_accuracy ▅▅▅▅▅▅▅▅▅▄▆▆▃▅██▆█▇▅▇▇▄▅▁
wandb:       val_loss ▃█▂▇▂▂▂▅▁▅▂▂▅▄▂▃▅▃▃▃▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.50223
wandb:     train_loss 1.12275
wandb:   val_accuracy 0.32667
wandb:       val_loss 1.1044
wandb: 
wandb: 🚀 View run elated-bee-199 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/dqgioxdb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_140937-dqgioxdb/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_142145-q5wt7qaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-breeze-200
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/q5wt7qaj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:32:14, 25.52s/it]  0%|          | 2/500 [00:49<3:25:33, 24.77s/it]  1%|          | 3/500 [01:14<3:24:54, 24.74s/it]  1%|          | 4/500 [01:40<3:27:37, 25.12s/it]  1%|          | 5/500 [02:07<3:34:43, 26.03s/it]  1%|          | 6/500 [02:34<3:37:28, 26.41s/it]  1%|▏         | 7/500 [03:06<3:49:57, 27.99s/it]  2%|▏         | 8/500 [03:34<3:50:27, 28.11s/it]  2%|▏         | 9/500 [04:05<3:58:21, 29.13s/it]  2%|▏         | 10/500 [04:31<3:49:53, 28.15s/it]  2%|▏         | 11/500 [05:00<3:49:56, 28.21s/it]  2%|▏         | 12/500 [05:32<3:59:14, 29.41s/it]  3%|▎         | 13/500 [05:58<3:49:58, 28.33s/it]  3%|▎         | 14/500 [06:26<3:48:05, 28.16s/it]  3%|▎         | 15/500 [06:54<3:49:26, 28.38s/it]  3%|▎         | 16/500 [07:23<3:49:01, 28.39s/it]  3%|▎         | 17/500 [07:50<3:44:24, 27.88s/it]  4%|▎         | 18/500 [08:18<3:45:07, 28.02s/it]  4%|▍         | 19/500 [08:50<3:55:03, 29.32s/it]  4%|▍         | 20/500 [09:16<3:46:46, 28.35s/it]  4%|▍         | 21/500 [09:40<3:35:46, 27.03s/it]  4%|▍         | 22/500 [10:15<3:54:10, 29.39s/it]  5%|▍         | 23/500 [10:50<4:06:36, 31.02s/it]  5%|▍         | 24/500 [11:16<3:54:04, 29.51s/it]  5%|▌         | 25/500 [11:50<4:03:20, 30.74s/it]  5%|▌         | 26/500 [12:17<3:54:00, 29.62s/it]  5%|▌         | 27/500 [12:42<3:43:19, 28.33s/it]  6%|▌         | 28/500 [13:10<3:42:49, 28.33s/it]  6%|▌         | 29/500 [13:39<3:43:13, 28.44s/it]  6%|▌         | 30/500 [14:09<3:46:14, 28.88s/it]  6%|▌         | 31/500 [14:34<3:36:57, 27.76s/it]  6%|▋         | 32/500 [15:08<3:50:57, 29.61s/it]  7%|▋         | 33/500 [15:37<3:49:36, 29.50s/it]  7%|▋         | 34/500 [16:06<3:46:46, 29.20s/it]  7%|▋         | 35/500 [16:34<3:43:54, 28.89s/it]  7%|▋         | 36/500 [17:00<3:36:28, 27.99s/it]  7%|▋         | 37/500 [17:29<3:38:41, 28.34s/it]  8%|▊         | 38/500 [17:54<3:31:50, 27.51s/it]  8%|▊         | 39/500 [18:22<3:30:40, 27.42s/it]  8%|▊         | 40/500 [18:48<3:27:31, 27.07s/it]  8%|▊         | 40/500 [18:48<3:36:18, 28.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▂▂▇▇██▆███▇▆█▇███████████████████████
wandb:     train_loss █▄▅█▁▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▄▂▁▅▄▆▅▄▆▆▆▄▃▇▃▇██▇▆██▇█▅██▇▅▆▇▆███▆▇▇
wandb:       val_loss ▂▂▂▂▂▂▁▃▂▃▁▁▅▆▁▃▆▅▁▁▅▃▄▁▃▄▆▅▇▄▄▂▁▃▃▁▃▅▆█
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.99851
wandb:     train_loss 0.0
wandb:   val_accuracy 0.50444
wandb:       val_loss 8.0933
wandb: 
wandb: 🚀 View run charmed-breeze-200 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/q5wt7qaj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_142145-q5wt7qaj/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_144116-kus1h402
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-resonance-201
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kus1h402
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:24, 26.38s/it]  0%|          | 2/500 [00:51<3:35:13, 25.93s/it]  1%|          | 3/500 [01:16<3:29:56, 25.34s/it]  1%|          | 4/500 [01:42<3:29:40, 25.36s/it]  1%|          | 5/500 [02:09<3:35:07, 26.08s/it]  1%|          | 6/500 [02:37<3:41:08, 26.86s/it]  1%|▏         | 7/500 [03:06<3:46:48, 27.60s/it]  2%|▏         | 8/500 [03:34<3:46:25, 27.61s/it]  2%|▏         | 9/500 [04:02<3:47:08, 27.76s/it]  2%|▏         | 10/500 [04:29<3:44:00, 27.43s/it]  2%|▏         | 11/500 [04:55<3:41:37, 27.19s/it]  2%|▏         | 12/500 [05:23<3:42:34, 27.37s/it]  3%|▎         | 13/500 [05:49<3:37:52, 26.84s/it]  3%|▎         | 14/500 [06:19<3:46:29, 27.96s/it]  3%|▎         | 15/500 [06:48<3:47:15, 28.11s/it]  3%|▎         | 16/500 [07:19<3:53:15, 28.92s/it]  3%|▎         | 17/500 [07:46<3:47:55, 28.31s/it]  4%|▎         | 18/500 [08:15<3:51:22, 28.80s/it]  4%|▍         | 19/500 [08:40<3:39:56, 27.44s/it]  4%|▍         | 20/500 [09:12<3:50:33, 28.82s/it]  4%|▍         | 21/500 [09:39<3:46:48, 28.41s/it]  4%|▍         | 22/500 [10:07<3:44:17, 28.15s/it]  5%|▍         | 23/500 [10:36<3:46:57, 28.55s/it]  5%|▍         | 24/500 [11:06<3:48:37, 28.82s/it]  5%|▌         | 25/500 [11:35<3:49:07, 28.94s/it]  5%|▌         | 26/500 [12:04<3:48:15, 28.89s/it]  5%|▌         | 27/500 [12:29<3:39:12, 27.81s/it]  6%|▌         | 28/500 [13:00<3:46:49, 28.83s/it]  6%|▌         | 29/500 [13:28<3:43:54, 28.52s/it]  6%|▌         | 30/500 [13:57<3:43:20, 28.51s/it]  6%|▌         | 31/500 [14:24<3:40:14, 28.18s/it]  6%|▋         | 32/500 [14:52<3:38:35, 28.02s/it]  7%|▋         | 33/500 [15:18<3:34:53, 27.61s/it]  7%|▋         | 34/500 [15:48<3:40:29, 28.39s/it]  7%|▋         | 35/500 [16:16<3:38:31, 28.20s/it]  7%|▋         | 36/500 [16:45<3:38:47, 28.29s/it]  7%|▋         | 37/500 [17:10<3:30:57, 27.34s/it]  8%|▊         | 38/500 [17:36<3:26:49, 26.86s/it]  8%|▊         | 39/500 [18:09<3:42:40, 28.98s/it]  8%|▊         | 40/500 [18:35<3:33:51, 27.89s/it]  8%|▊         | 41/500 [19:03<3:34:45, 28.07s/it]  8%|▊         | 42/500 [19:31<3:34:13, 28.07s/it]  9%|▊         | 43/500 [20:03<3:40:57, 29.01s/it]  9%|▉         | 44/500 [20:28<3:33:16, 28.06s/it]  9%|▉         | 45/500 [20:55<3:29:15, 27.59s/it]  9%|▉         | 46/500 [21:22<3:28:41, 27.58s/it]  9%|▉         | 47/500 [21:54<3:36:10, 28.63s/it] 10%|▉         | 48/500 [22:19<3:27:25, 27.53s/it] 10%|▉         | 49/500 [22:46<3:26:18, 27.45s/it] 10%|█         | 50/500 [23:10<3:18:44, 26.50s/it] 10%|█         | 51/500 [23:40<3:25:07, 27.41s/it] 10%|█         | 51/500 [23:40<3:28:23, 27.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.107 MB of 0.315 MB uploadedwandb: \ 0.107 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▃▃▃▄▄▅▆▇▇▇▆▇██▇█▇█▇▆█▆█▂▆▇▇███████▇██
wandb:     train_loss ▅▅█▇▅▆▅▅▅▃▄▆▃▃▆▃▃▅▄▄▅▅▅▃▂▄▁▂▅▄▂▃▄▃▅▇▄▄▄▃
wandb:   val_accuracy ▂▁▁▁▁▁▂▃▂▄▄▅▄▂▅▅▄▇▆█▅▂▇▅▂▇▁█▅▄▄▄▅▇▇▅▅▂▆▆
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁█▁▂▂▂▂▂▂▁▁▂▁▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 0.0
wandb: train_accuracy 0.82021
wandb:     train_loss 0.40162
wandb:   val_accuracy 0.50444
wandb:       val_loss 0.83364
wandb: 
wandb: 🚀 View run clear-resonance-201 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/kus1h402
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_144116-kus1h402/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_150539-zlscscn5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-salad-202
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zlscscn5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:07, 25.39s/it]  0%|          | 2/500 [00:50<3:28:00, 25.06s/it]  1%|          | 3/500 [01:14<3:24:39, 24.71s/it]  1%|          | 4/500 [01:41<3:31:34, 25.59s/it]  1%|          | 5/500 [02:10<3:40:47, 26.76s/it]  1%|          | 6/500 [02:37<3:42:13, 26.99s/it]  1%|▏         | 7/500 [03:06<3:45:33, 27.45s/it]  2%|▏         | 8/500 [03:34<3:47:08, 27.70s/it]  2%|▏         | 9/500 [04:01<3:45:35, 27.57s/it]  2%|▏         | 10/500 [04:31<3:51:50, 28.39s/it]  2%|▏         | 11/500 [04:55<3:39:41, 26.96s/it]  2%|▏         | 12/500 [05:29<3:55:40, 28.98s/it]  3%|▎         | 13/500 [05:54<3:45:21, 27.76s/it]  3%|▎         | 14/500 [06:19<3:39:35, 27.11s/it]  3%|▎         | 15/500 [06:47<3:41:54, 27.45s/it]  3%|▎         | 16/500 [07:13<3:36:59, 26.90s/it]  3%|▎         | 17/500 [07:47<3:52:23, 28.87s/it]  4%|▎         | 18/500 [08:11<3:40:06, 27.40s/it]  4%|▍         | 19/500 [08:40<3:44:00, 27.94s/it]  4%|▍         | 20/500 [09:06<3:39:43, 27.46s/it]  4%|▍         | 21/500 [09:35<3:42:05, 27.82s/it]  4%|▍         | 22/500 [10:08<3:54:16, 29.41s/it]  5%|▍         | 23/500 [10:33<3:44:32, 28.24s/it]  5%|▍         | 24/500 [11:05<3:51:53, 29.23s/it]  5%|▌         | 25/500 [11:30<3:41:04, 27.93s/it]  5%|▌         | 25/500 [11:30<3:38:35, 27.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.230 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▆▅▅▇▇▇█▇███
wandb:     train_loss ▁▇▁▁█▂▆▇▅▆▆▆▅▅▅▅▅▅▅▄▅▆▅▅▅
wandb:   val_accuracy ▄▄▄▄▄▄▄▄▄▃▅▆▃▅█▅▄▇▆▆█▅▅▄▁
wandb:       val_loss ▃█▂▇▂▂▂▅▁▅▂▂▅▅▂▃▅▃▃▄▃▂▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.0
wandb: train_accuracy 0.49629
wandb:     train_loss 1.12473
wandb:   val_accuracy 0.32889
wandb:       val_loss 1.10767
wandb: 
wandb: 🚀 View run solar-salad-202 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/zlscscn5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_150539-zlscscn5/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_151750-ra2bzymj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-valley-203
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ra2bzymj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:16:36, 30.85s/it]  0%|          | 2/500 [00:55<3:46:30, 27.29s/it]  1%|          | 3/500 [01:25<3:56:30, 28.55s/it]  1%|          | 4/500 [01:50<3:45:10, 27.24s/it]  1%|          | 5/500 [02:18<3:45:52, 27.38s/it]  1%|          | 6/500 [02:44<3:41:12, 26.87s/it]  1%|▏         | 7/500 [03:17<3:57:50, 28.95s/it]  2%|▏         | 8/500 [03:42<3:45:52, 27.54s/it]  2%|▏         | 9/500 [04:12<3:52:57, 28.47s/it]  2%|▏         | 10/500 [04:41<3:52:37, 28.48s/it]  2%|▏         | 11/500 [05:10<3:54:50, 28.81s/it]  2%|▏         | 12/500 [05:48<4:16:57, 31.59s/it]  3%|▎         | 13/500 [06:16<4:07:12, 30.46s/it]  3%|▎         | 14/500 [06:49<4:13:11, 31.26s/it]  3%|▎         | 15/500 [07:22<4:15:48, 31.65s/it]  3%|▎         | 16/500 [07:52<4:11:18, 31.15s/it]  3%|▎         | 17/500 [08:18<3:59:07, 29.70s/it]  4%|▎         | 18/500 [08:46<3:54:37, 29.21s/it]  4%|▍         | 19/500 [09:14<3:50:43, 28.78s/it]  4%|▍         | 20/500 [09:45<3:55:02, 29.38s/it]  4%|▍         | 21/500 [10:19<4:05:32, 30.76s/it]  4%|▍         | 22/500 [10:48<4:01:57, 30.37s/it]  5%|▍         | 23/500 [11:18<3:59:30, 30.13s/it]  5%|▍         | 24/500 [11:49<4:00:40, 30.34s/it]  5%|▌         | 25/500 [12:22<4:06:42, 31.16s/it]  5%|▌         | 26/500 [12:49<3:56:21, 29.92s/it]  5%|▌         | 27/500 [13:26<4:12:32, 32.03s/it]  6%|▌         | 28/500 [13:53<4:00:32, 30.58s/it]  6%|▌         | 29/500 [14:21<3:53:39, 29.77s/it]  6%|▌         | 30/500 [14:49<3:49:56, 29.35s/it]  6%|▌         | 31/500 [15:20<3:52:08, 29.70s/it]  6%|▋         | 32/500 [15:52<3:57:55, 30.50s/it]  7%|▋         | 33/500 [16:17<3:44:32, 28.85s/it]  7%|▋         | 34/500 [16:47<3:46:58, 29.22s/it]  7%|▋         | 35/500 [17:14<3:40:59, 28.51s/it]  7%|▋         | 36/500 [17:40<3:34:20, 27.72s/it]  7%|▋         | 37/500 [18:09<3:37:25, 28.18s/it]  8%|▊         | 38/500 [18:35<3:31:32, 27.47s/it]  8%|▊         | 39/500 [19:03<3:32:49, 27.70s/it]  8%|▊         | 40/500 [19:33<3:38:22, 28.48s/it]  8%|▊         | 40/500 [19:34<3:45:01, 29.35s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.028 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▁▄▃▅▅▆▅▇█▇▇▇▆▇███████████████████████
wandb:     train_loss ▅▃▅█▅▃▁▄▁█▄▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▁▁▅▄▆▇▆▅██▆▆▅▄▆▇▇▇▇▇▇█▆█▆▇█▇▇▆▆▆▇██▆▆▇
wandb:       val_loss ▂▂▂▃▁▂▁▄▁▂▁▁▅▅▁▃█▅▁▁▄▄▄▁▄▃▃▄▇▄▄▃▁▅▃▁▃▃▅▇
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 6e-05
wandb: train_accuracy 1.0
wandb:     train_loss 0.00148
wandb:   val_accuracy 0.51556
wandb:       val_loss 7.85573
wandb: 
wandb: 🚀 View run elated-valley-203 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ra2bzymj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_151750-ra2bzymj/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_153807-ga94n245
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-wildflower-204
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ga94n245
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:09, 25.27s/it]  0%|          | 2/500 [00:49<3:24:11, 24.60s/it]  1%|          | 3/500 [01:13<3:21:51, 24.37s/it]  1%|          | 4/500 [01:41<3:34:43, 25.98s/it]  1%|          | 5/500 [02:07<3:31:48, 25.67s/it]  1%|          | 6/500 [02:34<3:36:17, 26.27s/it]  1%|▏         | 7/500 [03:02<3:39:10, 26.68s/it]  2%|▏         | 8/500 [03:25<3:31:04, 25.74s/it]  2%|▏         | 9/500 [03:56<3:43:26, 27.30s/it]  2%|▏         | 10/500 [04:23<3:42:17, 27.22s/it]  2%|▏         | 11/500 [04:51<3:44:56, 27.60s/it]  2%|▏         | 12/500 [05:22<3:52:30, 28.59s/it]  3%|▎         | 13/500 [05:50<3:50:46, 28.43s/it]  3%|▎         | 14/500 [06:19<3:49:54, 28.38s/it]  3%|▎         | 15/500 [06:44<3:42:40, 27.55s/it]  3%|▎         | 16/500 [07:15<3:49:19, 28.43s/it]  3%|▎         | 17/500 [07:40<3:40:49, 27.43s/it]  4%|▎         | 18/500 [08:11<3:48:24, 28.43s/it]  4%|▎         | 18/500 [08:11<3:39:16, 27.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.021 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▃▄▄▅▄▄▇▇▇▅▇▅▅█
wandb:     train_loss ▅▄▆▆▄▅▆▂▅█▄▁▁▅▆▅▆▄
wandb:   val_accuracy ▂▁▂▄▅▅▅▆▅▆▇▇▆█▇█▆█
wandb:       val_loss ▄▁▄▃▃▃▃▄▃▂▂▁▅▁▂▅█▂
wandb: 
wandb: Run summary:
wandb:          epoch 17
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.80238
wandb:     train_loss 0.60779
wandb:   val_accuracy 0.52444
wandb:       val_loss 0.9617
wandb: 
wandb: 🚀 View run desert-wildflower-204 at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/ga94n245
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_153807-ga94n245/logs
Successfully processed 3_20140611
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240918_154700-p50nqn86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-star-205
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0916-seed-gridsearch/runs/p50nqn86
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:27:15, 24.92s/it]  0%|          | 2/500 [00:48<3:19:44, 24.06s/it]  1%|          | 3/500 [01:12<3:19:34, 24.09s/it]  1%|          | 4/500 [01:40<3:30:16, 25.44s/it]  1%|          | 5/500 [02:04<3:25:50, 24.95s/it]  1%|          | 6/500 [02:33<3:38:15, 26.51s/it]  1%|▏         | 7/500 [03:03<3:47:25, 27.68s/it]  2%|▏         | 8/500 [03:31<3:47:19, 27.72s/it]  2%|▏         | 9/500 [03:58<3:45:47, 27.59s/it]  2%|▏         | 10/500 [04:22<3:36:35, 26.52s/it]  2%|▏         | 11/500 [04:52<3:44:24, 27.54s/it]  2%|▏         | 12/500 [05:26<3:59:27, 29.44s/it]