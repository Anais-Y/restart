nohup: 忽略输入
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_165000-1mkddss6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-jazz-2540
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1mkddss6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:51, 17.42s/it]  0%|          | 2/500 [00:31<2:08:03, 15.43s/it]  1%|          | 3/500 [00:44<1:58:08, 14.26s/it]  1%|          | 4/500 [00:57<1:53:50, 13.77s/it]  1%|          | 5/500 [01:10<1:52:56, 13.69s/it]  1%|          | 6/500 [01:23<1:49:21, 13.28s/it]  1%|▏         | 7/500 [01:36<1:48:29, 13.20s/it]  2%|▏         | 8/500 [01:49<1:48:32, 13.24s/it]  2%|▏         | 9/500 [02:02<1:47:32, 13.14s/it]  2%|▏         | 10/500 [02:15<1:45:58, 12.98s/it]  2%|▏         | 11/500 [02:27<1:44:41, 12.85s/it]  2%|▏         | 12/500 [02:41<1:46:20, 13.07s/it]  3%|▎         | 13/500 [02:53<1:44:47, 12.91s/it]  3%|▎         | 14/500 [03:06<1:44:21, 12.88s/it]  3%|▎         | 15/500 [03:19<1:42:47, 12.72s/it]  3%|▎         | 16/500 [03:32<1:43:20, 12.81s/it]  3%|▎         | 17/500 [03:44<1:42:37, 12.75s/it]  4%|▎         | 18/500 [03:56<1:41:01, 12.58s/it]  4%|▍         | 19/500 [04:09<1:40:24, 12.52s/it]  4%|▍         | 20/500 [04:21<1:39:52, 12.48s/it]  4%|▍         | 21/500 [04:34<1:40:05, 12.54s/it]  4%|▍         | 22/500 [04:47<1:42:11, 12.83s/it]  5%|▍         | 23/500 [05:00<1:42:14, 12.86s/it]  5%|▍         | 24/500 [05:14<1:42:54, 12.97s/it]  5%|▌         | 25/500 [05:27<1:43:42, 13.10s/it]  5%|▌         | 26/500 [05:41<1:44:58, 13.29s/it]  5%|▌         | 27/500 [05:54<1:44:11, 13.22s/it]  6%|▌         | 28/500 [06:07<1:43:25, 13.15s/it]  6%|▌         | 29/500 [06:20<1:44:27, 13.31s/it]  6%|▌         | 30/500 [06:36<1:48:58, 13.91s/it]  6%|▌         | 31/500 [06:49<1:47:05, 13.70s/it]  6%|▋         | 32/500 [07:03<1:46:47, 13.69s/it]  7%|▋         | 33/500 [07:17<1:47:13, 13.78s/it]  7%|▋         | 34/500 [07:29<1:43:32, 13.33s/it]  7%|▋         | 35/500 [07:42<1:42:49, 13.27s/it]  7%|▋         | 36/500 [07:54<1:40:04, 12.94s/it]  7%|▋         | 37/500 [08:06<1:37:54, 12.69s/it]  8%|▊         | 38/500 [08:18<1:36:19, 12.51s/it]  8%|▊         | 39/500 [08:30<1:34:54, 12.35s/it]  8%|▊         | 40/500 [08:43<1:34:59, 12.39s/it]  8%|▊         | 41/500 [08:54<1:31:07, 11.91s/it]  8%|▊         | 42/500 [09:05<1:30:30, 11.86s/it]  9%|▊         | 43/500 [09:18<1:31:40, 12.04s/it]  9%|▉         | 44/500 [09:30<1:30:48, 11.95s/it]  9%|▉         | 45/500 [09:42<1:30:48, 11.97s/it]  9%|▉         | 46/500 [09:54<1:31:59, 12.16s/it]  9%|▉         | 47/500 [10:06<1:30:51, 12.03s/it] 10%|▉         | 48/500 [10:17<1:29:28, 11.88s/it] 10%|▉         | 48/500 [10:17<1:36:59, 12.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.020 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▃▄▅▄▆▄█▇▇███▇███████▇█▇██████████████
wandb:     train_loss ▂▃▄▃▁▂▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:   val_accuracy ▁▂▃▂▃▅▄▇▄▆▇▆▆▇▇▇▇▇█▇▇█▇▇▇▇█████▇▇▇▇▇▇▇▇▆
wandb:       val_loss ▂▂▁▁▃▃█▃▁▁▁▁▂▄▇▁▂▁▁▅▁▄▁▁▁▁▁▁▄▅▁▁▁▁▁▃▃▃▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.97028
wandb:     train_loss 0.01209
wandb:   val_accuracy 0.69778
wandb:       val_loss 0.34011
wandb: 
wandb: 🚀 View run azure-jazz-2540 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1mkddss6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_165000-1mkddss6/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_170112-ji6a06ho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-elevator-2543
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ji6a06ho
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:55:42, 13.91s/it]  0%|          | 2/500 [00:26<1:48:17, 13.05s/it]  1%|          | 3/500 [00:38<1:42:50, 12.41s/it]  1%|          | 4/500 [00:49<1:40:05, 12.11s/it]  1%|          | 5/500 [01:01<1:38:59, 12.00s/it]  1%|          | 6/500 [01:13<1:38:16, 11.94s/it]  1%|▏         | 7/500 [01:24<1:35:58, 11.68s/it]  2%|▏         | 8/500 [01:36<1:36:38, 11.79s/it]  2%|▏         | 9/500 [01:48<1:35:53, 11.72s/it]  2%|▏         | 10/500 [01:59<1:35:12, 11.66s/it]  2%|▏         | 11/500 [02:11<1:36:32, 11.85s/it]  2%|▏         | 12/500 [02:23<1:35:04, 11.69s/it]  3%|▎         | 13/500 [02:34<1:34:03, 11.59s/it]  3%|▎         | 14/500 [02:45<1:33:30, 11.54s/it]  3%|▎         | 15/500 [02:57<1:33:51, 11.61s/it]  3%|▎         | 16/500 [03:09<1:33:54, 11.64s/it]  3%|▎         | 17/500 [03:21<1:33:50, 11.66s/it]  4%|▎         | 18/500 [03:32<1:32:41, 11.54s/it]  4%|▍         | 19/500 [03:44<1:33:13, 11.63s/it]  4%|▍         | 20/500 [03:55<1:32:20, 11.54s/it]  4%|▍         | 21/500 [04:06<1:31:23, 11.45s/it]  4%|▍         | 22/500 [04:18<1:30:44, 11.39s/it]  5%|▍         | 23/500 [04:28<1:28:15, 11.10s/it]  5%|▍         | 24/500 [04:40<1:30:15, 11.38s/it]  5%|▌         | 25/500 [04:51<1:30:08, 11.39s/it]  5%|▌         | 26/500 [05:03<1:30:39, 11.47s/it]  5%|▌         | 27/500 [05:15<1:31:12, 11.57s/it]  5%|▌         | 27/500 [05:15<1:32:04, 11.68s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.021 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▂▃▅▇▇▂▄▃▇▅▆▁▂▁▃▄▇█▄▄▇▇▆
wandb:     train_loss ▃▃▃▃▃▃▃▃▃█▁▃▃▃▂▂▁▃▃▂▁▂▃▂▂▂▂
wandb:   val_accuracy ▄▄▅█▄▅▇█▇▄▆▄▆▇▅▁▄▂▄▄▅▇▄▄▅▅▅
wandb:       val_loss ▅▄▄▄▄▄▄▃▄█▁▄▅▄▅▅▄▅▆▅▄▂▅▅▄▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.6419
wandb:     train_loss 0.52757
wandb:   val_accuracy 0.40667
wandb:       val_loss 1.01389
wandb: 
wandb: 🚀 View run lilac-elevator-2543 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ji6a06ho
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_170112-ji6a06ho/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_170724-oj787qlw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-plant-2545
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oj787qlw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:01:49, 14.65s/it]  0%|          | 2/500 [00:27<1:52:13, 13.52s/it]  1%|          | 3/500 [00:40<1:50:48, 13.38s/it]  1%|          | 4/500 [00:52<1:47:06, 12.96s/it]  1%|          | 5/500 [01:05<1:45:39, 12.81s/it]  1%|          | 6/500 [01:18<1:44:57, 12.75s/it]  1%|▏         | 7/500 [01:30<1:43:31, 12.60s/it]  2%|▏         | 8/500 [01:42<1:42:08, 12.46s/it]  2%|▏         | 9/500 [01:54<1:40:53, 12.33s/it]  2%|▏         | 10/500 [02:07<1:41:49, 12.47s/it]  2%|▏         | 11/500 [02:19<1:41:32, 12.46s/it]  2%|▏         | 12/500 [02:33<1:44:15, 12.82s/it]  3%|▎         | 13/500 [02:45<1:41:56, 12.56s/it]  3%|▎         | 14/500 [02:57<1:41:33, 12.54s/it]  3%|▎         | 15/500 [03:12<1:45:19, 13.03s/it]  3%|▎         | 16/500 [03:24<1:43:55, 12.88s/it]  3%|▎         | 17/500 [03:37<1:43:26, 12.85s/it]  4%|▎         | 18/500 [03:50<1:43:57, 12.94s/it]  4%|▍         | 19/500 [04:04<1:45:14, 13.13s/it]  4%|▍         | 20/500 [04:15<1:41:37, 12.70s/it]  4%|▍         | 21/500 [04:28<1:42:12, 12.80s/it]  4%|▍         | 22/500 [04:42<1:43:29, 12.99s/it]  5%|▍         | 23/500 [04:54<1:41:49, 12.81s/it]  5%|▍         | 24/500 [05:07<1:41:11, 12.76s/it]  5%|▌         | 25/500 [05:20<1:41:18, 12.80s/it]  5%|▌         | 26/500 [05:32<1:39:34, 12.61s/it]  5%|▌         | 27/500 [05:44<1:38:34, 12.50s/it]  6%|▌         | 28/500 [05:57<1:39:24, 12.64s/it]  6%|▌         | 29/500 [06:10<1:39:15, 12.65s/it]  6%|▌         | 30/500 [06:23<1:39:24, 12.69s/it]  6%|▌         | 31/500 [06:35<1:39:03, 12.67s/it]  6%|▋         | 32/500 [06:47<1:37:56, 12.56s/it]  6%|▋         | 32/500 [06:47<1:39:25, 12.75s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.310 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.021 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁
wandb: train_accuracy ▂▁▁▁▁▁▁▁▄▁▁▁▆▁▁▂█▁▁▁▄▂▁▁▂▆█▅▄▄▅▂
wandb:     train_loss ▁▁▂▃▆▁▃▂▁▄▁▁▁▁█▆▁▆▄▁▁▂▃▄▁▂▁▁▁▁▁▂
wandb:   val_accuracy ▂▁▁▂▂▂▂▂▆▁▁▂▅▂▂▁▆▂▂▁▆▄▂▂▁█▇▇▆▅▇▃
wandb:       val_loss ▁▁▁▂▄▄▄▂▂▁▁▇▂▁▄▁▂▃▅█▁▁▂▂▃▁▂▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 31
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.37147
wandb:     train_loss 1.34989
wandb:   val_accuracy 0.42222
wandb:       val_loss 0.18164
wandb: 
wandb: 🚀 View run dazzling-plant-2545 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oj787qlw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_170724-oj787qlw/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_171459-m5if3nrg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-brook-2547
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m5if3nrg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:14:01, 16.12s/it]  0%|          | 2/500 [00:30<2:02:59, 14.82s/it]  1%|          | 3/500 [00:44<2:00:18, 14.52s/it]  1%|          | 4/500 [00:58<1:59:57, 14.51s/it]  1%|          | 5/500 [01:12<1:57:18, 14.22s/it]  1%|          | 6/500 [01:25<1:54:06, 13.86s/it]  1%|▏         | 7/500 [01:38<1:51:49, 13.61s/it]  2%|▏         | 8/500 [01:51<1:49:33, 13.36s/it]  2%|▏         | 9/500 [02:05<1:50:14, 13.47s/it]  2%|▏         | 10/500 [02:19<1:51:54, 13.70s/it]  2%|▏         | 11/500 [02:32<1:49:30, 13.44s/it]  2%|▏         | 12/500 [02:45<1:48:29, 13.34s/it]  3%|▎         | 13/500 [02:58<1:48:54, 13.42s/it]  3%|▎         | 14/500 [03:12<1:48:37, 13.41s/it]  3%|▎         | 15/500 [03:24<1:45:12, 13.02s/it]  3%|▎         | 16/500 [03:37<1:45:56, 13.13s/it]  3%|▎         | 17/500 [03:51<1:47:05, 13.30s/it]  4%|▎         | 18/500 [04:06<1:50:16, 13.73s/it]  4%|▍         | 19/500 [04:20<1:50:34, 13.79s/it]  4%|▍         | 20/500 [04:33<1:49:11, 13.65s/it]  4%|▍         | 21/500 [04:47<1:48:46, 13.63s/it]  4%|▍         | 22/500 [05:00<1:47:37, 13.51s/it]  5%|▍         | 23/500 [05:12<1:44:55, 13.20s/it]  5%|▍         | 24/500 [05:26<1:46:43, 13.45s/it]  5%|▌         | 25/500 [05:42<1:50:57, 14.01s/it]  5%|▌         | 26/500 [05:56<1:51:20, 14.09s/it]  5%|▌         | 27/500 [06:11<1:54:14, 14.49s/it]  6%|▌         | 28/500 [06:25<1:52:50, 14.34s/it]  6%|▌         | 29/500 [06:40<1:52:27, 14.33s/it]  6%|▌         | 30/500 [06:55<1:54:41, 14.64s/it]  6%|▌         | 31/500 [07:09<1:53:34, 14.53s/it]  6%|▋         | 32/500 [07:24<1:54:16, 14.65s/it]  7%|▋         | 33/500 [07:38<1:52:15, 14.42s/it]  7%|▋         | 34/500 [07:51<1:48:54, 14.02s/it]  7%|▋         | 35/500 [08:03<1:44:04, 13.43s/it]  7%|▋         | 36/500 [08:14<1:37:32, 12.61s/it]  7%|▋         | 37/500 [08:25<1:33:29, 12.12s/it]  8%|▊         | 38/500 [08:38<1:36:09, 12.49s/it]  8%|▊         | 39/500 [08:51<1:35:30, 12.43s/it]  8%|▊         | 40/500 [09:03<1:34:27, 12.32s/it]  8%|▊         | 41/500 [09:15<1:34:26, 12.34s/it]  8%|▊         | 42/500 [09:27<1:32:25, 12.11s/it]  9%|▊         | 43/500 [09:38<1:31:17, 11.99s/it]  9%|▉         | 44/500 [09:50<1:31:19, 12.02s/it]  9%|▉         | 45/500 [10:02<1:31:13, 12.03s/it]  9%|▉         | 46/500 [10:14<1:29:39, 11.85s/it]  9%|▉         | 47/500 [10:27<1:31:12, 12.08s/it] 10%|▉         | 48/500 [10:39<1:30:53, 12.07s/it] 10%|▉         | 48/500 [10:39<1:40:17, 13.31s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.307 MB uploadedwandb: | 0.010 MB of 0.307 MB uploadedwandb: / 0.029 MB of 0.307 MB uploadedwandb: - 0.307 MB of 0.307 MB uploadedwandb: \ 0.307 MB of 0.307 MB uploadedwandb: | 0.307 MB of 0.307 MB uploadedwandb: / 0.307 MB of 0.307 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▅▄▁▁▁▂▆▃▆▇▆▆▇▇▇██▇███████████████████
wandb:     train_loss ▂▂▃▃▂█▄▁▃▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▅▃▅▅▂▃▄▄▆▅▆▆▆▆▇▇▇▇▇▆▇▇██▇▇▇▇▇██▇▇▇█▇▇▇█
wandb:       val_loss ▂▂▂▁▃█▃█▁▂▃▁▁▅▅▂▃▁▁▃▁▃▁▁▁▂▁▁▄▄▂▁▃▁▁▂▅▃▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.99703
wandb:     train_loss 0.00041
wandb:   val_accuracy 0.79111
wandb:       val_loss 0.44295
wandb: 
wandb: 🚀 View run denim-brook-2547 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m5if3nrg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_171459-m5if3nrg/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_172625-z7zlzms2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-cherry-2549
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z7zlzms2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<1:59:09, 14.33s/it]  0%|          | 2/500 [00:28<1:56:46, 14.07s/it]  1%|          | 3/500 [00:41<1:55:05, 13.89s/it]  1%|          | 4/500 [00:55<1:53:53, 13.78s/it]  1%|          | 5/500 [01:07<1:49:30, 13.27s/it]  1%|          | 6/500 [01:21<1:48:54, 13.23s/it]  1%|▏         | 7/500 [01:33<1:47:34, 13.09s/it]  2%|▏         | 8/500 [01:46<1:45:30, 12.87s/it]  2%|▏         | 9/500 [01:58<1:43:37, 12.66s/it]  2%|▏         | 10/500 [02:11<1:43:33, 12.68s/it]  2%|▏         | 11/500 [02:24<1:45:15, 12.91s/it]  2%|▏         | 12/500 [02:35<1:40:35, 12.37s/it]  3%|▎         | 13/500 [02:47<1:37:54, 12.06s/it]  3%|▎         | 14/500 [02:58<1:34:57, 11.72s/it]  3%|▎         | 15/500 [03:10<1:35:31, 11.82s/it]  3%|▎         | 16/500 [03:21<1:35:12, 11.80s/it]  3%|▎         | 17/500 [03:33<1:34:50, 11.78s/it]  4%|▎         | 18/500 [03:46<1:36:39, 12.03s/it]  4%|▍         | 19/500 [03:58<1:37:10, 12.12s/it]  4%|▍         | 20/500 [04:11<1:38:12, 12.28s/it]  4%|▍         | 21/500 [04:23<1:38:19, 12.32s/it]  4%|▍         | 22/500 [04:36<1:38:30, 12.36s/it]  5%|▍         | 23/500 [04:49<1:40:38, 12.66s/it]  5%|▍         | 24/500 [05:01<1:39:54, 12.59s/it]  5%|▌         | 25/500 [05:14<1:40:16, 12.67s/it]  5%|▌         | 26/500 [05:27<1:39:48, 12.63s/it]  5%|▌         | 27/500 [05:39<1:39:00, 12.56s/it]  5%|▌         | 27/500 [05:39<1:39:10, 12.58s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.135 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▄▅▆▆▇▄▄▁▅▄▄▃██▅▄▅▃▂▄▅▅█▄▄
wandb:     train_loss ▂▂▂▂▂▂▂▂▃▂▁▂▂▃▂▂▂▄▃▂▂▂▂▃▃█▁
wandb:   val_accuracy ▅▅▄▅▆▅█▄▄▁▅▅▂▂▆▅▄▅▄▁▁▄▃▄▅▄▅
wandb:       val_loss ▄▃▃▃▃▃▃▄▅▃▁▃▃▃▃▄▃▆▄▄▃▄▃▄▃█▇
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.37296
wandb:     train_loss 0.02466
wandb:   val_accuracy 0.34889
wandb:       val_loss 2.51597
wandb: 
wandb: 🚀 View run kind-cherry-2549 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z7zlzms2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_172625-z7zlzms2/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_173247-2mz6mnc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-totem-2551
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2mz6mnc7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:14:04, 16.12s/it]  0%|          | 2/500 [00:29<2:00:45, 14.55s/it]  1%|          | 3/500 [00:43<1:57:32, 14.19s/it]  1%|          | 4/500 [00:56<1:53:08, 13.69s/it]  1%|          | 5/500 [01:09<1:51:35, 13.53s/it]  1%|          | 6/500 [01:23<1:51:47, 13.58s/it]  1%|▏         | 7/500 [01:36<1:50:16, 13.42s/it]  2%|▏         | 8/500 [01:49<1:50:48, 13.51s/it]  2%|▏         | 9/500 [02:02<1:48:56, 13.31s/it]  2%|▏         | 10/500 [02:15<1:47:19, 13.14s/it]  2%|▏         | 11/500 [02:28<1:45:18, 12.92s/it]  2%|▏         | 12/500 [02:40<1:44:59, 12.91s/it]  3%|▎         | 13/500 [02:53<1:43:45, 12.78s/it]  3%|▎         | 14/500 [03:05<1:42:05, 12.60s/it]  3%|▎         | 15/500 [03:18<1:42:32, 12.69s/it]  3%|▎         | 16/500 [03:29<1:39:12, 12.30s/it]  3%|▎         | 17/500 [03:41<1:36:43, 12.02s/it]  4%|▎         | 18/500 [03:52<1:34:18, 11.74s/it]  4%|▍         | 19/500 [04:05<1:36:40, 12.06s/it]  4%|▍         | 20/500 [04:18<1:38:55, 12.37s/it]  4%|▍         | 21/500 [04:30<1:39:43, 12.49s/it]  4%|▍         | 22/500 [04:43<1:40:22, 12.60s/it]  5%|▍         | 23/500 [04:57<1:42:39, 12.91s/it]  5%|▍         | 24/500 [05:10<1:42:12, 12.88s/it]  5%|▌         | 25/500 [05:23<1:43:16, 13.05s/it]  5%|▌         | 25/500 [05:23<1:42:30, 12.95s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.310 MB uploadedwandb: - 0.010 MB of 0.310 MB uploadedwandb: \ 0.229 MB of 0.310 MB uploadedwandb: | 0.308 MB of 0.310 MB uploadedwandb: / 0.308 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▆▁▁▁█▁█▁▁
wandb:     train_loss ▁▁▂▃▄█▂▁▄▄▁▁▃▁▄▂▁▃▁▁▁▂▁▂▁
wandb:   val_accuracy ▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁█▁▂▁▇▁▇▁▁
wandb:       val_loss ▁▁▂▂▄▆▃▁▁▃▁▅▃▁▃▁▂▃▂▃▁▂▁▂█
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.35364
wandb:     train_loss 0.0
wandb:   val_accuracy 0.33111
wandb:       val_loss 26.32526
wandb: 
wandb: 🚀 View run avid-totem-2551 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2mz6mnc7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_173247-2mz6mnc7/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_173859-gec4rw3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-moon-2553
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gec4rw3h
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:17, 13.74s/it]  0%|          | 2/500 [00:26<1:50:03, 13.26s/it]  1%|          | 3/500 [00:39<1:47:22, 12.96s/it]  1%|          | 4/500 [00:51<1:45:36, 12.78s/it]  1%|          | 5/500 [01:03<1:42:27, 12.42s/it]  1%|          | 6/500 [01:16<1:43:40, 12.59s/it]  1%|▏         | 7/500 [01:29<1:44:06, 12.67s/it]  2%|▏         | 8/500 [01:41<1:43:21, 12.60s/it]  2%|▏         | 9/500 [01:53<1:41:41, 12.43s/it]  2%|▏         | 10/500 [02:05<1:40:44, 12.34s/it]  2%|▏         | 11/500 [02:17<1:38:29, 12.08s/it]  2%|▏         | 12/500 [02:30<1:39:49, 12.27s/it]  3%|▎         | 13/500 [02:42<1:40:40, 12.40s/it]  3%|▎         | 14/500 [02:54<1:39:18, 12.26s/it]  3%|▎         | 15/500 [03:07<1:41:20, 12.54s/it]  3%|▎         | 16/500 [03:19<1:39:39, 12.35s/it]  3%|▎         | 17/500 [03:32<1:39:20, 12.34s/it]  4%|▎         | 18/500 [03:44<1:38:10, 12.22s/it]  4%|▍         | 19/500 [03:55<1:35:42, 11.94s/it]  4%|▍         | 20/500 [04:07<1:35:39, 11.96s/it]  4%|▍         | 21/500 [04:19<1:35:09, 11.92s/it]  4%|▍         | 22/500 [04:31<1:35:54, 12.04s/it]  5%|▍         | 23/500 [04:44<1:37:38, 12.28s/it]  5%|▍         | 24/500 [04:57<1:38:21, 12.40s/it]  5%|▌         | 25/500 [05:08<1:36:46, 12.22s/it]  5%|▌         | 26/500 [05:21<1:37:35, 12.35s/it]  5%|▌         | 27/500 [05:33<1:37:28, 12.37s/it]  6%|▌         | 28/500 [05:47<1:40:12, 12.74s/it]  6%|▌         | 29/500 [06:01<1:42:32, 13.06s/it]  6%|▌         | 30/500 [06:14<1:41:31, 12.96s/it]  6%|▌         | 31/500 [06:26<1:41:07, 12.94s/it]  6%|▋         | 32/500 [06:39<1:39:05, 12.70s/it]  7%|▋         | 33/500 [06:52<1:39:49, 12.82s/it]  7%|▋         | 34/500 [07:03<1:37:00, 12.49s/it]  7%|▋         | 35/500 [07:17<1:39:01, 12.78s/it]  7%|▋         | 36/500 [07:29<1:38:01, 12.68s/it]  7%|▋         | 37/500 [07:44<1:42:18, 13.26s/it]  8%|▊         | 38/500 [07:58<1:43:58, 13.50s/it]  8%|▊         | 39/500 [08:10<1:41:06, 13.16s/it]  8%|▊         | 40/500 [08:24<1:41:19, 13.22s/it]  8%|▊         | 41/500 [08:38<1:42:42, 13.43s/it]  8%|▊         | 42/500 [08:52<1:44:01, 13.63s/it]  9%|▊         | 43/500 [09:06<1:45:07, 13.80s/it]  9%|▉         | 44/500 [09:19<1:43:31, 13.62s/it]  9%|▉         | 45/500 [09:32<1:40:49, 13.30s/it]  9%|▉         | 46/500 [09:44<1:38:30, 13.02s/it]  9%|▉         | 47/500 [09:55<1:33:26, 12.38s/it] 10%|▉         | 48/500 [10:08<1:33:41, 12.44s/it] 10%|▉         | 48/500 [10:08<1:35:26, 12.67s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.133 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▂▅▂▁▆▅▄█▃▄▇▅▃█▅▇▃▅▄▄▃▆▇▄▅▅▆▅█▄▅▇▆▇▆▇▄▆▇▇
wandb:     train_loss ▂▂▃▂▂▄▅▁▅▃▁▁▅▂▂▁▁▁▄▃▆▂▁▁▁█▆▁▁▁▁▄▁▁▁▁▂▁▁▃
wandb:   val_accuracy ▁█▃▁▅▆▅▆▆▆▅▇▅▆▆▆▅▆▆▆▄▆▆▆▆▅▅▅▆▅▅▅▅▆▅▆▄▅▅▅
wandb:       val_loss ▂▂▂▃▃▄▃▆▁▃▄▁▃▅▆▃▃▁▁▅▁▄▂▂▁▃▁▂▇▆▂▁▃▄▁▁██▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.69539
wandb:     train_loss 2.11704
wandb:   val_accuracy 0.51111
wandb:       val_loss 4.9872
wandb: 
wandb: 🚀 View run vocal-moon-2553 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gec4rw3h
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_173859-gec4rw3h/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_174949-sct3lc3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-glade-2556
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sct3lc3m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:55:11, 13.85s/it]  0%|          | 2/500 [00:26<1:50:07, 13.27s/it]  1%|          | 3/500 [00:39<1:48:27, 13.09s/it]  1%|          | 4/500 [00:51<1:45:27, 12.76s/it]  1%|          | 5/500 [01:04<1:43:56, 12.60s/it]  1%|          | 6/500 [01:16<1:43:15, 12.54s/it]  1%|▏         | 7/500 [01:28<1:42:11, 12.44s/it]  2%|▏         | 8/500 [01:40<1:40:20, 12.24s/it]  2%|▏         | 9/500 [01:52<1:39:58, 12.22s/it]  2%|▏         | 10/500 [02:05<1:40:01, 12.25s/it]  2%|▏         | 11/500 [02:16<1:38:46, 12.12s/it]  2%|▏         | 12/500 [02:28<1:37:57, 12.04s/it]  3%|▎         | 13/500 [02:40<1:37:42, 12.04s/it]  3%|▎         | 14/500 [02:53<1:38:34, 12.17s/it]  3%|▎         | 15/500 [03:05<1:37:49, 12.10s/it]  3%|▎         | 16/500 [03:17<1:38:07, 12.17s/it]  3%|▎         | 17/500 [03:29<1:37:35, 12.12s/it]  4%|▎         | 18/500 [03:41<1:37:25, 12.13s/it]  4%|▍         | 19/500 [03:54<1:38:05, 12.24s/it]  4%|▍         | 20/500 [04:05<1:36:14, 12.03s/it]  4%|▍         | 21/500 [04:17<1:34:10, 11.80s/it]  4%|▍         | 22/500 [04:28<1:32:09, 11.57s/it]  5%|▍         | 23/500 [04:39<1:31:43, 11.54s/it]  5%|▍         | 24/500 [04:51<1:31:26, 11.53s/it]  5%|▌         | 25/500 [05:02<1:31:04, 11.50s/it]  5%|▌         | 25/500 [05:02<1:35:47, 12.10s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.231 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▂▂▁▁▁▂▂▇▁█▂▁▁▂▃▃▆▁▁▁▁▇
wandb:     train_loss ▂▂▂▂▂▂▂▂▇▆▁▁▁█▂▂▁▂▂▁▃▃▃▃▂
wandb:   val_accuracy ▁▁▁▁▁▁▁▁▁▁█▁▆▁▁▁▁▅▃▅▁▁▁▁▆
wandb:       val_loss ▂▂▂▂▂▂▂▂▁▃▁▂▂█▂▂▃▂▃▂▁▂▂▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.63447
wandb:     train_loss 1.45069
wandb:   val_accuracy 0.53111
wandb:       val_loss 0.34137
wandb: 
wandb: 🚀 View run upbeat-glade-2556 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sct3lc3m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_174949-sct3lc3m/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_175540-alnguxar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-bee-2558
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/alnguxar
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<1:58:18, 14.23s/it]  0%|          | 2/500 [00:26<1:49:09, 13.15s/it]  1%|          | 3/500 [00:39<1:49:19, 13.20s/it]  1%|          | 4/500 [00:53<1:50:02, 13.31s/it]  1%|          | 5/500 [01:07<1:51:02, 13.46s/it]  1%|          | 6/500 [01:19<1:48:52, 13.22s/it]  1%|▏         | 7/500 [01:33<1:50:20, 13.43s/it]  2%|▏         | 8/500 [01:46<1:48:26, 13.23s/it]  2%|▏         | 9/500 [01:59<1:47:24, 13.13s/it]  2%|▏         | 10/500 [02:12<1:48:15, 13.26s/it]  2%|▏         | 11/500 [02:26<1:49:47, 13.47s/it]  2%|▏         | 12/500 [02:41<1:51:48, 13.75s/it]  3%|▎         | 13/500 [02:53<1:48:43, 13.40s/it]  3%|▎         | 14/500 [03:06<1:46:56, 13.20s/it]  3%|▎         | 15/500 [03:22<1:52:15, 13.89s/it]  3%|▎         | 16/500 [03:39<1:59:35, 14.83s/it]  3%|▎         | 17/500 [03:53<1:58:23, 14.71s/it]  4%|▎         | 18/500 [04:07<1:56:45, 14.54s/it]  4%|▍         | 19/500 [04:20<1:52:14, 14.00s/it]  4%|▍         | 20/500 [04:37<1:59:07, 14.89s/it]  4%|▍         | 21/500 [04:51<1:58:06, 14.79s/it]  4%|▍         | 22/500 [05:07<1:58:51, 14.92s/it]  5%|▍         | 23/500 [05:21<1:56:45, 14.69s/it]  5%|▍         | 24/500 [05:36<1:56:41, 14.71s/it]  5%|▌         | 25/500 [05:50<1:56:01, 14.65s/it]  5%|▌         | 26/500 [06:06<1:58:18, 14.98s/it]  5%|▌         | 27/500 [06:21<1:59:23, 15.14s/it]  6%|▌         | 28/500 [06:34<1:53:11, 14.39s/it]  6%|▌         | 29/500 [06:49<1:53:21, 14.44s/it]  6%|▌         | 30/500 [07:03<1:52:03, 14.31s/it]  6%|▌         | 30/500 [07:03<1:50:28, 14.10s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.021 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▂▂▄▁▁▂█▂▁▂▂▁▂▁█▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁
wandb:     train_loss ▁▁▁▂▃▅▁▁▁▂▁▁▂▁▁▁▃▅▄▁▅▄▅▃▁▅█▁▅▄
wandb:   val_accuracy ▁▂▆▁▁▁█▂▁▁▁▁▂▁▇▂▁▁▁▁▁▁▁▂▁▁▁▁▁▃
wandb:       val_loss ▁▁▁▁▂▃▂▁▄▁▁▅▂▁▁▁▅▃▆█▁▂▃▂▆▃▅▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.3373
wandb:     train_loss 7.33076
wandb:   val_accuracy 0.41333
wandb:       val_loss 3.55899
wandb: 
wandb: 🚀 View run astral-bee-2558 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/alnguxar
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_175540-alnguxar/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_180331-06bye9ap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-disco-2559
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/06bye9ap
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:01, 17.56s/it]  0%|          | 2/500 [00:34<2:22:39, 17.19s/it]  1%|          | 3/500 [00:51<2:23:10, 17.28s/it]  1%|          | 4/500 [01:08<2:19:39, 16.89s/it]  1%|          | 5/500 [01:25<2:19:43, 16.94s/it]  1%|          | 6/500 [01:41<2:16:38, 16.60s/it]  1%|▏         | 7/500 [01:57<2:14:24, 16.36s/it]  2%|▏         | 8/500 [02:14<2:16:26, 16.64s/it]  2%|▏         | 9/500 [02:29<2:12:27, 16.19s/it]  2%|▏         | 10/500 [02:45<2:10:44, 16.01s/it]  2%|▏         | 11/500 [03:00<2:08:21, 15.75s/it]  2%|▏         | 12/500 [03:17<2:11:12, 16.13s/it]  3%|▎         | 13/500 [03:33<2:10:34, 16.09s/it]  3%|▎         | 14/500 [03:50<2:12:05, 16.31s/it]  3%|▎         | 15/500 [04:06<2:12:03, 16.34s/it]  3%|▎         | 16/500 [04:22<2:10:29, 16.18s/it]  3%|▎         | 17/500 [04:39<2:14:05, 16.66s/it]  4%|▎         | 18/500 [04:56<2:14:12, 16.71s/it]  4%|▍         | 19/500 [05:13<2:13:43, 16.68s/it]  4%|▍         | 20/500 [05:30<2:13:53, 16.74s/it]  4%|▍         | 21/500 [05:48<2:16:21, 17.08s/it]  4%|▍         | 22/500 [06:05<2:16:44, 17.16s/it]  5%|▍         | 23/500 [06:22<2:15:48, 17.08s/it]  5%|▍         | 24/500 [06:38<2:12:09, 16.66s/it]  5%|▌         | 25/500 [06:54<2:10:37, 16.50s/it]  5%|▌         | 26/500 [07:11<2:12:05, 16.72s/it]  5%|▌         | 27/500 [07:27<2:09:18, 16.40s/it]  6%|▌         | 28/500 [07:43<2:07:57, 16.27s/it]  6%|▌         | 29/500 [07:59<2:08:58, 16.43s/it]  6%|▌         | 30/500 [08:15<2:06:31, 16.15s/it]  6%|▌         | 30/500 [08:15<2:09:21, 16.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.307 MB uploadedwandb: - 0.010 MB of 0.307 MB uploadedwandb: \ 0.231 MB of 0.307 MB uploadedwandb: | 0.307 MB of 0.307 MB uploadedwandb: / 0.307 MB of 0.307 MB uploadedwandb: - 0.307 MB of 0.307 MB uploadedwandb: \ 0.307 MB of 0.307 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▁▃▆▃▅▃▆▇▅▇▇▇▇█▇▇█▇██████▇████
wandb:     train_loss ▃▃▁▁▁▁▁▁▁▄▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▃▅▅▅▅▆▇▇▅▆▇▇▇▇▇▇█▇▇█▇▇███▇██
wandb:       val_loss ▃▃▂▃▄▃▄▂▂▄▃▂▃▁█▁▁▁▄▂▁▁▄▂▂▇▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 1.0
wandb:     train_loss 0.00874
wandb:   val_accuracy 0.85778
wandb:       val_loss 0.01248
wandb: 
wandb: 🚀 View run azure-disco-2559 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/06bye9ap
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_180331-06bye9ap/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_181239-81qwhfzz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-bush-2561
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/81qwhfzz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:24, 17.48s/it]  0%|          | 2/500 [00:33<2:19:29, 16.81s/it]  1%|          | 3/500 [00:50<2:17:33, 16.61s/it]  1%|          | 4/500 [01:06<2:16:03, 16.46s/it]  1%|          | 5/500 [01:22<2:16:09, 16.50s/it]  1%|          | 6/500 [01:38<2:14:13, 16.30s/it]  1%|▏         | 7/500 [01:55<2:13:24, 16.24s/it]  2%|▏         | 8/500 [02:10<2:11:52, 16.08s/it]  2%|▏         | 9/500 [02:27<2:13:11, 16.28s/it]  2%|▏         | 10/500 [02:44<2:13:51, 16.39s/it]  2%|▏         | 11/500 [02:59<2:11:24, 16.12s/it]  2%|▏         | 12/500 [03:15<2:10:03, 15.99s/it]  3%|▎         | 13/500 [03:30<2:08:37, 15.85s/it]  3%|▎         | 14/500 [03:46<2:07:55, 15.79s/it]  3%|▎         | 15/500 [04:02<2:08:23, 15.88s/it]  3%|▎         | 16/500 [04:18<2:07:15, 15.78s/it]  3%|▎         | 17/500 [04:34<2:07:22, 15.82s/it]  4%|▎         | 18/500 [04:49<2:06:18, 15.72s/it]  4%|▍         | 19/500 [05:05<2:05:52, 15.70s/it]  4%|▍         | 20/500 [05:20<2:05:11, 15.65s/it]  4%|▍         | 21/500 [05:36<2:05:11, 15.68s/it]  4%|▍         | 22/500 [05:52<2:04:50, 15.67s/it]  5%|▍         | 23/500 [06:07<2:04:27, 15.66s/it]  5%|▍         | 24/500 [06:24<2:06:19, 15.92s/it]  5%|▌         | 25/500 [06:40<2:06:02, 15.92s/it]  5%|▌         | 26/500 [06:56<2:07:26, 16.13s/it]  5%|▌         | 27/500 [07:12<2:05:47, 15.96s/it]  6%|▌         | 28/500 [07:27<2:04:35, 15.84s/it]  6%|▌         | 28/500 [07:27<2:05:51, 16.00s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.021 MB of 0.312 MB uploadedwandb: \ 0.137 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▆▃▂▃▃▇▁▃▅▅▃▃▃▃▃▇▃▃▃▇▃█▄▄▃▃
wandb:     train_loss ▂▂▂▃▂█▁▂▂▄▂▁▃▂▂▂▅▂▃▂▃▁▂▁▂▃▂▇
wandb:   val_accuracy ▄▃█▃▂▃▃▆▁▃▇▇▅▃▂▃▃▆▄▂▁▇▂▇▃▄▁▄
wandb:       val_loss ▂▂▂▂▂▆█▃▂▅▂▁▄▃▃▃█▂▂▂▄▂▃▂▂▂▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.36999
wandb:     train_loss 5.73276
wandb:   val_accuracy 0.35111
wandb:       val_loss 2.42821
wandb: 
wandb: 🚀 View run solar-bush-2561 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/81qwhfzz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_181239-81qwhfzz/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_182051-0ju6zu66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-vortex-2562
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0ju6zu66
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:11, 18.06s/it]  0%|          | 2/500 [00:33<2:18:40, 16.71s/it]  1%|          | 3/500 [00:51<2:20:49, 17.00s/it]  1%|          | 4/500 [01:07<2:16:49, 16.55s/it]  1%|          | 5/500 [01:22<2:14:06, 16.26s/it]  1%|          | 6/500 [01:38<2:12:18, 16.07s/it]  1%|▏         | 7/500 [01:54<2:12:27, 16.12s/it]  2%|▏         | 8/500 [02:11<2:13:14, 16.25s/it]  2%|▏         | 9/500 [02:28<2:14:29, 16.44s/it]  2%|▏         | 10/500 [02:45<2:16:29, 16.71s/it]  2%|▏         | 11/500 [03:02<2:16:04, 16.70s/it]  2%|▏         | 12/500 [03:18<2:14:44, 16.57s/it]  3%|▎         | 13/500 [03:33<2:12:14, 16.29s/it]  3%|▎         | 14/500 [03:50<2:11:49, 16.27s/it]  3%|▎         | 15/500 [04:06<2:12:24, 16.38s/it]  3%|▎         | 16/500 [04:23<2:12:22, 16.41s/it]  3%|▎         | 17/500 [04:39<2:10:44, 16.24s/it]  4%|▎         | 18/500 [04:55<2:10:13, 16.21s/it]  4%|▍         | 19/500 [05:11<2:10:34, 16.29s/it]  4%|▍         | 20/500 [05:28<2:12:12, 16.53s/it]  4%|▍         | 21/500 [05:45<2:11:15, 16.44s/it]  4%|▍         | 22/500 [06:02<2:12:22, 16.62s/it]  5%|▍         | 23/500 [06:17<2:10:10, 16.37s/it]  5%|▍         | 24/500 [06:34<2:10:37, 16.47s/it]  5%|▌         | 25/500 [06:51<2:11:32, 16.62s/it]  5%|▌         | 26/500 [07:08<2:11:59, 16.71s/it]  5%|▌         | 27/500 [07:25<2:11:47, 16.72s/it]  6%|▌         | 28/500 [07:41<2:09:33, 16.47s/it]  6%|▌         | 29/500 [07:56<2:07:29, 16.24s/it]  6%|▌         | 30/500 [08:12<2:05:49, 16.06s/it]  6%|▌         | 31/500 [08:28<2:05:20, 16.03s/it]  6%|▋         | 32/500 [08:43<2:03:49, 15.87s/it]  7%|▋         | 33/500 [08:59<2:02:07, 15.69s/it]  7%|▋         | 34/500 [09:14<2:01:24, 15.63s/it]  7%|▋         | 35/500 [09:30<2:01:13, 15.64s/it]  7%|▋         | 36/500 [09:45<1:59:55, 15.51s/it]  7%|▋         | 37/500 [10:01<1:59:54, 15.54s/it]  7%|▋         | 37/500 [10:01<2:05:23, 16.25s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.020 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▂▃▂▃▄▅▄▆▆▅▅▇█▇▆▇▇▅█▇▇███▄█▆▇▇█▆▇██
wandb:     train_loss ▄▃▂▄▁▄▁▁▁█▁▁▃▂▂▁▁▁▁▂▁▁▂▂▁▁▁▂▁▁▄▇▁▂▂▁▂
wandb:   val_accuracy ▁▁▅▂▆▂▃▇▆▄█▇█▆▇▆▅▇▇▆▅▆▆▇▆▆▇▅▇▇▆▇▆▆▆▆▆
wandb:       val_loss ▂▃▂▂▂▂▃▁▂▂▁▂▂▁▃▁▂▁▂▂▁▁▂▂▂▁▁█▁▁▄▅▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.94651
wandb:     train_loss 0.39416
wandb:   val_accuracy 0.68667
wandb:       val_loss 0.14102
wandb: 
wandb: 🚀 View run trim-vortex-2562 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0ju6zu66
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_182051-0ju6zu66/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_183142-i8o2qwqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-wind-2564
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i8o2qwqz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:18, 17.47s/it]  0%|          | 2/500 [00:33<2:17:32, 16.57s/it]  1%|          | 3/500 [00:50<2:19:24, 16.83s/it]  1%|          | 4/500 [01:06<2:15:34, 16.40s/it]  1%|          | 5/500 [01:22<2:13:26, 16.17s/it]  1%|          | 6/500 [01:37<2:12:03, 16.04s/it]  1%|▏         | 7/500 [01:53<2:10:49, 15.92s/it]  2%|▏         | 8/500 [02:08<2:08:27, 15.67s/it]  2%|▏         | 9/500 [02:23<2:07:08, 15.54s/it]  2%|▏         | 10/500 [02:39<2:07:56, 15.67s/it]  2%|▏         | 11/500 [02:55<2:08:24, 15.76s/it]  2%|▏         | 12/500 [03:11<2:09:04, 15.87s/it]  3%|▎         | 13/500 [03:27<2:08:38, 15.85s/it]  3%|▎         | 14/500 [03:43<2:09:08, 15.94s/it]  3%|▎         | 15/500 [04:00<2:09:39, 16.04s/it]  3%|▎         | 16/500 [04:16<2:09:40, 16.08s/it]  3%|▎         | 17/500 [04:33<2:11:40, 16.36s/it]  4%|▎         | 18/500 [04:49<2:10:36, 16.26s/it]  4%|▍         | 19/500 [05:05<2:10:09, 16.24s/it]  4%|▍         | 20/500 [05:21<2:08:30, 16.06s/it]  4%|▍         | 21/500 [05:37<2:07:45, 16.00s/it]  4%|▍         | 22/500 [05:53<2:07:45, 16.04s/it]  5%|▍         | 23/500 [06:09<2:07:43, 16.07s/it]  5%|▍         | 24/500 [06:25<2:06:43, 15.97s/it]  5%|▌         | 25/500 [06:40<2:06:00, 15.92s/it]  5%|▌         | 26/500 [06:57<2:06:19, 15.99s/it]  5%|▌         | 27/500 [07:13<2:06:49, 16.09s/it]  6%|▌         | 28/500 [07:29<2:06:26, 16.07s/it]  6%|▌         | 29/500 [07:46<2:09:00, 16.43s/it]  6%|▌         | 30/500 [08:02<2:07:14, 16.24s/it]  6%|▌         | 31/500 [08:18<2:06:00, 16.12s/it]  6%|▋         | 32/500 [08:35<2:07:08, 16.30s/it]  7%|▋         | 33/500 [08:50<2:05:43, 16.15s/it]  7%|▋         | 34/500 [09:06<2:04:35, 16.04s/it]  7%|▋         | 35/500 [09:22<2:03:30, 15.94s/it]  7%|▋         | 36/500 [09:37<2:02:34, 15.85s/it]  7%|▋         | 37/500 [09:53<2:01:56, 15.80s/it]  8%|▊         | 38/500 [10:09<2:00:52, 15.70s/it]  8%|▊         | 39/500 [10:24<2:01:05, 15.76s/it]  8%|▊         | 40/500 [10:42<2:03:47, 16.15s/it]  8%|▊         | 41/500 [10:59<2:05:24, 16.39s/it]  8%|▊         | 42/500 [11:14<2:03:57, 16.24s/it]  9%|▊         | 43/500 [11:30<2:02:48, 16.12s/it]  9%|▉         | 44/500 [11:46<2:01:21, 15.97s/it]  9%|▉         | 45/500 [12:02<2:00:39, 15.91s/it]  9%|▉         | 45/500 [12:02<2:01:42, 16.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.020 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▂▁▆▃▄▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇██▇█▇▇▇█▇██▇██
wandb:     train_loss ▄▅█▂▆▇▁▁▆▁▄▁▂▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▂▁▃█▁▁▄▃▁▁
wandb:   val_accuracy ▁▂▁▄▄▄▅▆▇▆▅▆▆▇▆▇▇▆▆▇▆█▆█▆▇▇▇▇█▆▇▆█▇▇█▇▇▇
wandb:       val_loss ▂▂▂▂▁▁▁▃▁▁▂▂▁▃▁▁▁▁▁▁▂▁▂▁▁▁▁▁▄▁▁▁▁▁▁█▁▁▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.95245
wandb:     train_loss 0.01266
wandb:   val_accuracy 0.80222
wandb:       val_loss 0.00028
wandb: 
wandb: 🚀 View run deep-wind-2564 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i8o2qwqz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_183142-i8o2qwqz/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_184431-6d8jnjwo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-paper-2566
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6d8jnjwo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:40:18, 19.27s/it]  0%|          | 2/500 [00:34<2:22:20, 17.15s/it]  1%|          | 3/500 [00:50<2:16:40, 16.50s/it]  1%|          | 4/500 [01:07<2:15:51, 16.44s/it]  1%|          | 5/500 [01:22<2:14:09, 16.26s/it]  1%|          | 6/500 [01:38<2:12:38, 16.11s/it]  1%|▏         | 7/500 [01:54<2:12:29, 16.12s/it]  2%|▏         | 8/500 [02:11<2:12:15, 16.13s/it]  2%|▏         | 9/500 [02:27<2:12:16, 16.16s/it]  2%|▏         | 10/500 [02:43<2:11:27, 16.10s/it]  2%|▏         | 11/500 [02:59<2:10:37, 16.03s/it]  2%|▏         | 12/500 [03:15<2:11:13, 16.13s/it]  3%|▎         | 13/500 [03:31<2:09:51, 16.00s/it]  3%|▎         | 14/500 [03:47<2:09:22, 15.97s/it]  3%|▎         | 15/500 [04:02<2:08:27, 15.89s/it]  3%|▎         | 16/500 [04:18<2:07:11, 15.77s/it]  3%|▎         | 17/500 [04:33<2:06:04, 15.66s/it]  4%|▎         | 18/500 [04:50<2:07:48, 15.91s/it]  4%|▍         | 19/500 [05:06<2:07:43, 15.93s/it]  4%|▍         | 20/500 [05:21<2:06:32, 15.82s/it]  4%|▍         | 21/500 [05:37<2:05:48, 15.76s/it]  4%|▍         | 22/500 [05:52<2:04:59, 15.69s/it]  5%|▍         | 23/500 [06:08<2:04:15, 15.63s/it]  5%|▍         | 24/500 [06:24<2:04:46, 15.73s/it]  5%|▌         | 25/500 [06:41<2:06:50, 16.02s/it]  5%|▌         | 26/500 [06:56<2:05:33, 15.89s/it]  5%|▌         | 27/500 [07:13<2:07:55, 16.23s/it]  6%|▌         | 28/500 [07:29<2:07:43, 16.24s/it]  6%|▌         | 28/500 [07:29<2:06:23, 16.07s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.137 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▃▃▄▃▇▇▆▇▇▇▇▇▆▇██▇█▇▆█▇█▇▆
wandb:     train_loss █▇▆▇▅▇▄▅▆▆▅█▇▆▆▅▅▄▁▅▂▁▆▄▁▁▃▆
wandb:   val_accuracy ▁▁▄▃▃▇▆▆▇█▆▅▆▆▅▃▅▅▆▆▅▆▅▆▅▆▄▄
wandb:       val_loss ▄▄▄▄▄▂▄▄▂▅▄▁▅▁▆▂▃▂▃▃██▄▂▇▃▃▆
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.68945
wandb:     train_loss 0.99639
wandb:   val_accuracy 0.46444
wandb:       val_loss 1.49517
wandb: 
wandb: 🚀 View run rare-paper-2566 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6d8jnjwo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_184431-6d8jnjwo/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_185303-mfc5y32y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sound-2568
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mfc5y32y
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:41:53, 19.47s/it]  0%|          | 2/500 [00:36<2:27:54, 17.82s/it]  1%|          | 3/500 [00:53<2:24:23, 17.43s/it]  1%|          | 4/500 [01:09<2:21:34, 17.13s/it]  1%|          | 5/500 [01:25<2:18:21, 16.77s/it]  1%|          | 6/500 [01:42<2:18:56, 16.87s/it]  1%|▏         | 7/500 [01:59<2:16:43, 16.64s/it]  2%|▏         | 8/500 [02:14<2:14:13, 16.37s/it]  2%|▏         | 9/500 [02:30<2:12:38, 16.21s/it]  2%|▏         | 10/500 [02:46<2:11:24, 16.09s/it]  2%|▏         | 11/500 [03:02<2:10:10, 15.97s/it]  2%|▏         | 12/500 [03:17<2:09:04, 15.87s/it]  3%|▎         | 13/500 [03:33<2:08:26, 15.83s/it]  3%|▎         | 14/500 [03:49<2:07:31, 15.74s/it]  3%|▎         | 15/500 [04:05<2:08:00, 15.84s/it]  3%|▎         | 16/500 [04:21<2:09:40, 16.08s/it]  3%|▎         | 17/500 [04:38<2:09:47, 16.12s/it]  4%|▎         | 18/500 [04:54<2:08:58, 16.05s/it]  4%|▍         | 19/500 [05:10<2:09:08, 16.11s/it]  4%|▍         | 20/500 [05:25<2:07:12, 15.90s/it]  4%|▍         | 21/500 [05:42<2:09:53, 16.27s/it]  4%|▍         | 22/500 [05:58<2:08:26, 16.12s/it]  5%|▍         | 23/500 [06:14<2:07:44, 16.07s/it]  5%|▍         | 24/500 [06:30<2:06:50, 15.99s/it]  5%|▌         | 25/500 [06:46<2:06:21, 15.96s/it]  5%|▌         | 26/500 [07:02<2:05:58, 15.95s/it]  5%|▌         | 27/500 [07:18<2:06:17, 16.02s/it]  6%|▌         | 28/500 [07:34<2:06:23, 16.07s/it]  6%|▌         | 29/500 [07:50<2:05:51, 16.03s/it]  6%|▌         | 30/500 [08:06<2:05:37, 16.04s/it]  6%|▌         | 31/500 [08:23<2:07:45, 16.34s/it]  6%|▋         | 32/500 [08:39<2:06:36, 16.23s/it]  7%|▋         | 33/500 [08:55<2:04:49, 16.04s/it]  7%|▋         | 34/500 [09:10<2:03:30, 15.90s/it]  7%|▋         | 35/500 [09:26<2:03:14, 15.90s/it]  7%|▋         | 36/500 [09:42<2:02:50, 15.88s/it]  7%|▋         | 37/500 [09:58<2:01:59, 15.81s/it]  7%|▋         | 37/500 [09:58<2:04:44, 16.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.232 MB of 0.314 MB uploadedwandb: \ 0.232 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▁▂▂▃▃▇█▃██▇▇▇▇▇▇▇█▇█▇▆▇▇█▅▇█▇██▇▇▇▇
wandb:     train_loss ▃▃▃▆▁▃▁▂▂█▂▂▂▂▁▁▂▁▁▁▁▁▂▃▁▁▁▁▁▂▃▃▄▁▃▁▁
wandb:   val_accuracy ▁▂▂▁▂▆▄▇▆▅▆▇▇▆▇▆▆▇█▇▇█▇▇▇██▅██▇█▇▇▇▇▇
wandb:       val_loss ▃▃▃▂▄▂▃▃▃▃▂▄▂▁▆▂▁▁▂▃▁▂▅▃▅▁▁█▁▁▃▅▁▂▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.85141
wandb:     train_loss 0.15812
wandb:   val_accuracy 0.67778
wandb:       val_loss 0.14986
wandb: 
wandb: 🚀 View run comic-sound-2568 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mfc5y32y
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_185303-mfc5y32y/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_190351-4ix6nqvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-morning-2570
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4ix6nqvr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:36:49, 18.86s/it]  0%|          | 2/500 [00:35<2:26:55, 17.70s/it]  1%|          | 3/500 [00:52<2:21:38, 17.10s/it]  1%|          | 4/500 [01:08<2:19:40, 16.90s/it]  1%|          | 5/500 [01:24<2:17:27, 16.66s/it]  1%|          | 6/500 [01:41<2:15:55, 16.51s/it]  1%|▏         | 7/500 [01:57<2:15:43, 16.52s/it]  2%|▏         | 8/500 [02:13<2:14:30, 16.40s/it]  2%|▏         | 9/500 [02:29<2:13:05, 16.26s/it]  2%|▏         | 10/500 [02:45<2:11:00, 16.04s/it]  2%|▏         | 11/500 [03:01<2:10:45, 16.04s/it]  2%|▏         | 12/500 [03:17<2:09:30, 15.92s/it]  3%|▎         | 13/500 [03:33<2:09:48, 15.99s/it]  3%|▎         | 14/500 [03:48<2:08:22, 15.85s/it]  3%|▎         | 15/500 [04:05<2:09:09, 15.98s/it]  3%|▎         | 16/500 [04:21<2:08:56, 15.98s/it]  3%|▎         | 17/500 [04:37<2:09:44, 16.12s/it]  4%|▎         | 18/500 [04:53<2:08:36, 16.01s/it]  4%|▍         | 19/500 [05:07<2:05:20, 15.63s/it]  4%|▍         | 20/500 [05:23<2:03:55, 15.49s/it]  4%|▍         | 21/500 [05:39<2:05:00, 15.66s/it]  4%|▍         | 22/500 [05:55<2:06:01, 15.82s/it]  5%|▍         | 23/500 [06:11<2:05:54, 15.84s/it]  5%|▍         | 24/500 [06:28<2:08:18, 16.17s/it]  5%|▌         | 25/500 [06:43<2:06:41, 16.00s/it]  5%|▌         | 26/500 [06:59<2:05:45, 15.92s/it]  5%|▌         | 27/500 [07:15<2:05:17, 15.89s/it]  6%|▌         | 28/500 [07:31<2:05:43, 15.98s/it]  6%|▌         | 29/500 [07:47<2:04:51, 15.91s/it]  6%|▌         | 29/500 [07:47<2:06:29, 16.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.021 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▁▂▁▂▂▆▂▁▂▃▃▅▅▄▃▁█▇▆▇▄▅▅█▆▃▇
wandb:     train_loss ▂▂▂▂▃▁▃▂▁▁▃▄▂▁▁▁▂▁▁▁▁▁▁█▁▁▁▁▁
wandb:   val_accuracy ▁▃▁▂▁▁▆▇▅▅▅▅▆▆▆▆▅▂█▇▆▇▆▇▇█▇▅▇
wandb:       val_loss ▃▂▃▃▂▃▃▆▆▃▃▄▁▁▆▁▂▄▁▄▁▆▇▃█▁▁▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.7786
wandb:     train_loss 0.01202
wandb:   val_accuracy 0.61111
wandb:       val_loss 0.01714
wandb: 
wandb: 🚀 View run curious-morning-2570 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4ix6nqvr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_190351-4ix6nqvr/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_191226-ilyvrvby
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-paper-2572
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ilyvrvby
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:52, 18.14s/it]  0%|          | 2/500 [00:34<2:23:33, 17.30s/it]  1%|          | 3/500 [00:51<2:19:16, 16.81s/it]  1%|          | 4/500 [01:06<2:15:59, 16.45s/it]  1%|          | 5/500 [01:22<2:13:23, 16.17s/it]  1%|          | 6/500 [01:38<2:12:37, 16.11s/it]  1%|▏         | 7/500 [01:54<2:10:32, 15.89s/it]  2%|▏         | 8/500 [02:09<2:10:04, 15.86s/it]  2%|▏         | 9/500 [02:25<2:10:06, 15.90s/it]  2%|▏         | 10/500 [02:41<2:08:57, 15.79s/it]  2%|▏         | 11/500 [02:57<2:08:59, 15.83s/it]  2%|▏         | 12/500 [03:13<2:09:02, 15.87s/it]  3%|▎         | 13/500 [03:28<2:08:16, 15.80s/it]  3%|▎         | 14/500 [03:44<2:07:14, 15.71s/it]  3%|▎         | 15/500 [03:59<2:06:27, 15.64s/it]  3%|▎         | 16/500 [04:15<2:04:50, 15.48s/it]  3%|▎         | 17/500 [04:30<2:03:40, 15.36s/it]  4%|▎         | 18/500 [04:46<2:04:57, 15.56s/it]  4%|▍         | 19/500 [05:01<2:04:49, 15.57s/it]  4%|▍         | 20/500 [05:18<2:07:07, 15.89s/it]  4%|▍         | 21/500 [05:34<2:07:59, 16.03s/it]  4%|▍         | 22/500 [05:50<2:06:38, 15.90s/it]  5%|▍         | 23/500 [06:06<2:05:56, 15.84s/it]  5%|▍         | 24/500 [06:23<2:08:27, 16.19s/it]  5%|▌         | 25/500 [06:38<2:07:00, 16.04s/it]  5%|▌         | 26/500 [06:53<2:04:46, 15.79s/it]  5%|▌         | 27/500 [07:09<2:04:40, 15.82s/it]  6%|▌         | 28/500 [07:26<2:06:17, 16.05s/it]  6%|▌         | 29/500 [07:42<2:06:45, 16.15s/it]  6%|▌         | 30/500 [07:58<2:05:38, 16.04s/it]  6%|▌         | 31/500 [08:14<2:04:23, 15.91s/it]  6%|▋         | 32/500 [08:31<2:06:33, 16.22s/it]  7%|▋         | 33/500 [08:47<2:05:32, 16.13s/it]  7%|▋         | 34/500 [09:02<2:03:44, 15.93s/it]  7%|▋         | 35/500 [09:18<2:02:56, 15.86s/it]  7%|▋         | 36/500 [09:34<2:03:55, 16.02s/it]  7%|▋         | 37/500 [09:49<2:02:10, 15.83s/it]  8%|▊         | 38/500 [10:05<2:02:05, 15.86s/it]  8%|▊         | 39/500 [10:21<2:02:10, 15.90s/it]  8%|▊         | 40/500 [10:38<2:03:20, 16.09s/it]  8%|▊         | 41/500 [10:53<2:01:32, 15.89s/it]  8%|▊         | 42/500 [11:09<2:01:08, 15.87s/it]  9%|▊         | 43/500 [11:25<1:59:45, 15.72s/it]  9%|▉         | 44/500 [11:40<1:59:28, 15.72s/it]  9%|▉         | 45/500 [11:56<1:58:56, 15.68s/it]  9%|▉         | 46/500 [12:11<1:58:11, 15.62s/it]  9%|▉         | 47/500 [12:27<1:58:13, 15.66s/it] 10%|▉         | 48/500 [12:43<1:57:47, 15.64s/it] 10%|▉         | 49/500 [12:59<1:58:41, 15.79s/it] 10%|█         | 50/500 [13:15<1:59:00, 15.87s/it] 10%|█         | 51/500 [13:30<1:57:52, 15.75s/it] 10%|█         | 51/500 [13:31<1:59:01, 15.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.319 MB uploadedwandb: | 0.020 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb: \ 0.319 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▅▄▅▄▆██▇▇▅▄▅▅▃▃▄▃▄▅▃▅▄▄▃▂▂▃▃▃▃▂▃▃▃▄▃▃▃
wandb:     train_loss ▅▄▄▄▄▂▃▃▃▃▄▃▃▃▂▃▂▂▅▂▁▄▃██▄▆▃▁▂▁▅▇▁█▅▅▄▂▄
wandb:   val_accuracy ▃▂▆▅█▆▆▆▆▇▇▅▃▅▄▁▄▄▄▃▅▄▄▄▄▃▃▂▃▄▄▃▁▃▂▃▄▃▃▃
wandb:       val_loss ▃▃▃▂▂▃▃▂▂▂▃▄▂▂▂▂▁▆▂▆▂▁▁▂▁▂▂▁▂▃▂▇▄▄▂▁▆█▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.42051
wandb:     train_loss 1.2059
wandb:   val_accuracy 0.38889
wandb:       val_loss 0.79023
wandb: 
wandb: 🚀 View run azure-paper-2572 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ilyvrvby
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_191226-ilyvrvby/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_192641-to81y2i1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-lion-2574
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/to81y2i1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:08, 17.45s/it]  0%|          | 2/500 [00:32<2:15:16, 16.30s/it]  1%|          | 3/500 [00:48<2:13:41, 16.14s/it]  1%|          | 4/500 [01:04<2:11:43, 15.93s/it]  1%|          | 5/500 [01:20<2:11:10, 15.90s/it]  1%|          | 6/500 [01:35<2:10:00, 15.79s/it]  1%|▏         | 7/500 [01:51<2:09:28, 15.76s/it]  2%|▏         | 8/500 [02:07<2:10:23, 15.90s/it]  2%|▏         | 9/500 [02:23<2:09:22, 15.81s/it]  2%|▏         | 10/500 [02:39<2:08:35, 15.75s/it]  2%|▏         | 11/500 [02:54<2:07:22, 15.63s/it]  2%|▏         | 12/500 [03:10<2:07:13, 15.64s/it]  3%|▎         | 13/500 [03:25<2:07:22, 15.69s/it]  3%|▎         | 14/500 [03:42<2:08:08, 15.82s/it]  3%|▎         | 15/500 [03:58<2:09:43, 16.05s/it]  3%|▎         | 16/500 [04:14<2:09:12, 16.02s/it]  3%|▎         | 17/500 [04:31<2:10:20, 16.19s/it]  4%|▎         | 18/500 [04:46<2:08:35, 16.01s/it]  4%|▍         | 19/500 [05:01<2:06:32, 15.79s/it]  4%|▍         | 20/500 [05:18<2:07:00, 15.88s/it]  4%|▍         | 21/500 [05:33<2:05:56, 15.78s/it]  4%|▍         | 22/500 [05:49<2:05:24, 15.74s/it]  5%|▍         | 23/500 [06:05<2:05:50, 15.83s/it]  5%|▍         | 24/500 [06:20<2:04:59, 15.75s/it]  5%|▌         | 25/500 [06:36<2:05:07, 15.81s/it]  5%|▌         | 26/500 [06:52<2:05:29, 15.88s/it]  5%|▌         | 27/500 [07:09<2:05:48, 15.96s/it]  6%|▌         | 28/500 [07:24<2:04:56, 15.88s/it]  6%|▌         | 29/500 [07:40<2:04:50, 15.90s/it]  6%|▌         | 30/500 [07:55<2:03:08, 15.72s/it]  6%|▌         | 31/500 [08:12<2:03:57, 15.86s/it]  6%|▋         | 32/500 [08:28<2:05:15, 16.06s/it]  7%|▋         | 33/500 [08:44<2:03:57, 15.93s/it]  7%|▋         | 34/500 [09:00<2:05:05, 16.11s/it]  7%|▋         | 35/500 [09:17<2:05:51, 16.24s/it]  7%|▋         | 36/500 [09:34<2:07:02, 16.43s/it]  7%|▋         | 37/500 [09:52<2:11:32, 17.05s/it]  8%|▊         | 38/500 [10:10<2:13:30, 17.34s/it]  8%|▊         | 39/500 [10:26<2:10:22, 16.97s/it]  8%|▊         | 40/500 [10:43<2:08:33, 16.77s/it]  8%|▊         | 41/500 [11:00<2:09:02, 16.87s/it]  8%|▊         | 42/500 [11:18<2:11:17, 17.20s/it]  9%|▊         | 43/500 [11:34<2:09:10, 16.96s/it]  9%|▊         | 43/500 [11:34<2:03:02, 16.15s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.020 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▂▂▂▃▅▁▃▅▅▅▅▄▅▄▃▄▄▅▄▄▂▄▄█▇▁▃▃▇▇▄▂▂▃▆▄▇▆▄▇
wandb:     train_loss ▂▂▁▂▁▂▂▁▂▁▂▃▃▁▁▃▁▂▁▁▃▁▁▁▁▅▅█▂▁▂▂▆█▁▁▁▁▁▁
wandb:   val_accuracy ▃▃▃▅█▁▅█▆▆▆▅▅▅▄▅▅▅▅▅▄▅▅▆▆▃▄▅▆▆▅▄▄▆▆▆▆▆▆▆
wandb:       val_loss ▂▃▃▂▂▂▂▂▂▃▂▄▃▅▂▃▂▁▃▁█▇▃▃▁▁▁▂▃▄▂▄▆█▄▂▃▇▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.78158
wandb:     train_loss 0.00125
wandb:   val_accuracy 0.55556
wandb:       val_loss 2.56364
wandb: 
wandb: 🚀 View run serene-lion-2574 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/to81y2i1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_192641-to81y2i1/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_193903-j10q3aat
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sun-2576
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j10q3aat
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:43, 26.42s/it]  0%|          | 2/500 [00:51<3:30:23, 25.35s/it]  1%|          | 3/500 [01:15<3:27:16, 25.02s/it]  1%|          | 4/500 [01:40<3:24:53, 24.78s/it]  1%|          | 5/500 [02:04<3:22:38, 24.56s/it]  1%|          | 6/500 [02:28<3:22:07, 24.55s/it]  1%|▏         | 7/500 [02:53<3:21:41, 24.55s/it]  2%|▏         | 8/500 [03:18<3:22:24, 24.68s/it]  2%|▏         | 9/500 [03:42<3:21:43, 24.65s/it]  2%|▏         | 10/500 [04:07<3:21:02, 24.62s/it]  2%|▏         | 11/500 [04:32<3:21:49, 24.76s/it]  2%|▏         | 12/500 [04:57<3:21:38, 24.79s/it]  3%|▎         | 13/500 [05:22<3:21:11, 24.79s/it]  3%|▎         | 14/500 [05:46<3:20:31, 24.76s/it]  3%|▎         | 15/500 [06:12<3:21:12, 24.89s/it]  3%|▎         | 16/500 [06:36<3:19:48, 24.77s/it]  3%|▎         | 17/500 [07:01<3:19:12, 24.75s/it]  4%|▎         | 18/500 [07:25<3:18:29, 24.71s/it]  4%|▍         | 19/500 [07:50<3:18:05, 24.71s/it]  4%|▍         | 20/500 [08:14<3:16:46, 24.60s/it]  4%|▍         | 21/500 [08:38<3:15:13, 24.45s/it]  4%|▍         | 22/500 [09:03<3:13:52, 24.34s/it]  5%|▍         | 23/500 [09:28<3:15:21, 24.57s/it]  5%|▍         | 24/500 [09:53<3:16:16, 24.74s/it]  5%|▌         | 25/500 [10:18<3:15:59, 24.76s/it]  5%|▌         | 26/500 [10:42<3:15:16, 24.72s/it]  5%|▌         | 27/500 [11:07<3:14:15, 24.64s/it]  6%|▌         | 28/500 [11:31<3:13:17, 24.57s/it]  6%|▌         | 29/500 [11:55<3:11:39, 24.41s/it]  6%|▌         | 30/500 [12:19<3:10:12, 24.28s/it]  6%|▌         | 30/500 [12:19<3:13:07, 24.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▇▁▁▂▁▇▂▇▅▇▅▆▅▅█▅▇▄█▇▅▅▇▇█▇▆█
wandb:     train_loss ▅▅▃▂▆▁▇▁██▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▆▂▂▄▁▇▄▆▅▇▅▇█▅▆▆▆▅█▆▆▆▇▆█▆▆▇
wandb:       val_loss ▄▂▂▃▂▂▄▁▁▇▄▁▅▁█▁▃▄▁▄▄▁▃▁▂▅▁▁▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.95542
wandb:     train_loss 0.05652
wandb:   val_accuracy 0.71556
wandb:       val_loss 0.62161
wandb: 
wandb: 🚀 View run deft-sun-2576 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j10q3aat
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_193903-j10q3aat/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_195209-7dip5y7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-resonance-2578
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7dip5y7z
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:34, 26.40s/it]  0%|          | 2/500 [00:50<3:27:15, 24.97s/it]  1%|          | 3/500 [01:14<3:24:00, 24.63s/it]  1%|          | 4/500 [01:38<3:21:22, 24.36s/it]  1%|          | 5/500 [02:02<3:19:58, 24.24s/it]  1%|          | 6/500 [02:26<3:19:36, 24.24s/it]  1%|▏         | 7/500 [02:50<3:17:37, 24.05s/it]  2%|▏         | 8/500 [03:14<3:16:16, 23.94s/it]  2%|▏         | 9/500 [03:37<3:14:51, 23.81s/it]  2%|▏         | 10/500 [04:01<3:13:28, 23.69s/it]  2%|▏         | 11/500 [04:24<3:12:34, 23.63s/it]  2%|▏         | 12/500 [04:48<3:13:24, 23.78s/it]  3%|▎         | 13/500 [05:12<3:13:09, 23.80s/it]  3%|▎         | 14/500 [05:36<3:12:32, 23.77s/it]  3%|▎         | 15/500 [06:00<3:12:19, 23.79s/it]  3%|▎         | 16/500 [06:23<3:11:24, 23.73s/it]  3%|▎         | 17/500 [06:47<3:12:07, 23.87s/it]  4%|▎         | 18/500 [07:12<3:12:24, 23.95s/it]  4%|▍         | 19/500 [07:35<3:11:54, 23.94s/it]  4%|▍         | 20/500 [07:59<3:11:43, 23.97s/it]  4%|▍         | 21/500 [08:23<3:10:39, 23.88s/it]  4%|▍         | 22/500 [08:47<3:09:31, 23.79s/it]  5%|▍         | 23/500 [09:10<3:08:51, 23.75s/it]  5%|▍         | 24/500 [09:34<3:08:08, 23.72s/it]  5%|▌         | 25/500 [09:58<3:07:55, 23.74s/it]  5%|▌         | 26/500 [10:22<3:07:50, 23.78s/it]  5%|▌         | 27/500 [10:45<3:07:21, 23.77s/it]  6%|▌         | 28/500 [11:10<3:07:52, 23.88s/it]  6%|▌         | 29/500 [11:33<3:07:23, 23.87s/it]  6%|▌         | 30/500 [11:57<3:06:26, 23.80s/it]  6%|▌         | 31/500 [12:21<3:05:27, 23.73s/it]  6%|▋         | 32/500 [12:44<3:04:57, 23.71s/it]  7%|▋         | 33/500 [13:08<3:04:01, 23.64s/it]  7%|▋         | 34/500 [13:31<3:03:19, 23.60s/it]  7%|▋         | 35/500 [13:55<3:03:31, 23.68s/it]  7%|▋         | 36/500 [14:19<3:03:25, 23.72s/it]  7%|▋         | 37/500 [14:43<3:03:38, 23.80s/it]  8%|▊         | 38/500 [15:07<3:03:04, 23.78s/it]  8%|▊         | 39/500 [15:30<3:01:52, 23.67s/it]  8%|▊         | 40/500 [15:54<3:01:35, 23.69s/it]  8%|▊         | 41/500 [16:18<3:02:17, 23.83s/it]  8%|▊         | 42/500 [16:42<3:02:28, 23.91s/it]  9%|▊         | 43/500 [17:06<3:01:55, 23.88s/it]  9%|▉         | 44/500 [17:30<3:02:45, 24.05s/it]  9%|▉         | 44/500 [17:30<3:01:30, 23.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb: train_accuracy ▁▃▂▃▅▅▅▁▆▆▇▇▇▇▅▇▇▇▇▇▇▇▇█▆▇▇▇█▇█▇▇███████
wandb:     train_loss ▄▅▅▅▅▄▅█▅▄▄▄▂▁▁▂▂▂▁▁▃▂▃▃▃▂▁▂▃▁▃▂▃▁▂▁▂▁▁▂
wandb:   val_accuracy ▁▃▂▃▅▄▄▁▅▆▇▆▇▆▅▇▇██▇███▇▅█▆█▇█▇█▇▇█▇▇▇▇█
wandb:       val_loss ▅▆▅▅▄▄▆▆▅▄▃▄▄▃█▄▁▃█▇█▃▅▂▄▁▇▄▂▄▃▂▅▁▁▃▄█▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 43
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.90193
wandb:     train_loss 0.1926
wandb:   val_accuracy 0.62444
wandb:       val_loss 0.97101
wandb: 
wandb: 🚀 View run dandy-resonance-2578 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7dip5y7z
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_195209-7dip5y7z/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_201024-7o8v3qyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-serenity-2579
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7o8v3qyg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:32, 25.80s/it]  0%|          | 2/500 [00:50<3:26:25, 24.87s/it]  1%|          | 3/500 [01:14<3:26:05, 24.88s/it]  1%|          | 4/500 [01:39<3:24:30, 24.74s/it]  1%|          | 5/500 [02:03<3:21:32, 24.43s/it]  1%|          | 6/500 [02:27<3:19:25, 24.22s/it]  1%|▏         | 7/500 [02:51<3:18:38, 24.18s/it]  2%|▏         | 8/500 [03:14<3:17:06, 24.04s/it]  2%|▏         | 9/500 [03:38<3:15:58, 23.95s/it]  2%|▏         | 10/500 [04:02<3:15:28, 23.94s/it]  2%|▏         | 11/500 [04:26<3:15:35, 24.00s/it]  2%|▏         | 12/500 [04:50<3:14:40, 23.93s/it]  3%|▎         | 13/500 [05:14<3:13:38, 23.86s/it]  3%|▎         | 14/500 [05:37<3:12:44, 23.79s/it]  3%|▎         | 15/500 [06:01<3:12:35, 23.83s/it]  3%|▎         | 16/500 [06:26<3:14:18, 24.09s/it]  3%|▎         | 17/500 [06:50<3:12:45, 23.94s/it]  4%|▎         | 18/500 [07:13<3:12:17, 23.94s/it]  4%|▍         | 19/500 [07:38<3:13:22, 24.12s/it]  4%|▍         | 20/500 [08:02<3:12:48, 24.10s/it]  4%|▍         | 21/500 [08:26<3:13:01, 24.18s/it]  4%|▍         | 22/500 [08:50<3:12:05, 24.11s/it]  5%|▍         | 23/500 [09:15<3:12:02, 24.16s/it]  5%|▍         | 24/500 [09:38<3:10:47, 24.05s/it]  5%|▌         | 25/500 [10:03<3:10:22, 24.05s/it]  5%|▌         | 26/500 [10:26<3:09:30, 23.99s/it]  5%|▌         | 27/500 [10:50<3:07:57, 23.84s/it]  6%|▌         | 28/500 [11:14<3:07:34, 23.84s/it]  6%|▌         | 29/500 [11:38<3:07:30, 23.89s/it]  6%|▌         | 30/500 [12:02<3:07:03, 23.88s/it]  6%|▌         | 31/500 [12:26<3:08:21, 24.10s/it]  6%|▋         | 32/500 [12:50<3:06:53, 23.96s/it]  7%|▋         | 33/500 [13:13<3:05:46, 23.87s/it]  7%|▋         | 34/500 [13:38<3:05:47, 23.92s/it]  7%|▋         | 35/500 [14:01<3:05:03, 23.88s/it]  7%|▋         | 36/500 [14:25<3:04:16, 23.83s/it]  7%|▋         | 37/500 [14:49<3:03:20, 23.76s/it]  8%|▊         | 38/500 [15:13<3:03:31, 23.84s/it]  8%|▊         | 39/500 [15:36<3:03:08, 23.84s/it]  8%|▊         | 40/500 [16:00<3:02:35, 23.82s/it]  8%|▊         | 41/500 [16:24<3:01:47, 23.76s/it]  8%|▊         | 42/500 [16:48<3:01:21, 23.76s/it]  9%|▊         | 43/500 [17:11<3:00:59, 23.76s/it]  9%|▉         | 44/500 [17:35<3:00:45, 23.78s/it]  9%|▉         | 44/500 [17:35<3:02:21, 23.99s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.232 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb: train_accuracy ▂▂▅▃▅▂▁▄▄▆▄▅▆▆▆▆███▇████████████████████
wandb:     train_loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▄▄▇▁▁▄▅▅▆▆▆▆▇▇▇▇▇█▇█▇█▇▇█▇▇█▇▇█▇▇▇▇▇▇▇
wandb:       val_loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 43
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.99851
wandb:     train_loss 0.0139
wandb:   val_accuracy 0.72667
wandb:       val_loss 0.39609
wandb: 
wandb: 🚀 View run ethereal-serenity-2579 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7o8v3qyg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_201024-7o8v3qyg/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_202849-29xjc729
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-grass-2580
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29xjc729
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:37, 26.29s/it]  0%|          | 2/500 [00:50<3:29:53, 25.29s/it]  1%|          | 3/500 [01:15<3:25:39, 24.83s/it]  1%|          | 4/500 [01:39<3:22:25, 24.49s/it]  1%|          | 5/500 [02:03<3:20:57, 24.36s/it]  1%|          | 6/500 [02:27<3:19:50, 24.27s/it]  1%|▏         | 7/500 [02:51<3:17:58, 24.09s/it]  2%|▏         | 8/500 [03:14<3:16:40, 23.99s/it]  2%|▏         | 9/500 [03:38<3:16:11, 23.97s/it]  2%|▏         | 10/500 [04:02<3:15:18, 23.92s/it]  2%|▏         | 11/500 [04:26<3:14:18, 23.84s/it]  2%|▏         | 12/500 [04:50<3:14:02, 23.86s/it]  3%|▎         | 13/500 [05:13<3:13:24, 23.83s/it]  3%|▎         | 14/500 [05:37<3:12:42, 23.79s/it]  3%|▎         | 15/500 [06:01<3:13:35, 23.95s/it]  3%|▎         | 16/500 [06:25<3:12:26, 23.86s/it]  3%|▎         | 17/500 [06:49<3:11:54, 23.84s/it]  4%|▎         | 18/500 [07:13<3:11:13, 23.80s/it]  4%|▍         | 19/500 [07:37<3:11:49, 23.93s/it]  4%|▍         | 20/500 [08:00<3:10:19, 23.79s/it]  4%|▍         | 21/500 [08:25<3:11:04, 23.93s/it]  4%|▍         | 22/500 [08:48<3:10:39, 23.93s/it]  5%|▍         | 23/500 [09:12<3:10:26, 23.95s/it]  5%|▍         | 24/500 [09:36<3:09:52, 23.93s/it]  5%|▌         | 25/500 [10:00<3:09:04, 23.88s/it]  5%|▌         | 26/500 [10:24<3:08:40, 23.88s/it]  5%|▌         | 27/500 [10:48<3:08:09, 23.87s/it]  6%|▌         | 28/500 [11:12<3:07:42, 23.86s/it]  6%|▌         | 29/500 [11:36<3:07:15, 23.85s/it]  6%|▌         | 30/500 [12:00<3:09:02, 24.13s/it]  6%|▌         | 31/500 [12:24<3:07:30, 23.99s/it]  6%|▋         | 32/500 [12:48<3:06:40, 23.93s/it]  7%|▋         | 33/500 [13:11<3:05:32, 23.84s/it]  7%|▋         | 34/500 [13:35<3:04:52, 23.80s/it]  7%|▋         | 35/500 [13:59<3:04:32, 23.81s/it]  7%|▋         | 36/500 [14:22<3:03:21, 23.71s/it]  7%|▋         | 37/500 [14:46<3:02:25, 23.64s/it]  8%|▊         | 38/500 [15:09<3:01:45, 23.60s/it]  8%|▊         | 39/500 [15:33<3:01:56, 23.68s/it]  8%|▊         | 40/500 [15:57<3:01:44, 23.70s/it]  8%|▊         | 40/500 [15:57<3:03:31, 23.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.020 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▂▆▁▇▅▄▇█▇▇▇▆▅▇████▇███▇▇██████▇███▇███
wandb:     train_loss ▃▁▁▁█▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▃▄▃▄▅▅▅▆▇▇▇▇▅▆▆█▇▇████▆▇█▇███▇▆████▇▇▇
wandb:       val_loss ▃▆▃█▁▁▃▂▁▃▁▁▃▂▂▄▂▁▁▇▅▁▄▁▃▃▃▁▂▄▁▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 0.00041
wandb: train_accuracy 1.0
wandb:     train_loss 0.00069
wandb:   val_accuracy 0.81778
wandb:       val_loss 0.12131
wandb: 
wandb: 🚀 View run rare-grass-2580 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29xjc729
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_202849-29xjc729/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_204533-4dcmqo4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-planet-2582
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4dcmqo4r
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:13, 26.24s/it]  0%|          | 2/500 [00:50<3:25:59, 24.82s/it]  1%|          | 3/500 [01:14<3:23:25, 24.56s/it]  1%|          | 4/500 [01:38<3:21:22, 24.36s/it]  1%|          | 5/500 [02:02<3:20:46, 24.34s/it]  1%|          | 6/500 [02:26<3:19:28, 24.23s/it]  1%|▏         | 7/500 [02:51<3:19:27, 24.27s/it]  2%|▏         | 8/500 [03:15<3:20:35, 24.46s/it]  2%|▏         | 9/500 [03:39<3:18:58, 24.31s/it]  2%|▏         | 10/500 [04:03<3:17:32, 24.19s/it]  2%|▏         | 11/500 [04:27<3:15:43, 24.01s/it]  2%|▏         | 12/500 [04:51<3:15:30, 24.04s/it]  3%|▎         | 13/500 [05:15<3:14:30, 23.96s/it]  3%|▎         | 14/500 [05:39<3:14:14, 23.98s/it]  3%|▎         | 15/500 [06:03<3:13:51, 23.98s/it]  3%|▎         | 16/500 [06:26<3:12:03, 23.81s/it]  3%|▎         | 17/500 [06:50<3:11:03, 23.73s/it]  4%|▎         | 18/500 [07:13<3:10:26, 23.71s/it]  4%|▍         | 19/500 [07:37<3:10:16, 23.74s/it]  4%|▍         | 20/500 [08:01<3:09:40, 23.71s/it]  4%|▍         | 21/500 [08:24<3:09:01, 23.68s/it]  4%|▍         | 22/500 [08:49<3:10:17, 23.89s/it]  5%|▍         | 23/500 [09:13<3:09:55, 23.89s/it]  5%|▍         | 24/500 [09:37<3:09:35, 23.90s/it]  5%|▌         | 25/500 [10:01<3:09:09, 23.89s/it]  5%|▌         | 26/500 [10:24<3:08:48, 23.90s/it]  5%|▌         | 27/500 [10:48<3:07:37, 23.80s/it]  6%|▌         | 28/500 [11:12<3:07:46, 23.87s/it]  6%|▌         | 29/500 [11:36<3:07:37, 23.90s/it]  6%|▌         | 30/500 [12:00<3:07:16, 23.91s/it]  6%|▌         | 31/500 [12:24<3:08:00, 24.05s/it]  6%|▋         | 32/500 [12:48<3:07:38, 24.06s/it]  7%|▋         | 33/500 [13:12<3:06:26, 23.95s/it]  7%|▋         | 34/500 [13:36<3:05:41, 23.91s/it]  7%|▋         | 35/500 [14:00<3:05:00, 23.87s/it]  7%|▋         | 36/500 [14:24<3:04:25, 23.85s/it]  7%|▋         | 37/500 [14:47<3:04:15, 23.88s/it]  8%|▊         | 38/500 [15:11<3:03:36, 23.85s/it]  8%|▊         | 39/500 [15:35<3:02:22, 23.74s/it]  8%|▊         | 40/500 [15:59<3:02:33, 23.81s/it]  8%|▊         | 41/500 [16:23<3:02:48, 23.90s/it]  8%|▊         | 42/500 [16:46<3:01:42, 23.80s/it]  9%|▊         | 43/500 [17:10<3:00:35, 23.71s/it]  9%|▉         | 44/500 [17:34<3:00:06, 23.70s/it]  9%|▉         | 45/500 [17:57<2:59:15, 23.64s/it]  9%|▉         | 46/500 [18:21<2:59:05, 23.67s/it]  9%|▉         | 47/500 [18:44<2:58:20, 23.62s/it] 10%|▉         | 48/500 [19:08<2:58:18, 23.67s/it] 10%|▉         | 49/500 [19:32<2:57:46, 23.65s/it] 10%|█         | 50/500 [19:55<2:57:22, 23.65s/it] 10%|█         | 51/500 [20:19<2:56:44, 23.62s/it] 10%|█         | 51/500 [20:19<2:58:55, 23.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.139 MB of 0.314 MB uploadedwandb: - 0.139 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▃▃▆▆▅▄▆▆▆▆▇▆▆▆▇▆▆▆▇▇▇█▇▇▇██▇█▇▇▇▇█▁▆█▇
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▂
wandb:   val_accuracy ▁▂▂▃▄▄▄▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇██
wandb:       val_loss ▁▂▁▁▁▁▁▂▁▂▁▁▁▁▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▂▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.8737
wandb:     train_loss 1.97907
wandb:   val_accuracy 0.73778
wandb:       val_loss 0.81351
wandb: 
wandb: 🚀 View run stoic-planet-2582 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4dcmqo4r
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_204533-4dcmqo4r/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_210636-7tv1oeuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-bird-2584
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7tv1oeuw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:40:14, 26.48s/it]  0%|          | 2/500 [00:50<3:27:35, 25.01s/it]  1%|          | 3/500 [01:15<3:25:44, 24.84s/it]  1%|          | 4/500 [01:39<3:22:55, 24.55s/it]  1%|          | 5/500 [02:03<3:20:54, 24.35s/it]  1%|          | 6/500 [02:27<3:19:49, 24.27s/it]  1%|▏         | 7/500 [02:50<3:16:51, 23.96s/it]  2%|▏         | 8/500 [03:14<3:15:13, 23.81s/it]  2%|▏         | 9/500 [03:37<3:14:47, 23.80s/it]  2%|▏         | 10/500 [04:01<3:13:38, 23.71s/it]  2%|▏         | 11/500 [04:24<3:12:38, 23.64s/it]  2%|▏         | 12/500 [04:49<3:13:49, 23.83s/it]  3%|▎         | 13/500 [05:13<3:13:30, 23.84s/it]  3%|▎         | 14/500 [05:37<3:14:20, 23.99s/it]  3%|▎         | 15/500 [06:01<3:13:46, 23.97s/it]  3%|▎         | 16/500 [06:24<3:12:42, 23.89s/it]  3%|▎         | 17/500 [06:48<3:11:27, 23.78s/it]  4%|▎         | 18/500 [07:12<3:10:30, 23.71s/it]  4%|▍         | 19/500 [07:35<3:09:53, 23.69s/it]  4%|▍         | 20/500 [07:59<3:09:16, 23.66s/it]  4%|▍         | 21/500 [08:23<3:10:20, 23.84s/it]  4%|▍         | 22/500 [08:47<3:11:02, 23.98s/it]  5%|▍         | 23/500 [09:11<3:10:44, 23.99s/it]  5%|▍         | 24/500 [09:35<3:09:50, 23.93s/it]  5%|▌         | 25/500 [09:59<3:08:45, 23.84s/it]  5%|▌         | 26/500 [10:22<3:07:58, 23.79s/it]  5%|▌         | 27/500 [10:47<3:08:54, 23.96s/it]  6%|▌         | 28/500 [11:11<3:08:22, 23.95s/it]  6%|▌         | 29/500 [11:34<3:06:56, 23.82s/it]  6%|▌         | 30/500 [11:58<3:06:28, 23.81s/it]  6%|▌         | 31/500 [12:22<3:06:02, 23.80s/it]  6%|▋         | 32/500 [12:46<3:06:15, 23.88s/it]  7%|▋         | 33/500 [13:10<3:06:27, 23.96s/it]  7%|▋         | 34/500 [13:34<3:05:05, 23.83s/it]  7%|▋         | 35/500 [13:57<3:04:09, 23.76s/it]  7%|▋         | 36/500 [14:21<3:03:30, 23.73s/it]  7%|▋         | 37/500 [14:45<3:03:01, 23.72s/it]  8%|▊         | 38/500 [15:08<3:02:09, 23.66s/it]  8%|▊         | 39/500 [15:32<3:01:52, 23.67s/it]  8%|▊         | 40/500 [15:55<3:01:34, 23.68s/it]  8%|▊         | 40/500 [15:56<3:03:14, 23.90s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.232 MB of 0.312 MB uploadedwandb: - 0.232 MB of 0.312 MB uploadedwandb: \ 0.232 MB of 0.312 MB uploadedwandb: | 0.232 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▅▅▅▅▃▅▇▇▃██████▇▇█▇█▆██▆█▇▇████▇████▇██
wandb:     train_loss ▇▆▂▂▅▁▄▄▇█▂▂▂▁▁▁▁▁▂▁▁▃▁▁▆▁▂▁▁▃▁▁▄▂▁▃▁▂▁▁
wandb:   val_accuracy ▁▅▆▅▅▄▇▆▇▆███▇▇█▇▇█▇▇▇██▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇
wandb:       val_loss ▂▂▁▂▁▂▂▁█▁▂▁▂▁▁▂▄▁▁▄▂▁▃▁▂▁▂▁▄▄▁▁▁▃▂▃▁▁▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.96434
wandb:     train_loss 0.09104
wandb:   val_accuracy 0.65778
wandb:       val_loss 2.7962
wandb: 
wandb: 🚀 View run wandering-bird-2584 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7tv1oeuw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_210636-7tv1oeuw/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_212311-25kcf4gu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-waterfall-2586
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/25kcf4gu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:41, 25.69s/it]  0%|          | 2/500 [00:49<3:23:06, 24.47s/it]  1%|          | 3/500 [01:14<3:25:17, 24.78s/it]  1%|          | 4/500 [01:38<3:21:27, 24.37s/it]  1%|          | 5/500 [02:02<3:19:38, 24.20s/it]  1%|          | 6/500 [02:26<3:19:07, 24.18s/it]  1%|▏         | 7/500 [02:49<3:16:59, 23.98s/it]  2%|▏         | 8/500 [03:13<3:16:03, 23.91s/it]  2%|▏         | 9/500 [03:37<3:15:32, 23.90s/it]  2%|▏         | 10/500 [04:01<3:15:53, 23.99s/it]  2%|▏         | 11/500 [04:25<3:14:43, 23.89s/it]  2%|▏         | 12/500 [04:49<3:13:55, 23.84s/it]  3%|▎         | 13/500 [05:12<3:12:36, 23.73s/it]  3%|▎         | 14/500 [05:36<3:11:58, 23.70s/it]  3%|▎         | 15/500 [05:59<3:11:16, 23.66s/it]  3%|▎         | 16/500 [06:23<3:10:44, 23.64s/it]  3%|▎         | 17/500 [06:47<3:10:49, 23.70s/it]  4%|▎         | 18/500 [07:10<3:09:55, 23.64s/it]  4%|▍         | 19/500 [07:34<3:09:39, 23.66s/it]  4%|▍         | 20/500 [07:57<3:08:57, 23.62s/it]  4%|▍         | 21/500 [08:21<3:08:05, 23.56s/it]  4%|▍         | 22/500 [08:44<3:07:58, 23.59s/it]  5%|▍         | 23/500 [09:09<3:08:55, 23.76s/it]  5%|▍         | 24/500 [09:32<3:08:12, 23.72s/it]  5%|▌         | 25/500 [09:56<3:07:27, 23.68s/it]  5%|▌         | 26/500 [10:19<3:06:31, 23.61s/it]  5%|▌         | 27/500 [10:43<3:05:47, 23.57s/it]  6%|▌         | 28/500 [11:06<3:05:34, 23.59s/it]  6%|▌         | 29/500 [11:30<3:05:26, 23.62s/it]  6%|▌         | 30/500 [11:54<3:04:45, 23.59s/it]  6%|▌         | 31/500 [12:17<3:04:35, 23.61s/it]  6%|▋         | 32/500 [12:41<3:05:01, 23.72s/it]  7%|▋         | 33/500 [13:05<3:04:02, 23.65s/it]  7%|▋         | 34/500 [13:28<3:03:57, 23.69s/it]  7%|▋         | 35/500 [13:52<3:03:39, 23.70s/it]  7%|▋         | 36/500 [14:16<3:02:41, 23.62s/it]  7%|▋         | 37/500 [14:39<3:02:25, 23.64s/it]  8%|▊         | 38/500 [15:03<3:02:36, 23.71s/it]  8%|▊         | 39/500 [15:28<3:03:49, 23.92s/it]  8%|▊         | 40/500 [15:51<3:03:00, 23.87s/it]  8%|▊         | 40/500 [15:52<3:02:29, 23.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.020 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▅▃▆▆▂▆▇▆▇▆▇▇█▇▇▇▇▇██▇▆██▇▇██▇▇█▇▇█▇██
wandb:     train_loss ▄▂▁▁▇▁▁█▁▃▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▅▄▆▃▄▄▄▄▅▆▆▇█▇▇▇▇▆█▆▆▇▆██▆▇█▇▇▇▆█▇▆▇▇▇
wandb:       val_loss ▃▃▁▅▁▁▄▃▁▅▁▁▃▂▁▁▁▂▁▇▆▁█▁▆▂▄▁▁▂▁▆▁▁▁▁▃▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.94354
wandb:     train_loss 7e-05
wandb:   val_accuracy 0.78667
wandb:       val_loss 0.01375
wandb: 
wandb: 🚀 View run classic-waterfall-2586 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/25kcf4gu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_212311-25kcf4gu/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_213943-29fwwg3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-deluge-2588
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29fwwg3d
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:49:33, 27.60s/it]  0%|          | 2/500 [00:51<3:31:19, 25.46s/it]  1%|          | 3/500 [01:16<3:28:15, 25.14s/it]  1%|          | 4/500 [01:40<3:25:31, 24.86s/it]  1%|          | 5/500 [02:05<3:24:31, 24.79s/it]  1%|          | 6/500 [02:29<3:22:52, 24.64s/it]  1%|▏         | 7/500 [02:54<3:22:54, 24.69s/it]  2%|▏         | 8/500 [03:18<3:21:06, 24.53s/it]  2%|▏         | 9/500 [03:43<3:20:58, 24.56s/it]  2%|▏         | 10/500 [04:07<3:18:43, 24.33s/it]  2%|▏         | 11/500 [04:30<3:16:24, 24.10s/it]  2%|▏         | 12/500 [04:54<3:14:53, 23.96s/it]  3%|▎         | 13/500 [05:17<3:13:05, 23.79s/it]  3%|▎         | 14/500 [05:41<3:12:31, 23.77s/it]  3%|▎         | 15/500 [06:05<3:11:46, 23.73s/it]  3%|▎         | 16/500 [06:29<3:11:44, 23.77s/it]  3%|▎         | 17/500 [06:52<3:11:02, 23.73s/it]  4%|▎         | 18/500 [07:16<3:10:47, 23.75s/it]  4%|▍         | 19/500 [07:40<3:10:31, 23.77s/it]  4%|▍         | 20/500 [08:03<3:09:34, 23.70s/it]  4%|▍         | 21/500 [08:27<3:09:03, 23.68s/it]  4%|▍         | 22/500 [08:51<3:08:37, 23.68s/it]  5%|▍         | 23/500 [09:14<3:07:56, 23.64s/it]  5%|▍         | 24/500 [09:38<3:07:25, 23.62s/it]  5%|▌         | 25/500 [10:01<3:06:19, 23.54s/it]  5%|▌         | 25/500 [10:01<3:10:30, 24.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.020 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▆█▂▇▆▆▆█▆▇█▇▆▆█▅▄▇▅▄▄▅
wandb:     train_loss ▄▄▄▄▅█▃▅▆▅▄▄▄▁▁▇█▄▃▂▇▃▄▄▃
wandb:   val_accuracy ▂▆▄█▇▁▆▃▆▅▅▂▆▆▄▃▄▇▂▄▅▃▃▄▂
wandb:       val_loss ▃▄▂▃▁▇▃▅▁▄▄▂▂▂▄▁▂▄▂█▆▁█▂█
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.48143
wandb:     train_loss 0.8343
wandb:   val_accuracy 0.34222
wandb:       val_loss 1.62011
wandb: 
wandb: 🚀 View run dazzling-deluge-2588 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29fwwg3d
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_213943-29fwwg3d/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_215030-jkfqga6i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-pyramid-2589
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jkfqga6i
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<4:00:11, 28.88s/it]  0%|          | 2/500 [00:53<3:39:55, 26.50s/it]  1%|          | 3/500 [01:18<3:32:57, 25.71s/it]  1%|          | 4/500 [01:42<3:27:41, 25.12s/it]  1%|          | 5/500 [02:06<3:23:21, 24.65s/it]  1%|          | 6/500 [02:30<3:21:07, 24.43s/it]  1%|▏         | 7/500 [02:54<3:18:51, 24.20s/it]  2%|▏         | 8/500 [03:17<3:16:47, 24.00s/it]  2%|▏         | 9/500 [03:41<3:16:44, 24.04s/it]  2%|▏         | 10/500 [04:05<3:15:57, 24.00s/it]  2%|▏         | 11/500 [04:30<3:16:07, 24.06s/it]  2%|▏         | 12/500 [04:59<3:28:51, 25.68s/it]  3%|▎         | 13/500 [05:23<3:23:58, 25.13s/it]  3%|▎         | 14/500 [05:46<3:19:54, 24.68s/it]  3%|▎         | 15/500 [06:10<3:17:22, 24.42s/it]  3%|▎         | 16/500 [06:34<3:15:32, 24.24s/it]  3%|▎         | 17/500 [06:58<3:14:25, 24.15s/it]  4%|▎         | 18/500 [07:23<3:16:47, 24.50s/it]  4%|▍         | 19/500 [07:47<3:14:49, 24.30s/it]  4%|▍         | 20/500 [08:11<3:12:25, 24.05s/it]  4%|▍         | 21/500 [08:34<3:10:50, 23.91s/it]  4%|▍         | 22/500 [08:58<3:09:18, 23.76s/it]  5%|▍         | 23/500 [09:22<3:09:38, 23.85s/it]  5%|▍         | 24/500 [09:46<3:09:58, 23.95s/it]  5%|▌         | 25/500 [10:10<3:10:00, 24.00s/it]  5%|▌         | 26/500 [10:34<3:09:37, 24.00s/it]  5%|▌         | 27/500 [10:59<3:10:38, 24.18s/it]  6%|▌         | 28/500 [11:23<3:10:55, 24.27s/it]  6%|▌         | 29/500 [11:47<3:10:49, 24.31s/it]  6%|▌         | 30/500 [12:12<3:10:59, 24.38s/it]  6%|▌         | 31/500 [12:37<3:12:02, 24.57s/it]  6%|▋         | 32/500 [13:02<3:12:05, 24.63s/it]  7%|▋         | 33/500 [13:26<3:10:48, 24.52s/it]  7%|▋         | 34/500 [13:51<3:10:23, 24.51s/it]  7%|▋         | 35/500 [14:15<3:09:32, 24.46s/it]  7%|▋         | 36/500 [14:39<3:08:19, 24.35s/it]  7%|▋         | 37/500 [15:04<3:08:43, 24.46s/it]  8%|▊         | 38/500 [15:37<3:29:31, 27.21s/it]  8%|▊         | 39/500 [16:02<3:23:18, 26.46s/it]  8%|▊         | 40/500 [16:26<3:17:20, 25.74s/it]  8%|▊         | 41/500 [16:57<3:27:54, 27.18s/it]  8%|▊         | 42/500 [17:26<3:31:52, 27.76s/it]  9%|▊         | 43/500 [17:51<3:26:46, 27.15s/it]  9%|▉         | 44/500 [18:16<3:20:30, 26.38s/it]  9%|▉         | 45/500 [18:40<3:15:27, 25.77s/it]  9%|▉         | 46/500 [19:05<3:11:56, 25.37s/it]  9%|▉         | 47/500 [19:29<3:09:26, 25.09s/it] 10%|▉         | 48/500 [19:54<3:08:10, 24.98s/it] 10%|▉         | 49/500 [20:18<3:06:26, 24.80s/it] 10%|█         | 50/500 [20:43<3:05:07, 24.68s/it] 10%|█         | 51/500 [21:08<3:04:50, 24.70s/it] 10%|█         | 52/500 [21:33<3:06:28, 24.97s/it] 11%|█         | 53/500 [21:58<3:05:15, 24.87s/it] 11%|█         | 54/500 [22:23<3:05:40, 24.98s/it] 11%|█         | 55/500 [22:48<3:05:18, 24.98s/it] 11%|█         | 56/500 [23:12<3:03:34, 24.81s/it] 11%|█▏        | 57/500 [23:38<3:04:33, 25.00s/it] 12%|█▏        | 58/500 [24:03<3:05:08, 25.13s/it] 12%|█▏        | 59/500 [24:28<3:03:35, 24.98s/it] 12%|█▏        | 59/500 [24:28<3:02:55, 24.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.030 MB uploadedwandb: / 0.020 MB of 0.310 MB uploadedwandb: - 0.295 MB of 0.310 MB uploadedwandb: \ 0.295 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▆▁▃▂▄▂▃▂▂▃▁▇▃▃▅▃▅▇█▇██▇██▇▇▇████▇▇█▇▇
wandb:     train_loss ▃▂▃▂▁▃█▃▃▁▁█▁▅▃▁▃▁▆▁▁▁▂▁▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▂█▃▅▃█▅▇▆▇█▆▇██▇█▇▇▇▇█▇▇▇▇▇▆▇▇▇▇▇▇▆▇▇▆
wandb:       val_loss ▂▂▂▁▃▃▁▃▁▁▄▅▃█▅▆▁▄▅▄▂▂▁▄▂▁▁▄▄▂▄▂▄▁▁▁▅▂▇▆
wandb: 
wandb: Run summary:
wandb:          epoch 58
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.88113
wandb:     train_loss 0.00368
wandb:   val_accuracy 0.53778
wandb:       val_loss 4.32709
wandb: 
wandb: 🚀 View run lucky-pyramid-2589 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jkfqga6i
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_215030-jkfqga6i/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_221540-i5741574
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-bush-2591
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i5741574
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:22:34, 17.14s/it]  0%|          | 2/500 [00:37<2:35:41, 18.76s/it]  1%|          | 3/500 [00:52<2:23:37, 17.34s/it]  1%|          | 4/500 [01:08<2:17:07, 16.59s/it]  1%|          | 5/500 [01:24<2:15:11, 16.39s/it]  1%|          | 6/500 [01:40<2:13:28, 16.21s/it]  1%|▏         | 7/500 [01:55<2:12:10, 16.09s/it]  2%|▏         | 8/500 [02:11<2:10:14, 15.88s/it]  2%|▏         | 9/500 [02:26<2:09:01, 15.77s/it]  2%|▏         | 10/500 [02:42<2:08:14, 15.70s/it]  2%|▏         | 11/500 [02:58<2:08:17, 15.74s/it]  2%|▏         | 12/500 [03:13<2:08:04, 15.75s/it]  3%|▎         | 13/500 [03:29<2:08:25, 15.82s/it]  3%|▎         | 14/500 [03:45<2:07:36, 15.75s/it]  3%|▎         | 15/500 [04:01<2:06:59, 15.71s/it]  3%|▎         | 16/500 [04:17<2:07:46, 15.84s/it]  3%|▎         | 17/500 [04:32<2:06:53, 15.76s/it]  4%|▎         | 18/500 [04:52<2:16:58, 17.05s/it]  4%|▍         | 19/500 [05:08<2:12:56, 16.58s/it]  4%|▍         | 20/500 [05:23<2:10:00, 16.25s/it]  4%|▍         | 21/500 [05:43<2:18:26, 17.34s/it]  4%|▍         | 22/500 [05:59<2:13:56, 16.81s/it]  5%|▍         | 23/500 [06:14<2:10:51, 16.46s/it]  5%|▍         | 24/500 [06:30<2:08:43, 16.23s/it]  5%|▌         | 25/500 [06:46<2:07:37, 16.12s/it]  5%|▌         | 26/500 [07:02<2:06:01, 15.95s/it]  5%|▌         | 27/500 [07:17<2:04:43, 15.82s/it]  6%|▌         | 28/500 [07:33<2:04:03, 15.77s/it]  6%|▌         | 29/500 [07:50<2:08:17, 16.34s/it]  6%|▌         | 30/500 [08:06<2:06:20, 16.13s/it]  6%|▌         | 31/500 [08:22<2:04:58, 15.99s/it]  6%|▋         | 32/500 [08:37<2:03:32, 15.84s/it]  7%|▋         | 33/500 [08:57<2:13:11, 17.11s/it]  7%|▋         | 34/500 [09:13<2:09:42, 16.70s/it]  7%|▋         | 35/500 [09:28<2:06:21, 16.30s/it]  7%|▋         | 36/500 [09:44<2:04:01, 16.04s/it]  7%|▋         | 37/500 [09:59<2:02:24, 15.86s/it]  8%|▊         | 38/500 [10:15<2:01:59, 15.84s/it]  8%|▊         | 39/500 [10:30<2:00:37, 15.70s/it]  8%|▊         | 40/500 [10:46<1:59:59, 15.65s/it]  8%|▊         | 41/500 [11:01<1:59:20, 15.60s/it]  8%|▊         | 42/500 [11:17<1:59:14, 15.62s/it]  9%|▊         | 43/500 [11:33<1:58:26, 15.55s/it]  9%|▉         | 44/500 [11:48<1:57:56, 15.52s/it]  9%|▉         | 45/500 [12:04<1:57:49, 15.54s/it]  9%|▉         | 46/500 [12:19<1:57:58, 15.59s/it]  9%|▉         | 47/500 [12:35<1:57:15, 15.53s/it] 10%|▉         | 48/500 [12:50<1:56:45, 15.50s/it] 10%|▉         | 49/500 [13:06<1:56:24, 15.49s/it] 10%|█         | 50/500 [13:22<1:58:09, 15.76s/it] 10%|█         | 51/500 [13:38<1:57:38, 15.72s/it] 10%|█         | 52/500 [13:53<1:56:45, 15.64s/it] 11%|█         | 53/500 [14:09<1:56:14, 15.60s/it] 11%|█         | 54/500 [14:24<1:55:27, 15.53s/it] 11%|█         | 55/500 [14:39<1:54:36, 15.45s/it] 11%|█         | 56/500 [14:55<1:55:05, 15.55s/it] 11%|█▏        | 57/500 [15:11<1:55:02, 15.58s/it] 12%|█▏        | 58/500 [15:26<1:54:15, 15.51s/it] 12%|█▏        | 59/500 [15:46<2:03:07, 16.75s/it] 12%|█▏        | 60/500 [16:01<2:00:41, 16.46s/it] 12%|█▏        | 60/500 [16:01<1:57:34, 16.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.029 MB of 0.314 MB uploadedwandb: - 0.029 MB of 0.314 MB uploadedwandb: \ 0.029 MB of 0.314 MB uploadedwandb: | 0.029 MB of 0.314 MB uploadedwandb: / 0.186 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▅▁▂▃▄▃▂▃▆▄▆▇▁▆▆▅▅▇▆█▇█▆▅██▇█▇▇▇█▇██▇█▇▇
wandb:     train_loss ▂▂▁▃▂▃▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▁▃▅▄▅▄▄▅▅▅▆▃▄▆▆▇▆▅██▇▅▅▇▇▇▇▆▆█▇▇█▇▇█▇▇
wandb:       val_loss ▃▂▄▂▃▄▃▃▁▁▁▂▁▄▁▄▇█▁▁▁▅▁▄▁▁▆▁▃▁▃▂▄▄▃▁▄▁▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.94651
wandb:     train_loss 0.00142
wandb:   val_accuracy 0.75111
wandb:       val_loss 0.0069
wandb: 
wandb: 🚀 View run dark-bush-2591 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i5741574
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_221540-i5741574/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_223226-ft50b45t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-water-2592
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ft50b45t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:20, 18.68s/it]  0%|          | 2/500 [00:34<2:22:09, 17.13s/it]  1%|          | 3/500 [00:51<2:20:29, 16.96s/it]  1%|          | 4/500 [01:07<2:18:06, 16.71s/it]  1%|          | 5/500 [01:23<2:14:46, 16.34s/it]  1%|          | 6/500 [01:39<2:13:05, 16.16s/it]  1%|▏         | 7/500 [01:54<2:10:38, 15.90s/it]  2%|▏         | 8/500 [02:10<2:11:27, 16.03s/it]  2%|▏         | 9/500 [02:27<2:11:34, 16.08s/it]  2%|▏         | 10/500 [02:43<2:11:49, 16.14s/it]  2%|▏         | 11/500 [02:59<2:12:19, 16.24s/it]  2%|▏         | 12/500 [03:15<2:11:24, 16.16s/it]  3%|▎         | 13/500 [03:32<2:12:29, 16.32s/it]  3%|▎         | 14/500 [03:49<2:14:18, 16.58s/it]  3%|▎         | 15/500 [04:05<2:12:24, 16.38s/it]  3%|▎         | 16/500 [04:22<2:13:20, 16.53s/it]  3%|▎         | 17/500 [04:38<2:12:42, 16.48s/it]  4%|▎         | 18/500 [05:01<2:26:24, 18.23s/it]  4%|▍         | 19/500 [05:17<2:21:22, 17.64s/it]  4%|▍         | 20/500 [05:38<2:29:40, 18.71s/it]  4%|▍         | 21/500 [05:56<2:27:49, 18.52s/it]  4%|▍         | 22/500 [06:14<2:25:17, 18.24s/it]  5%|▍         | 23/500 [06:30<2:20:06, 17.62s/it]  5%|▍         | 24/500 [06:47<2:17:49, 17.37s/it]  5%|▌         | 25/500 [07:04<2:16:00, 17.18s/it]  5%|▌         | 26/500 [07:20<2:14:23, 17.01s/it]  5%|▌         | 27/500 [07:36<2:11:57, 16.74s/it]  6%|▌         | 28/500 [07:52<2:09:38, 16.48s/it]  6%|▌         | 28/500 [07:52<2:12:47, 16.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.021 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▂▃▅▅▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▆▇████
wandb:     train_loss █▇▇▇▅▇▄▄▆▆▃▇▆▅▇▆▄▆▁▆▁▁▇▂▁▁▄▄
wandb:   val_accuracy ▁▁▃▂▃█▇▅▅▄▅▅▅▅▆▄▆▆▆▆▆▆▅▆▇▆▇▇
wandb:       val_loss ▆▆▆▅▅▄▆▆▄▆▅▁▄▂▇▄▄▄▄▄█▆▆▄▄▄▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.80832
wandb:     train_loss 0.64695
wandb:   val_accuracy 0.57778
wandb:       val_loss 0.62561
wandb: 
wandb: 🚀 View run astral-water-2592 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ft50b45t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_223226-ft50b45t/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_224109-cllc1dwi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-bee-2594
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cllc1dwi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:37:31, 18.94s/it]  0%|          | 2/500 [00:35<2:23:35, 17.30s/it]  1%|          | 3/500 [00:51<2:19:56, 16.90s/it]  1%|          | 4/500 [01:08<2:19:06, 16.83s/it]  1%|          | 5/500 [01:24<2:16:52, 16.59s/it]  1%|          | 6/500 [01:41<2:16:41, 16.60s/it]  1%|▏         | 7/500 [01:57<2:14:57, 16.43s/it]  2%|▏         | 8/500 [02:13<2:13:44, 16.31s/it]  2%|▏         | 9/500 [02:30<2:16:02, 16.62s/it]  2%|▏         | 10/500 [02:47<2:16:41, 16.74s/it]  2%|▏         | 11/500 [03:04<2:16:34, 16.76s/it]  2%|▏         | 12/500 [03:21<2:17:32, 16.91s/it]  3%|▎         | 13/500 [03:38<2:16:48, 16.86s/it]  3%|▎         | 14/500 [03:55<2:17:36, 16.99s/it]  3%|▎         | 15/500 [04:11<2:14:40, 16.66s/it]  3%|▎         | 16/500 [04:27<2:12:09, 16.38s/it]  3%|▎         | 17/500 [04:43<2:10:40, 16.23s/it]  4%|▎         | 18/500 [04:58<2:09:08, 16.08s/it]  4%|▍         | 19/500 [05:14<2:08:38, 16.05s/it]  4%|▍         | 20/500 [05:30<2:07:47, 15.97s/it]  4%|▍         | 21/500 [05:45<2:06:10, 15.81s/it]  4%|▍         | 22/500 [06:01<2:05:09, 15.71s/it]  5%|▍         | 23/500 [06:17<2:04:32, 15.67s/it]  5%|▍         | 24/500 [06:32<2:03:57, 15.63s/it]  5%|▌         | 25/500 [06:48<2:03:57, 15.66s/it]  5%|▌         | 26/500 [07:03<2:02:46, 15.54s/it]  5%|▌         | 27/500 [07:19<2:03:09, 15.62s/it]  6%|▌         | 28/500 [07:34<2:02:29, 15.57s/it]  6%|▌         | 29/500 [07:50<2:02:31, 15.61s/it]  6%|▌         | 29/500 [07:50<2:07:21, 16.22s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.106 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▅▄▂▅▄▅▅▃▇▆▁▆▆▄▇▆▆▆▅▆█▃▇▆▇▇▅█
wandb:     train_loss ▄▄▄▃▂▃▂▂▃▃▄█▃▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▆▃▁▄▃▃▃▂█▆▃▇█▄▇▅▇▆▅▆▇▅▇▆▇▆▅▇
wandb:       val_loss ▃▃▃▃▃▂▂▅▄▂▃▄▁▁▆▁▂▁▁▃▂▁█▃▃▂▁▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.9584
wandb:     train_loss 0.00028
wandb:   val_accuracy 0.68667
wandb:       val_loss 0.10681
wandb: 
wandb: 🚀 View run fine-bee-2594 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cllc1dwi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_224109-cllc1dwi/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_224942-2e42qz6g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-deluge-2595
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2e42qz6g
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:31, 18.22s/it]  0%|          | 2/500 [00:34<2:20:30, 16.93s/it]  1%|          | 3/500 [00:50<2:16:58, 16.54s/it]  1%|          | 4/500 [01:05<2:10:59, 15.85s/it]  1%|          | 5/500 [01:20<2:08:06, 15.53s/it]  1%|          | 6/500 [01:35<2:06:21, 15.35s/it]  1%|▏         | 7/500 [01:49<2:04:33, 15.16s/it]  2%|▏         | 8/500 [02:04<2:03:56, 15.11s/it]  2%|▏         | 9/500 [02:19<2:03:43, 15.12s/it]  2%|▏         | 10/500 [02:34<2:02:48, 15.04s/it]  2%|▏         | 11/500 [02:49<2:01:56, 14.96s/it]  2%|▏         | 12/500 [03:04<2:01:22, 14.92s/it]  3%|▎         | 13/500 [03:19<2:00:45, 14.88s/it]  3%|▎         | 14/500 [03:34<2:01:10, 14.96s/it]  3%|▎         | 15/500 [03:49<2:01:00, 14.97s/it]  3%|▎         | 16/500 [04:04<2:01:15, 15.03s/it]  3%|▎         | 17/500 [04:19<2:01:11, 15.05s/it]  4%|▎         | 18/500 [04:34<2:00:56, 15.05s/it]  4%|▍         | 19/500 [04:49<2:00:01, 14.97s/it]  4%|▍         | 20/500 [05:04<1:59:41, 14.96s/it]  4%|▍         | 21/500 [05:19<1:59:09, 14.93s/it]  4%|▍         | 22/500 [05:34<1:58:54, 14.93s/it]  5%|▍         | 23/500 [05:49<1:58:53, 14.95s/it]  5%|▍         | 24/500 [06:04<1:58:39, 14.96s/it]  5%|▌         | 25/500 [06:19<1:58:42, 15.00s/it]  5%|▌         | 26/500 [06:34<1:58:13, 14.97s/it]  5%|▌         | 27/500 [06:49<1:58:48, 15.07s/it]  6%|▌         | 28/500 [07:04<1:59:36, 15.20s/it]  6%|▌         | 29/500 [07:20<2:00:11, 15.31s/it]  6%|▌         | 30/500 [07:35<1:59:20, 15.23s/it]  6%|▌         | 31/500 [07:51<1:59:32, 15.29s/it]  6%|▋         | 32/500 [08:05<1:57:57, 15.12s/it]  7%|▋         | 33/500 [08:20<1:57:17, 15.07s/it]  7%|▋         | 34/500 [08:35<1:56:37, 15.02s/it]  7%|▋         | 35/500 [08:50<1:56:07, 14.98s/it]  7%|▋         | 36/500 [09:05<1:56:12, 15.03s/it]  7%|▋         | 37/500 [09:20<1:55:30, 14.97s/it]  8%|▊         | 38/500 [09:35<1:55:46, 15.04s/it]  8%|▊         | 39/500 [09:50<1:55:55, 15.09s/it]  8%|▊         | 40/500 [10:05<1:55:05, 15.01s/it]  8%|▊         | 41/500 [10:21<1:56:14, 15.20s/it]  8%|▊         | 42/500 [10:36<1:55:38, 15.15s/it]  9%|▊         | 43/500 [10:51<1:56:02, 15.24s/it]  9%|▉         | 44/500 [11:07<1:55:51, 15.24s/it]  9%|▉         | 45/500 [11:23<1:57:35, 15.51s/it]  9%|▉         | 46/500 [11:38<1:56:46, 15.43s/it]  9%|▉         | 47/500 [11:54<1:57:45, 15.60s/it] 10%|▉         | 48/500 [12:09<1:57:07, 15.55s/it] 10%|▉         | 49/500 [12:24<1:55:23, 15.35s/it] 10%|█         | 50/500 [12:39<1:54:03, 15.21s/it] 10%|█         | 51/500 [12:54<1:54:02, 15.24s/it] 10%|█         | 52/500 [13:10<1:53:31, 15.20s/it] 11%|█         | 53/500 [13:25<1:52:44, 15.13s/it] 11%|█         | 54/500 [13:40<1:52:50, 15.18s/it] 11%|█         | 55/500 [13:55<1:53:26, 15.29s/it] 11%|█         | 56/500 [14:10<1:52:02, 15.14s/it] 11%|█▏        | 57/500 [14:25<1:51:24, 15.09s/it] 12%|█▏        | 58/500 [14:40<1:50:44, 15.03s/it] 12%|█▏        | 59/500 [14:55<1:50:50, 15.08s/it] 12%|█▏        | 60/500 [15:10<1:50:15, 15.03s/it] 12%|█▏        | 61/500 [15:25<1:49:35, 14.98s/it] 12%|█▏        | 62/500 [15:40<1:49:51, 15.05s/it] 13%|█▎        | 63/500 [15:55<1:49:45, 15.07s/it] 13%|█▎        | 64/500 [16:11<1:49:43, 15.10s/it] 13%|█▎        | 65/500 [16:26<1:49:41, 15.13s/it] 13%|█▎        | 66/500 [16:40<1:48:33, 15.01s/it] 13%|█▎        | 67/500 [16:55<1:47:46, 14.93s/it] 14%|█▎        | 68/500 [17:10<1:47:08, 14.88s/it] 14%|█▍        | 69/500 [17:25<1:46:55, 14.89s/it] 14%|█▍        | 70/500 [17:40<1:46:28, 14.86s/it] 14%|█▍        | 70/500 [17:40<1:48:32, 15.15s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.319 MB uploadedwandb: / 0.010 MB of 0.319 MB uploadedwandb: - 0.140 MB of 0.319 MB uploadedwandb: \ 0.319 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▁▄▁▃▂▄▄▅▆▇▄▂▆█▇▇▆▆▇▅█▇█▇▆▆▇▅▆▇▆▇▇▆█▆▇
wandb:     train_loss ▃▃▂▄▄▁▅█▁▁▁▁▁▇▁▁▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁
wandb:   val_accuracy ▁▃▂▂▅▄▅▄▅▅▇▅▅▅▅▆▇▇▆▆▆▇▆█▆█▆▆▆▆▅▇▇▆▆▆▆▇▆▇
wandb:       val_loss ▂▂▄▄▅▅▃▁▆▁▁▄▂▄▆▁▁▂▅▁▁▁▅▁▆▁▆▆▆▅▁█▄▄▁▆▅▁▅▆
wandb: 
wandb: Run summary:
wandb:          epoch 69
wandb:  learning_rate 0.00021
wandb: train_accuracy 0.91679
wandb:     train_loss 0.04991
wandb:   val_accuracy 0.68
wandb:       val_loss 3.45839
wandb: 
wandb: 🚀 View run major-deluge-2595 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2e42qz6g
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_224942-2e42qz6g/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_230806-icyp4y0m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-cloud-2597
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/icyp4y0m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:19:06, 16.73s/it]  0%|          | 2/500 [00:33<2:18:06, 16.64s/it]  1%|          | 3/500 [00:50<2:18:52, 16.77s/it]  1%|          | 4/500 [01:06<2:16:06, 16.46s/it]  1%|          | 5/500 [01:22<2:14:42, 16.33s/it]  1%|          | 6/500 [01:39<2:16:26, 16.57s/it]  1%|▏         | 7/500 [01:56<2:16:34, 16.62s/it]  2%|▏         | 8/500 [02:12<2:15:26, 16.52s/it]  2%|▏         | 9/500 [02:28<2:14:42, 16.46s/it]  2%|▏         | 10/500 [02:44<2:13:33, 16.35s/it]  2%|▏         | 11/500 [03:00<2:12:34, 16.27s/it]  2%|▏         | 12/500 [03:17<2:12:24, 16.28s/it]  3%|▎         | 13/500 [03:33<2:12:00, 16.26s/it]  3%|▎         | 14/500 [03:49<2:11:32, 16.24s/it]  3%|▎         | 15/500 [04:05<2:09:48, 16.06s/it]  3%|▎         | 16/500 [04:21<2:08:48, 15.97s/it]  3%|▎         | 17/500 [04:37<2:10:12, 16.17s/it]  4%|▎         | 18/500 [04:53<2:09:28, 16.12s/it]  4%|▍         | 19/500 [05:09<2:09:12, 16.12s/it]  4%|▍         | 20/500 [05:26<2:10:27, 16.31s/it]  4%|▍         | 21/500 [05:43<2:10:53, 16.40s/it]  4%|▍         | 22/500 [05:59<2:10:05, 16.33s/it]  5%|▍         | 23/500 [06:15<2:09:52, 16.34s/it]  5%|▍         | 24/500 [06:36<2:19:18, 17.56s/it]  5%|▌         | 25/500 [06:52<2:15:36, 17.13s/it]  5%|▌         | 26/500 [07:08<2:12:52, 16.82s/it]  5%|▌         | 27/500 [07:24<2:10:35, 16.57s/it]  6%|▌         | 28/500 [07:40<2:09:11, 16.42s/it]  6%|▌         | 29/500 [07:56<2:08:12, 16.33s/it]  6%|▌         | 30/500 [08:12<2:07:14, 16.24s/it]  6%|▌         | 31/500 [08:28<2:06:27, 16.18s/it]  6%|▋         | 32/500 [08:44<2:06:18, 16.19s/it]  7%|▋         | 33/500 [09:00<2:05:27, 16.12s/it]  7%|▋         | 34/500 [09:16<2:05:16, 16.13s/it]  7%|▋         | 35/500 [09:33<2:06:05, 16.27s/it]  7%|▋         | 36/500 [09:49<2:05:20, 16.21s/it]  7%|▋         | 37/500 [10:05<2:04:13, 16.10s/it]  8%|▊         | 38/500 [10:21<2:04:38, 16.19s/it]  8%|▊         | 39/500 [10:37<2:02:23, 15.93s/it]  8%|▊         | 40/500 [10:53<2:02:40, 16.00s/it]  8%|▊         | 41/500 [11:11<2:06:56, 16.59s/it]  8%|▊         | 42/500 [11:28<2:07:15, 16.67s/it]  9%|▊         | 43/500 [11:44<2:06:56, 16.67s/it]  9%|▉         | 44/500 [12:01<2:06:03, 16.59s/it]  9%|▉         | 45/500 [12:17<2:05:46, 16.59s/it]  9%|▉         | 45/500 [12:17<2:04:19, 16.39s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.138 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▂▃▄▅▆▇▇▇▇▇▇▇▇▇█▇█▇▇▆▇▆▆▇██▇▅▇█▆▇▇▇███
wandb:     train_loss █▇▇▇▆▇▅▅▆▄▆▆▆▄▄▃▂▂▁▁▂▂▁▁▆▁▅▄▃▅▁▄▁▁▁▁▁▁▁▆
wandb:   val_accuracy ▁▁▂▂▄▇█▆▇█▇█▇▇▇█▇█▇▇▇▇▅▆▇▆▇▇▇▇▅▇▇▅▇▆█▇▇▆
wandb:       val_loss ▃▃▃▃▃▃▃▃▃▃▃▃▂▅▂▂▃▂▁▃▄▃▄▃▄▁▂▃▅▁▂▄▂▂▃█▂▃▆▁
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.88707
wandb:     train_loss 0.82856
wandb:   val_accuracy 0.57556
wandb:       val_loss 0.27038
wandb: 
wandb: 🚀 View run golden-cloud-2597 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/icyp4y0m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_230806-icyp4y0m/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_232123-7uqaj79b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-microwave-2599
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7uqaj79b
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:35, 18.71s/it]  0%|          | 2/500 [00:34<2:22:17, 17.14s/it]  1%|          | 3/500 [00:51<2:19:08, 16.80s/it]  1%|          | 4/500 [01:06<2:14:40, 16.29s/it]  1%|          | 5/500 [01:22<2:12:21, 16.04s/it]  1%|          | 6/500 [01:37<2:10:31, 15.85s/it]  1%|▏         | 7/500 [01:52<2:08:13, 15.61s/it]  2%|▏         | 8/500 [02:12<2:17:41, 16.79s/it]  2%|▏         | 9/500 [02:27<2:14:24, 16.42s/it]  2%|▏         | 10/500 [02:42<2:10:59, 16.04s/it]  2%|▏         | 11/500 [02:59<2:11:56, 16.19s/it]  2%|▏         | 12/500 [03:14<2:08:59, 15.86s/it]  3%|▎         | 13/500 [03:29<2:06:49, 15.63s/it]  3%|▎         | 14/500 [03:45<2:06:14, 15.59s/it]  3%|▎         | 15/500 [04:00<2:05:54, 15.58s/it]  3%|▎         | 16/500 [04:15<2:04:10, 15.39s/it]  3%|▎         | 17/500 [04:30<2:03:16, 15.31s/it]  4%|▎         | 18/500 [04:45<2:02:14, 15.22s/it]  4%|▍         | 19/500 [05:00<2:01:27, 15.15s/it]  4%|▍         | 20/500 [05:16<2:02:23, 15.30s/it]  4%|▍         | 21/500 [05:32<2:03:07, 15.42s/it]  4%|▍         | 22/500 [05:47<2:02:52, 15.42s/it]  4%|▍         | 22/500 [05:47<2:05:52, 15.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.021 MB of 0.316 MB uploadedwandb: | 0.293 MB of 0.316 MB uploadedwandb: / 0.293 MB of 0.316 MB uploadedwandb: - 0.293 MB of 0.316 MB uploadedwandb: \ 0.293 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▄▁▄▆▄▆▆▅▆▆▃▁▁▂▁▄▆▇▅█▇
wandb:     train_loss ▃▃▃▂▁▂▂▅▂▁▂▆█▄▂▃▁▂▂▂▁▁
wandb:   val_accuracy ▃█▄▄▅▆▇█▇██▇▃▃▂▂▁▄▄▃▅▆
wandb:       val_loss ▂▂▂▂▂▁▂▄▄▂▂▃█▃▃▂▂▂▃▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.75483
wandb:     train_loss 0.40704
wandb:   val_accuracy 0.53556
wandb:       val_loss 0.99143
wandb: 
wandb: 🚀 View run splendid-microwave-2599 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7uqaj79b
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_232123-7uqaj79b/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_232754-ry5tkd52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-dawn-2600
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ry5tkd52
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:29:01, 17.92s/it]  0%|          | 2/500 [00:34<2:20:26, 16.92s/it]  1%|          | 3/500 [00:50<2:18:35, 16.73s/it]  1%|          | 4/500 [01:06<2:16:07, 16.47s/it]  1%|          | 5/500 [01:22<2:13:14, 16.15s/it]  1%|          | 6/500 [01:38<2:13:04, 16.16s/it]  1%|▏         | 7/500 [01:55<2:14:16, 16.34s/it]  2%|▏         | 8/500 [02:10<2:12:24, 16.15s/it]  2%|▏         | 9/500 [02:26<2:10:27, 15.94s/it]  2%|▏         | 10/500 [02:42<2:10:28, 15.98s/it]  2%|▏         | 11/500 [02:59<2:11:42, 16.16s/it]  2%|▏         | 12/500 [03:15<2:12:04, 16.24s/it]  3%|▎         | 13/500 [03:31<2:10:19, 16.06s/it]  3%|▎         | 14/500 [03:46<2:08:54, 15.91s/it]  3%|▎         | 15/500 [04:03<2:09:55, 16.07s/it]  3%|▎         | 16/500 [04:19<2:10:34, 16.19s/it]  3%|▎         | 17/500 [04:35<2:10:12, 16.17s/it]  4%|▎         | 18/500 [04:51<2:07:54, 15.92s/it]  4%|▍         | 19/500 [05:06<2:07:29, 15.90s/it]  4%|▍         | 20/500 [05:22<2:07:16, 15.91s/it]  4%|▍         | 21/500 [05:44<2:21:39, 17.74s/it]  4%|▍         | 22/500 [06:01<2:18:22, 17.37s/it]  5%|▍         | 23/500 [06:18<2:16:35, 17.18s/it]  5%|▍         | 24/500 [06:35<2:16:12, 17.17s/it]  5%|▌         | 25/500 [06:51<2:12:59, 16.80s/it]  5%|▌         | 26/500 [07:08<2:13:11, 16.86s/it]  5%|▌         | 27/500 [07:24<2:11:31, 16.68s/it]  6%|▌         | 28/500 [07:40<2:09:31, 16.47s/it]  6%|▌         | 29/500 [07:56<2:08:11, 16.33s/it]  6%|▌         | 30/500 [08:13<2:08:57, 16.46s/it]  6%|▌         | 31/500 [08:29<2:07:09, 16.27s/it]  6%|▋         | 32/500 [08:44<2:05:25, 16.08s/it]  7%|▋         | 33/500 [09:01<2:07:02, 16.32s/it]  7%|▋         | 34/500 [09:17<2:05:20, 16.14s/it]  7%|▋         | 35/500 [09:33<2:04:40, 16.09s/it]  7%|▋         | 36/500 [09:49<2:03:56, 16.03s/it]  7%|▋         | 37/500 [10:04<2:02:49, 15.92s/it]  8%|▊         | 38/500 [10:20<2:02:13, 15.87s/it]  8%|▊         | 39/500 [10:37<2:04:02, 16.15s/it]  8%|▊         | 40/500 [10:54<2:05:54, 16.42s/it]  8%|▊         | 41/500 [11:11<2:06:55, 16.59s/it]  8%|▊         | 42/500 [11:26<2:03:52, 16.23s/it]  9%|▊         | 43/500 [11:42<2:01:31, 15.96s/it]  9%|▊         | 43/500 [11:42<2:04:22, 16.33s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.020 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▂▂▆▇▄▇▁▅▅█▂▁▂▄▁█▂▅▁▂▂▁▄▁▆▃▁▃▁▂▅▁▁▁▁▆▁▁▂▂
wandb:     train_loss ▂▂▁▂▂▁▃▂▁▂▃▅▃▁▁▂▁▂▁▄▂▁▄▂▂▂▄▇█▁▂▄▃▁▄▁▃▄▁▁
wandb:   val_accuracy ▁▃▆▅▄█▃▅▇█▄▂▃▅▂▇▄▅▃▃▄▃▅▂▅▄▃▄▃▄▆▂▃▃▂▆▂▃▄▄
wandb:       val_loss ▂▂▂▂▁▁▂▄▁▂▂▄▂▄▂▁▁▁▂▁▆▅▂█▁▁▁▂▂▄▁▂▃▂▂▁▂█▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.38187
wandb:     train_loss 0.11377
wandb:   val_accuracy 0.47333
wandb:       val_loss 5.55323
wandb: 
wandb: 🚀 View run wise-dawn-2600 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ry5tkd52
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_232754-ry5tkd52/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_234028-43on7rxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sky-2602
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/43on7rxp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:47, 18.37s/it]  0%|          | 2/500 [00:33<2:15:56, 16.38s/it]  1%|          | 3/500 [00:48<2:11:48, 15.91s/it]  1%|          | 4/500 [01:03<2:08:52, 15.59s/it]  1%|          | 5/500 [01:18<2:06:54, 15.38s/it]  1%|          | 6/500 [01:33<2:05:54, 15.29s/it]  1%|▏         | 7/500 [01:49<2:05:17, 15.25s/it]  2%|▏         | 8/500 [02:04<2:04:32, 15.19s/it]  2%|▏         | 9/500 [02:19<2:04:27, 15.21s/it]  2%|▏         | 10/500 [02:34<2:03:50, 15.16s/it]  2%|▏         | 11/500 [02:49<2:03:50, 15.20s/it]  2%|▏         | 12/500 [03:04<2:03:32, 15.19s/it]  3%|▎         | 13/500 [03:19<2:02:25, 15.08s/it]  3%|▎         | 14/500 [03:34<2:02:27, 15.12s/it]  3%|▎         | 15/500 [03:50<2:02:14, 15.12s/it]  3%|▎         | 16/500 [04:05<2:02:00, 15.13s/it]  3%|▎         | 17/500 [04:20<2:01:28, 15.09s/it]  4%|▎         | 18/500 [04:35<2:01:17, 15.10s/it]  4%|▍         | 19/500 [04:50<2:01:33, 15.16s/it]  4%|▍         | 20/500 [05:05<2:00:14, 15.03s/it]  4%|▍         | 21/500 [05:20<1:59:45, 15.00s/it]  4%|▍         | 22/500 [05:35<1:59:21, 14.98s/it]  5%|▍         | 23/500 [05:50<2:00:21, 15.14s/it]  5%|▍         | 24/500 [06:06<2:00:53, 15.24s/it]  5%|▌         | 25/500 [06:21<1:59:48, 15.13s/it]  5%|▌         | 26/500 [06:36<1:59:24, 15.11s/it]  5%|▌         | 27/500 [06:51<1:59:49, 15.20s/it]  6%|▌         | 28/500 [07:06<1:59:10, 15.15s/it]  6%|▌         | 29/500 [07:21<1:58:27, 15.09s/it]  6%|▌         | 30/500 [07:36<1:58:08, 15.08s/it]  6%|▌         | 31/500 [07:51<1:57:32, 15.04s/it]  6%|▋         | 32/500 [08:06<1:56:23, 14.92s/it]  7%|▋         | 33/500 [08:21<1:56:12, 14.93s/it]  7%|▋         | 34/500 [08:36<1:55:50, 14.92s/it]  7%|▋         | 35/500 [08:51<1:56:39, 15.05s/it]  7%|▋         | 36/500 [09:06<1:56:35, 15.08s/it]  7%|▋         | 37/500 [09:21<1:55:29, 14.97s/it]  8%|▊         | 38/500 [09:35<1:54:28, 14.87s/it]  8%|▊         | 39/500 [09:50<1:54:14, 14.87s/it]  8%|▊         | 40/500 [10:06<1:57:06, 15.27s/it]  8%|▊         | 41/500 [10:22<1:56:25, 15.22s/it]  8%|▊         | 42/500 [10:37<1:56:11, 15.22s/it]  9%|▊         | 43/500 [10:52<1:56:38, 15.31s/it]  9%|▉         | 44/500 [11:07<1:55:12, 15.16s/it]  9%|▉         | 45/500 [11:22<1:54:51, 15.15s/it]  9%|▉         | 46/500 [11:38<1:55:41, 15.29s/it]  9%|▉         | 47/500 [11:53<1:54:04, 15.11s/it]  9%|▉         | 47/500 [11:53<1:54:33, 15.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.317 MB uploadedwandb: - 0.138 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▁▅▅▇█▇▆▇▇▅▆▅▂▄▃▅▂▄▂▃▃▅▃▅▄▆▄▃▃▄▄▃▃▃▄▃▄▄▅
wandb:     train_loss ▄▃▃▃▃▃▃▃▂▂▄▃▃▂▂▂▂▅▂▂▆▃▁▃▅▆▆▂▆▄▁▃▁▂▁▅▁█▄▄
wandb:   val_accuracy ▂▂▃▄▅██▆▆▆▃▅▃▁▄▁▃▂▂▂▂▂▄▃▃▃▅▃▃▂▄▄▃▃▃▃▃▃▃▄
wandb:       val_loss ▃▃▃▃▃▂▄▃▃▃▃▃▅▂▂▂▂▂██▂█▃▆▂▂▁▅▂▃▃▃▂▂▃▅▄▂▂▇
wandb: 
wandb: Run summary:
wandb:          epoch 46
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.50223
wandb:     train_loss 1.30013
wandb:   val_accuracy 0.40889
wandb:       val_loss 1.87439
wandb: 
wandb: 🚀 View run sage-sky-2602 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/43on7rxp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_234028-43on7rxp/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241003_235302-8zaahujm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-forest-2604
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8zaahujm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:55, 17.67s/it]  0%|          | 2/500 [00:33<2:17:43, 16.59s/it]  1%|          | 3/500 [00:49<2:16:07, 16.43s/it]  1%|          | 4/500 [01:06<2:16:14, 16.48s/it]  1%|          | 5/500 [01:22<2:14:07, 16.26s/it]  1%|          | 6/500 [01:38<2:12:53, 16.14s/it]  1%|▏         | 7/500 [01:53<2:11:33, 16.01s/it]  2%|▏         | 8/500 [02:09<2:11:04, 15.98s/it]  2%|▏         | 9/500 [02:25<2:09:32, 15.83s/it]  2%|▏         | 10/500 [02:40<2:08:29, 15.73s/it]  2%|▏         | 11/500 [02:56<2:07:50, 15.69s/it]  2%|▏         | 12/500 [03:12<2:08:23, 15.79s/it]  3%|▎         | 13/500 [03:28<2:08:06, 15.78s/it]  3%|▎         | 14/500 [03:44<2:08:24, 15.85s/it]  3%|▎         | 15/500 [04:00<2:09:11, 15.98s/it]  3%|▎         | 16/500 [04:16<2:09:49, 16.09s/it]  3%|▎         | 17/500 [04:32<2:09:21, 16.07s/it]  4%|▎         | 18/500 [04:48<2:09:02, 16.06s/it]  4%|▍         | 19/500 [05:04<2:08:17, 16.00s/it]  4%|▍         | 20/500 [05:20<2:07:48, 15.98s/it]  4%|▍         | 21/500 [05:36<2:08:21, 16.08s/it]  4%|▍         | 22/500 [05:53<2:08:54, 16.18s/it]  5%|▍         | 23/500 [06:09<2:08:47, 16.20s/it]  5%|▍         | 24/500 [06:25<2:08:17, 16.17s/it]  5%|▌         | 25/500 [06:42<2:09:05, 16.31s/it]  5%|▌         | 26/500 [06:58<2:08:07, 16.22s/it]  5%|▌         | 27/500 [07:13<2:06:06, 16.00s/it]  6%|▌         | 28/500 [07:29<2:06:15, 16.05s/it]  6%|▌         | 29/500 [07:46<2:06:44, 16.15s/it]  6%|▌         | 30/500 [08:03<2:08:36, 16.42s/it]  6%|▌         | 31/500 [08:20<2:08:49, 16.48s/it]  6%|▋         | 32/500 [08:41<2:19:04, 17.83s/it]  7%|▋         | 33/500 [08:57<2:16:39, 17.56s/it]  7%|▋         | 34/500 [09:14<2:14:57, 17.38s/it]  7%|▋         | 35/500 [09:30<2:11:26, 16.96s/it]  7%|▋         | 36/500 [09:46<2:09:01, 16.68s/it]  7%|▋         | 37/500 [10:02<2:06:13, 16.36s/it]  8%|▊         | 38/500 [10:18<2:04:58, 16.23s/it]  8%|▊         | 39/500 [10:34<2:03:44, 16.10s/it]  8%|▊         | 40/500 [10:50<2:02:41, 16.00s/it]  8%|▊         | 41/500 [11:05<2:01:32, 15.89s/it]  8%|▊         | 42/500 [11:21<2:01:16, 15.89s/it]  9%|▊         | 43/500 [11:37<2:01:28, 15.95s/it]  9%|▉         | 44/500 [11:53<2:02:03, 16.06s/it]  9%|▉         | 45/500 [12:10<2:03:01, 16.22s/it]  9%|▉         | 46/500 [12:26<2:02:27, 16.18s/it]  9%|▉         | 47/500 [12:42<2:02:03, 16.17s/it] 10%|▉         | 48/500 [12:58<2:01:07, 16.08s/it] 10%|▉         | 49/500 [13:14<2:01:01, 16.10s/it] 10%|▉         | 49/500 [13:14<2:01:56, 16.22s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.020 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▃▃▃▃▃▂▃▃▂▃▄▁▃▁▇▂▃▄▂▂▃▂▂▆▇▆▃▂▅▃█▅▂▂█▅▅█▆
wandb:     train_loss ▂▃▁▂▂▁▂▄▂▃▄▂▂█▁▁▁▄▁▃▂▁▂▃▄▁▁▆▃▁▆▁▇▁▅▁▁▁▁▁
wandb:   val_accuracy ▂▂▂▂▁▂▂▂▂▃▂▃▂▂▁▇▁▄▆▂▂▂▂▃▆█▇▅▂▆▄█▆▄▄█▆▆█▆
wandb:       val_loss ▂▃▂▂▂▆▂▃▂▂▄▁▂█▂▁▂▁▃▂▃█▂▁▁▁▁▁▃▁▁▂▃▂▂▁▁▃▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.66568
wandb:     train_loss 0.19928
wandb:   val_accuracy 0.66
wandb:       val_loss 1.55843
wandb: 
wandb: 🚀 View run charmed-forest-2604 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8zaahujm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241003_235302-8zaahujm/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_000705-cy8wf9cy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-mountain-2606
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cy8wf9cy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:30, 25.79s/it]  0%|          | 2/500 [00:49<3:24:36, 24.65s/it]  1%|          | 3/500 [01:13<3:20:34, 24.21s/it]  1%|          | 4/500 [01:37<3:18:27, 24.01s/it]  1%|          | 5/500 [02:01<3:18:32, 24.07s/it]  1%|          | 6/500 [02:24<3:16:48, 23.90s/it]  1%|▏         | 7/500 [02:48<3:15:30, 23.79s/it]  2%|▏         | 8/500 [03:11<3:14:16, 23.69s/it]  2%|▏         | 9/500 [03:35<3:14:14, 23.74s/it]  2%|▏         | 10/500 [03:59<3:15:14, 23.91s/it]  2%|▏         | 11/500 [04:23<3:14:42, 23.89s/it]  2%|▏         | 12/500 [04:52<3:25:31, 25.27s/it]  3%|▎         | 13/500 [05:15<3:20:55, 24.75s/it]  3%|▎         | 14/500 [05:39<3:18:45, 24.54s/it]  3%|▎         | 15/500 [06:03<3:15:34, 24.19s/it]  3%|▎         | 16/500 [06:30<3:22:16, 25.08s/it]  3%|▎         | 17/500 [06:53<3:18:20, 24.64s/it]  4%|▎         | 18/500 [07:17<3:14:59, 24.27s/it]  4%|▍         | 19/500 [07:40<3:12:19, 23.99s/it]  4%|▍         | 20/500 [08:05<3:12:38, 24.08s/it]  4%|▍         | 21/500 [08:28<3:11:25, 23.98s/it]  4%|▍         | 22/500 [08:52<3:10:38, 23.93s/it]  5%|▍         | 23/500 [09:15<3:08:15, 23.68s/it]  5%|▍         | 24/500 [09:38<3:06:51, 23.55s/it]  5%|▌         | 25/500 [10:02<3:06:13, 23.52s/it]  5%|▌         | 26/500 [10:25<3:05:33, 23.49s/it]  5%|▌         | 27/500 [10:49<3:04:53, 23.45s/it]  6%|▌         | 28/500 [11:13<3:06:54, 23.76s/it]  6%|▌         | 29/500 [11:40<3:13:16, 24.62s/it]  6%|▌         | 30/500 [12:04<3:11:03, 24.39s/it]  6%|▌         | 30/500 [12:04<3:09:04, 24.14s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.240 MB uploadedwandb: - 0.138 MB of 0.240 MB uploadedwandb: \ 0.138 MB of 0.240 MB uploadedwandb: | 0.240 MB of 0.240 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▂▇▁▁▁▃▃▇▇▆▆▆▆▇▆▅▄▅▆▇▇▆█▆▆▄▄▄
wandb:     train_loss ▄▅▃▂▇▁█▄▇▃▁▁▁▁▁▁▃▁▅▁▁▁▁▁▁▁▁▁▄▄
wandb:   val_accuracy ▁▃▄▅▂▃▂▄▄▅▆▅▆▇▅▆▄▅▄▅▇▅▆▆▆▅█▄▄▄
wandb:       val_loss ▃▃▄▃▂▄▄▄▁▁▁▁▅▁▁▁▃▄▃▄▅▁▄▁▂█▁▂▄▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.59881
wandb:     train_loss 1.09861
wandb:   val_accuracy 0.51778
wandb:       val_loss 1.90061
wandb: 
wandb: 🚀 View run logical-mountain-2606 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cy8wf9cy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_000705-cy8wf9cy/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_001956-gpb265ii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-pond-2608
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gpb265ii
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:09, 25.27s/it]  0%|          | 2/500 [00:50<3:27:10, 24.96s/it]  1%|          | 3/500 [01:14<3:25:10, 24.77s/it]  1%|          | 4/500 [01:38<3:23:11, 24.58s/it]  1%|          | 5/500 [02:03<3:24:11, 24.75s/it]  1%|          | 6/500 [02:28<3:22:16, 24.57s/it]  1%|▏         | 7/500 [02:51<3:19:53, 24.33s/it]  2%|▏         | 8/500 [03:15<3:18:21, 24.19s/it]  2%|▏         | 9/500 [03:46<3:34:26, 26.21s/it]  2%|▏         | 10/500 [04:10<3:29:06, 25.61s/it]  2%|▏         | 11/500 [04:34<3:25:06, 25.17s/it]  2%|▏         | 12/500 [04:58<3:21:51, 24.82s/it]  3%|▎         | 13/500 [05:23<3:20:27, 24.70s/it]  3%|▎         | 14/500 [05:47<3:19:24, 24.62s/it]  3%|▎         | 15/500 [06:12<3:19:13, 24.65s/it]  3%|▎         | 16/500 [06:38<3:21:14, 24.95s/it]  3%|▎         | 17/500 [07:02<3:18:58, 24.72s/it]  4%|▎         | 18/500 [07:27<3:19:29, 24.83s/it]  4%|▍         | 19/500 [07:52<3:19:14, 24.85s/it]  4%|▍         | 20/500 [08:17<3:18:28, 24.81s/it]  4%|▍         | 21/500 [08:42<3:19:16, 24.96s/it]  4%|▍         | 22/500 [09:06<3:16:44, 24.70s/it]  5%|▍         | 23/500 [09:36<3:28:36, 26.24s/it]  5%|▍         | 24/500 [10:02<3:28:29, 26.28s/it]  5%|▌         | 25/500 [10:27<3:23:58, 25.77s/it]  5%|▌         | 26/500 [10:53<3:23:40, 25.78s/it]  5%|▌         | 27/500 [11:18<3:23:05, 25.76s/it]  6%|▌         | 28/500 [11:44<3:23:39, 25.89s/it]  6%|▌         | 29/500 [12:09<3:21:05, 25.62s/it]  6%|▌         | 30/500 [12:34<3:19:03, 25.41s/it]  6%|▌         | 31/500 [13:02<3:22:58, 25.97s/it]  6%|▋         | 32/500 [13:27<3:21:43, 25.86s/it]  7%|▋         | 33/500 [13:56<3:28:00, 26.72s/it]  7%|▋         | 34/500 [14:21<3:23:44, 26.23s/it]  7%|▋         | 35/500 [14:44<3:16:39, 25.37s/it]  7%|▋         | 36/500 [15:08<3:11:33, 24.77s/it]  7%|▋         | 37/500 [15:32<3:09:47, 24.60s/it]  8%|▊         | 38/500 [15:56<3:07:49, 24.39s/it]  8%|▊         | 39/500 [16:19<3:05:31, 24.15s/it]  8%|▊         | 40/500 [16:43<3:04:43, 24.09s/it]  8%|▊         | 41/500 [17:07<3:03:26, 23.98s/it]  8%|▊         | 42/500 [17:31<3:02:23, 23.89s/it]  9%|▊         | 43/500 [17:55<3:01:41, 23.86s/it]  9%|▉         | 44/500 [18:19<3:02:15, 23.98s/it]  9%|▉         | 45/500 [18:43<3:01:14, 23.90s/it]  9%|▉         | 46/500 [19:07<3:00:56, 23.91s/it]  9%|▉         | 47/500 [19:35<3:11:12, 25.33s/it] 10%|▉         | 48/500 [19:59<3:06:48, 24.80s/it] 10%|▉         | 49/500 [20:22<3:03:54, 24.47s/it] 10%|▉         | 49/500 [20:27<3:08:18, 25.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.310 MB of 0.314 MB uploadedwandb: \ 0.310 MB of 0.314 MB uploadedwandb: | 0.310 MB of 0.314 MB uploadedwandb: / 0.310 MB of 0.314 MB uploadedwandb: - 0.310 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▃▆▅▆▆▆▇▇▇▆▆▇▆▆▇▆▇▇█▇██▇▇█▂▂▂▂▂▃▄▄▄▅▄▆
wandb:     train_loss ▄▅▅▅▅▅▄▆▅▄▄▁▁▂▄▂▁▁▃▃▄▃▃▁▃▂▃▂▆▃▃█▃▆▁▃▃▄▄▂
wandb:   val_accuracy ▃▃▃▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇██▁▁▂▁▂▂▄▄▄▄▄▄
wandb:       val_loss ▅▅▄▄▄▅▄▅▃▅▄▄▃▄▃▃▆▇█▃▃▃▂▃▂▂▁▃▆▅▆▄▅▄▄▅▆▇▄▅
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.74443
wandb:     train_loss 0.3629
wandb:   val_accuracy 0.45778
wandb:       val_loss 1.13491
wandb: 
wandb: 🚀 View run dauntless-pond-2608 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gpb265ii
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_001956-gpb265ii/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_004117-gelfx8uo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-wind-2611
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gelfx8uo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:37:59, 26.21s/it]  0%|          | 2/500 [00:51<3:31:55, 25.53s/it]  1%|          | 3/500 [01:15<3:25:09, 24.77s/it]  1%|          | 4/500 [01:39<3:21:56, 24.43s/it]  1%|          | 5/500 [02:02<3:19:15, 24.15s/it]  1%|          | 6/500 [02:26<3:18:09, 24.07s/it]  1%|▏         | 7/500 [02:50<3:16:19, 23.89s/it]  2%|▏         | 8/500 [03:13<3:15:41, 23.86s/it]  2%|▏         | 9/500 [03:37<3:14:59, 23.83s/it]  2%|▏         | 10/500 [04:00<3:13:08, 23.65s/it]  2%|▏         | 11/500 [04:24<3:12:58, 23.68s/it]  2%|▏         | 12/500 [04:47<3:10:45, 23.45s/it]  3%|▎         | 13/500 [05:11<3:10:38, 23.49s/it]  3%|▎         | 14/500 [05:35<3:11:26, 23.64s/it]  3%|▎         | 15/500 [05:58<3:10:42, 23.59s/it]  3%|▎         | 16/500 [06:22<3:10:18, 23.59s/it]  3%|▎         | 17/500 [06:45<3:09:03, 23.49s/it]  4%|▎         | 18/500 [07:09<3:09:02, 23.53s/it]  4%|▍         | 19/500 [07:33<3:10:19, 23.74s/it]  4%|▍         | 20/500 [07:56<3:09:05, 23.64s/it]  4%|▍         | 21/500 [08:20<3:09:01, 23.68s/it]  4%|▍         | 22/500 [08:44<3:08:22, 23.65s/it]  5%|▍         | 23/500 [09:07<3:08:36, 23.72s/it]  5%|▍         | 24/500 [09:31<3:06:56, 23.56s/it]  5%|▌         | 25/500 [09:55<3:08:03, 23.75s/it]  5%|▌         | 26/500 [10:18<3:07:08, 23.69s/it]  5%|▌         | 27/500 [10:42<3:06:49, 23.70s/it]  6%|▌         | 28/500 [11:06<3:05:43, 23.61s/it]  6%|▌         | 29/500 [11:29<3:04:58, 23.56s/it]  6%|▌         | 30/500 [11:52<3:03:52, 23.47s/it]  6%|▌         | 30/500 [11:52<3:06:07, 23.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.020 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▃▂▁▃▃▁▇▃▃▆▅▇▆▇▇▄▃▇▇▇▆▇▆▄█▅▇█▇
wandb:     train_loss ▄▂▃▄▃▂█▂▇▆▁▂▁▁▁▁▅▁▁▁▅▂▁▁▃▁▂▁▁▃
wandb:   val_accuracy ▁▄▁▂▅▅▃▆▅▄▆▅▇▆▇▇▅▅▆▆▆▆▇▆▅█▅▆▆▆
wandb:       val_loss ▄▄▅▄▂▂▅▂▂▃▄▁▄▁▁▂█▄▃▆▆▁▅▁▅▅▇▁▆▇
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.80684
wandb:     train_loss 1.09965
wandb:   val_accuracy 0.64889
wandb:       val_loss 2.2049
wandb: 
wandb: 🚀 View run frosty-wind-2611 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gelfx8uo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_004117-gelfx8uo/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_005358-9xq19sdn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-energy-2613
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9xq19sdn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:21, 25.41s/it]  0%|          | 2/500 [00:48<3:19:37, 24.05s/it]  1%|          | 3/500 [01:11<3:16:41, 23.75s/it]  1%|          | 4/500 [01:35<3:14:30, 23.53s/it]  1%|          | 5/500 [01:58<3:13:33, 23.46s/it]  1%|          | 6/500 [02:22<3:13:39, 23.52s/it]  1%|▏         | 7/500 [02:45<3:13:53, 23.60s/it]  2%|▏         | 8/500 [03:09<3:12:49, 23.52s/it]  2%|▏         | 9/500 [03:32<3:12:07, 23.48s/it]  2%|▏         | 10/500 [04:00<3:22:26, 24.79s/it]  2%|▏         | 11/500 [04:23<3:19:04, 24.43s/it]  2%|▏         | 12/500 [04:47<3:15:29, 24.04s/it]  3%|▎         | 13/500 [05:10<3:13:31, 23.84s/it]  3%|▎         | 14/500 [05:33<3:11:47, 23.68s/it]  3%|▎         | 15/500 [05:57<3:10:49, 23.61s/it]  3%|▎         | 16/500 [06:20<3:10:42, 23.64s/it]  3%|▎         | 17/500 [06:44<3:09:21, 23.52s/it]  4%|▎         | 18/500 [07:07<3:08:42, 23.49s/it]  4%|▍         | 19/500 [07:30<3:08:07, 23.47s/it]  4%|▍         | 20/500 [07:54<3:07:14, 23.40s/it]  4%|▍         | 21/500 [08:17<3:05:45, 23.27s/it]  4%|▍         | 22/500 [08:40<3:05:33, 23.29s/it]  5%|▍         | 23/500 [09:03<3:04:46, 23.24s/it]  5%|▍         | 24/500 [09:27<3:05:54, 23.43s/it]  5%|▌         | 25/500 [09:50<3:05:30, 23.43s/it]  5%|▌         | 26/500 [10:16<3:10:50, 24.16s/it]  5%|▌         | 27/500 [10:40<3:08:31, 23.91s/it]  6%|▌         | 28/500 [11:03<3:07:09, 23.79s/it]  6%|▌         | 29/500 [11:27<3:05:43, 23.66s/it]  6%|▌         | 30/500 [11:50<3:05:21, 23.66s/it]  6%|▌         | 31/500 [12:13<3:03:37, 23.49s/it]  6%|▋         | 32/500 [12:36<3:02:27, 23.39s/it]  7%|▋         | 33/500 [13:00<3:01:58, 23.38s/it]  7%|▋         | 34/500 [13:23<3:01:18, 23.34s/it]  7%|▋         | 35/500 [13:47<3:01:27, 23.41s/it]  7%|▋         | 36/500 [14:10<3:00:21, 23.32s/it]  7%|▋         | 37/500 [14:32<2:58:41, 23.16s/it]  8%|▊         | 38/500 [14:56<2:58:04, 23.13s/it]  8%|▊         | 39/500 [15:19<2:58:22, 23.22s/it]  8%|▊         | 40/500 [15:42<2:57:53, 23.20s/it]  8%|▊         | 41/500 [16:06<2:57:52, 23.25s/it]  8%|▊         | 42/500 [16:29<2:58:38, 23.40s/it]  9%|▊         | 43/500 [16:52<2:57:36, 23.32s/it]  9%|▉         | 44/500 [17:16<2:57:43, 23.38s/it]  9%|▉         | 45/500 [17:40<2:58:03, 23.48s/it]  9%|▉         | 46/500 [18:03<2:57:03, 23.40s/it]  9%|▉         | 47/500 [18:26<2:56:39, 23.40s/it] 10%|▉         | 48/500 [18:50<2:55:57, 23.36s/it] 10%|▉         | 49/500 [19:13<2:55:20, 23.33s/it] 10%|█         | 50/500 [19:36<2:54:12, 23.23s/it] 10%|█         | 51/500 [19:59<2:54:29, 23.32s/it] 10%|█         | 51/500 [19:59<2:56:02, 23.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.139 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▄▁▂▁▁▂███▆█▇▇█▆▇▇██████████████████████
wandb:     train_loss ▆▄█▆▆▆▆▅▁▆▆▁▁▁▆▁▁▄▂▁▆▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▆▁▁▁
wandb:   val_accuracy ▁▁▁▃▂▂▃███▇▇▅▆▅▅▇▆▇██▆▇██▇████████▇▇████
wandb:       val_loss ▄▄▅▄▆▅▄▇▆▄▇▅█▄▂▃▅▄▂▂▂▄▃▂▃▁▄▁▃▂▃▃▆▂▂▂▄▂▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.68796
wandb:     train_loss 0.01034
wandb:   val_accuracy 0.53111
wandb:       val_loss 1.09861
wandb: 
wandb: 🚀 View run devout-energy-2613 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9xq19sdn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_005358-9xq19sdn/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_011437-4p2qw906
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-cosmos-2616
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4p2qw906
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:49, 25.35s/it]  0%|          | 2/500 [00:48<3:20:52, 24.20s/it]  1%|          | 3/500 [01:12<3:18:43, 23.99s/it]  1%|          | 4/500 [01:35<3:16:00, 23.71s/it]  1%|          | 5/500 [01:59<3:14:16, 23.55s/it]  1%|          | 6/500 [02:22<3:14:42, 23.65s/it]  1%|▏         | 7/500 [02:46<3:13:31, 23.55s/it]  2%|▏         | 8/500 [03:09<3:11:50, 23.39s/it]  2%|▏         | 9/500 [03:32<3:11:14, 23.37s/it]  2%|▏         | 10/500 [03:56<3:11:19, 23.43s/it]  2%|▏         | 11/500 [04:19<3:10:46, 23.41s/it]  2%|▏         | 12/500 [04:42<3:10:03, 23.37s/it]  3%|▎         | 13/500 [05:06<3:09:34, 23.36s/it]  3%|▎         | 14/500 [05:31<3:15:16, 24.11s/it]  3%|▎         | 15/500 [06:01<3:27:44, 25.70s/it]  3%|▎         | 16/500 [06:24<3:21:51, 25.02s/it]  3%|▎         | 17/500 [06:47<3:16:49, 24.45s/it]  4%|▎         | 18/500 [07:11<3:13:48, 24.13s/it]  4%|▍         | 19/500 [07:34<3:11:22, 23.87s/it]  4%|▍         | 20/500 [08:02<3:20:57, 25.12s/it]  4%|▍         | 21/500 [08:25<3:15:48, 24.53s/it]  4%|▍         | 22/500 [08:49<3:12:29, 24.16s/it]  5%|▍         | 23/500 [09:12<3:10:10, 23.92s/it]  5%|▍         | 24/500 [09:35<3:08:40, 23.78s/it]  5%|▌         | 25/500 [09:58<3:06:11, 23.52s/it]  5%|▌         | 26/500 [10:22<3:06:38, 23.63s/it]  5%|▌         | 27/500 [10:50<3:16:17, 24.90s/it]  6%|▌         | 28/500 [11:14<3:12:49, 24.51s/it]  6%|▌         | 29/500 [11:37<3:09:38, 24.16s/it]  6%|▌         | 30/500 [12:00<3:06:51, 23.86s/it]  6%|▌         | 31/500 [12:24<3:05:37, 23.75s/it]  6%|▋         | 32/500 [12:47<3:04:51, 23.70s/it]  7%|▋         | 33/500 [13:10<3:02:45, 23.48s/it]  7%|▋         | 34/500 [13:38<3:12:48, 24.82s/it]  7%|▋         | 35/500 [14:01<3:08:39, 24.34s/it]  7%|▋         | 36/500 [14:30<3:18:26, 25.66s/it]  7%|▋         | 37/500 [14:59<3:25:09, 26.59s/it]  8%|▊         | 38/500 [15:22<3:17:44, 25.68s/it]  8%|▊         | 39/500 [15:46<3:12:10, 25.01s/it]  8%|▊         | 40/500 [16:09<3:07:42, 24.48s/it]  8%|▊         | 41/500 [16:32<3:04:40, 24.14s/it]  8%|▊         | 42/500 [16:56<3:02:24, 23.90s/it]  9%|▊         | 43/500 [17:19<3:01:25, 23.82s/it]  9%|▉         | 44/500 [17:43<2:59:47, 23.66s/it]  9%|▉         | 45/500 [18:11<3:09:18, 24.96s/it]  9%|▉         | 46/500 [18:34<3:05:40, 24.54s/it]  9%|▉         | 47/500 [18:57<3:01:46, 24.08s/it] 10%|▉         | 48/500 [19:25<3:10:28, 25.28s/it] 10%|▉         | 49/500 [19:49<3:05:34, 24.69s/it] 10%|█         | 50/500 [20:12<3:02:08, 24.28s/it] 10%|█         | 51/500 [20:35<2:59:17, 23.96s/it] 10%|█         | 52/500 [20:59<2:57:36, 23.79s/it] 11%|█         | 53/500 [21:22<2:55:49, 23.60s/it] 11%|█         | 54/500 [21:45<2:54:27, 23.47s/it] 11%|█         | 55/500 [22:08<2:53:02, 23.33s/it] 11%|█         | 56/500 [22:36<3:03:26, 24.79s/it] 11%|█▏        | 57/500 [23:00<3:00:04, 24.39s/it] 12%|█▏        | 58/500 [23:28<3:08:10, 25.54s/it] 12%|█▏        | 59/500 [23:51<3:02:43, 24.86s/it] 12%|█▏        | 59/500 [23:51<2:58:20, 24.26s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.030 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.233 MB of 0.313 MB uploadedwandb: | 0.233 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▅▆▅▅▁▄▆▇▇▇▇▇▇▇▇▇███▇█▇███▇███████████
wandb:     train_loss ▅▆▆▆▅▅█▆▅▃▂▃▂▂▂▃▃▃▆▁▃▂▅▃▅▂▁▂▂▅▁▁▂▁▁▁▁▁▃▁
wandb:   val_accuracy ▁▂▂▄▅▅▆▁▄▆▇▇▇█▇█▇▇███▇█▇▇█▇▇▇████▇▇▇▇▇▇▇
wandb:       val_loss ▃▃▂▂▂▂▃▃▃▂▂▃▁▄▃▃▂▁▂▂▂▂▁▁▂▁▂▃▃▂▄▂▃▅█▁▄▁▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 58
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.95542
wandb:     train_loss 0.00827
wandb:   val_accuracy 0.59556
wandb:       val_loss 0.87218
wandb: 
wandb: 🚀 View run twilight-cosmos-2616 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4p2qw906
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_011437-4p2qw906/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_013913-gk650tbp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-dust-2619
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gk650tbp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:18:13, 31.05s/it]  0%|          | 2/500 [00:59<4:06:34, 29.71s/it]  1%|          | 3/500 [01:24<3:47:45, 27.50s/it]  1%|          | 4/500 [01:48<3:36:44, 26.22s/it]  1%|          | 5/500 [02:12<3:29:24, 25.38s/it]  1%|          | 6/500 [02:36<3:24:26, 24.83s/it]  1%|▏         | 7/500 [03:00<3:20:45, 24.43s/it]  2%|▏         | 8/500 [03:24<3:20:10, 24.41s/it]  2%|▏         | 9/500 [03:48<3:18:01, 24.20s/it]  2%|▏         | 10/500 [04:11<3:16:00, 24.00s/it]  2%|▏         | 11/500 [04:36<3:16:09, 24.07s/it]  2%|▏         | 12/500 [04:59<3:15:13, 24.00s/it]  3%|▎         | 13/500 [05:27<3:23:56, 25.13s/it]  3%|▎         | 14/500 [05:51<3:20:09, 24.71s/it]  3%|▎         | 15/500 [06:14<3:16:59, 24.37s/it]  3%|▎         | 16/500 [06:43<3:27:19, 25.70s/it]  3%|▎         | 17/500 [07:07<3:22:11, 25.12s/it]  4%|▎         | 18/500 [07:31<3:18:24, 24.70s/it]  4%|▍         | 19/500 [07:55<3:16:37, 24.53s/it]  4%|▍         | 20/500 [08:18<3:13:10, 24.15s/it]  4%|▍         | 21/500 [08:42<3:11:38, 24.00s/it]  4%|▍         | 22/500 [09:06<3:11:53, 24.09s/it]  5%|▍         | 23/500 [09:30<3:10:11, 23.92s/it]  5%|▍         | 24/500 [09:53<3:08:54, 23.81s/it]  5%|▌         | 25/500 [10:17<3:08:07, 23.76s/it]  5%|▌         | 26/500 [10:40<3:06:19, 23.58s/it]  5%|▌         | 27/500 [11:04<3:06:14, 23.63s/it]  6%|▌         | 28/500 [11:28<3:06:13, 23.67s/it]  6%|▌         | 29/500 [11:55<3:15:50, 24.95s/it]  6%|▌         | 30/500 [12:25<3:25:12, 26.20s/it]  6%|▌         | 31/500 [12:48<3:19:10, 25.48s/it]  6%|▋         | 32/500 [13:12<3:14:19, 24.91s/it]  7%|▋         | 33/500 [13:36<3:10:47, 24.51s/it]  7%|▋         | 34/500 [13:59<3:07:48, 24.18s/it]  7%|▋         | 35/500 [14:22<3:05:27, 23.93s/it]  7%|▋         | 36/500 [14:46<3:03:54, 23.78s/it]  7%|▋         | 37/500 [15:09<3:02:53, 23.70s/it]  8%|▊         | 38/500 [15:36<3:09:20, 24.59s/it]  8%|▊         | 38/500 [15:36<3:09:45, 24.64s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.138 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▃▆▅▇▆▅▄▅▄▃▁▃▅██▇▅▃▃▅▇▆▇████▇▆██▆███▇█▇
wandb:     train_loss ▂▁▂▂▁▁▃▁▂█▁▂▂▂▂▂▃▁▂▂▃▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▂
wandb:   val_accuracy ▄█▆▆▆▇▇▇▇▄▁▄▆▇▇▇▇▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇
wandb:       val_loss ▂▂▂▂▂▂▂▃▂█▃▁▃▂▂▁▅▂▂▃▃▁▃▂▃▂▂▂▄▃▂▃▂▃▂▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.54235
wandb:     train_loss 1.24757
wandb:   val_accuracy 0.52444
wandb:       val_loss 0.55307
wandb: 
wandb: 🚀 View run lucky-dust-2619 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gk650tbp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_013913-gk650tbp/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_015539-zt93qky2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-vortex-2621
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zt93qky2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:06, 25.14s/it]  0%|          | 2/500 [00:48<3:19:45, 24.07s/it]  1%|          | 3/500 [01:12<3:18:45, 24.00s/it]  1%|          | 4/500 [01:35<3:17:01, 23.83s/it]  1%|          | 5/500 [01:59<3:15:10, 23.66s/it]  1%|          | 6/500 [02:22<3:14:41, 23.65s/it]  1%|▏         | 7/500 [02:46<3:13:54, 23.60s/it]  2%|▏         | 8/500 [03:09<3:12:21, 23.46s/it]  2%|▏         | 9/500 [03:34<3:14:49, 23.81s/it]  2%|▏         | 10/500 [03:57<3:13:28, 23.69s/it]  2%|▏         | 11/500 [04:20<3:11:47, 23.53s/it]  2%|▏         | 12/500 [04:43<3:10:32, 23.43s/it]  3%|▎         | 13/500 [05:07<3:09:25, 23.34s/it]  3%|▎         | 14/500 [05:30<3:08:47, 23.31s/it]  3%|▎         | 15/500 [05:53<3:07:52, 23.24s/it]  3%|▎         | 16/500 [06:16<3:07:06, 23.19s/it]  3%|▎         | 17/500 [06:40<3:07:43, 23.32s/it]  4%|▎         | 18/500 [07:04<3:09:06, 23.54s/it]  4%|▍         | 19/500 [07:31<3:18:32, 24.77s/it]  4%|▍         | 20/500 [08:00<3:26:49, 25.85s/it]  4%|▍         | 21/500 [08:23<3:20:29, 25.11s/it]  4%|▍         | 22/500 [08:52<3:28:28, 26.17s/it]  5%|▍         | 23/500 [09:15<3:20:44, 25.25s/it]  5%|▍         | 24/500 [09:39<3:16:45, 24.80s/it]  5%|▌         | 25/500 [10:02<3:13:15, 24.41s/it]  5%|▌         | 26/500 [10:25<3:10:05, 24.06s/it]  5%|▌         | 27/500 [10:49<3:07:57, 23.84s/it]  6%|▌         | 28/500 [11:12<3:06:08, 23.66s/it]  6%|▌         | 29/500 [11:35<3:05:06, 23.58s/it]  6%|▌         | 30/500 [12:04<3:17:27, 25.21s/it]  6%|▌         | 31/500 [12:28<3:12:42, 24.65s/it]  6%|▋         | 32/500 [12:51<3:08:43, 24.20s/it]  7%|▋         | 33/500 [13:14<3:05:55, 23.89s/it]  7%|▋         | 34/500 [13:37<3:04:18, 23.73s/it]  7%|▋         | 35/500 [14:01<3:04:39, 23.83s/it]  7%|▋         | 36/500 [14:25<3:02:47, 23.64s/it]  7%|▋         | 37/500 [14:48<3:01:13, 23.48s/it]  8%|▊         | 38/500 [15:11<3:00:23, 23.43s/it]  8%|▊         | 39/500 [15:34<2:59:25, 23.35s/it]  8%|▊         | 40/500 [15:58<2:59:14, 23.38s/it]  8%|▊         | 40/500 [15:58<3:03:43, 23.96s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.020 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▃▂▂▄▆▂▇▇▇▆▇▆▅▅█▄█▇▇▇▇█▆▇▅▇▅▇█▆▆▆▇▅▇▆▇█
wandb:     train_loss ▂▁▁▂▄▁▁▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁█▁▃▁▂
wandb:   val_accuracy ▁▁▅▄▃▅▅▄▅▅▆▆▇▆▅▆▇▆▆▅█▆▅▇▄▇▄▅▄█▆▅▅▇██▆▅▇▆
wandb:       val_loss ▂▄▁▄▂▁▃▁▁▂▂▁▂▂▁▂▁▃▁▇▅▁▇▁█▂█▁▃▁▁▆▁▂▁▂▃▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.94948
wandb:     train_loss 0.62668
wandb:   val_accuracy 0.70889
wandb:       val_loss 0.12166
wandb: 
wandb: 🚀 View run distinctive-vortex-2621 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zt93qky2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_015539-zt93qky2/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_021218-jpmqbf3j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-shadow-2623
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jpmqbf3j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:35:32, 25.92s/it]  0%|          | 2/500 [00:50<3:26:11, 24.84s/it]  1%|          | 3/500 [01:13<3:21:48, 24.36s/it]  1%|          | 4/500 [01:37<3:18:27, 24.01s/it]  1%|          | 5/500 [02:00<3:16:40, 23.84s/it]  1%|          | 6/500 [02:24<3:16:19, 23.85s/it]  1%|▏         | 7/500 [02:49<3:17:38, 24.05s/it]  2%|▏         | 8/500 [03:13<3:17:12, 24.05s/it]  2%|▏         | 9/500 [03:37<3:16:24, 24.00s/it]  2%|▏         | 10/500 [04:00<3:14:26, 23.81s/it]  2%|▏         | 11/500 [04:24<3:13:47, 23.78s/it]  2%|▏         | 12/500 [04:48<3:14:02, 23.86s/it]  3%|▎         | 13/500 [05:11<3:12:53, 23.76s/it]  3%|▎         | 14/500 [05:35<3:11:47, 23.68s/it]  3%|▎         | 15/500 [05:59<3:11:41, 23.72s/it]  3%|▎         | 16/500 [06:22<3:10:37, 23.63s/it]  3%|▎         | 17/500 [06:45<3:09:51, 23.59s/it]  4%|▎         | 18/500 [07:09<3:09:33, 23.60s/it]  4%|▍         | 19/500 [07:33<3:09:30, 23.64s/it]  4%|▍         | 20/500 [07:57<3:09:20, 23.67s/it]  4%|▍         | 21/500 [08:20<3:08:36, 23.62s/it]  4%|▍         | 22/500 [08:44<3:09:02, 23.73s/it]  5%|▍         | 23/500 [09:08<3:08:08, 23.67s/it]  5%|▍         | 24/500 [09:31<3:07:15, 23.60s/it]  5%|▌         | 25/500 [09:55<3:06:51, 23.60s/it]  5%|▌         | 26/500 [10:18<3:06:20, 23.59s/it]  5%|▌         | 27/500 [10:42<3:06:42, 23.68s/it]  6%|▌         | 28/500 [11:06<3:05:58, 23.64s/it]  6%|▌         | 29/500 [11:30<3:06:14, 23.72s/it]  6%|▌         | 30/500 [11:53<3:06:02, 23.75s/it]  6%|▌         | 31/500 [12:17<3:05:51, 23.78s/it]  6%|▋         | 32/500 [12:47<3:18:57, 25.51s/it]  7%|▋         | 33/500 [13:10<3:14:05, 24.94s/it]  7%|▋         | 34/500 [13:34<3:10:05, 24.48s/it]  7%|▋         | 35/500 [13:57<3:06:30, 24.06s/it]  7%|▋         | 36/500 [14:20<3:04:57, 23.92s/it]  7%|▋         | 37/500 [14:44<3:04:24, 23.90s/it]  8%|▊         | 38/500 [15:08<3:04:02, 23.90s/it]  8%|▊         | 39/500 [15:32<3:03:00, 23.82s/it]  8%|▊         | 40/500 [15:55<3:02:15, 23.77s/it]  8%|▊         | 41/500 [16:19<3:01:18, 23.70s/it]  8%|▊         | 42/500 [16:43<3:01:01, 23.71s/it]  9%|▊         | 43/500 [17:06<3:00:03, 23.64s/it]  9%|▉         | 44/500 [17:30<2:59:47, 23.66s/it]  9%|▉         | 45/500 [17:54<2:59:35, 23.68s/it]  9%|▉         | 46/500 [18:18<3:00:00, 23.79s/it]  9%|▉         | 47/500 [18:41<2:59:19, 23.75s/it] 10%|▉         | 48/500 [19:05<2:58:29, 23.69s/it] 10%|▉         | 49/500 [19:28<2:57:38, 23.63s/it] 10%|█         | 50/500 [19:52<2:57:17, 23.64s/it] 10%|█         | 51/500 [20:16<2:56:34, 23.60s/it] 10%|█         | 52/500 [20:39<2:55:25, 23.49s/it] 11%|█         | 53/500 [21:02<2:55:26, 23.55s/it] 11%|█         | 54/500 [21:26<2:55:34, 23.62s/it] 11%|█         | 54/500 [21:26<2:57:07, 23.83s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.139 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▄▄▂▃▃▄▅▆█▇▇▇█▇▇▇█▇██▇██▇██▇██▁▃▃▄▄▄▄▄▄▄
wandb:     train_loss ▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▂▂▂▁▁▁▂▁▂▂▂▁▁▂█▁▂▁▂▂▂▁▂▂
wandb:   val_accuracy ▄▆▃▃▄▃▃▅▅▇▇▇▆▇▆█▆▇▆▇▇▇▇▇▇▇█▇▇▇▄▁▂▃▃▃▃▃▃▃
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▁▁▂▂▁▁▂▁▁▁▂█▂▂▁▂▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 53
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.5052
wandb:     train_loss 1.66252
wandb:   val_accuracy 0.32667
wandb:       val_loss 1.18798
wandb: 
wandb: 🚀 View run atomic-shadow-2623 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jpmqbf3j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_021218-jpmqbf3j/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_023434-r9zb4x1k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-monkey-2625
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r9zb4x1k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:40:58, 26.57s/it]  0%|          | 2/500 [00:50<3:26:59, 24.94s/it]  1%|          | 3/500 [01:19<3:44:02, 27.05s/it]  1%|          | 4/500 [01:43<3:33:34, 25.84s/it]  1%|          | 5/500 [02:08<3:28:50, 25.31s/it]  1%|          | 6/500 [02:32<3:25:32, 24.97s/it]  1%|▏         | 7/500 [02:56<3:22:29, 24.64s/it]  2%|▏         | 8/500 [03:20<3:20:11, 24.41s/it]  2%|▏         | 9/500 [03:44<3:19:13, 24.35s/it]  2%|▏         | 10/500 [04:08<3:17:33, 24.19s/it]  2%|▏         | 11/500 [04:32<3:16:59, 24.17s/it]  2%|▏         | 12/500 [04:57<3:17:56, 24.34s/it]  3%|▎         | 13/500 [05:21<3:16:56, 24.26s/it]  3%|▎         | 14/500 [05:50<3:28:10, 25.70s/it]  3%|▎         | 15/500 [06:19<3:36:51, 26.83s/it]  3%|▎         | 16/500 [06:44<3:29:46, 26.00s/it]  3%|▎         | 17/500 [07:07<3:24:07, 25.36s/it]  4%|▎         | 18/500 [07:31<3:20:11, 24.92s/it]  4%|▍         | 19/500 [07:55<3:16:53, 24.56s/it]  4%|▍         | 20/500 [08:19<3:14:47, 24.35s/it]  4%|▍         | 21/500 [08:42<3:12:42, 24.14s/it]  4%|▍         | 22/500 [09:06<3:11:56, 24.09s/it]  5%|▍         | 23/500 [09:31<3:12:28, 24.21s/it]  5%|▍         | 24/500 [09:55<3:10:59, 24.07s/it]  5%|▌         | 25/500 [10:19<3:10:55, 24.12s/it]  5%|▌         | 25/500 [10:19<3:16:12, 24.78s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.011 MB of 0.316 MB uploadedwandb: / 0.011 MB of 0.316 MB uploadedwandb: - 0.025 MB of 0.316 MB uploadedwandb: \ 0.025 MB of 0.316 MB uploadedwandb: | 0.025 MB of 0.316 MB uploadedwandb: / 0.025 MB of 0.316 MB uploadedwandb: - 0.025 MB of 0.316 MB uploadedwandb: \ 0.025 MB of 0.316 MB uploadedwandb: | 0.025 MB of 0.316 MB uploadedwandb: / 0.025 MB of 0.316 MB uploadedwandb: - 0.025 MB of 0.316 MB uploadedwandb: \ 0.025 MB of 0.316 MB uploadedwandb: | 0.025 MB of 0.316 MB uploadedwandb: / 0.025 MB of 0.316 MB uploadedwandb: - 0.025 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.028 MB of 0.316 MB uploadedwandb: / 0.028 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.028 MB of 0.316 MB uploadedwandb: | 0.199 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▂▁▁▄▃▆▆▆▆▄▆▇▆▇▆▇▇▇▇▇▆▇█▇▅
wandb:     train_loss ▅▃▄▂▄▂▁▂█▇▄▃▂▃▁▃▆▁▅▁▁▃▁▂▃
wandb:   val_accuracy ▂▁▁▇▃▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▆▇▇▇
wandb:       val_loss ▄▄▄▄▂▁▆▂▁▅▂▁▅▂▄▂▆▄▂█▇▁▅▁█
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.66716
wandb:     train_loss 0.52374
wandb:   val_accuracy 0.60667
wandb:       val_loss 2.54836
wandb: 
wandb: 🚀 View run easy-monkey-2625 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r9zb4x1k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_023434-r9zb4x1k/logs
Successfully processed 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024644-jqsv2x54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-serenity-2626
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jqsv2x54
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run mild-serenity-2626 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jqsv2x54
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024644-jqsv2x54/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024724-a7p3q8fc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-planet-2627
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/a7p3q8fc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.031 MB uploadedwandb: \ 0.020 MB of 0.031 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run lilac-planet-2627 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/a7p3q8fc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024724-a7p3q8fc/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024800-kgrb2i1k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-field-2628
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kgrb2i1k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.010 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run gallant-field-2628 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kgrb2i1k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024800-kgrb2i1k/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024833-djfwx31r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-shape-2629
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/djfwx31r
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run splendid-shape-2629 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/djfwx31r
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024833-djfwx31r/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024903-utq245jz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-pine-2630
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/utq245jz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run rosy-pine-2630 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/utq245jz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024903-utq245jz/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_024934-zz26v93l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-breeze-2631
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zz26v93l
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run fancy-breeze-2631 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zz26v93l
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_024934-zz26v93l/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_025004-d4h2nl7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-microwave-2632
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d4h2nl7r
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run wandering-microwave-2632 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d4h2nl7r
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_025004-d4h2nl7r/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_025035-xlosidtj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-galaxy-2633
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xlosidtj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run dry-galaxy-2633 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xlosidtj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_025035-xlosidtj/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20241004_025105-hvukz7oo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-waterfall-2634
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/hvukz7oo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run trim-waterfall-2634 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/hvukz7oo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241004_025105-hvukz7oo/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 7_20131027
grid_search.sh: 行 42: e：未找到命令
