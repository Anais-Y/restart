nohup: 忽略输入
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_011200-noa8ynnv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-fog-272
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/noa8ynnv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:43, 17.64s/it]  0%|          | 2/500 [00:34<2:22:10, 17.13s/it]  1%|          | 3/500 [00:51<2:20:30, 16.96s/it]  1%|          | 4/500 [01:06<2:15:49, 16.43s/it]  1%|          | 5/500 [01:23<2:15:20, 16.41s/it]  1%|          | 6/500 [01:39<2:15:10, 16.42s/it]  1%|▏         | 7/500 [01:57<2:19:52, 17.02s/it]  2%|▏         | 8/500 [02:13<2:16:11, 16.61s/it]  2%|▏         | 9/500 [02:29<2:13:38, 16.33s/it]  2%|▏         | 10/500 [02:45<2:13:45, 16.38s/it]  2%|▏         | 11/500 [03:01<2:12:15, 16.23s/it]  2%|▏         | 12/500 [03:18<2:12:18, 16.27s/it]  3%|▎         | 13/500 [03:34<2:11:38, 16.22s/it]  3%|▎         | 14/500 [03:49<2:10:02, 16.05s/it]  3%|▎         | 15/500 [04:04<2:07:36, 15.79s/it]  3%|▎         | 16/500 [04:16<1:56:51, 14.49s/it]  3%|▎         | 17/500 [04:28<1:49:35, 13.61s/it]  4%|▎         | 18/500 [04:39<1:44:04, 12.96s/it]  4%|▍         | 19/500 [04:50<1:38:18, 12.26s/it]  4%|▍         | 20/500 [05:00<1:33:52, 11.73s/it]  4%|▍         | 21/500 [05:11<1:30:51, 11.38s/it]  4%|▍         | 22/500 [05:21<1:28:44, 11.14s/it]  5%|▍         | 23/500 [05:32<1:27:00, 10.94s/it]  5%|▍         | 24/500 [05:42<1:26:12, 10.87s/it]  5%|▌         | 25/500 [05:54<1:27:07, 11.00s/it]  5%|▌         | 26/500 [06:05<1:26:58, 11.01s/it]  5%|▌         | 27/500 [06:18<1:31:18, 11.58s/it]  5%|▌         | 27/500 [06:18<1:50:26, 14.01s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.208 MB of 0.509 MB uploadedwandb: | 0.208 MB of 0.509 MB uploadedwandb: / 0.505 MB of 0.509 MB uploadedwandb: - 0.505 MB of 0.509 MB uploadedwandb: \ 0.505 MB of 0.509 MB uploadedwandb: | 0.505 MB of 0.509 MB uploadedwandb: / 0.509 MB of 0.509 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▅▁▁▂▆▁▁▁▁▃▁▁▁▁▂▁█▄▁▁▁▁▁▁
wandb:     train_loss ▁▁▁▁▁▆▁▁▅▂▁▂▁▂▂▁▁▁▁▁▁▂█▂▁▂▁
wandb:   val_accuracy ▂▂▂█▁▂▁▅▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▂
wandb:       val_loss ▁▂▁▁▁▅▁▁▄▁▁▁▁▂▁▁▁▃▁▁▂▁█▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.35513
wandb:     train_loss 0.00244
wandb:   val_accuracy 0.33333
wandb:       val_loss 2.21888
wandb: 
wandb: 🚀 View run frosty-fog-272 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/noa8ynnv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_011200-noa8ynnv/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_011911-bqjgq21t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-pond-274
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bqjgq21t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:06:54, 15.26s/it]  0%|          | 2/500 [00:28<1:55:54, 13.96s/it]  1%|          | 3/500 [00:43<1:59:17, 14.40s/it]  1%|          | 4/500 [00:56<1:53:54, 13.78s/it]  1%|          | 5/500 [01:09<1:51:59, 13.58s/it]  1%|          | 6/500 [01:23<1:52:48, 13.70s/it]  1%|▏         | 7/500 [01:36<1:51:10, 13.53s/it]  2%|▏         | 8/500 [01:48<1:48:27, 13.23s/it]  2%|▏         | 9/500 [02:01<1:46:52, 13.06s/it]  2%|▏         | 10/500 [02:13<1:44:30, 12.80s/it]  2%|▏         | 11/500 [02:26<1:43:22, 12.68s/it]  2%|▏         | 12/500 [02:39<1:45:07, 12.93s/it]  3%|▎         | 13/500 [02:54<1:48:26, 13.36s/it]  3%|▎         | 14/500 [03:07<1:48:32, 13.40s/it]  3%|▎         | 15/500 [03:21<1:49:26, 13.54s/it]  3%|▎         | 16/500 [03:37<1:55:01, 14.26s/it]  3%|▎         | 17/500 [03:51<1:53:56, 14.15s/it]  4%|▎         | 18/500 [04:05<1:52:34, 14.01s/it]  4%|▍         | 19/500 [04:18<1:51:50, 13.95s/it]  4%|▍         | 20/500 [04:32<1:50:34, 13.82s/it]  4%|▍         | 21/500 [04:46<1:51:14, 13.93s/it]  4%|▍         | 22/500 [05:00<1:50:12, 13.83s/it]  5%|▍         | 23/500 [05:13<1:49:50, 13.82s/it]  5%|▍         | 24/500 [05:28<1:51:11, 14.02s/it]  5%|▌         | 25/500 [05:43<1:52:57, 14.27s/it]  5%|▌         | 26/500 [05:57<1:53:22, 14.35s/it]  5%|▌         | 27/500 [06:12<1:54:46, 14.56s/it]  6%|▌         | 28/500 [06:27<1:54:35, 14.57s/it]  6%|▌         | 29/500 [06:41<1:52:53, 14.38s/it]  6%|▌         | 29/500 [06:41<1:48:38, 13.84s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.137 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▄▁▁▅▁▆▁▁█▂▁▁▁▂▄▁▁▃▃▁▄▃▃▁▄
wandb:     train_loss ▂▂▂▂▂▂▁▂▇▁█▂▂▂▁▁▅▂▁▂▆▂▂▁▁▁▁▅▂
wandb:   val_accuracy ▂▂▃▄▅▂▂▆▂█▂▁▅▁▂▂▂▂▃▂▂▂▁▂▂▁▁▂▂
wandb:       val_loss ▁▁▁▁▁▁▁▁▄▁▅▁▁▁▂▁▂▁▁▂▅▁▁█▁▁▁▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.46211
wandb:     train_loss 0.9816
wandb:   val_accuracy 0.34889
wandb:       val_loss 0.77232
wandb: 
wandb: 🚀 View run toasty-pond-274 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bqjgq21t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_011911-bqjgq21t/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_012636-jrvwm1eb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-shape-275
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jrvwm1eb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:26:43, 24.86s/it]  0%|          | 2/500 [00:37<2:25:20, 17.51s/it]  1%|          | 3/500 [00:49<2:05:58, 15.21s/it]  1%|          | 4/500 [01:02<1:57:54, 14.26s/it]  1%|          | 5/500 [01:14<1:49:47, 13.31s/it]  1%|          | 6/500 [01:25<1:44:58, 12.75s/it]  1%|▏         | 7/500 [01:37<1:42:01, 12.42s/it]  2%|▏         | 8/500 [01:49<1:40:25, 12.25s/it]  2%|▏         | 9/500 [02:01<1:39:54, 12.21s/it]  2%|▏         | 10/500 [02:14<1:40:33, 12.31s/it]  2%|▏         | 11/500 [02:26<1:39:37, 12.22s/it]  2%|▏         | 12/500 [02:37<1:38:06, 12.06s/it]  3%|▎         | 13/500 [02:49<1:37:45, 12.04s/it]  3%|▎         | 14/500 [03:01<1:36:32, 11.92s/it]  3%|▎         | 15/500 [03:12<1:34:57, 11.75s/it]  3%|▎         | 16/500 [03:24<1:35:08, 11.79s/it]  3%|▎         | 17/500 [03:37<1:36:33, 11.99s/it]  4%|▎         | 18/500 [03:48<1:35:25, 11.88s/it]  4%|▍         | 19/500 [04:00<1:35:20, 11.89s/it]  4%|▍         | 20/500 [04:12<1:34:34, 11.82s/it]  4%|▍         | 21/500 [04:24<1:34:09, 11.79s/it]  4%|▍         | 22/500 [04:36<1:34:45, 11.89s/it]  5%|▍         | 23/500 [04:47<1:33:22, 11.74s/it]  5%|▍         | 24/500 [04:59<1:33:58, 11.85s/it]  5%|▌         | 25/500 [05:11<1:33:16, 11.78s/it]  5%|▌         | 26/500 [05:23<1:33:59, 11.90s/it]  5%|▌         | 27/500 [05:35<1:33:49, 11.90s/it]  6%|▌         | 28/500 [05:46<1:32:32, 11.76s/it]  6%|▌         | 29/500 [05:58<1:31:58, 11.72s/it]  6%|▌         | 30/500 [06:09<1:30:25, 11.54s/it]  6%|▌         | 31/500 [06:21<1:30:04, 11.52s/it]  6%|▋         | 32/500 [06:32<1:30:39, 11.62s/it]  7%|▋         | 33/500 [06:46<1:34:17, 12.11s/it]  7%|▋         | 34/500 [06:58<1:35:31, 12.30s/it]  7%|▋         | 35/500 [07:11<1:36:10, 12.41s/it]  7%|▋         | 36/500 [07:24<1:37:32, 12.61s/it]  7%|▋         | 37/500 [07:36<1:36:05, 12.45s/it]  8%|▊         | 38/500 [07:48<1:35:02, 12.34s/it]  8%|▊         | 39/500 [08:00<1:34:01, 12.24s/it]  8%|▊         | 40/500 [08:13<1:33:48, 12.24s/it]  8%|▊         | 41/500 [08:24<1:31:54, 12.01s/it]  8%|▊         | 42/500 [08:36<1:31:18, 11.96s/it]  9%|▊         | 43/500 [08:48<1:31:13, 11.98s/it]  9%|▉         | 44/500 [09:00<1:30:51, 11.95s/it]  9%|▉         | 45/500 [09:12<1:30:51, 11.98s/it]  9%|▉         | 46/500 [09:24<1:30:28, 11.96s/it]  9%|▉         | 47/500 [09:35<1:29:39, 11.88s/it] 10%|▉         | 48/500 [09:47<1:29:43, 11.91s/it] 10%|▉         | 49/500 [10:00<1:31:02, 12.11s/it] 10%|█         | 50/500 [10:13<1:32:03, 12.27s/it] 10%|█         | 51/500 [10:25<1:33:02, 12.43s/it] 10%|█         | 52/500 [10:38<1:33:10, 12.48s/it] 11%|█         | 53/500 [10:50<1:31:54, 12.34s/it] 11%|█         | 53/500 [10:50<1:31:27, 12.28s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.019 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▁▁▄▁▁▂▁▁▁▁▂▇▁▁▃▁▁▅▄▃▁▃▄▃▇▄█▄▁█▄▇▃▄▁▂▄▁▄
wandb:     train_loss ▁▁▂▁▂▁▁▁█▂▄▁▁▁▂▁▁▂▁▂▁▃▂▂▁▁▁▁▁▃▁▁▁▂▁▃▁▁▁▁
wandb:   val_accuracy ▁▁▁█▁▁▂▁▁▁▁▂▂▁▁▆▁▁▁▇▂▁▇▃▃▁▃▂▅▁▂▄▂▁▃▁▁▃▁▃
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 52
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.60773
wandb:     train_loss 0.00557
wandb:   val_accuracy 0.37333
wandb:       val_loss 0.57594
wandb: 
wandb: 🚀 View run polar-shape-275 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jrvwm1eb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_012636-jrvwm1eb/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_013805-y424gzvz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-snow-278
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y424gzvz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:04:49, 15.01s/it]  0%|          | 2/500 [00:27<1:54:35, 13.81s/it]  1%|          | 3/500 [00:41<1:51:36, 13.47s/it]  1%|          | 4/500 [00:53<1:47:22, 12.99s/it]  1%|          | 5/500 [01:05<1:44:39, 12.69s/it]  1%|          | 6/500 [01:17<1:43:43, 12.60s/it]  1%|▏         | 7/500 [01:30<1:43:14, 12.56s/it]  2%|▏         | 8/500 [01:42<1:42:26, 12.49s/it]  2%|▏         | 9/500 [01:56<1:44:20, 12.75s/it]  2%|▏         | 10/500 [02:08<1:44:26, 12.79s/it]  2%|▏         | 11/500 [02:21<1:43:16, 12.67s/it]  2%|▏         | 12/500 [02:33<1:40:59, 12.42s/it]  3%|▎         | 13/500 [02:44<1:39:18, 12.23s/it]  3%|▎         | 14/500 [02:56<1:38:34, 12.17s/it]  3%|▎         | 15/500 [03:09<1:39:36, 12.32s/it]  3%|▎         | 16/500 [03:22<1:40:36, 12.47s/it]  3%|▎         | 17/500 [03:35<1:40:54, 12.54s/it]  4%|▎         | 18/500 [03:47<1:39:54, 12.44s/it]  4%|▍         | 19/500 [03:59<1:39:21, 12.39s/it]  4%|▍         | 20/500 [04:11<1:38:59, 12.37s/it]  4%|▍         | 21/500 [04:24<1:38:40, 12.36s/it]  4%|▍         | 22/500 [04:38<1:42:03, 12.81s/it]  5%|▍         | 23/500 [04:51<1:42:12, 12.86s/it]  5%|▍         | 24/500 [05:03<1:40:39, 12.69s/it]  5%|▌         | 25/500 [05:15<1:39:18, 12.54s/it]  5%|▌         | 26/500 [05:28<1:39:01, 12.54s/it]  5%|▌         | 27/500 [05:40<1:38:29, 12.49s/it]  5%|▌         | 27/500 [05:40<1:39:26, 12.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.021 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▃▁▂▂▂▂▃▃▄▆▆▂▂▇▇██▆▇█▄▅▇▇█
wandb:     train_loss ▃▃▂▆▆▃▅█▅▃▁▃▂█▆▂▁▁▁▁▁▁▂▅▁▁▁
wandb:   val_accuracy ▄▅█▄▃▄▄▄▁▃▄▅▄▄▄▃▄▃▃▄▇▄▄▅▆▅▆
wandb:       val_loss ▁▁▁▁▂▁▁▁▁▂▁▁▁▃▃▂▂▃▃▂▅▅▄▄▄█▄
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.93611
wandb:     train_loss 0.00436
wandb:   val_accuracy 0.42667
wandb:       val_loss 6.92534
wandb: 
wandb: 🚀 View run dark-snow-278 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y424gzvz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_013805-y424gzvz/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_014429-6fhoq0us
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-haze-280
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6fhoq0us
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:08:17, 15.43s/it]  0%|          | 2/500 [00:27<1:52:51, 13.60s/it]  1%|          | 3/500 [00:41<1:52:19, 13.56s/it]  1%|          | 4/500 [00:53<1:47:53, 13.05s/it]  1%|          | 5/500 [01:04<1:42:45, 12.46s/it]  1%|          | 6/500 [01:16<1:39:07, 12.04s/it]  1%|▏         | 7/500 [01:27<1:37:55, 11.92s/it]  2%|▏         | 8/500 [01:39<1:36:09, 11.73s/it]  2%|▏         | 9/500 [01:50<1:34:26, 11.54s/it]  2%|▏         | 10/500 [02:01<1:33:49, 11.49s/it]  2%|▏         | 11/500 [02:13<1:33:42, 11.50s/it]  2%|▏         | 12/500 [02:24<1:34:09, 11.58s/it]  3%|▎         | 13/500 [02:36<1:34:01, 11.58s/it]  3%|▎         | 14/500 [02:48<1:35:32, 11.80s/it]  3%|▎         | 15/500 [03:00<1:35:24, 11.80s/it]  3%|▎         | 16/500 [03:12<1:34:18, 11.69s/it]  3%|▎         | 17/500 [03:23<1:33:03, 11.56s/it]  4%|▎         | 18/500 [03:34<1:32:20, 11.49s/it]  4%|▍         | 19/500 [03:46<1:32:21, 11.52s/it]  4%|▍         | 20/500 [03:58<1:33:12, 11.65s/it]  4%|▍         | 21/500 [04:09<1:31:33, 11.47s/it]  4%|▍         | 22/500 [04:19<1:28:59, 11.17s/it]  5%|▍         | 23/500 [04:29<1:26:25, 10.87s/it]  5%|▍         | 24/500 [04:40<1:25:34, 10.79s/it]  5%|▌         | 25/500 [04:51<1:25:20, 10.78s/it]  5%|▌         | 26/500 [05:02<1:25:15, 10.79s/it]  5%|▌         | 27/500 [05:12<1:24:48, 10.76s/it]  6%|▌         | 28/500 [05:23<1:25:28, 10.87s/it]  6%|▌         | 29/500 [05:34<1:25:40, 10.91s/it]  6%|▌         | 29/500 [05:34<1:30:39, 11.55s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.169 MB of 0.316 MB uploadedwandb: - 0.169 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▅▄▇▆▇▇██▅▃▁▂▅▁▁▁▁▁▃▁▁▃▄▂▁▁
wandb:     train_loss ▂▂▂▂▂▂▁▂▂▁▂▂▂▃▁▂▂▁▂▂▃▂▂▁▁▂▁▅█
wandb:   val_accuracy ▂▂▂▃▅████▄▅▇▁▁▁▄▁▁▁▁▁▂▂▁▂▃▂▁▁
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▃▁▁█▁▁▁▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 13.08838
wandb:   val_accuracy 0.32222
wandb:       val_loss 15.96069
wandb: 
wandb: 🚀 View run driven-haze-280 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6fhoq0us
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_014429-6fhoq0us/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_015049-48gqxp0p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-valley-282
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/48gqxp0p
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:31, 13.77s/it]  0%|          | 2/500 [00:25<1:45:10, 12.67s/it]  1%|          | 3/500 [00:37<1:42:54, 12.42s/it]  1%|          | 4/500 [00:50<1:42:56, 12.45s/it]  1%|          | 5/500 [01:02<1:41:31, 12.31s/it]  1%|          | 6/500 [01:13<1:39:20, 12.07s/it]  1%|▏         | 7/500 [01:25<1:38:16, 11.96s/it]  2%|▏         | 8/500 [01:37<1:38:39, 12.03s/it]  2%|▏         | 9/500 [01:49<1:38:13, 12.00s/it]  2%|▏         | 10/500 [02:01<1:38:24, 12.05s/it]  2%|▏         | 11/500 [02:13<1:37:35, 11.97s/it]  2%|▏         | 12/500 [02:25<1:37:33, 11.99s/it]  3%|▎         | 13/500 [02:37<1:37:36, 12.02s/it]  3%|▎         | 14/500 [02:50<1:38:09, 12.12s/it]  3%|▎         | 15/500 [03:02<1:37:56, 12.12s/it]  3%|▎         | 16/500 [03:14<1:38:15, 12.18s/it]  3%|▎         | 17/500 [03:27<1:38:25, 12.23s/it]  4%|▎         | 18/500 [03:39<1:38:31, 12.26s/it]  4%|▍         | 19/500 [03:51<1:39:02, 12.35s/it]  4%|▍         | 20/500 [04:04<1:38:49, 12.35s/it]  4%|▍         | 20/500 [04:04<1:37:51, 12.23s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.136 MB of 0.314 MB uploadedwandb: | 0.136 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁
wandb: train_accuracy ▁▂▃▅▃▄▁▅▄▆▆▆█▅▇▆▆▇▇▇
wandb:     train_loss ▄▃▄▂▆▃▇▂▄▂▂▆▂▂▂█▁▂▃▁
wandb:   val_accuracy ▁▅█▅▄▅▄▃▅▃▃▂▂▅▃▃▄▃▂▃
wandb:       val_loss ▂▂▂▁▂▃▁▂▁▄▁▂▁█▅▄▂▅▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 19
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.79792
wandb:     train_loss 0.01823
wandb:   val_accuracy 0.39556
wandb:       val_loss 2.11939
wandb: 
wandb: 🚀 View run apricot-valley-282 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/48gqxp0p
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_015049-48gqxp0p/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_015536-kysbwq09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-pine-283
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kysbwq09
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:06:00, 15.15s/it]  0%|          | 2/500 [00:28<1:57:04, 14.11s/it]  1%|          | 3/500 [00:41<1:52:10, 13.54s/it]  1%|          | 4/500 [00:53<1:46:45, 12.91s/it]  1%|          | 5/500 [01:05<1:44:46, 12.70s/it]  1%|          | 6/500 [01:18<1:44:01, 12.63s/it]  1%|▏         | 7/500 [01:30<1:42:30, 12.48s/it]  2%|▏         | 8/500 [01:42<1:42:07, 12.45s/it]  2%|▏         | 9/500 [01:54<1:41:04, 12.35s/it]  2%|▏         | 10/500 [02:07<1:40:56, 12.36s/it]  2%|▏         | 11/500 [02:20<1:41:53, 12.50s/it]  2%|▏         | 12/500 [02:32<1:40:32, 12.36s/it]  3%|▎         | 13/500 [02:45<1:41:46, 12.54s/it]  3%|▎         | 14/500 [02:57<1:42:19, 12.63s/it]  3%|▎         | 15/500 [03:10<1:42:42, 12.71s/it]  3%|▎         | 16/500 [03:23<1:42:27, 12.70s/it]  3%|▎         | 17/500 [03:35<1:39:26, 12.35s/it]  4%|▎         | 18/500 [03:46<1:37:34, 12.15s/it]  4%|▍         | 19/500 [03:58<1:36:10, 12.00s/it]  4%|▍         | 20/500 [04:10<1:37:19, 12.17s/it]  4%|▍         | 21/500 [04:23<1:37:47, 12.25s/it]  4%|▍         | 22/500 [04:35<1:38:11, 12.33s/it]  5%|▍         | 23/500 [04:48<1:39:05, 12.46s/it]  5%|▍         | 24/500 [05:01<1:40:21, 12.65s/it]  5%|▌         | 25/500 [05:14<1:40:03, 12.64s/it]  5%|▌         | 26/500 [05:26<1:39:42, 12.62s/it]  5%|▌         | 27/500 [05:39<1:39:20, 12.60s/it]  5%|▌         | 27/500 [05:39<1:39:07, 12.57s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.028 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.021 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▂▂▁▂▅▅█▃▆▆▁▆▃▁▆█▇█▅▄▅▃▄▇
wandb:     train_loss ▃▂▂▄▅▂▆▁▃▁▁▁█▅▂▁▁▁▁▁▁▂▁▁▁▁▁
wandb:   val_accuracy ▂▄█▂▂▂▃▅▃▂▃▄▂▂▃▂▂▆▁▃▄▁▂▃▂▆▂
wandb:       val_loss ▁▁▁▇▁▁▁▁▁▂▁▂▁▂▂▆▂▃▄▂▅▄▃▅▄█▄
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.82021
wandb:     train_loss 0.00021
wandb:   val_accuracy 0.32889
wandb:       val_loss 6.88751
wandb: 
wandb: 🚀 View run misunderstood-pine-283 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kysbwq09
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_015536-kysbwq09/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_020209-39ix3h4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-galaxy-285
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/39ix3h4u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:14:27, 16.17s/it]  0%|          | 2/500 [00:30<2:04:02, 14.94s/it]  1%|          | 3/500 [00:44<2:01:55, 14.72s/it]  1%|          | 4/500 [00:58<1:57:08, 14.17s/it]  1%|          | 5/500 [01:11<1:53:47, 13.79s/it]  1%|          | 6/500 [01:24<1:53:38, 13.80s/it]  1%|▏         | 7/500 [01:38<1:53:21, 13.80s/it]  2%|▏         | 8/500 [01:53<1:54:25, 13.95s/it]  2%|▏         | 9/500 [02:07<1:56:13, 14.20s/it]  2%|▏         | 10/500 [02:22<1:56:19, 14.24s/it]  2%|▏         | 11/500 [02:36<1:56:29, 14.29s/it]  2%|▏         | 12/500 [02:51<1:57:42, 14.47s/it]  3%|▎         | 13/500 [03:06<1:58:25, 14.59s/it]  3%|▎         | 14/500 [03:20<1:58:25, 14.62s/it]  3%|▎         | 15/500 [03:36<1:59:32, 14.79s/it]  3%|▎         | 16/500 [03:50<1:57:06, 14.52s/it]  3%|▎         | 17/500 [04:03<1:55:16, 14.32s/it]  4%|▎         | 18/500 [04:19<1:57:29, 14.63s/it]  4%|▍         | 19/500 [04:35<2:00:56, 15.09s/it]  4%|▍         | 20/500 [04:52<2:04:22, 15.55s/it]  4%|▍         | 21/500 [05:06<2:00:52, 15.14s/it]  4%|▍         | 22/500 [05:18<1:53:34, 14.26s/it]  5%|▍         | 23/500 [05:30<1:47:13, 13.49s/it]  5%|▍         | 24/500 [05:42<1:43:20, 13.03s/it]  5%|▌         | 25/500 [05:54<1:40:55, 12.75s/it]  5%|▌         | 26/500 [06:07<1:43:05, 13.05s/it]  5%|▌         | 27/500 [06:20<1:42:13, 12.97s/it]  6%|▌         | 28/500 [06:33<1:40:48, 12.81s/it]  6%|▌         | 29/500 [06:45<1:40:35, 12.81s/it]  6%|▌         | 29/500 [06:46<1:49:54, 14.00s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.021 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▄▃▆▆▇▇▇▇██▇█▇▇█████▇▆▇▇▇█▆
wandb:     train_loss ▆▇▆▆▆▆▅▆▆▃▆▆▅▅█▆▂▄▁▂▆▄▆▅▃▁▄▆▆
wandb:   val_accuracy ▁▁▁▂▅██▇▇▄▅▄▄▅▄▄▄▄▃▄▃▄▄▃▃▃▃▃▃
wandb:       val_loss ▄▃▃▃▃▃▃▃▃▄▃▂▁▆▄▃▂▅▂▃▇▄▇▄█▆▅▆▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.60624
wandb:     train_loss 1.05344
wandb:   val_accuracy 0.43111
wandb:       val_loss 0.97515
wandb: 
wandb: 🚀 View run royal-galaxy-285 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/39ix3h4u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_020209-39ix3h4u/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_020938-qafjphuq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-energy-287
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qafjphuq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:56:05, 13.96s/it]  0%|          | 2/500 [00:26<1:47:12, 12.92s/it]  1%|          | 3/500 [00:38<1:45:00, 12.68s/it]  1%|          | 4/500 [00:50<1:43:39, 12.54s/it]  1%|          | 5/500 [01:02<1:41:27, 12.30s/it]  1%|          | 6/500 [01:15<1:42:56, 12.50s/it]  1%|▏         | 7/500 [01:28<1:42:57, 12.53s/it]  2%|▏         | 8/500 [01:40<1:41:14, 12.35s/it]  2%|▏         | 9/500 [01:51<1:39:07, 12.11s/it]  2%|▏         | 10/500 [02:03<1:38:16, 12.03s/it]  2%|▏         | 11/500 [02:15<1:36:50, 11.88s/it]  2%|▏         | 12/500 [02:27<1:36:53, 11.91s/it]  3%|▎         | 13/500 [02:38<1:35:52, 11.81s/it]  3%|▎         | 14/500 [02:50<1:35:57, 11.85s/it]  3%|▎         | 15/500 [03:02<1:34:40, 11.71s/it]  3%|▎         | 16/500 [03:14<1:35:19, 11.82s/it]  3%|▎         | 17/500 [03:26<1:36:17, 11.96s/it]  4%|▎         | 18/500 [03:38<1:35:16, 11.86s/it]  4%|▍         | 19/500 [03:49<1:34:24, 11.78s/it]  4%|▍         | 20/500 [04:02<1:37:10, 12.15s/it]  4%|▍         | 21/500 [04:14<1:36:42, 12.11s/it]  4%|▍         | 22/500 [04:27<1:38:15, 12.33s/it]  5%|▍         | 23/500 [04:39<1:38:03, 12.33s/it]  5%|▍         | 24/500 [04:52<1:38:20, 12.40s/it]  5%|▌         | 25/500 [05:06<1:43:02, 13.02s/it]  5%|▌         | 26/500 [05:19<1:42:19, 12.95s/it]  5%|▌         | 27/500 [05:31<1:39:13, 12.59s/it]  6%|▌         | 28/500 [05:42<1:36:20, 12.25s/it]  6%|▌         | 29/500 [05:54<1:34:05, 11.99s/it]  6%|▌         | 30/500 [06:05<1:31:16, 11.65s/it]  6%|▌         | 31/500 [06:15<1:29:12, 11.41s/it]  6%|▋         | 32/500 [06:30<1:36:50, 12.42s/it]  7%|▋         | 33/500 [06:43<1:37:04, 12.47s/it]  7%|▋         | 34/500 [06:57<1:39:54, 12.86s/it]  7%|▋         | 35/500 [07:10<1:41:09, 13.05s/it]  7%|▋         | 36/500 [07:23<1:41:12, 13.09s/it]  7%|▋         | 37/500 [07:35<1:39:01, 12.83s/it]  8%|▊         | 38/500 [07:48<1:37:16, 12.63s/it]  8%|▊         | 39/500 [08:01<1:38:46, 12.86s/it]  8%|▊         | 40/500 [08:14<1:39:38, 13.00s/it]  8%|▊         | 41/500 [08:28<1:41:41, 13.29s/it]  8%|▊         | 42/500 [08:41<1:40:55, 13.22s/it]  9%|▊         | 43/500 [08:55<1:40:51, 13.24s/it]  9%|▉         | 44/500 [09:07<1:39:25, 13.08s/it]  9%|▉         | 45/500 [09:20<1:36:59, 12.79s/it]  9%|▉         | 46/500 [09:32<1:37:00, 12.82s/it]  9%|▉         | 47/500 [09:45<1:36:16, 12.75s/it] 10%|▉         | 48/500 [09:58<1:36:16, 12.78s/it] 10%|▉         | 49/500 [10:11<1:36:25, 12.83s/it] 10%|█         | 50/500 [10:25<1:39:28, 13.26s/it] 10%|█         | 51/500 [10:38<1:37:34, 13.04s/it] 10%|█         | 52/500 [10:50<1:35:11, 12.75s/it] 11%|█         | 53/500 [11:02<1:34:26, 12.68s/it] 11%|█         | 54/500 [11:15<1:34:55, 12.77s/it] 11%|█         | 55/500 [11:29<1:36:01, 12.95s/it] 11%|█         | 56/500 [11:41<1:34:47, 12.81s/it] 11%|█▏        | 57/500 [11:54<1:34:26, 12.79s/it] 12%|█▏        | 58/500 [12:06<1:32:41, 12.58s/it] 12%|█▏        | 59/500 [12:19<1:33:00, 12.65s/it] 12%|█▏        | 60/500 [12:31<1:32:40, 12.64s/it] 12%|█▏        | 61/500 [12:44<1:33:16, 12.75s/it] 12%|█▏        | 62/500 [12:58<1:35:24, 13.07s/it] 12%|█▏        | 62/500 [12:58<1:31:40, 12.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.020 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▁▁▄▂▁▁▁▁▁▁▁▁▁▂▁▅▁▁▁▁▁▁▁▆▃▁▁▁▁▃█▃▇▁▇▁█▁▁
wandb:     train_loss ▁▁▁▁▁▁▁▅▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▅▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▃▃▄▃▃▂▃▃▃▃▃▄▃▃▃▃▃▃▃▃▅▃▃▇▃▃▃▃▃▃▆▃▅▃█▄▁▃▃
wandb:       val_loss ▁▁▅▁▁▃▁▆▁▁▁▁▁█▁▁▂▁▁▁▁▂▁▄▁▁▁▁▁▁▂▁▂▂▁▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.3581
wandb:     train_loss 2.03586
wandb:   val_accuracy 0.32889
wandb:       val_loss 0.83513
wandb: 
wandb: 🚀 View run daily-energy-287 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qafjphuq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_020938-qafjphuq/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_022318-7mf0fhsl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-fire-289
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7mf0fhsl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:03:28, 14.85s/it]  0%|          | 2/500 [00:27<1:54:23, 13.78s/it]  1%|          | 3/500 [00:42<1:59:01, 14.37s/it]  1%|          | 4/500 [00:55<1:54:05, 13.80s/it]  1%|          | 5/500 [01:07<1:47:10, 12.99s/it]  1%|          | 6/500 [01:19<1:43:10, 12.53s/it]  1%|▏         | 7/500 [01:31<1:43:56, 12.65s/it]  2%|▏         | 8/500 [01:43<1:41:23, 12.36s/it]  2%|▏         | 9/500 [01:54<1:37:12, 11.88s/it]  2%|▏         | 10/500 [02:05<1:34:11, 11.53s/it]  2%|▏         | 11/500 [02:15<1:30:51, 11.15s/it]  2%|▏         | 12/500 [02:27<1:32:22, 11.36s/it]  3%|▎         | 13/500 [02:39<1:33:32, 11.53s/it]  3%|▎         | 14/500 [02:51<1:36:02, 11.86s/it]  3%|▎         | 15/500 [03:04<1:37:02, 12.00s/it]  3%|▎         | 16/500 [03:16<1:36:27, 11.96s/it]  3%|▎         | 17/500 [03:28<1:37:29, 12.11s/it]  4%|▎         | 18/500 [03:40<1:36:46, 12.05s/it]  4%|▍         | 19/500 [03:51<1:35:08, 11.87s/it]  4%|▍         | 20/500 [04:04<1:36:23, 12.05s/it]  4%|▍         | 21/500 [04:16<1:36:20, 12.07s/it]  4%|▍         | 22/500 [04:27<1:34:32, 11.87s/it]  5%|▍         | 23/500 [04:40<1:35:58, 12.07s/it]  5%|▍         | 24/500 [04:54<1:39:53, 12.59s/it]  5%|▌         | 25/500 [05:06<1:38:09, 12.40s/it]  5%|▌         | 26/500 [05:18<1:38:08, 12.42s/it]  5%|▌         | 27/500 [05:31<1:38:05, 12.44s/it]  6%|▌         | 28/500 [05:43<1:37:40, 12.42s/it]  6%|▌         | 29/500 [05:58<1:43:03, 13.13s/it]  6%|▌         | 30/500 [06:12<1:44:40, 13.36s/it]  6%|▌         | 31/500 [06:25<1:45:00, 13.43s/it]  6%|▋         | 32/500 [06:38<1:43:03, 13.21s/it]  7%|▋         | 33/500 [06:53<1:46:00, 13.62s/it]  7%|▋         | 34/500 [07:07<1:46:28, 13.71s/it]  7%|▋         | 35/500 [07:19<1:44:12, 13.45s/it]  7%|▋         | 36/500 [07:34<1:46:18, 13.75s/it]  7%|▋         | 37/500 [07:49<1:48:18, 14.04s/it]  8%|▊         | 38/500 [08:02<1:46:30, 13.83s/it]  8%|▊         | 39/500 [08:16<1:47:21, 13.97s/it]  8%|▊         | 40/500 [08:30<1:47:23, 14.01s/it]  8%|▊         | 41/500 [08:43<1:44:09, 13.62s/it]  8%|▊         | 42/500 [08:57<1:45:07, 13.77s/it]  9%|▊         | 43/500 [09:12<1:47:17, 14.09s/it]  9%|▉         | 44/500 [09:25<1:45:30, 13.88s/it]  9%|▉         | 45/500 [09:39<1:44:14, 13.75s/it]  9%|▉         | 46/500 [09:54<1:47:30, 14.21s/it]  9%|▉         | 47/500 [10:09<1:48:34, 14.38s/it] 10%|▉         | 48/500 [10:22<1:45:40, 14.03s/it] 10%|▉         | 49/500 [10:34<1:40:46, 13.41s/it] 10%|█         | 50/500 [10:44<1:33:07, 12.42s/it] 10%|█         | 51/500 [10:54<1:27:34, 11.70s/it] 10%|█         | 52/500 [11:05<1:25:30, 11.45s/it] 11%|█         | 53/500 [11:18<1:28:36, 11.89s/it] 11%|█         | 54/500 [11:29<1:26:46, 11.67s/it] 11%|█         | 55/500 [11:40<1:25:23, 11.51s/it] 11%|█         | 56/500 [11:53<1:27:24, 11.81s/it] 11%|█▏        | 57/500 [12:04<1:26:33, 11.72s/it] 12%|█▏        | 58/500 [12:16<1:26:45, 11.78s/it] 12%|█▏        | 59/500 [12:31<1:33:02, 12.66s/it] 12%|█▏        | 60/500 [12:44<1:34:40, 12.91s/it] 12%|█▏        | 61/500 [12:57<1:33:48, 12.82s/it] 12%|█▏        | 62/500 [13:09<1:32:14, 12.64s/it] 13%|█▎        | 63/500 [13:22<1:32:05, 12.64s/it] 13%|█▎        | 64/500 [13:36<1:35:26, 13.13s/it] 13%|█▎        | 65/500 [13:49<1:35:38, 13.19s/it] 13%|█▎        | 66/500 [14:05<1:39:25, 13.75s/it] 13%|█▎        | 67/500 [14:19<1:40:38, 13.94s/it] 14%|█▎        | 68/500 [14:32<1:38:56, 13.74s/it] 14%|█▍        | 69/500 [14:46<1:39:37, 13.87s/it] 14%|█▍        | 70/500 [15:00<1:38:37, 13.76s/it] 14%|█▍        | 71/500 [15:14<1:39:49, 13.96s/it] 14%|█▍        | 72/500 [15:27<1:37:53, 13.72s/it] 14%|█▍        | 72/500 [15:27<1:31:56, 12.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.234 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▁▁▁▄▃▆▄▆▆▇▆▇▅▂▄▂▃██▂▇▃▅▆▁█▄▄▃▆▅▅▅▇▃▃▂
wandb:     train_loss ▂▂▂▃▄▇▂▂▂▁▁▁▁▁▁▂▅▁▁▁▅▃▂▁▁▁▅▁▁▁▁▁▆▁▁▁▁▁██
wandb:   val_accuracy ▃█▅▃▃▂▃▁▃▃▃▄▃▄▄▃▃▄▃▃▄▅▂▄▃▅▅▁▄▄▅▅▄▅▄▄▅▄▅▃
wandb:       val_loss ▂▂▁▂▂▁▁▁▃▂▁▆▅▆▅▄▃▃▃▄▃▂█▇▄▁▃▂▁▂▁▃▄▃▆▁▃▁▄▅
wandb: 
wandb: Run summary:
wandb:          epoch 71
wandb:  learning_rate 0.00021
wandb: train_accuracy 0.4101
wandb:     train_loss 4.58551
wandb:   val_accuracy 0.35778
wandb:       val_loss 6.80936
wandb: 
wandb: 🚀 View run denim-fire-289 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7mf0fhsl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_022318-7mf0fhsl/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_023930-n4tyba09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-frog-292
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n4tyba09
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:05:40, 15.11s/it]  0%|          | 2/500 [00:29<2:01:23, 14.62s/it]  1%|          | 3/500 [00:41<1:50:06, 13.29s/it]  1%|          | 4/500 [00:51<1:41:45, 12.31s/it]  1%|          | 5/500 [01:02<1:36:55, 11.75s/it]  1%|          | 6/500 [01:14<1:37:44, 11.87s/it]  1%|▏         | 7/500 [01:25<1:34:38, 11.52s/it]  2%|▏         | 8/500 [01:38<1:38:14, 11.98s/it]  2%|▏         | 9/500 [01:51<1:39:57, 12.22s/it]  2%|▏         | 10/500 [02:03<1:40:54, 12.36s/it]  2%|▏         | 11/500 [02:16<1:42:11, 12.54s/it]  2%|▏         | 12/500 [02:30<1:44:10, 12.81s/it]  3%|▎         | 13/500 [02:42<1:42:17, 12.60s/it]  3%|▎         | 14/500 [02:55<1:43:15, 12.75s/it]  3%|▎         | 15/500 [03:07<1:41:48, 12.59s/it]  3%|▎         | 16/500 [03:22<1:45:45, 13.11s/it]  3%|▎         | 17/500 [03:35<1:46:02, 13.17s/it]  4%|▎         | 18/500 [03:49<1:48:42, 13.53s/it]  4%|▍         | 19/500 [04:03<1:49:23, 13.65s/it]  4%|▍         | 20/500 [04:18<1:51:40, 13.96s/it]  4%|▍         | 21/500 [04:32<1:51:35, 13.98s/it]  4%|▍         | 22/500 [04:46<1:52:42, 14.15s/it]  5%|▍         | 23/500 [04:59<1:49:05, 13.72s/it]  5%|▍         | 24/500 [05:12<1:47:41, 13.57s/it]  5%|▌         | 25/500 [05:25<1:46:01, 13.39s/it]  5%|▌         | 26/500 [05:40<1:49:16, 13.83s/it]  5%|▌         | 27/500 [05:54<1:49:32, 13.90s/it]  6%|▌         | 28/500 [06:08<1:49:47, 13.96s/it]  6%|▌         | 29/500 [06:21<1:45:52, 13.49s/it]  6%|▌         | 30/500 [06:34<1:44:33, 13.35s/it]  6%|▌         | 31/500 [06:47<1:44:49, 13.41s/it]  6%|▋         | 32/500 [07:01<1:45:32, 13.53s/it]  7%|▋         | 33/500 [07:15<1:45:38, 13.57s/it]  7%|▋         | 34/500 [07:28<1:45:37, 13.60s/it]  7%|▋         | 35/500 [07:43<1:46:37, 13.76s/it]  7%|▋         | 36/500 [07:58<1:51:09, 14.37s/it]  7%|▋         | 37/500 [08:12<1:48:20, 14.04s/it]  8%|▊         | 38/500 [08:25<1:46:18, 13.81s/it]  8%|▊         | 39/500 [08:39<1:46:25, 13.85s/it]  8%|▊         | 40/500 [08:52<1:43:52, 13.55s/it]  8%|▊         | 41/500 [09:06<1:45:43, 13.82s/it]  8%|▊         | 42/500 [09:20<1:44:35, 13.70s/it]  9%|▊         | 43/500 [09:33<1:43:28, 13.59s/it]  9%|▉         | 44/500 [09:49<1:49:20, 14.39s/it]  9%|▉         | 45/500 [10:00<1:41:56, 13.44s/it]  9%|▉         | 46/500 [10:11<1:34:51, 12.54s/it]  9%|▉         | 47/500 [10:21<1:29:55, 11.91s/it] 10%|▉         | 48/500 [10:34<1:32:08, 12.23s/it] 10%|▉         | 49/500 [10:47<1:33:40, 12.46s/it] 10%|█         | 50/500 [11:00<1:33:04, 12.41s/it] 10%|█         | 51/500 [11:13<1:34:34, 12.64s/it] 10%|█         | 52/500 [11:26<1:35:36, 12.80s/it] 11%|█         | 53/500 [11:39<1:35:40, 12.84s/it] 11%|█         | 54/500 [11:51<1:34:53, 12.77s/it] 11%|█         | 55/500 [12:03<1:32:24, 12.46s/it] 11%|█         | 56/500 [12:15<1:31:33, 12.37s/it] 11%|█         | 56/500 [12:15<1:37:14, 13.14s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.139 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▄▂▂▁▇▁▂▁▇▂▂▂▂▂▂▃▄▅██▃▃▂▄▂▃▄▃▄▃▄▂▄▄▄▂
wandb:     train_loss ▁▁▁▁▁▁▂▁▁▁▁▁▁▂▄▁▂▁▁▁▂▁▁▁▁▁▂▂█▁▁▁▁▁▁▄▂▁▁▁
wandb:   val_accuracy ▃▃▂▅▆▂▂▁█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▂▂▂▂▂▂▃▂▂▂▂▂▃▂▄▂
wandb:       val_loss ▂▂▂▂▂▂▂▂▁▂▂▂▁▂▄▂▃█▁▂▂▂▁▂▁▂▂▂▁▂▁▁▁▂▂▆▂▂▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 7e-05
wandb:   val_accuracy 0.32444
wandb:       val_loss 4.08017
wandb: 
wandb: 🚀 View run graceful-frog-292 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n4tyba09
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_023930-n4tyba09/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_025233-1ku5tt3j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-salad-295
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1ku5tt3j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:08:59, 15.51s/it]  0%|          | 2/500 [00:29<2:02:14, 14.73s/it]  1%|          | 3/500 [00:44<2:00:55, 14.60s/it]  1%|          | 4/500 [00:57<1:55:47, 14.01s/it]  1%|          | 5/500 [01:09<1:51:11, 13.48s/it]  1%|          | 6/500 [01:22<1:49:42, 13.33s/it]  1%|▏         | 7/500 [01:36<1:49:13, 13.29s/it]  2%|▏         | 8/500 [01:48<1:47:54, 13.16s/it]  2%|▏         | 9/500 [02:02<1:48:34, 13.27s/it]  2%|▏         | 10/500 [02:16<1:50:51, 13.58s/it]  2%|▏         | 11/500 [02:30<1:50:55, 13.61s/it]  2%|▏         | 12/500 [02:43<1:50:32, 13.59s/it]  3%|▎         | 13/500 [02:57<1:50:51, 13.66s/it]  3%|▎         | 14/500 [03:13<1:55:11, 14.22s/it]  3%|▎         | 15/500 [03:28<1:56:32, 14.42s/it]  3%|▎         | 16/500 [03:43<1:58:22, 14.67s/it]  3%|▎         | 17/500 [03:58<1:58:44, 14.75s/it]  4%|▎         | 18/500 [04:13<1:59:08, 14.83s/it]  4%|▍         | 19/500 [04:27<1:56:57, 14.59s/it]  4%|▍         | 20/500 [04:42<1:57:31, 14.69s/it]  4%|▍         | 20/500 [04:42<1:52:54, 14.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.136 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁
wandb: train_accuracy ▁▁▆▆▄▃▃▃▃▄▄█▇▄▅▅▅▅▇▄
wandb:     train_loss ▃▃▃▂▄▃▃▂▃▂▂▄▁▂▄█▁▄▂▂
wandb:   val_accuracy ▂▂▇█▇▅▃▄▂▄▃▄▅▂▂▁▁▂▂▃
wandb:       val_loss ▂▂▂▁▃▂▂▂▃▅▂▃▂█▅▅▂▅▇▂
wandb: 
wandb: Run summary:
wandb:          epoch 19
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.50074
wandb:     train_loss 0.33478
wandb:   val_accuracy 0.38667
wandb:       val_loss 1.06798
wandb: 
wandb: 🚀 View run playful-salad-295 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1ku5tt3j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_025233-1ku5tt3j/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_025757-s3xlfh6m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-fire-296
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s3xlfh6m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:52:23, 13.51s/it]  0%|          | 2/500 [00:24<1:41:16, 12.20s/it]  1%|          | 3/500 [00:35<1:36:39, 11.67s/it]  1%|          | 4/500 [00:46<1:33:24, 11.30s/it]  1%|          | 5/500 [00:58<1:34:05, 11.40s/it]  1%|          | 6/500 [01:09<1:34:40, 11.50s/it]  1%|▏         | 7/500 [01:21<1:34:13, 11.47s/it]  2%|▏         | 8/500 [01:32<1:34:47, 11.56s/it]  2%|▏         | 9/500 [01:44<1:35:25, 11.66s/it]  2%|▏         | 10/500 [01:57<1:36:47, 11.85s/it]  2%|▏         | 11/500 [02:09<1:36:49, 11.88s/it]  2%|▏         | 12/500 [02:20<1:35:24, 11.73s/it]  3%|▎         | 13/500 [02:32<1:35:58, 11.82s/it]  3%|▎         | 14/500 [02:44<1:35:24, 11.78s/it]  3%|▎         | 15/500 [02:56<1:35:23, 11.80s/it]  3%|▎         | 16/500 [03:08<1:35:58, 11.90s/it]  3%|▎         | 17/500 [03:20<1:35:36, 11.88s/it]  4%|▎         | 18/500 [03:31<1:35:22, 11.87s/it]  4%|▍         | 19/500 [03:43<1:34:41, 11.81s/it]  4%|▍         | 20/500 [03:55<1:34:24, 11.80s/it]  4%|▍         | 21/500 [04:07<1:34:06, 11.79s/it]  4%|▍         | 22/500 [04:18<1:33:25, 11.73s/it]  5%|▍         | 23/500 [04:30<1:33:11, 11.72s/it]  5%|▍         | 24/500 [04:41<1:32:25, 11.65s/it]  5%|▌         | 25/500 [04:54<1:34:21, 11.92s/it]  5%|▌         | 25/500 [04:54<1:33:13, 11.78s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.021 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▁▃▅▁▂▁██▆▁▅▄▃▁▅▃▇▁▂▆█▁
wandb:     train_loss ▂▁▂▁▂▂▃▁▂▁▁▁▂▁▁▂▁▁█▁▁▄▁▁▁
wandb:   val_accuracy ▂█▇▅▅▄▁▂▂▁▂▃▂▅▂▂▂▅▄▃▂▂▁▃▂
wandb:       val_loss ▁▁▁▁▁▁▂▁▁▃▁▂▁▄▂▂▂▄█▂▄▃▄█▃
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.35364
wandb:     train_loss 0.02494
wandb:   val_accuracy 0.33111
wandb:       val_loss 4.03305
wandb: 
wandb: 🚀 View run lilac-fire-296 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s3xlfh6m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_025757-s3xlfh6m/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_030335-z34xhg3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-fire-298
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z34xhg3c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:00:24, 14.48s/it]  0%|          | 2/500 [00:27<1:51:41, 13.46s/it]  1%|          | 3/500 [00:40<1:52:06, 13.54s/it]  1%|          | 4/500 [00:53<1:48:22, 13.11s/it]  1%|          | 5/500 [01:05<1:45:48, 12.83s/it]  1%|          | 6/500 [01:17<1:43:11, 12.53s/it]  1%|▏         | 7/500 [01:29<1:42:36, 12.49s/it]  2%|▏         | 8/500 [01:42<1:42:06, 12.45s/it]  2%|▏         | 9/500 [01:54<1:40:58, 12.34s/it]  2%|▏         | 10/500 [02:06<1:40:30, 12.31s/it]  2%|▏         | 11/500 [02:18<1:39:37, 12.22s/it]  2%|▏         | 12/500 [02:30<1:37:45, 12.02s/it]  3%|▎         | 13/500 [02:42<1:37:59, 12.07s/it]  3%|▎         | 14/500 [02:54<1:38:17, 12.13s/it]  3%|▎         | 15/500 [03:07<1:40:00, 12.37s/it]  3%|▎         | 16/500 [03:19<1:39:40, 12.36s/it]  3%|▎         | 17/500 [03:31<1:38:21, 12.22s/it]  4%|▎         | 18/500 [03:44<1:38:16, 12.23s/it]  4%|▍         | 19/500 [03:56<1:37:36, 12.18s/it]  4%|▍         | 20/500 [04:08<1:37:47, 12.22s/it]  4%|▍         | 21/500 [04:20<1:37:14, 12.18s/it]  4%|▍         | 22/500 [04:32<1:35:55, 12.04s/it]  5%|▍         | 23/500 [04:44<1:35:11, 11.97s/it]  5%|▍         | 24/500 [04:56<1:35:12, 12.00s/it]  5%|▌         | 25/500 [05:08<1:34:43, 11.97s/it]  5%|▌         | 26/500 [05:19<1:33:33, 11.84s/it]  5%|▌         | 27/500 [05:31<1:33:12, 11.82s/it]  6%|▌         | 28/500 [05:43<1:34:15, 11.98s/it]  6%|▌         | 29/500 [05:56<1:34:41, 12.06s/it]  6%|▌         | 30/500 [06:07<1:33:08, 11.89s/it]  6%|▌         | 31/500 [06:20<1:35:54, 12.27s/it]  6%|▋         | 32/500 [06:33<1:37:04, 12.45s/it]  7%|▋         | 33/500 [06:45<1:34:52, 12.19s/it]  7%|▋         | 34/500 [06:56<1:33:35, 12.05s/it]  7%|▋         | 35/500 [07:08<1:32:34, 11.95s/it]  7%|▋         | 36/500 [07:20<1:32:07, 11.91s/it]  7%|▋         | 37/500 [07:32<1:31:37, 11.87s/it]  8%|▊         | 38/500 [07:43<1:30:41, 11.78s/it]  8%|▊         | 39/500 [07:56<1:31:41, 11.93s/it]  8%|▊         | 40/500 [08:07<1:30:54, 11.86s/it]  8%|▊         | 41/500 [08:19<1:29:54, 11.75s/it]  8%|▊         | 42/500 [08:31<1:31:23, 11.97s/it]  9%|▊         | 43/500 [08:43<1:30:53, 11.93s/it]  9%|▉         | 44/500 [08:55<1:30:26, 11.90s/it]  9%|▉         | 45/500 [09:06<1:29:28, 11.80s/it]  9%|▉         | 46/500 [09:18<1:28:44, 11.73s/it]  9%|▉         | 47/500 [09:29<1:27:44, 11.62s/it] 10%|▉         | 48/500 [09:41<1:27:42, 11.64s/it] 10%|▉         | 49/500 [09:52<1:26:49, 11.55s/it] 10%|█         | 50/500 [10:04<1:26:00, 11.47s/it] 10%|█         | 51/500 [10:15<1:25:00, 11.36s/it] 10%|█         | 52/500 [10:26<1:24:38, 11.33s/it] 10%|█         | 52/500 [10:26<1:29:57, 12.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.030 MB of 0.317 MB uploadedwandb: - 0.030 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▁▂▅██▇█▆▆▅▅▅▆▆▆▆▅▅▅▅▅▆▅▅▅▅▅▆▅▄▅▆▅▅▆▅▅▅▆
wandb:     train_loss ▄▄▃▃▃▃▄▂▃▃▃▅▅▄▃▃▃▃▆▃▂▃▃▇▄▃▄▃▁▄▁▁█▂▁▃▆▃▃▁
wandb:   val_accuracy ▁▁▁▁██▇█▇▅▄▅▄▄▅▅▄▄▁▄▂▄▂▃▄▄▄▃▃▃▂▄▂▃▃▄▃▃▄▅
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 51
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.50074
wandb:     train_loss 0.59405
wandb:   val_accuracy 0.54444
wandb:       val_loss 0.69623
wandb: 
wandb: 🚀 View run classic-fire-298 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z34xhg3c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_030335-z34xhg3c/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_031441-k2qf6v33
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-mountain-300
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k2qf6v33
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:29, 13.77s/it]  0%|          | 2/500 [00:25<1:43:43, 12.50s/it]  1%|          | 3/500 [00:37<1:43:41, 12.52s/it]  1%|          | 4/500 [00:50<1:43:35, 12.53s/it]  1%|          | 5/500 [01:01<1:40:14, 12.15s/it]  1%|          | 6/500 [01:14<1:39:50, 12.13s/it]  1%|▏         | 7/500 [01:25<1:39:03, 12.05s/it]  2%|▏         | 8/500 [01:37<1:37:39, 11.91s/it]  2%|▏         | 9/500 [01:49<1:36:36, 11.80s/it]  2%|▏         | 10/500 [02:01<1:37:30, 11.94s/it]  2%|▏         | 11/500 [02:13<1:36:48, 11.88s/it]  2%|▏         | 12/500 [02:24<1:35:39, 11.76s/it]  3%|▎         | 13/500 [02:36<1:35:17, 11.74s/it]  3%|▎         | 14/500 [02:48<1:35:13, 11.76s/it]  3%|▎         | 15/500 [03:00<1:37:08, 12.02s/it]  3%|▎         | 16/500 [03:12<1:36:54, 12.01s/it]  3%|▎         | 17/500 [03:24<1:35:59, 11.93s/it]  4%|▎         | 18/500 [03:37<1:38:07, 12.22s/it]  4%|▍         | 19/500 [03:49<1:37:06, 12.11s/it]  4%|▍         | 20/500 [04:02<1:38:53, 12.36s/it]  4%|▍         | 20/500 [04:02<1:36:50, 12.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.230 MB of 0.313 MB uploadedwandb: / 0.230 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁
wandb: train_accuracy ▄▂▂▂▂▁▂▂▂▂▁▅▄▂▄▃▂▄█▃
wandb:     train_loss ▂▁▂▂▁▂▁▁▄▁▂▂▂█▁▁▄▂▁▃
wandb:   val_accuracy ▃▃▃▂▃▅▃▃▃▁▂▁▁▃▁▁▃▂█▁
wandb:       val_loss ▁▁▁▁█▁▃▁▂▁▁▁▁▃▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 19
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.34918
wandb:     train_loss 2.75318
wandb:   val_accuracy 0.3
wandb:       val_loss 0.91144
wandb: 
wandb: 🚀 View run soft-mountain-300 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k2qf6v33
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_031441-k2qf6v33/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_031925-51q3tjjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-feather-302
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/51q3tjjr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:36:56, 18.87s/it]  0%|          | 2/500 [00:34<2:20:49, 16.97s/it]  1%|          | 3/500 [00:50<2:16:11, 16.44s/it]  1%|          | 4/500 [01:05<2:11:14, 15.88s/it]  1%|          | 5/500 [01:20<2:09:58, 15.75s/it]  1%|          | 6/500 [01:36<2:08:14, 15.57s/it]  1%|▏         | 7/500 [01:51<2:06:58, 15.45s/it]  2%|▏         | 8/500 [02:06<2:05:45, 15.34s/it]  2%|▏         | 9/500 [02:21<2:05:07, 15.29s/it]  2%|▏         | 10/500 [02:37<2:06:23, 15.48s/it]  2%|▏         | 11/500 [02:53<2:06:49, 15.56s/it]  2%|▏         | 12/500 [03:08<2:05:09, 15.39s/it]  3%|▎         | 13/500 [03:23<2:05:07, 15.42s/it]  3%|▎         | 14/500 [03:38<2:03:51, 15.29s/it]  3%|▎         | 15/500 [03:54<2:03:50, 15.32s/it]  3%|▎         | 16/500 [04:09<2:03:07, 15.26s/it]  3%|▎         | 17/500 [04:24<2:02:55, 15.27s/it]  4%|▎         | 18/500 [04:39<2:02:08, 15.20s/it]  4%|▍         | 19/500 [04:55<2:02:52, 15.33s/it]  4%|▍         | 20/500 [05:10<2:01:32, 15.19s/it]  4%|▍         | 21/500 [05:24<2:00:31, 15.10s/it]  4%|▍         | 22/500 [05:39<1:58:50, 14.92s/it]  5%|▍         | 23/500 [05:54<1:57:49, 14.82s/it]  5%|▍         | 24/500 [06:08<1:56:57, 14.74s/it]  5%|▌         | 25/500 [06:23<1:58:06, 14.92s/it]  5%|▌         | 26/500 [06:39<1:58:48, 15.04s/it]  5%|▌         | 27/500 [06:54<1:59:02, 15.10s/it]  6%|▌         | 28/500 [07:09<1:58:21, 15.05s/it]  6%|▌         | 29/500 [07:24<1:58:40, 15.12s/it]  6%|▌         | 29/500 [07:24<2:00:23, 15.34s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.168 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▃▁▃▂▄▆▇▇▇▇▇▇▇███▇▇███████▆
wandb:     train_loss ▇█▇▅▇▆█▂▆▃▁▂▁▇▃▁▁▁▁▁▁▁▁▃▁▁▁▁▅
wandb:   val_accuracy ▂█▇▅▄▃▅▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▂▃
wandb:       val_loss ▁▁▂▁▁▁▃▂▃▁▅▂▁▁▃▁▅▄▂▃█▇▄▅▅▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.77415
wandb:     train_loss 0.70688
wandb:   val_accuracy 0.37333
wandb:       val_loss 0.78056
wandb: 
wandb: 🚀 View run fluent-feather-302 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/51q3tjjr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_031925-51q3tjjr/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_032734-jz8wuloi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-durian-304
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jz8wuloi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:43:29, 19.66s/it]  0%|          | 2/500 [00:36<2:29:48, 18.05s/it]  1%|          | 3/500 [00:53<2:25:44, 17.59s/it]  1%|          | 4/500 [01:10<2:23:45, 17.39s/it]  1%|          | 5/500 [01:27<2:20:27, 17.02s/it]  1%|          | 6/500 [01:44<2:20:25, 17.06s/it]  1%|▏         | 7/500 [02:00<2:19:22, 16.96s/it]  2%|▏         | 8/500 [02:17<2:18:36, 16.90s/it]  2%|▏         | 9/500 [02:34<2:19:12, 17.01s/it]  2%|▏         | 10/500 [02:52<2:19:21, 17.06s/it]  2%|▏         | 11/500 [03:09<2:20:31, 17.24s/it]  2%|▏         | 12/500 [03:26<2:18:52, 17.07s/it]  3%|▎         | 13/500 [03:43<2:19:25, 17.18s/it]  3%|▎         | 14/500 [04:00<2:18:21, 17.08s/it]  3%|▎         | 15/500 [04:18<2:19:31, 17.26s/it]  3%|▎         | 16/500 [04:33<2:14:51, 16.72s/it]  3%|▎         | 17/500 [04:49<2:11:10, 16.29s/it]  4%|▎         | 18/500 [05:04<2:09:14, 16.09s/it]  4%|▍         | 19/500 [05:21<2:11:27, 16.40s/it]  4%|▍         | 20/500 [05:43<2:24:39, 18.08s/it]  4%|▍         | 21/500 [06:00<2:20:26, 17.59s/it]  4%|▍         | 22/500 [06:17<2:18:49, 17.43s/it]  5%|▍         | 23/500 [06:33<2:15:07, 17.00s/it]  5%|▍         | 24/500 [06:49<2:12:42, 16.73s/it]  5%|▌         | 25/500 [07:06<2:13:16, 16.83s/it]  5%|▌         | 26/500 [07:23<2:13:44, 16.93s/it]  5%|▌         | 27/500 [07:40<2:13:28, 16.93s/it]  6%|▌         | 28/500 [07:57<2:12:21, 16.82s/it]  6%|▌         | 29/500 [08:13<2:09:53, 16.55s/it]  6%|▌         | 29/500 [08:13<2:13:30, 17.01s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.231 MB of 0.315 MB uploadedwandb: \ 0.231 MB of 0.315 MB uploadedwandb: | 0.231 MB of 0.315 MB uploadedwandb: / 0.231 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▁▄▄▆▅▅▆▆▆▆▆▆▇▆▇▇▇████▁▃▄▆▇
wandb:     train_loss ▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:   val_accuracy ▂▂▁▁▃▂█▂▂█▅▄▇▄▃▇▂▂▂▂▂▂▁▁▂▁▂▅▅
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▁▂█▁▁▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.65973
wandb:     train_loss 0.96517
wandb:   val_accuracy 0.51333
wandb:       val_loss 1.54092
wandb: 
wandb: 🚀 View run feasible-durian-304 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jz8wuloi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_032734-jz8wuloi/logs
Successfully processed 2_20140404
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_033636-af8qgn0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-forest-306
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/af8qgn0o
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:07, 18.17s/it]  0%|          | 2/500 [00:34<2:20:13, 16.90s/it]  1%|          | 3/500 [00:50<2:16:58, 16.54s/it]  1%|          | 4/500 [01:06<2:15:46, 16.42s/it]  1%|          | 5/500 [01:23<2:16:49, 16.58s/it]  1%|          | 6/500 [01:39<2:16:32, 16.58s/it]  1%|▏         | 7/500 [01:56<2:15:13, 16.46s/it]  2%|▏         | 8/500 [02:11<2:13:07, 16.23s/it]  2%|▏         | 9/500 [02:27<2:11:52, 16.12s/it]  2%|▏         | 10/500 [02:44<2:13:05, 16.30s/it]  2%|▏         | 11/500 [03:00<2:12:04, 16.20s/it]  2%|▏         | 12/500 [03:16<2:11:12, 16.13s/it]  3%|▎         | 13/500 [03:32<2:10:20, 16.06s/it]  3%|▎         | 14/500 [03:48<2:09:10, 15.95s/it]  3%|▎         | 15/500 [04:03<2:08:56, 15.95s/it]  3%|▎         | 16/500 [04:19<2:08:17, 15.90s/it]  3%|▎         | 17/500 [04:35<2:07:48, 15.88s/it]  4%|▎         | 18/500 [04:51<2:07:35, 15.88s/it]  4%|▍         | 19/500 [05:07<2:08:11, 15.99s/it]  4%|▍         | 20/500 [05:24<2:08:39, 16.08s/it]  4%|▍         | 21/500 [05:40<2:10:23, 16.33s/it]  4%|▍         | 22/500 [05:57<2:09:28, 16.25s/it]  5%|▍         | 23/500 [06:14<2:11:03, 16.49s/it]  5%|▍         | 24/500 [06:30<2:11:47, 16.61s/it]  5%|▌         | 25/500 [06:48<2:12:41, 16.76s/it]  5%|▌         | 26/500 [07:04<2:10:39, 16.54s/it]  5%|▌         | 27/500 [07:20<2:09:24, 16.41s/it]  6%|▌         | 28/500 [07:36<2:08:53, 16.38s/it]  6%|▌         | 29/500 [07:53<2:10:35, 16.64s/it]  6%|▌         | 29/500 [07:53<2:08:14, 16.34s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.306 MB uploadedwandb: - 0.021 MB of 0.306 MB uploadedwandb: \ 0.306 MB of 0.306 MB uploadedwandb: | 0.306 MB of 0.306 MB uploadedwandb: / 0.306 MB of 0.306 MB uploadedwandb: - 0.306 MB of 0.306 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▄▅▂▄▆▆▄▆▅▆▄▆▇▇▇▆▇█████████
wandb:     train_loss ▃▃▄▃▆▁▄▄▁▃▁█▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▂▂█▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb:       val_loss ▂▂▂▁▂▂▁▂▂▁▃▂▁▃▂▂▅▄▄▃█▄▄▅▄▄▂▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.97771
wandb:     train_loss 0.217
wandb:   val_accuracy 0.29333
wandb:       val_loss 5.6722
wandb: 
wandb: 🚀 View run decent-forest-306 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/af8qgn0o
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_033636-af8qgn0o/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_034515-trcpy5s2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-disco-307
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/trcpy5s2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:08, 17.33s/it]  0%|          | 2/500 [00:33<2:15:56, 16.38s/it]  1%|          | 3/500 [00:49<2:14:17, 16.21s/it]  1%|          | 4/500 [01:04<2:12:18, 16.00s/it]  1%|          | 5/500 [01:19<2:09:37, 15.71s/it]  1%|          | 6/500 [01:34<2:07:13, 15.45s/it]  1%|▏         | 7/500 [01:50<2:06:24, 15.38s/it]  2%|▏         | 8/500 [02:05<2:07:10, 15.51s/it]  2%|▏         | 9/500 [02:21<2:06:57, 15.51s/it]  2%|▏         | 10/500 [02:38<2:10:25, 15.97s/it]  2%|▏         | 11/500 [02:54<2:09:29, 15.89s/it]  2%|▏         | 12/500 [03:09<2:08:24, 15.79s/it]  3%|▎         | 13/500 [03:25<2:07:54, 15.76s/it]  3%|▎         | 14/500 [03:45<2:18:56, 17.15s/it]  3%|▎         | 15/500 [04:01<2:14:51, 16.68s/it]  3%|▎         | 16/500 [04:17<2:12:56, 16.48s/it]  3%|▎         | 17/500 [04:33<2:11:26, 16.33s/it]  4%|▎         | 18/500 [04:50<2:12:26, 16.49s/it]  4%|▍         | 19/500 [05:06<2:11:44, 16.43s/it]  4%|▍         | 20/500 [05:22<2:10:49, 16.35s/it]  4%|▍         | 21/500 [05:38<2:09:21, 16.20s/it]  4%|▍         | 22/500 [05:54<2:07:28, 16.00s/it]  5%|▍         | 23/500 [06:09<2:06:35, 15.92s/it]  5%|▍         | 23/500 [06:09<2:07:49, 16.08s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.136 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▄▁▁▁▁▃▅▁▄▇▄▇█▇▅▆██▆█▇█
wandb:     train_loss ▂▁▃▄▄▁▂▂█▁▁▄▁▁▁▃▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▃▃▃▃▃▁▁▃▂▂▃█▂▁▃▄▁▄▃▄▃▁
wandb:       val_loss ▂▂▅▃▅▃▁▅▆▁▄▆▂▃▄▃▄▄▃▃▇█▇
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.91382
wandb:     train_loss 0.01039
wandb:   val_accuracy 0.28222
wandb:       val_loss 5.72148
wandb: 
wandb: 🚀 View run comic-disco-307 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/trcpy5s2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_034515-trcpy5s2/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_035208-ik2gr1wp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-blaze-309
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ik2gr1wp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:03, 17.68s/it]  0%|          | 2/500 [00:33<2:18:01, 16.63s/it]  1%|          | 3/500 [00:49<2:14:50, 16.28s/it]  1%|          | 4/500 [01:05<2:13:40, 16.17s/it]  1%|          | 5/500 [01:21<2:12:46, 16.09s/it]  1%|          | 6/500 [01:37<2:12:04, 16.04s/it]  1%|▏         | 7/500 [01:53<2:12:26, 16.12s/it]  2%|▏         | 8/500 [02:09<2:11:01, 15.98s/it]  2%|▏         | 9/500 [02:25<2:11:09, 16.03s/it]  2%|▏         | 10/500 [02:41<2:10:37, 16.00s/it]  2%|▏         | 11/500 [02:57<2:10:01, 15.95s/it]  2%|▏         | 12/500 [03:12<2:08:42, 15.82s/it]  3%|▎         | 13/500 [03:29<2:10:35, 16.09s/it]  3%|▎         | 14/500 [03:44<2:08:42, 15.89s/it]  3%|▎         | 15/500 [04:00<2:07:40, 15.79s/it]  3%|▎         | 16/500 [04:16<2:06:58, 15.74s/it]  3%|▎         | 17/500 [04:31<2:06:19, 15.69s/it]  4%|▎         | 18/500 [04:47<2:05:15, 15.59s/it]  4%|▍         | 19/500 [05:02<2:05:05, 15.60s/it]  4%|▍         | 20/500 [05:18<2:05:44, 15.72s/it]  4%|▍         | 21/500 [05:34<2:05:40, 15.74s/it]  4%|▍         | 22/500 [05:49<2:05:02, 15.69s/it]  5%|▍         | 23/500 [06:06<2:06:12, 15.87s/it]  5%|▍         | 24/500 [06:22<2:05:41, 15.84s/it]  5%|▌         | 25/500 [06:38<2:05:45, 15.88s/it]  5%|▌         | 26/500 [06:53<2:05:28, 15.88s/it]  5%|▌         | 27/500 [07:10<2:06:48, 16.09s/it]  6%|▌         | 28/500 [07:26<2:06:26, 16.07s/it]  6%|▌         | 29/500 [07:41<2:04:10, 15.82s/it]  6%|▌         | 29/500 [07:41<2:05:01, 15.93s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.021 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▄▄▆▅▆▅▆▆▆▆▆▆▇▇▇▇████████▇
wandb:     train_loss █▇▆▆▆▄▆▆▆▅▃█▅▂▇▄▄▄▆▄▅▄▃▇▂▅▁▃▄
wandb:   val_accuracy ▂▂▂▅▅▂█▃▃█▄▃▅▄▃▅▂▂▂▁▃▂▁▁▂▂▁▁▂
wandb:       val_loss ▃▃▃▃▂▂▂▃▂▂▃▂▁▃▂▃▄▄▃▃█▄▄▄▂▄▆▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.74443
wandb:     train_loss 0.72852
wandb:   val_accuracy 0.34444
wandb:       val_loss 2.22952
wandb: 
wandb: 🚀 View run devout-blaze-309 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ik2gr1wp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_035208-ik2gr1wp/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_040034-g2kgiqje
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-forest-310
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g2kgiqje
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:29:18, 17.95s/it]  0%|          | 2/500 [00:33<2:18:52, 16.73s/it]  1%|          | 3/500 [00:50<2:18:20, 16.70s/it]  1%|          | 4/500 [01:07<2:17:58, 16.69s/it]  1%|          | 5/500 [01:23<2:17:47, 16.70s/it]  1%|          | 6/500 [01:40<2:17:34, 16.71s/it]  1%|▏         | 7/500 [01:57<2:18:11, 16.82s/it]  2%|▏         | 8/500 [02:13<2:16:12, 16.61s/it]  2%|▏         | 9/500 [02:29<2:14:34, 16.45s/it]  2%|▏         | 10/500 [02:46<2:14:12, 16.43s/it]  2%|▏         | 11/500 [03:02<2:13:22, 16.36s/it]  2%|▏         | 12/500 [03:21<2:19:57, 17.21s/it]  3%|▎         | 13/500 [03:37<2:16:58, 16.88s/it]  3%|▎         | 14/500 [03:53<2:15:03, 16.67s/it]  3%|▎         | 15/500 [04:10<2:14:30, 16.64s/it]  3%|▎         | 16/500 [04:27<2:15:41, 16.82s/it]  3%|▎         | 17/500 [04:43<2:13:28, 16.58s/it]  4%|▎         | 18/500 [05:00<2:13:06, 16.57s/it]  4%|▍         | 19/500 [05:16<2:12:10, 16.49s/it]  4%|▍         | 20/500 [05:32<2:10:40, 16.34s/it]  4%|▍         | 21/500 [05:48<2:09:06, 16.17s/it]  4%|▍         | 22/500 [06:04<2:08:32, 16.14s/it]  5%|▍         | 23/500 [06:20<2:08:46, 16.20s/it]  5%|▍         | 24/500 [06:37<2:09:02, 16.27s/it]  5%|▌         | 25/500 [06:53<2:09:03, 16.30s/it]  5%|▌         | 26/500 [07:09<2:08:07, 16.22s/it]  5%|▌         | 27/500 [07:26<2:08:40, 16.32s/it]  6%|▌         | 28/500 [07:42<2:07:17, 16.18s/it]  6%|▌         | 29/500 [07:58<2:07:07, 16.19s/it]  6%|▌         | 29/500 [07:58<2:09:27, 16.49s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.137 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▃▂▄▄▃▄▅▄▅▆▆▆▆▆▆▇▆▆▇▇▇▇▇▆██
wandb:     train_loss ▃▄▄▄▆▂▄█▂▃▁▅▃▁▄▂▄▂▁▂▃▂▂▅▁▁▁▁▂
wandb:   val_accuracy ▁▂▄▅▇█▇▇▆▇▄▆▇▅▅▄▅▆▅▆▅▄▅▄▄▃▂▄▃
wandb:       val_loss ▃▂▂▂▂▂▃▃▂▁▃▂▁▂▃▁▅▅▂▂█▃▂▅▁▄▃▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.94056
wandb:     train_loss 0.22046
wandb:   val_accuracy 0.39333
wandb:       val_loss 2.8655
wandb: 
wandb: 🚀 View run sweet-forest-310 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g2kgiqje
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_040034-g2kgiqje/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_040918-qv36zi88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-galaxy-311
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qv36zi88
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:08:45, 29.91s/it]  0%|          | 2/500 [00:45<2:59:17, 21.60s/it]  1%|          | 3/500 [01:01<2:38:00, 19.08s/it]  1%|          | 4/500 [01:17<2:25:53, 17.65s/it]  1%|          | 5/500 [01:32<2:19:34, 16.92s/it]  1%|          | 6/500 [01:48<2:15:14, 16.43s/it]  1%|▏         | 7/500 [02:03<2:12:06, 16.08s/it]  2%|▏         | 8/500 [02:19<2:09:54, 15.84s/it]  2%|▏         | 9/500 [02:34<2:08:37, 15.72s/it]  2%|▏         | 10/500 [02:50<2:07:55, 15.67s/it]  2%|▏         | 11/500 [03:05<2:07:30, 15.65s/it]  2%|▏         | 12/500 [03:20<2:06:34, 15.56s/it]  3%|▎         | 13/500 [03:36<2:07:24, 15.70s/it]  3%|▎         | 14/500 [03:52<2:06:01, 15.56s/it]  3%|▎         | 15/500 [04:08<2:06:23, 15.64s/it]  3%|▎         | 16/500 [04:23<2:04:53, 15.48s/it]  3%|▎         | 17/500 [04:38<2:05:20, 15.57s/it]  4%|▎         | 18/500 [04:54<2:04:19, 15.48s/it]  4%|▍         | 19/500 [05:09<2:03:35, 15.42s/it]  4%|▍         | 20/500 [05:25<2:03:47, 15.47s/it]  4%|▍         | 21/500 [05:40<2:03:21, 15.45s/it]  4%|▍         | 22/500 [05:55<2:02:43, 15.40s/it]  5%|▍         | 23/500 [06:11<2:02:11, 15.37s/it]  5%|▍         | 24/500 [06:26<2:01:31, 15.32s/it]  5%|▌         | 25/500 [06:41<2:00:39, 15.24s/it]  5%|▌         | 26/500 [06:56<2:01:08, 15.33s/it]  5%|▌         | 27/500 [07:12<2:01:46, 15.45s/it]  6%|▌         | 28/500 [07:27<2:01:15, 15.42s/it]  6%|▌         | 29/500 [07:43<2:01:26, 15.47s/it]  6%|▌         | 29/500 [07:43<2:05:28, 15.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.231 MB of 0.312 MB uploadedwandb: / 0.231 MB of 0.312 MB uploadedwandb: - 0.231 MB of 0.312 MB uploadedwandb: \ 0.231 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▁▂▁▄▃▃▄▇▃▆▇▇▆██▇█▅█▆█▇█▇██▇
wandb:     train_loss ▃▂▄▇█▁▃▄▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:   val_accuracy ▅▄▃▃▄▂▂▂▄▁▄▆▄▂▃▄▃▃▂▂▄▅▄▂▄▃▃▅█
wandb:       val_loss ▂▂▄▂▅▄▁▆▃▂▆▁▁▅▆▃▅▅▂▃█▃▅▆▆▄▆▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.90045
wandb:     train_loss 4.91182
wandb:   val_accuracy 0.43778
wandb:       val_loss 1.9827
wandb: 
wandb: 🚀 View run comfy-galaxy-311 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qv36zi88
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_040918-qv36zi88/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_041743-ez01ol6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-pond-313
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ez01ol6q
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:39:51, 19.22s/it]  0%|          | 2/500 [00:35<2:24:45, 17.44s/it]  1%|          | 3/500 [00:52<2:22:57, 17.26s/it]  1%|          | 4/500 [01:08<2:18:31, 16.76s/it]  1%|          | 5/500 [01:24<2:17:34, 16.68s/it]  1%|          | 6/500 [01:41<2:15:57, 16.51s/it]  1%|▏         | 7/500 [01:57<2:14:31, 16.37s/it]  2%|▏         | 8/500 [02:12<2:12:30, 16.16s/it]  2%|▏         | 9/500 [02:30<2:14:52, 16.48s/it]  2%|▏         | 10/500 [02:46<2:13:55, 16.40s/it]  2%|▏         | 11/500 [03:01<2:11:00, 16.08s/it]  2%|▏         | 12/500 [03:16<2:07:49, 15.72s/it]  3%|▎         | 13/500 [03:32<2:06:58, 15.64s/it]  3%|▎         | 14/500 [03:47<2:07:04, 15.69s/it]  3%|▎         | 15/500 [04:03<2:07:00, 15.71s/it]  3%|▎         | 16/500 [04:19<2:06:32, 15.69s/it]  3%|▎         | 17/500 [04:34<2:05:08, 15.55s/it]  4%|▎         | 18/500 [04:50<2:06:02, 15.69s/it]  4%|▍         | 19/500 [05:06<2:05:41, 15.68s/it]  4%|▍         | 20/500 [05:21<2:05:39, 15.71s/it]  4%|▍         | 21/500 [05:37<2:05:18, 15.70s/it]  4%|▍         | 22/500 [05:53<2:04:21, 15.61s/it]  5%|▍         | 23/500 [06:08<2:03:59, 15.60s/it]  5%|▍         | 24/500 [06:24<2:03:40, 15.59s/it]  5%|▌         | 25/500 [06:39<2:03:13, 15.57s/it]  5%|▌         | 26/500 [06:55<2:03:06, 15.58s/it]  5%|▌         | 27/500 [07:10<2:02:50, 15.58s/it]  6%|▌         | 28/500 [07:26<2:02:52, 15.62s/it]  6%|▌         | 29/500 [07:42<2:02:52, 15.65s/it]  6%|▌         | 29/500 [07:42<2:05:08, 15.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▃▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇█████████
wandb:     train_loss █▇▆▇▇▅▇▆▆▆▅▇▆▃█▅▅▅▆▄▆▇▄▇▁▅▂▄▅
wandb:   val_accuracy ▂▂▂▄▄▃█▄▅▇▄▃▅▄▄▅▂▂▂▁▂▂▂▁▁▂▁▁▃
wandb:       val_loss ▃▃▃▃▂▂▂▃▂▁▃▂▁▃▂▂▄▄▃▃█▄▄▄▂▄▆▂▆
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.75483
wandb:     train_loss 0.72697
wandb:   val_accuracy 0.38
wandb:       val_loss 2.0069
wandb: 
wandb: 🚀 View run playful-pond-313 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ez01ol6q
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_041743-ez01ol6q/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_042619-tzbl19qz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-donkey-315
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/tzbl19qz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:34, 17.75s/it]  0%|          | 2/500 [00:34<2:21:44, 17.08s/it]  1%|          | 3/500 [00:51<2:20:27, 16.96s/it]  1%|          | 4/500 [01:08<2:20:10, 16.96s/it]  1%|          | 5/500 [01:25<2:20:25, 17.02s/it]  1%|          | 6/500 [01:41<2:17:56, 16.75s/it]  1%|▏         | 7/500 [01:57<2:15:51, 16.54s/it]  2%|▏         | 8/500 [02:13<2:14:53, 16.45s/it]  2%|▏         | 9/500 [02:30<2:14:27, 16.43s/it]  2%|▏         | 10/500 [02:46<2:13:15, 16.32s/it]  2%|▏         | 11/500 [03:02<2:12:07, 16.21s/it]  2%|▏         | 12/500 [03:18<2:10:41, 16.07s/it]  3%|▎         | 13/500 [03:33<2:10:08, 16.03s/it]  3%|▎         | 14/500 [03:50<2:10:13, 16.08s/it]  3%|▎         | 15/500 [04:06<2:09:52, 16.07s/it]  3%|▎         | 16/500 [04:22<2:09:15, 16.02s/it]  3%|▎         | 17/500 [04:38<2:09:19, 16.07s/it]  4%|▎         | 18/500 [04:54<2:09:12, 16.08s/it]  4%|▍         | 19/500 [05:10<2:08:50, 16.07s/it]  4%|▍         | 20/500 [05:26<2:09:43, 16.22s/it]  4%|▍         | 21/500 [05:43<2:09:27, 16.22s/it]  4%|▍         | 22/500 [05:59<2:08:39, 16.15s/it]  5%|▍         | 23/500 [06:15<2:08:44, 16.19s/it]  5%|▍         | 24/500 [06:31<2:08:24, 16.19s/it]  5%|▌         | 25/500 [06:47<2:07:50, 16.15s/it]  5%|▌         | 26/500 [07:03<2:07:01, 16.08s/it]  5%|▌         | 27/500 [07:19<2:06:42, 16.07s/it]  6%|▌         | 28/500 [07:35<2:05:25, 15.94s/it]  6%|▌         | 29/500 [07:51<2:05:31, 15.99s/it]  6%|▌         | 30/500 [08:07<2:06:07, 16.10s/it]  6%|▌         | 31/500 [08:24<2:06:15, 16.15s/it]  6%|▋         | 32/500 [08:39<2:05:12, 16.05s/it]  7%|▋         | 33/500 [08:56<2:05:29, 16.12s/it]  7%|▋         | 34/500 [09:11<2:04:21, 16.01s/it]  7%|▋         | 35/500 [09:27<2:02:51, 15.85s/it]  7%|▋         | 36/500 [09:43<2:03:11, 15.93s/it]  7%|▋         | 37/500 [09:59<2:03:36, 16.02s/it]  8%|▊         | 38/500 [10:16<2:04:51, 16.22s/it]  8%|▊         | 39/500 [10:32<2:04:32, 16.21s/it]  8%|▊         | 40/500 [10:48<2:04:19, 16.22s/it]  8%|▊         | 41/500 [11:05<2:03:52, 16.19s/it]  8%|▊         | 42/500 [11:21<2:03:31, 16.18s/it]  9%|▊         | 43/500 [11:37<2:03:37, 16.23s/it]  9%|▊         | 43/500 [11:37<2:03:33, 16.22s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.020 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▂▃▃▃▃▂▁▁▄▂▃▅▄▂▆▃▃▃▅▄▅▆▅▅▃▆▆▅▆▆▆▇▇▇▆▆▆▇██
wandb:     train_loss ▂▂▂▂▂▃▂▂▂▂▅▂▆▃▁▃▁▂▁▁▁▁▄▁▁▂▁▁▁▁▇▁▅▁▂▁▁█▁▁
wandb:   val_accuracy ▁▅▇▄▃▂▂▂▇▃▄▂█▄▅▃▆▄▆▅▅▂▅▅▂▃▃▄▃▃▄▅▄▃▄▄▃▃▃▂
wandb:       val_loss ▃▃▃▄▂▃▆▄▅▄▅▂▁▄▁█▆▂▆▅▃▂█▂▅▁▆▄▂▃▁▄▁▆▂▆▇▄▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.81426
wandb:     train_loss 0.00182
wandb:   val_accuracy 0.38222
wandb:       val_loss 2.57849
wandb: 
wandb: 🚀 View run stellar-donkey-315 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/tzbl19qz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_042619-tzbl19qz/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_043837-77mefyh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-paper-317
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/77mefyh9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:23:50, 17.30s/it]  0%|          | 2/500 [00:33<2:16:38, 16.46s/it]  1%|          | 3/500 [00:49<2:16:00, 16.42s/it]  1%|          | 4/500 [01:05<2:13:48, 16.19s/it]  1%|          | 5/500 [01:21<2:12:51, 16.10s/it]  1%|          | 6/500 [01:36<2:11:11, 15.94s/it]  1%|▏         | 7/500 [01:52<2:09:58, 15.82s/it]  2%|▏         | 8/500 [02:08<2:10:25, 15.91s/it]  2%|▏         | 9/500 [02:24<2:09:13, 15.79s/it]  2%|▏         | 10/500 [02:39<2:08:35, 15.75s/it]  2%|▏         | 11/500 [02:55<2:08:23, 15.75s/it]  2%|▏         | 12/500 [03:11<2:08:15, 15.77s/it]  3%|▎         | 13/500 [03:27<2:08:29, 15.83s/it]  3%|▎         | 14/500 [03:42<2:07:29, 15.74s/it]  3%|▎         | 15/500 [03:58<2:07:34, 15.78s/it]  3%|▎         | 16/500 [04:14<2:07:02, 15.75s/it]  3%|▎         | 17/500 [04:30<2:06:30, 15.71s/it]  4%|▎         | 18/500 [04:45<2:06:35, 15.76s/it]  4%|▍         | 19/500 [05:01<2:05:56, 15.71s/it]  4%|▍         | 20/500 [05:17<2:05:14, 15.65s/it]  4%|▍         | 21/500 [05:32<2:04:42, 15.62s/it]  4%|▍         | 22/500 [05:47<2:03:40, 15.52s/it]  5%|▍         | 23/500 [06:03<2:03:26, 15.53s/it]  5%|▍         | 24/500 [06:19<2:03:43, 15.60s/it]  5%|▌         | 25/500 [06:34<2:03:54, 15.65s/it]  5%|▌         | 26/500 [06:50<2:03:21, 15.62s/it]  5%|▌         | 27/500 [07:06<2:04:04, 15.74s/it]  6%|▌         | 28/500 [07:22<2:03:33, 15.71s/it]  6%|▌         | 29/500 [07:37<2:02:49, 15.65s/it]  6%|▌         | 29/500 [07:37<2:03:53, 15.78s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.021 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.028 MB of 0.309 MB uploadedwandb: - 0.028 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.028 MB of 0.309 MB uploadedwandb: - 0.028 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.028 MB of 0.309 MB uploadedwandb: - 0.028 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.028 MB of 0.309 MB uploadedwandb: - 0.028 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.028 MB of 0.309 MB uploadedwandb: - 0.028 MB of 0.309 MB uploadedwandb: \ 0.028 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▄▄▄▆▇▅▇▇▆▆▇▇▇▇▆▆▇▇███▇█▆▇█▇
wandb:     train_loss ▃▃▄▂▄▁▂█▅▁▅▁▁▂▃▁▂▂▁█▁▁▂▁▁▁▃▂▁
wandb:   val_accuracy ▃▇▄██▄▄▃▁▁▃▁▂▂▁▂▄▂▃▄▂▂▅▃▃▄▄▆▇
wandb:       val_loss ▂▁▂▁▁▁▁▄▄▁▆▂▁▁▄▁▅▄▂▄█▃▂▅▃▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.85736
wandb:     train_loss 0.00833
wandb:   val_accuracy 0.44444
wandb:       val_loss 0.16754
wandb: 
wandb: 🚀 View run eager-paper-317 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/77mefyh9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_043837-77mefyh9/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_044720-2qgvjd3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-water-318
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2qgvjd3l
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:07:19, 22.52s/it]  0%|          | 2/500 [00:38<2:37:05, 18.93s/it]  1%|          | 3/500 [00:55<2:26:51, 17.73s/it]  1%|          | 4/500 [01:11<2:21:30, 17.12s/it]  1%|          | 5/500 [01:27<2:19:09, 16.87s/it]  1%|          | 6/500 [01:43<2:16:31, 16.58s/it]  1%|▏         | 7/500 [01:59<2:13:56, 16.30s/it]  2%|▏         | 8/500 [02:15<2:12:39, 16.18s/it]  2%|▏         | 9/500 [02:31<2:11:59, 16.13s/it]  2%|▏         | 10/500 [02:47<2:10:27, 15.97s/it]  2%|▏         | 11/500 [03:02<2:09:20, 15.87s/it]  2%|▏         | 12/500 [03:18<2:08:57, 15.85s/it]  3%|▎         | 13/500 [03:34<2:08:40, 15.85s/it]  3%|▎         | 14/500 [03:50<2:09:06, 15.94s/it]  3%|▎         | 15/500 [04:06<2:07:50, 15.82s/it]  3%|▎         | 16/500 [04:22<2:08:34, 15.94s/it]  3%|▎         | 17/500 [04:38<2:08:45, 15.99s/it]  4%|▎         | 18/500 [04:54<2:07:46, 15.91s/it]  4%|▍         | 19/500 [05:10<2:07:39, 15.92s/it]  4%|▍         | 20/500 [05:26<2:07:45, 15.97s/it]  4%|▍         | 21/500 [05:41<2:06:46, 15.88s/it]  4%|▍         | 22/500 [05:57<2:06:30, 15.88s/it]  5%|▍         | 23/500 [06:13<2:05:44, 15.82s/it]  5%|▍         | 24/500 [06:28<2:04:43, 15.72s/it]  5%|▌         | 25/500 [06:45<2:05:32, 15.86s/it]  5%|▌         | 26/500 [07:01<2:06:08, 15.97s/it]  5%|▌         | 27/500 [07:17<2:05:51, 15.96s/it]  6%|▌         | 28/500 [07:32<2:04:45, 15.86s/it]  6%|▌         | 29/500 [07:48<2:04:07, 15.81s/it]  6%|▌         | 29/500 [07:48<2:06:51, 16.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.011 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▆▇▇▇█▇▅▆█▅▆▇▅▂▂▂▂▂▂▂▃▂▃▂▃▂
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂█▁▃▁▂▁▁▁▁▃
wandb:   val_accuracy ▂▂▂▆███▇█▆▇▆▅▆▆▆▂▁▁▁▁▁▃▁▂▁▁▁▁
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃█▂▇▁▂▁▂▂▁▇
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 5.9113
wandb:   val_accuracy 0.32667
wandb:       val_loss 8.49822
wandb: 
wandb: 🚀 View run worthy-water-318 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2qgvjd3l
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_044720-2qgvjd3l/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_045606-zv2i99tg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-dawn-319
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zv2i99tg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:34:53, 18.62s/it]  0%|          | 2/500 [00:38<2:41:28, 19.45s/it]  1%|          | 3/500 [00:54<2:27:51, 17.85s/it]  1%|          | 4/500 [01:14<2:35:12, 18.77s/it]  1%|          | 5/500 [01:30<2:24:48, 17.55s/it]  1%|          | 6/500 [01:45<2:17:53, 16.75s/it]  1%|▏         | 7/500 [02:00<2:14:15, 16.34s/it]  2%|▏         | 8/500 [02:15<2:10:37, 15.93s/it]  2%|▏         | 9/500 [02:31<2:08:35, 15.71s/it]  2%|▏         | 10/500 [02:46<2:06:57, 15.55s/it]  2%|▏         | 11/500 [03:01<2:06:18, 15.50s/it]  2%|▏         | 12/500 [03:17<2:05:37, 15.45s/it]  3%|▎         | 13/500 [03:32<2:05:07, 15.42s/it]  3%|▎         | 14/500 [03:47<2:05:01, 15.44s/it]  3%|▎         | 15/500 [04:02<2:03:35, 15.29s/it]  3%|▎         | 16/500 [04:17<2:02:33, 15.19s/it]  3%|▎         | 17/500 [04:33<2:03:14, 15.31s/it]  4%|▎         | 18/500 [04:48<2:02:11, 15.21s/it]  4%|▍         | 19/500 [05:03<2:01:15, 15.13s/it]  4%|▍         | 20/500 [05:18<2:00:57, 15.12s/it]  4%|▍         | 21/500 [05:33<2:00:36, 15.11s/it]  4%|▍         | 22/500 [05:48<1:59:54, 15.05s/it]  5%|▍         | 23/500 [06:03<1:58:48, 14.94s/it]  5%|▍         | 24/500 [06:17<1:58:24, 14.92s/it]  5%|▌         | 25/500 [06:32<1:58:24, 14.96s/it]  5%|▌         | 26/500 [06:48<2:00:14, 15.22s/it]  5%|▌         | 27/500 [07:03<1:58:49, 15.07s/it]  6%|▌         | 28/500 [07:18<1:57:27, 14.93s/it]  6%|▌         | 29/500 [07:32<1:56:10, 14.80s/it]  6%|▌         | 30/500 [07:47<1:56:41, 14.90s/it]  6%|▌         | 31/500 [08:03<1:57:20, 15.01s/it]  6%|▋         | 32/500 [08:18<1:58:07, 15.14s/it]  7%|▋         | 33/500 [08:33<1:58:22, 15.21s/it]  7%|▋         | 34/500 [08:48<1:57:23, 15.11s/it]  7%|▋         | 35/500 [09:03<1:56:44, 15.06s/it]  7%|▋         | 36/500 [09:18<1:56:43, 15.09s/it]  7%|▋         | 37/500 [09:33<1:55:48, 15.01s/it]  8%|▊         | 38/500 [09:48<1:55:11, 14.96s/it]  8%|▊         | 39/500 [10:03<1:54:57, 14.96s/it]  8%|▊         | 40/500 [10:18<1:54:46, 14.97s/it]  8%|▊         | 41/500 [10:33<1:54:49, 15.01s/it]  8%|▊         | 42/500 [10:48<1:54:50, 15.04s/it]  9%|▊         | 43/500 [11:03<1:54:50, 15.08s/it]  9%|▉         | 44/500 [11:18<1:54:42, 15.09s/it]  9%|▉         | 45/500 [11:34<1:54:54, 15.15s/it]  9%|▉         | 46/500 [11:48<1:53:37, 15.02s/it]  9%|▉         | 47/500 [12:03<1:53:05, 14.98s/it] 10%|▉         | 48/500 [12:18<1:52:33, 14.94s/it] 10%|▉         | 48/500 [12:18<1:55:56, 15.39s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.020 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▂▃▁▄▃▅▆▆▇▇▆█▇▇▇██▇████████▇█▇█▇▇███▇█
wandb:     train_loss ▄▄▄▃▄▆▄▂▂▁▁▁▂▁▁▁▁▁▂▁█▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁▂▁▁▂
wandb:   val_accuracy ▂▅▄▄▃▂█▄▄▃▃▂▁▁▁▁▄▃▄▃▃▂▃▃▄▃▃▄▄▄▄▄▄▄▄▄▄▄▄▃
wandb:       val_loss ▂▂▂▂▁▁▂▂▁▂▁▂▃▁▃▂▂▅▃▂▃▃▁▁▁▂▁▃▂▃▁▃▃▁▁▁█▇▄▂
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.92422
wandb:     train_loss 0.3455
wandb:   val_accuracy 0.4
wandb:       val_loss 1.54259
wandb: 
wandb: 🚀 View run sleek-dawn-319 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zv2i99tg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_045606-zv2i99tg/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_050939-sujx09jp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-rain-322
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sujx09jp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:05:27, 22.30s/it]  0%|          | 2/500 [00:38<2:34:10, 18.58s/it]  1%|          | 3/500 [00:54<2:23:21, 17.31s/it]  1%|          | 4/500 [01:09<2:18:00, 16.69s/it]  1%|          | 5/500 [01:25<2:15:25, 16.42s/it]  1%|          | 6/500 [01:41<2:13:17, 16.19s/it]  1%|▏         | 7/500 [01:56<2:10:51, 15.93s/it]  2%|▏         | 8/500 [02:12<2:09:38, 15.81s/it]  2%|▏         | 9/500 [02:27<2:08:33, 15.71s/it]  2%|▏         | 10/500 [02:43<2:07:28, 15.61s/it]  2%|▏         | 11/500 [02:58<2:07:20, 15.62s/it]  2%|▏         | 12/500 [03:14<2:06:08, 15.51s/it]  3%|▎         | 13/500 [03:30<2:06:35, 15.60s/it]  3%|▎         | 14/500 [03:45<2:06:08, 15.57s/it]  3%|▎         | 15/500 [04:00<2:05:36, 15.54s/it]  3%|▎         | 16/500 [04:16<2:05:38, 15.58s/it]  3%|▎         | 17/500 [04:31<2:04:40, 15.49s/it]  4%|▎         | 18/500 [04:47<2:04:39, 15.52s/it]  4%|▍         | 19/500 [05:02<2:03:56, 15.46s/it]  4%|▍         | 20/500 [05:18<2:03:36, 15.45s/it]  4%|▍         | 21/500 [05:34<2:04:43, 15.62s/it]  4%|▍         | 22/500 [05:50<2:06:55, 15.93s/it]  5%|▍         | 23/500 [06:06<2:05:32, 15.79s/it]  5%|▍         | 24/500 [06:21<2:04:29, 15.69s/it]  5%|▌         | 25/500 [06:37<2:04:04, 15.67s/it]  5%|▌         | 26/500 [06:52<2:02:47, 15.54s/it]  5%|▌         | 27/500 [07:08<2:03:15, 15.64s/it]  6%|▌         | 28/500 [07:25<2:05:07, 15.91s/it]  6%|▌         | 29/500 [07:40<2:04:19, 15.84s/it]  6%|▌         | 29/500 [07:40<2:04:45, 15.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▄▄▅▄▄▁▃▄▅▆▇▇▆▆▇▅█▇▆▇▇█▆█▇▆▇▇
wandb:     train_loss ▄▃▅▃▄▃▅▂▂▃▄▂▁▄▅▁▃▂█▃▁▂▁▁▁▃█▅▁
wandb:   val_accuracy ▁▃▃▂▇▆▄█▇▁▅▂▁▅▂▅▃▂▆▂▄▂▅▂▅▂▂▃▃
wandb:       val_loss ▃▂▃▂▂▂▅▂▄▂▇▂▁▂▄▁▆▄▄▅▄▂▁█▅▄▇▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.77266
wandb:     train_loss 0.00681
wandb:   val_accuracy 0.40444
wandb:       val_loss 2.96095
wandb: 
wandb: 🚀 View run vocal-rain-322 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sujx09jp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_050939-sujx09jp/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_051806-5qr1fjq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-surf-323
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5qr1fjq8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:42:13, 19.51s/it]  0%|          | 2/500 [00:34<2:18:11, 16.65s/it]  1%|          | 3/500 [00:50<2:15:10, 16.32s/it]  1%|          | 4/500 [01:05<2:12:08, 15.98s/it]  1%|          | 5/500 [01:20<2:10:00, 15.76s/it]  1%|          | 6/500 [01:36<2:08:48, 15.64s/it]  1%|▏         | 7/500 [01:51<2:07:32, 15.52s/it]  2%|▏         | 8/500 [02:06<2:06:35, 15.44s/it]  2%|▏         | 9/500 [02:22<2:05:56, 15.39s/it]  2%|▏         | 10/500 [02:37<2:05:00, 15.31s/it]  2%|▏         | 11/500 [02:52<2:04:14, 15.24s/it]  2%|▏         | 12/500 [03:07<2:03:55, 15.24s/it]  3%|▎         | 13/500 [03:23<2:04:29, 15.34s/it]  3%|▎         | 14/500 [03:38<2:03:57, 15.30s/it]  3%|▎         | 15/500 [03:53<2:04:04, 15.35s/it]  3%|▎         | 16/500 [04:08<2:02:46, 15.22s/it]  3%|▎         | 17/500 [04:24<2:04:57, 15.52s/it]  4%|▎         | 18/500 [04:41<2:06:04, 15.69s/it]  4%|▍         | 19/500 [04:56<2:05:48, 15.69s/it]  4%|▍         | 20/500 [05:12<2:05:30, 15.69s/it]  4%|▍         | 21/500 [05:27<2:04:31, 15.60s/it]  4%|▍         | 22/500 [05:43<2:04:08, 15.58s/it]  5%|▍         | 23/500 [05:59<2:04:03, 15.61s/it]  5%|▍         | 24/500 [06:14<2:03:22, 15.55s/it]  5%|▌         | 25/500 [06:30<2:03:10, 15.56s/it]  5%|▌         | 26/500 [06:46<2:04:00, 15.70s/it]  5%|▌         | 27/500 [07:01<2:02:32, 15.54s/it]  6%|▌         | 28/500 [07:17<2:04:59, 15.89s/it]  6%|▌         | 29/500 [07:33<2:04:52, 15.91s/it]  6%|▌         | 29/500 [07:33<2:02:52, 15.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▆▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇████
wandb:     train_loss ██▆▇▇▃▇▇▆▅▃▇▅▂█▄▆▃▆▃▆▇▃▆▄▅▁▃▄
wandb:   val_accuracy ▁▁▁██▁▅▆▄▇▅▅█▇▆█▇▄▇▄▇▆▆▆▇▅▅▅▆
wandb:       val_loss ▄▄▄▄▃▃▂▄▂▂▄▃▁▃▂▂▆▅▃▃█▃▃▅▁▄▂▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.65973
wandb:     train_loss 0.7986
wandb:   val_accuracy 0.58667
wandb:       val_loss 1.22552
wandb: 
wandb: 🚀 View run driven-surf-323 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5qr1fjq8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_051806-5qr1fjq8/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_052621-xuyf5hij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sun-325
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xuyf5hij
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:49, 17.77s/it]  0%|          | 2/500 [00:34<2:20:11, 16.89s/it]  1%|          | 3/500 [00:50<2:17:19, 16.58s/it]  1%|          | 4/500 [01:05<2:14:12, 16.24s/it]  1%|          | 5/500 [01:21<2:11:37, 15.95s/it]  1%|          | 6/500 [01:37<2:10:35, 15.86s/it]  1%|▏         | 7/500 [01:53<2:11:04, 15.95s/it]  2%|▏         | 8/500 [02:08<2:10:02, 15.86s/it]  2%|▏         | 9/500 [02:24<2:10:08, 15.90s/it]  2%|▏         | 10/500 [02:40<2:09:43, 15.88s/it]  2%|▏         | 11/500 [02:58<2:13:19, 16.36s/it]  2%|▏         | 12/500 [03:15<2:15:07, 16.61s/it]  3%|▎         | 13/500 [03:32<2:17:02, 16.88s/it]  3%|▎         | 14/500 [03:47<2:12:11, 16.32s/it]  3%|▎         | 15/500 [04:03<2:09:40, 16.04s/it]  3%|▎         | 16/500 [04:18<2:08:33, 15.94s/it]  3%|▎         | 17/500 [04:34<2:07:19, 15.82s/it]  4%|▎         | 18/500 [04:50<2:06:36, 15.76s/it]  4%|▍         | 19/500 [05:05<2:05:47, 15.69s/it]  4%|▍         | 20/500 [05:21<2:04:57, 15.62s/it]  4%|▍         | 21/500 [05:36<2:04:40, 15.62s/it]  4%|▍         | 22/500 [05:52<2:04:15, 15.60s/it]  5%|▍         | 23/500 [06:07<2:03:15, 15.51s/it]  5%|▍         | 24/500 [06:22<2:02:13, 15.41s/it]  5%|▌         | 25/500 [06:38<2:01:57, 15.41s/it]  5%|▌         | 26/500 [06:53<2:01:37, 15.40s/it]  5%|▌         | 27/500 [07:09<2:02:02, 15.48s/it]  6%|▌         | 28/500 [07:24<2:01:58, 15.50s/it]  6%|▌         | 29/500 [07:40<2:02:40, 15.63s/it]  6%|▌         | 30/500 [07:56<2:02:01, 15.58s/it]  6%|▌         | 31/500 [08:11<2:01:11, 15.50s/it]  6%|▋         | 32/500 [08:27<2:01:04, 15.52s/it]  7%|▋         | 33/500 [08:42<2:01:20, 15.59s/it]  7%|▋         | 34/500 [08:58<2:01:16, 15.61s/it]  7%|▋         | 35/500 [09:13<2:00:18, 15.52s/it]  7%|▋         | 36/500 [09:29<1:59:39, 15.47s/it]  7%|▋         | 37/500 [09:44<1:59:37, 15.50s/it]  8%|▊         | 38/500 [10:00<1:59:15, 15.49s/it]  8%|▊         | 39/500 [10:15<1:59:26, 15.55s/it]  8%|▊         | 40/500 [10:31<2:00:05, 15.66s/it]  8%|▊         | 41/500 [10:47<1:59:04, 15.57s/it]  8%|▊         | 42/500 [11:02<1:59:00, 15.59s/it]  9%|▊         | 43/500 [11:18<1:59:31, 15.69s/it]  9%|▉         | 44/500 [11:34<1:58:51, 15.64s/it]  9%|▉         | 45/500 [11:49<1:58:15, 15.59s/it]  9%|▉         | 46/500 [12:05<1:57:36, 15.54s/it]  9%|▉         | 47/500 [12:20<1:57:27, 15.56s/it] 10%|▉         | 48/500 [12:36<1:57:43, 15.63s/it] 10%|▉         | 49/500 [12:51<1:56:59, 15.56s/it] 10%|█         | 50/500 [13:07<1:56:55, 15.59s/it] 10%|█         | 51/500 [13:23<1:56:29, 15.57s/it] 10%|█         | 52/500 [13:38<1:56:38, 15.62s/it] 11%|█         | 53/500 [13:54<1:56:32, 15.64s/it] 11%|█         | 54/500 [14:09<1:55:33, 15.55s/it] 11%|█         | 55/500 [14:25<1:55:44, 15.60s/it] 11%|█         | 56/500 [14:40<1:54:55, 15.53s/it] 11%|█         | 56/500 [14:40<1:56:24, 15.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.130 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▁▁▂▄▅▆▂▇▅▄▆▅▅▇▇▆█▆▇▇▇▇▇█▆▅▆▇▆███▅▇█▆▇▇
wandb:     train_loss ▂▂▃▃▁▂▂▂▅▁▁▁▁▅▁▃▁▂▁▁▄▁▁▁▂▁▂▅▂█▃▁▁▂▁▁▁▁▂▁
wandb:   val_accuracy ▄▄▄▄▄▄▅▆▄▃▃▃▁▄▃▂▆▁▂▇▂▅▂▄▁▇▃▃▅▂█▄▂▂▄█▄▄▆█
wandb:       val_loss ▂▂▃▃▂▃▃▂▄▁▄▃▄▄▄▅▄▇▂▁█▅▅▃▃▄▄▄▁▄▆▄▄▅▁▁▄▅▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.82021
wandb:     train_loss 0.17322
wandb:   val_accuracy 0.49111
wandb:       val_loss 1.19805
wandb: 
wandb: 🚀 View run copper-sun-325 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xuyf5hij
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_052621-xuyf5hij/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_054149-lq4mrpd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-pond-327
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lq4mrpd6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:22, 25.66s/it]  0%|          | 2/500 [00:49<3:25:25, 24.75s/it]  1%|          | 3/500 [01:13<3:22:33, 24.45s/it]  1%|          | 4/500 [01:38<3:21:37, 24.39s/it]  1%|          | 5/500 [02:02<3:19:45, 24.21s/it]  1%|          | 6/500 [02:25<3:17:36, 24.00s/it]  1%|▏         | 7/500 [02:49<3:15:40, 23.81s/it]  2%|▏         | 8/500 [03:18<3:29:10, 25.51s/it]  2%|▏         | 9/500 [03:49<3:42:55, 27.24s/it]  2%|▏         | 10/500 [04:13<3:34:55, 26.32s/it]  2%|▏         | 11/500 [04:37<3:29:02, 25.65s/it]  2%|▏         | 12/500 [05:01<3:24:31, 25.15s/it]  3%|▎         | 13/500 [05:25<3:21:08, 24.78s/it]  3%|▎         | 14/500 [05:49<3:18:23, 24.49s/it]  3%|▎         | 15/500 [06:13<3:15:56, 24.24s/it]  3%|▎         | 16/500 [06:36<3:14:25, 24.10s/it]  3%|▎         | 17/500 [07:00<3:13:38, 24.05s/it]  4%|▎         | 18/500 [07:24<3:12:15, 23.93s/it]  4%|▍         | 19/500 [07:48<3:11:34, 23.90s/it]  4%|▍         | 20/500 [08:11<3:10:05, 23.76s/it]  4%|▍         | 21/500 [08:35<3:09:15, 23.71s/it]  4%|▍         | 22/500 [08:59<3:09:43, 23.82s/it]  5%|▍         | 23/500 [09:23<3:09:13, 23.80s/it]  5%|▍         | 24/500 [09:46<3:08:46, 23.80s/it]  5%|▌         | 25/500 [10:12<3:13:00, 24.38s/it]  5%|▌         | 25/500 [10:12<3:14:01, 24.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.027 MB uploadedwandb: - 0.010 MB of 0.311 MB uploadedwandb: \ 0.137 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▄▃▃▁▃▂▁▅▇▆▆██▃▇▇▇█▃█▇█
wandb:     train_loss ▂▂▁▁▁▁▆▁▃█▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁
wandb:   val_accuracy ▂▆▃▆█▆▂▆▅▂▆▂▃▄▁▂▅▄▁▄▁▃▂▃▃
wandb:       val_loss ▁▁▁▁▁▂▂▂▁▄▁▂▃▁▂▅█▁▂▂▂▂▂▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.99851
wandb:     train_loss 0.00015
wandb:   val_accuracy 0.39778
wandb:       val_loss 0.68508
wandb: 
wandb: 🚀 View run misunderstood-pond-327 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lq4mrpd6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_054149-lq4mrpd6/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_055252-g2algllg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-microwave-329
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g2algllg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:59, 26.33s/it]  0%|          | 2/500 [00:50<3:27:42, 25.03s/it]  1%|          | 3/500 [01:14<3:25:09, 24.77s/it]  1%|          | 4/500 [01:39<3:23:33, 24.62s/it]  1%|          | 5/500 [02:03<3:21:29, 24.42s/it]  1%|          | 6/500 [02:27<3:19:05, 24.18s/it]  1%|▏         | 7/500 [02:50<3:17:46, 24.07s/it]  2%|▏         | 8/500 [03:14<3:17:00, 24.03s/it]  2%|▏         | 9/500 [03:38<3:16:24, 24.00s/it]  2%|▏         | 10/500 [04:03<3:17:51, 24.23s/it]  2%|▏         | 11/500 [04:27<3:16:34, 24.12s/it]  2%|▏         | 12/500 [04:51<3:15:29, 24.04s/it]  3%|▎         | 13/500 [05:15<3:15:16, 24.06s/it]  3%|▎         | 14/500 [05:39<3:13:54, 23.94s/it]  3%|▎         | 15/500 [06:02<3:12:54, 23.87s/it]  3%|▎         | 16/500 [06:26<3:13:03, 23.93s/it]  3%|▎         | 17/500 [06:50<3:12:17, 23.89s/it]  4%|▎         | 18/500 [07:14<3:11:49, 23.88s/it]  4%|▍         | 19/500 [07:38<3:12:39, 24.03s/it]  4%|▍         | 20/500 [08:03<3:12:42, 24.09s/it]  4%|▍         | 21/500 [08:27<3:12:18, 24.09s/it]  4%|▍         | 22/500 [08:51<3:11:24, 24.03s/it]  5%|▍         | 23/500 [09:15<3:11:22, 24.07s/it]  5%|▍         | 24/500 [09:39<3:11:04, 24.09s/it]  5%|▌         | 25/500 [10:03<3:10:52, 24.11s/it]  5%|▌         | 26/500 [10:28<3:11:24, 24.23s/it]  5%|▌         | 27/500 [10:51<3:09:56, 24.09s/it]  6%|▌         | 28/500 [11:15<3:09:20, 24.07s/it]  6%|▌         | 29/500 [11:43<3:16:55, 25.09s/it]  6%|▌         | 30/500 [12:07<3:13:58, 24.76s/it]  6%|▌         | 31/500 [12:31<3:11:14, 24.47s/it]  6%|▋         | 32/500 [12:54<3:08:59, 24.23s/it]  7%|▋         | 33/500 [13:20<3:12:58, 24.79s/it]  7%|▋         | 34/500 [13:44<3:10:30, 24.53s/it]  7%|▋         | 35/500 [14:08<3:09:00, 24.39s/it]  7%|▋         | 36/500 [14:32<3:07:24, 24.23s/it]  7%|▋         | 37/500 [14:56<3:05:42, 24.07s/it]  8%|▊         | 38/500 [15:20<3:05:09, 24.05s/it]  8%|▊         | 39/500 [15:44<3:05:09, 24.10s/it]  8%|▊         | 40/500 [16:08<3:04:17, 24.04s/it]  8%|▊         | 41/500 [16:32<3:03:51, 24.03s/it]  8%|▊         | 42/500 [16:56<3:02:34, 23.92s/it]  9%|▊         | 43/500 [17:24<3:12:04, 25.22s/it]  9%|▉         | 44/500 [17:48<3:08:57, 24.86s/it]  9%|▉         | 45/500 [18:11<3:05:32, 24.47s/it]  9%|▉         | 46/500 [18:36<3:04:41, 24.41s/it]  9%|▉         | 47/500 [19:00<3:03:02, 24.24s/it] 10%|▉         | 48/500 [19:23<3:01:31, 24.10s/it] 10%|▉         | 49/500 [19:47<3:00:10, 23.97s/it] 10%|█         | 50/500 [20:11<2:59:38, 23.95s/it] 10%|█         | 51/500 [20:35<3:00:22, 24.10s/it] 10%|█         | 52/500 [20:59<2:59:06, 23.99s/it] 11%|█         | 53/500 [21:23<2:58:30, 23.96s/it] 11%|█         | 54/500 [21:47<2:58:17, 23.99s/it] 11%|█         | 55/500 [22:11<2:57:35, 23.94s/it] 11%|█         | 55/500 [22:11<2:59:32, 24.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁
wandb: train_accuracy ▁▃▄▅▅▅▄▂▄▄▄▅▅▅▅▆▅▆▇▆▇▆▆▇█▇█▅▇▇█▇██▆███▇█
wandb:     train_loss ▆▇▆▄▆▇▆▇▆▅█▆▇█▄▆▂▃▄▄▂▆▅▃▅▆▂▁▅▁▁█▁▃▁▂▂▂▄▄
wandb:   val_accuracy ▁▂▆▇█▇▇▂▃▄▄▆▆▅▆▃▆▄▂▄▂▃▃▁▁▂▁▃▂▂▁▂▁▂▃▂▁▁▂▁
wandb:       val_loss ▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▄▂▂▂▄▃▂▃▃▄▂▃▅▁▂▂▃▅▄▅▃█▆▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 54
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.82169
wandb:     train_loss 0.71727
wandb:   val_accuracy 0.33556
wandb:       val_loss 1.27816
wandb: 
wandb: 🚀 View run cool-microwave-329 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g2algllg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_055252-g2algllg/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_061551-wzlgee7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-vortex-331
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wzlgee7n
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:07, 25.75s/it]  0%|          | 2/500 [00:49<3:24:02, 24.58s/it]  1%|          | 3/500 [01:13<3:22:39, 24.47s/it]  1%|          | 4/500 [01:38<3:21:24, 24.36s/it]  1%|          | 5/500 [02:01<3:19:19, 24.16s/it]  1%|          | 6/500 [02:25<3:18:24, 24.10s/it]  1%|▏         | 7/500 [02:49<3:18:10, 24.12s/it]  2%|▏         | 8/500 [03:14<3:17:48, 24.12s/it]  2%|▏         | 9/500 [03:38<3:17:23, 24.12s/it]  2%|▏         | 10/500 [04:01<3:15:54, 23.99s/it]  2%|▏         | 11/500 [04:25<3:15:07, 23.94s/it]  2%|▏         | 12/500 [04:49<3:14:46, 23.95s/it]  3%|▎         | 13/500 [05:13<3:14:22, 23.95s/it]  3%|▎         | 14/500 [05:37<3:14:42, 24.04s/it]  3%|▎         | 15/500 [06:02<3:15:55, 24.24s/it]  3%|▎         | 16/500 [06:27<3:16:03, 24.30s/it]  3%|▎         | 17/500 [06:51<3:15:31, 24.29s/it]  4%|▎         | 18/500 [07:15<3:14:28, 24.21s/it]  4%|▍         | 19/500 [07:39<3:13:39, 24.16s/it]  4%|▍         | 20/500 [08:03<3:12:18, 24.04s/it]  4%|▍         | 21/500 [08:26<3:11:14, 23.96s/it]  4%|▍         | 22/500 [08:51<3:11:55, 24.09s/it]  5%|▍         | 23/500 [09:15<3:10:57, 24.02s/it]  5%|▍         | 24/500 [09:39<3:10:24, 24.00s/it]  5%|▌         | 25/500 [10:02<3:09:06, 23.89s/it]  5%|▌         | 26/500 [10:26<3:07:20, 23.71s/it]  5%|▌         | 27/500 [10:49<3:06:18, 23.63s/it]  6%|▌         | 28/500 [11:13<3:06:19, 23.69s/it]  6%|▌         | 29/500 [11:37<3:06:31, 23.76s/it]  6%|▌         | 30/500 [12:01<3:06:35, 23.82s/it]  6%|▌         | 31/500 [12:24<3:05:46, 23.77s/it]  6%|▋         | 32/500 [12:48<3:04:53, 23.70s/it]  7%|▋         | 33/500 [13:12<3:04:34, 23.72s/it]  7%|▋         | 34/500 [13:36<3:06:14, 23.98s/it]  7%|▋         | 35/500 [14:01<3:06:29, 24.06s/it]  7%|▋         | 36/500 [14:24<3:05:44, 24.02s/it]  7%|▋         | 37/500 [14:48<3:04:25, 23.90s/it]  8%|▊         | 38/500 [15:12<3:03:18, 23.81s/it]  8%|▊         | 39/500 [15:37<3:07:24, 24.39s/it]  8%|▊         | 40/500 [16:02<3:06:46, 24.36s/it]  8%|▊         | 41/500 [16:29<3:13:59, 25.36s/it]  8%|▊         | 42/500 [16:53<3:10:16, 24.93s/it]  9%|▊         | 43/500 [17:17<3:07:30, 24.62s/it]  9%|▉         | 44/500 [17:41<3:05:01, 24.34s/it]  9%|▉         | 45/500 [18:05<3:04:17, 24.30s/it]  9%|▉         | 46/500 [18:29<3:02:47, 24.16s/it]  9%|▉         | 47/500 [18:53<3:01:13, 24.00s/it] 10%|▉         | 48/500 [19:16<3:00:08, 23.91s/it] 10%|▉         | 49/500 [19:40<3:00:18, 23.99s/it] 10%|█         | 50/500 [20:07<3:04:44, 24.63s/it] 10%|█         | 51/500 [20:31<3:02:53, 24.44s/it] 10%|█         | 52/500 [20:54<3:00:50, 24.22s/it] 11%|█         | 53/500 [21:18<2:59:21, 24.07s/it] 11%|█         | 54/500 [21:46<3:08:10, 25.31s/it] 11%|█         | 55/500 [22:10<3:04:01, 24.81s/it] 11%|█         | 56/500 [22:34<3:01:32, 24.53s/it] 11%|█▏        | 57/500 [22:58<2:59:33, 24.32s/it] 11%|█▏        | 57/500 [22:58<2:58:30, 24.18s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.029 MB uploadedwandb: - 0.020 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▂▃▂▂▃▂▃▃▅▃▄▃▃▄▆▅▅▅▅▅▇▇▇▃▇▇▇▇██████▇████
wandb:     train_loss ▂▂▃▁▄▁▃▁▁▁▃▃█▁▃▁▁▁█▁▁▁▁▁▅▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▄▇▄▃▄▃▃▂▂▃▃▃▃▃▃▃▂▃▃▃▂▁▂▃▂▂█▂▂▅▂▅▅▃▂▁▂▁▂
wandb:       val_loss ▂▂▃▃▅▂▇▅█▁▇▄▆▅▇█▅▄▆▆▁▄▂▄▄▄▃▁▁▁▄▄▄▄▄▇▆▅▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.96582
wandb:     train_loss 0.00085
wandb:   val_accuracy 0.28222
wandb:       val_loss 1.90822
wandb: 
wandb: 🚀 View run fresh-vortex-331 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wzlgee7n
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_061551-wzlgee7n/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_063942-da30bxoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sky-334
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/da30bxoz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:36:57, 26.09s/it]  0%|          | 2/500 [00:49<3:25:44, 24.79s/it]  1%|          | 3/500 [01:14<3:23:57, 24.62s/it]  1%|          | 4/500 [01:38<3:21:43, 24.40s/it]  1%|          | 5/500 [02:02<3:20:08, 24.26s/it]  1%|          | 6/500 [02:26<3:19:49, 24.27s/it]  1%|▏         | 7/500 [02:51<3:20:07, 24.36s/it]  2%|▏         | 8/500 [03:15<3:19:20, 24.31s/it]  2%|▏         | 9/500 [03:39<3:19:00, 24.32s/it]  2%|▏         | 10/500 [04:08<3:29:38, 25.67s/it]  2%|▏         | 11/500 [04:33<3:27:15, 25.43s/it]  2%|▏         | 12/500 [04:57<3:23:23, 25.01s/it]  3%|▎         | 13/500 [05:21<3:20:12, 24.67s/it]  3%|▎         | 14/500 [05:45<3:18:47, 24.54s/it]  3%|▎         | 15/500 [06:09<3:16:51, 24.35s/it]  3%|▎         | 16/500 [06:33<3:14:59, 24.17s/it]  3%|▎         | 17/500 [06:56<3:12:56, 23.97s/it]  4%|▎         | 18/500 [07:20<3:13:08, 24.04s/it]  4%|▍         | 19/500 [07:44<3:11:57, 23.94s/it]  4%|▍         | 20/500 [08:08<3:11:40, 23.96s/it]  4%|▍         | 21/500 [08:32<3:10:40, 23.88s/it]  4%|▍         | 22/500 [08:56<3:10:51, 23.96s/it]  5%|▍         | 23/500 [09:20<3:09:26, 23.83s/it]  5%|▍         | 24/500 [09:44<3:09:24, 23.87s/it]  5%|▌         | 25/500 [10:07<3:08:58, 23.87s/it]  5%|▌         | 26/500 [10:31<3:08:52, 23.91s/it]  5%|▌         | 27/500 [10:55<3:08:24, 23.90s/it]  6%|▌         | 28/500 [11:19<3:07:25, 23.83s/it]  6%|▌         | 29/500 [11:43<3:07:15, 23.85s/it]  6%|▌         | 30/500 [12:07<3:07:52, 23.98s/it]  6%|▌         | 30/500 [12:07<3:10:02, 24.26s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.020 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▃▄▄▄▁▅▄▁▆▆▆▇██▆██▅██▇██▆███▇
wandb:     train_loss ▂▂▁▂▁▁▆▁▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▆▆▁▇█▃▂█▃▄▅▃▂▂▃▄▄▄▃▃▃▅▄▄▃▆▂▄▄
wandb:       val_loss ▂▁▁▁▁▁▂▂▁▃▂▁▄▁▃▆█▁▂▂▃▃▁▅▂▂▂▂▆▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.94502
wandb:     train_loss 0.00915
wandb:   val_accuracy 0.46889
wandb:       val_loss 4.71517
wandb: 
wandb: 🚀 View run honest-sky-334 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/da30bxoz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_063942-da30bxoz/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_065232-vecycsul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-firebrand-336
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vecycsul
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:18, 26.25s/it]  0%|          | 2/500 [00:51<3:31:15, 25.45s/it]  1%|          | 3/500 [01:15<3:25:16, 24.78s/it]  1%|          | 4/500 [01:39<3:22:52, 24.54s/it]  1%|          | 5/500 [02:03<3:20:28, 24.30s/it]  1%|          | 6/500 [02:27<3:19:02, 24.18s/it]  1%|▏         | 7/500 [02:50<3:16:48, 23.95s/it]  2%|▏         | 8/500 [03:14<3:15:34, 23.85s/it]  2%|▏         | 9/500 [03:38<3:15:27, 23.88s/it]  2%|▏         | 10/500 [04:02<3:14:56, 23.87s/it]  2%|▏         | 11/500 [04:25<3:14:28, 23.86s/it]  2%|▏         | 12/500 [04:49<3:14:06, 23.86s/it]  3%|▎         | 13/500 [05:13<3:14:15, 23.93s/it]  3%|▎         | 14/500 [05:37<3:14:14, 23.98s/it]  3%|▎         | 15/500 [06:01<3:13:01, 23.88s/it]  3%|▎         | 16/500 [06:25<3:12:24, 23.85s/it]  3%|▎         | 17/500 [06:52<3:20:38, 24.92s/it]  4%|▎         | 18/500 [07:16<3:18:17, 24.68s/it]  4%|▍         | 19/500 [07:40<3:15:52, 24.43s/it]  4%|▍         | 20/500 [08:04<3:14:13, 24.28s/it]  4%|▍         | 21/500 [08:28<3:12:05, 24.06s/it]  4%|▍         | 22/500 [08:52<3:11:04, 23.99s/it]  5%|▍         | 23/500 [09:15<3:09:32, 23.84s/it]  5%|▍         | 24/500 [09:39<3:08:46, 23.80s/it]  5%|▌         | 25/500 [10:02<3:07:52, 23.73s/it]  5%|▌         | 26/500 [10:26<3:07:04, 23.68s/it]  5%|▌         | 27/500 [10:49<3:05:45, 23.56s/it]  6%|▌         | 28/500 [11:17<3:16:28, 24.98s/it]  6%|▌         | 29/500 [11:49<3:32:14, 27.04s/it]  6%|▌         | 30/500 [12:13<3:24:32, 26.11s/it]  6%|▌         | 31/500 [12:37<3:18:24, 25.38s/it]  6%|▋         | 32/500 [13:01<3:14:27, 24.93s/it]  7%|▋         | 33/500 [13:25<3:11:22, 24.59s/it]  7%|▋         | 34/500 [13:48<3:09:11, 24.36s/it]  7%|▋         | 35/500 [14:12<3:07:23, 24.18s/it]  7%|▋         | 36/500 [14:36<3:06:00, 24.05s/it]  7%|▋         | 37/500 [15:00<3:04:46, 23.95s/it]  8%|▊         | 38/500 [15:23<3:03:26, 23.82s/it]  8%|▊         | 39/500 [15:47<3:02:59, 23.82s/it]  8%|▊         | 40/500 [16:11<3:02:32, 23.81s/it]  8%|▊         | 41/500 [16:35<3:02:28, 23.85s/it]  8%|▊         | 42/500 [16:58<3:01:51, 23.83s/it]  9%|▊         | 43/500 [17:22<3:00:49, 23.74s/it]  9%|▉         | 44/500 [17:47<3:03:35, 24.16s/it]  9%|▉         | 45/500 [18:11<3:03:12, 24.16s/it]  9%|▉         | 46/500 [18:40<3:13:53, 25.62s/it]  9%|▉         | 47/500 [19:09<3:20:59, 26.62s/it] 10%|▉         | 48/500 [19:33<3:14:12, 25.78s/it] 10%|▉         | 49/500 [19:57<3:08:51, 25.12s/it] 10%|█         | 50/500 [20:20<3:05:16, 24.70s/it] 10%|█         | 51/500 [20:44<3:02:43, 24.42s/it] 10%|█         | 52/500 [21:08<3:00:57, 24.24s/it] 11%|█         | 53/500 [21:32<2:59:09, 24.05s/it] 11%|█         | 54/500 [22:00<3:09:08, 25.44s/it] 11%|█         | 55/500 [22:24<3:05:08, 24.96s/it] 11%|█         | 56/500 [22:48<3:01:24, 24.51s/it] 11%|█▏        | 57/500 [23:16<3:09:46, 25.70s/it] 12%|█▏        | 58/500 [23:45<3:15:40, 26.56s/it] 12%|█▏        | 58/500 [23:52<3:01:56, 24.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▃▆▅▅▅▅▄▆▅▅▅▅▇▆▇▆▇▇▇█▇██▇▇███▆▇▇████▇▇
wandb:     train_loss ▅▅▆▃▆▂▅█▁▅▅▅▃▃▄▃▄▄▄▃▅▂▂▁▁▂▂▅▄▂▃▆▂▄▅▄▆▄▆▄
wandb:   val_accuracy ▁▁▅▇██▇██▅▆▆▄▅▄▄▄▃▃▃▃▃▃▃▂▂▃▃▃▃▃▂▃▃▃▁▃▂▃▃
wandb:       val_loss ▃▃▃▃▃▂▂▂▂▂▃▂▂▂▃▆▃▄▃▂▂▅▂▄▄▂▁▂▁▁▃▂▅▅█▇▂▃▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 57
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.73403
wandb:     train_loss 0.68356
wandb:   val_accuracy 0.45778
wandb:       val_loss 1.90977
wandb: 
wandb: 🚀 View run fanciful-firebrand-336 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vecycsul
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_065232-vecycsul/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_071711-oyc4vvkg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-morning-339
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oyc4vvkg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:42:15, 26.72s/it]  0%|          | 2/500 [00:50<3:29:24, 25.23s/it]  1%|          | 3/500 [01:15<3:26:25, 24.92s/it]  1%|          | 4/500 [01:39<3:24:20, 24.72s/it]  1%|          | 5/500 [02:08<3:34:32, 26.00s/it]  1%|          | 6/500 [02:33<3:31:24, 25.68s/it]  1%|▏         | 7/500 [03:04<3:45:58, 27.50s/it]  2%|▏         | 8/500 [03:28<3:37:02, 26.47s/it]  2%|▏         | 9/500 [03:52<3:30:57, 25.78s/it]  2%|▏         | 10/500 [04:17<3:26:59, 25.35s/it]  2%|▏         | 11/500 [04:41<3:24:37, 25.11s/it]  2%|▏         | 12/500 [05:06<3:22:34, 24.91s/it]  3%|▎         | 13/500 [05:31<3:21:48, 24.86s/it]  3%|▎         | 14/500 [05:55<3:20:13, 24.72s/it]  3%|▎         | 15/500 [06:28<3:39:29, 27.15s/it]  3%|▎         | 16/500 [06:52<3:32:47, 26.38s/it]  3%|▎         | 17/500 [07:22<3:39:18, 27.24s/it]  4%|▎         | 18/500 [07:46<3:32:01, 26.39s/it]  4%|▍         | 19/500 [08:15<3:38:46, 27.29s/it]  4%|▍         | 20/500 [08:46<3:47:11, 28.40s/it]  4%|▍         | 21/500 [09:12<3:39:07, 27.45s/it]  4%|▍         | 22/500 [09:37<3:32:28, 26.67s/it]  5%|▍         | 23/500 [10:01<3:26:30, 25.98s/it]  5%|▍         | 24/500 [10:26<3:22:56, 25.58s/it]  5%|▌         | 25/500 [10:50<3:19:55, 25.25s/it]  5%|▌         | 26/500 [11:15<3:18:06, 25.08s/it]  5%|▌         | 27/500 [11:39<3:16:09, 24.88s/it]  6%|▌         | 28/500 [12:03<3:14:34, 24.73s/it]  6%|▌         | 29/500 [12:28<3:14:34, 24.79s/it]  6%|▌         | 30/500 [12:55<3:17:23, 25.20s/it]  6%|▌         | 31/500 [13:20<3:16:22, 25.12s/it]  6%|▋         | 32/500 [13:44<3:15:00, 25.00s/it]  7%|▋         | 33/500 [14:09<3:13:28, 24.86s/it]  7%|▋         | 34/500 [14:34<3:12:54, 24.84s/it]  7%|▋         | 35/500 [14:58<3:12:33, 24.85s/it]  7%|▋         | 36/500 [15:23<3:10:26, 24.63s/it]  7%|▋         | 37/500 [15:47<3:10:40, 24.71s/it]  8%|▊         | 38/500 [16:12<3:09:30, 24.61s/it]  8%|▊         | 39/500 [16:42<3:21:41, 26.25s/it]  8%|▊         | 40/500 [17:06<3:16:43, 25.66s/it]  8%|▊         | 41/500 [17:37<3:29:11, 27.34s/it]  8%|▊         | 42/500 [18:07<3:34:40, 28.12s/it]  9%|▊         | 43/500 [18:32<3:26:08, 27.06s/it]  9%|▉         | 44/500 [19:01<3:31:06, 27.78s/it]  9%|▉         | 45/500 [19:26<3:23:56, 26.89s/it]  9%|▉         | 45/500 [19:32<3:17:30, 26.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▃▂▂▅▂▃▃▃▃▄▅▃▄▃▆▄▄▄▆▄▆▅▆▅██▇▇█▆███▇████
wandb:     train_loss ▂▂▃▂▁▁▂▁▂▁▁▁▂▃▃▂▁▂▂▁▁▁▁█▁▄▁▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃█▆▃▂▅▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▄▂▂▁▄▂▃▂▂▂▂▂▁▁▄▁▂▃▁
wandb:       val_loss ▂▂▂▂▃▃▅▂▂▅█▁▁▆▄▁▄▄▆█▄▄▄▆▅▁▂▃▃▃▄▃▄▃▃▂▁▁▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.95988
wandb:     train_loss 0.03364
wandb:   val_accuracy 0.28
wandb:       val_loss 2.04251
wandb: 
wandb: 🚀 View run azure-morning-339 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oyc4vvkg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_071711-oyc4vvkg/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_073732-ncf85jmk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-elevator-340
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ncf85jmk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:24:50, 31.84s/it]  0%|          | 2/500 [01:03<4:22:29, 31.62s/it]  1%|          | 3/500 [01:29<3:59:58, 28.97s/it]  1%|          | 4/500 [01:53<3:45:31, 27.28s/it]  1%|          | 5/500 [02:18<3:37:14, 26.33s/it]  1%|          | 6/500 [02:51<3:55:00, 28.54s/it]  1%|▏         | 7/500 [03:23<4:03:48, 29.67s/it]  2%|▏         | 8/500 [03:55<4:09:32, 30.43s/it]  2%|▏         | 9/500 [04:24<4:06:25, 30.11s/it]  2%|▏         | 10/500 [04:49<3:51:27, 28.34s/it]  2%|▏         | 11/500 [05:13<3:39:57, 26.99s/it]  2%|▏         | 12/500 [05:37<3:32:39, 26.15s/it]  3%|▎         | 13/500 [06:02<3:29:15, 25.78s/it]  3%|▎         | 14/500 [06:29<3:31:18, 26.09s/it]  3%|▎         | 15/500 [06:53<3:27:18, 25.65s/it]  3%|▎         | 16/500 [07:17<3:22:11, 25.07s/it]  3%|▎         | 17/500 [07:41<3:19:26, 24.77s/it]  4%|▎         | 18/500 [08:05<3:17:35, 24.60s/it]  4%|▍         | 19/500 [08:34<3:27:49, 25.92s/it]  4%|▍         | 20/500 [08:59<3:24:04, 25.51s/it]  4%|▍         | 21/500 [09:23<3:20:31, 25.12s/it]  4%|▍         | 22/500 [09:48<3:19:14, 25.01s/it]  5%|▍         | 23/500 [10:12<3:18:02, 24.91s/it]  5%|▍         | 24/500 [10:37<3:16:26, 24.76s/it]  5%|▌         | 25/500 [11:01<3:14:25, 24.56s/it]  5%|▌         | 26/500 [11:25<3:12:46, 24.40s/it]  5%|▌         | 27/500 [11:49<3:12:07, 24.37s/it]  6%|▌         | 28/500 [12:13<3:11:32, 24.35s/it]  6%|▌         | 29/500 [12:38<3:10:38, 24.29s/it]  6%|▌         | 30/500 [13:02<3:11:14, 24.41s/it]  6%|▌         | 30/500 [13:02<3:24:24, 26.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.010 MB of 0.308 MB uploadedwandb: \ 0.137 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▃▄▄▃▁▄▃▁▅▄▃▇▄▆▄▇▆▇▆▅▆█▄█▆▄██
wandb:     train_loss ▂▂▁▂▁▁█▁▂▇▂▂▂▁▁▁▄▁▁▁▁▁▁▁▁▁▁▃▁▁
wandb:   val_accuracy ▂▅▆▁▅█▃▇█▃▆▇▅▃▅▆▄▃▄▄▅▆▅▆▄▅▅▃▄▆
wandb:       val_loss ▂▁▁▁▁▁▂▂▁▃▁▁▄▁▁▅█▂▂▂▂▂▁▃▃▁▂▁▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 1.0
wandb:     train_loss 0.01467
wandb:   val_accuracy 0.52889
wandb:       val_loss 3.18106
wandb: 
wandb: 🚀 View run fast-elevator-340 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ncf85jmk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_073732-ncf85jmk/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_075128-c0zcvwgd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-fog-342
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c0zcvwgd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:37, 25.45s/it]  0%|          | 2/500 [00:49<3:25:04, 24.71s/it]  1%|          | 3/500 [01:14<3:23:38, 24.58s/it]  1%|          | 4/500 [01:38<3:23:12, 24.58s/it]  1%|          | 5/500 [02:03<3:22:15, 24.52s/it]  1%|          | 6/500 [02:29<3:28:08, 25.28s/it]  1%|▏         | 7/500 [02:53<3:24:14, 24.86s/it]  2%|▏         | 8/500 [03:25<3:41:30, 27.01s/it]  2%|▏         | 9/500 [03:49<3:34:12, 26.18s/it]  2%|▏         | 10/500 [04:13<3:28:46, 25.56s/it]  2%|▏         | 11/500 [04:38<3:24:42, 25.12s/it]  2%|▏         | 12/500 [05:02<3:21:30, 24.78s/it]  3%|▎         | 13/500 [05:30<3:30:30, 25.94s/it]  3%|▎         | 14/500 [05:54<3:25:17, 25.34s/it]  3%|▎         | 15/500 [06:18<3:21:28, 24.93s/it]  3%|▎         | 16/500 [06:42<3:19:12, 24.69s/it]  3%|▎         | 17/500 [07:06<3:17:22, 24.52s/it]  4%|▎         | 18/500 [07:36<3:28:33, 25.96s/it]  4%|▍         | 19/500 [08:00<3:23:49, 25.42s/it]  4%|▍         | 20/500 [08:24<3:21:08, 25.14s/it]  4%|▍         | 21/500 [08:54<3:31:22, 26.48s/it]  4%|▍         | 22/500 [09:19<3:26:46, 25.96s/it]  5%|▍         | 23/500 [09:43<3:23:07, 25.55s/it]  5%|▍         | 24/500 [10:07<3:19:12, 25.11s/it]  5%|▌         | 25/500 [10:31<3:15:56, 24.75s/it]  5%|▌         | 26/500 [10:55<3:13:57, 24.55s/it]  5%|▌         | 27/500 [11:20<3:12:42, 24.44s/it]  6%|▌         | 28/500 [11:49<3:25:13, 26.09s/it]  6%|▌         | 29/500 [12:20<3:35:01, 27.39s/it]  6%|▌         | 30/500 [12:44<3:26:54, 26.41s/it]  6%|▌         | 31/500 [13:08<3:21:06, 25.73s/it]  6%|▋         | 32/500 [13:33<3:17:48, 25.36s/it]  7%|▋         | 33/500 [13:57<3:14:03, 24.93s/it]  7%|▋         | 34/500 [14:21<3:11:40, 24.68s/it]  7%|▋         | 35/500 [14:53<3:30:04, 27.11s/it]  7%|▋         | 36/500 [15:17<3:22:18, 26.16s/it]  7%|▋         | 37/500 [15:49<3:34:19, 27.77s/it]  8%|▊         | 38/500 [16:13<3:25:08, 26.64s/it]  8%|▊         | 39/500 [16:37<3:19:09, 25.92s/it]  8%|▊         | 40/500 [17:07<3:27:22, 27.05s/it]  8%|▊         | 41/500 [17:31<3:21:21, 26.32s/it]  8%|▊         | 42/500 [17:56<3:15:44, 25.64s/it]  9%|▊         | 43/500 [18:20<3:12:29, 25.27s/it]  9%|▉         | 44/500 [18:44<3:09:54, 24.99s/it]  9%|▉         | 45/500 [19:09<3:09:13, 24.95s/it]  9%|▉         | 46/500 [19:34<3:07:57, 24.84s/it]  9%|▉         | 47/500 [20:03<3:17:14, 26.12s/it] 10%|▉         | 48/500 [20:27<3:11:55, 25.48s/it] 10%|▉         | 49/500 [20:51<3:07:58, 25.01s/it] 10%|█         | 50/500 [21:15<3:05:35, 24.75s/it] 10%|█         | 51/500 [21:39<3:03:59, 24.59s/it] 10%|█         | 52/500 [22:03<3:02:20, 24.42s/it] 11%|█         | 53/500 [22:27<3:01:10, 24.32s/it] 11%|█         | 54/500 [22:51<2:59:52, 24.20s/it] 11%|█         | 55/500 [23:15<2:59:17, 24.17s/it] 11%|█         | 56/500 [23:39<2:58:29, 24.12s/it] 11%|█▏        | 57/500 [24:03<2:57:17, 24.01s/it] 12%|█▏        | 58/500 [24:27<2:56:42, 23.99s/it] 12%|█▏        | 59/500 [24:51<2:56:14, 23.98s/it] 12%|█▏        | 60/500 [25:19<3:04:22, 25.14s/it] 12%|█▏        | 60/500 [25:19<3:05:41, 25.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.020 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▃▄▅▅▂▅▅▅▆▆▇▇▅▇▅▆▅▆█▆▇▇▇▇▇▇▇███▇▇██████
wandb:     train_loss ▄▅▅▃▅▂▄▄▁▄▆▅▃▂▂▃▃█▁▄▁▃▁▇▂▃▃▂▅▃▃▂▂▂▁▇▄▆▆▁
wandb:   val_accuracy ▃▃▂▅▃█▃▆▄▇▄▆▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▂
wandb:       val_loss ▃▃▂▃▃▂▂▃▂▁▃▂▃▂▆▃▃▄▅▁▄▃▄▂▄▆▄▁▄▁▃▇▄█▅▃▃▆▄▆
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.79049
wandb:     train_loss 0.17808
wandb:   val_accuracy 0.30444
wandb:       val_loss 2.65445
wandb: 
wandb: 🚀 View run lively-fog-342 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c0zcvwgd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_075128-c0zcvwgd/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_081730-4cy8k50q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-salad-343
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4cy8k50q
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:58, 25.85s/it]  0%|          | 2/500 [00:50<3:27:18, 24.98s/it]  1%|          | 3/500 [01:14<3:26:07, 24.89s/it]  1%|          | 4/500 [01:39<3:24:20, 24.72s/it]  1%|          | 5/500 [02:03<3:23:00, 24.61s/it]  1%|          | 6/500 [02:28<3:23:01, 24.66s/it]  1%|▏         | 7/500 [02:52<3:21:46, 24.56s/it]  2%|▏         | 8/500 [03:18<3:22:38, 24.71s/it]  2%|▏         | 9/500 [03:42<3:21:23, 24.61s/it]  2%|▏         | 10/500 [04:07<3:22:07, 24.75s/it]  2%|▏         | 11/500 [04:32<3:22:23, 24.83s/it]  2%|▏         | 12/500 [05:02<3:35:41, 26.52s/it]  3%|▎         | 13/500 [05:27<3:30:42, 25.96s/it]  3%|▎         | 14/500 [05:52<3:27:24, 25.61s/it]  3%|▎         | 15/500 [06:21<3:35:35, 26.67s/it]  3%|▎         | 16/500 [06:51<3:42:42, 27.61s/it]  3%|▎         | 17/500 [07:20<3:46:51, 28.18s/it]  4%|▎         | 18/500 [07:45<3:37:34, 27.08s/it]  4%|▍         | 19/500 [08:14<3:43:23, 27.87s/it]  4%|▍         | 20/500 [08:45<3:48:32, 28.57s/it]  4%|▍         | 21/500 [09:09<3:37:40, 27.27s/it]  4%|▍         | 22/500 [09:34<3:32:33, 26.68s/it]  5%|▍         | 23/500 [10:04<3:38:21, 27.47s/it]  5%|▍         | 24/500 [10:34<3:46:01, 28.49s/it]  5%|▌         | 25/500 [11:04<3:47:33, 28.74s/it]  5%|▌         | 26/500 [11:33<3:47:56, 28.85s/it]  5%|▌         | 27/500 [11:57<3:37:13, 27.55s/it]  6%|▌         | 28/500 [12:27<3:42:34, 28.29s/it]  6%|▌         | 29/500 [12:52<3:33:11, 27.16s/it]  6%|▌         | 29/500 [12:57<3:30:28, 26.81s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.020 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▄▄▄▄▄▃▃▃▃▆▃▄▄▅▅▅▂▁▂▆▆▇▇▇▇██▇
wandb:     train_loss ▂▂▂▂▁▂▂▁▂▁▁▁▁▂▂▃▁█▂▂▂▂▁▁▁▂▁▁▁
wandb:   val_accuracy ▂▆▆▂▃▂▂▂▂▁▁▁▁▁▁▁▁▂▃▁█▇▇▆▄▅▆▆▄
wandb:       val_loss ▂▂▂▂▃▃▄▂▇▂▄█▁▁▆▄▁▇▂▂▂▃▂▂▁▂▃▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.83507
wandb:     train_loss 0.12367
wandb:   val_accuracy 0.40889
wandb:       val_loss 0.08287
wandb: 
wandb: 🚀 View run super-salad-343 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4cy8k50q
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_081730-4cy8k50q/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_083110-mddv32r3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-plant-344
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mddv32r3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:00, 25.37s/it]  0%|          | 2/500 [00:52<3:36:53, 26.13s/it]  1%|          | 3/500 [01:16<3:30:02, 25.36s/it]  1%|          | 4/500 [01:40<3:26:19, 24.96s/it]  1%|          | 5/500 [02:09<3:36:57, 26.30s/it]  1%|          | 6/500 [02:33<3:29:53, 25.49s/it]  1%|▏         | 7/500 [02:57<3:24:55, 24.94s/it]  2%|▏         | 8/500 [03:26<3:35:21, 26.26s/it]  2%|▏         | 9/500 [03:50<3:30:35, 25.73s/it]  2%|▏         | 10/500 [04:14<3:25:45, 25.19s/it]  2%|▏         | 11/500 [04:39<3:22:51, 24.89s/it]  2%|▏         | 12/500 [05:02<3:19:35, 24.54s/it]  3%|▎         | 13/500 [05:31<3:30:33, 25.94s/it]  3%|▎         | 14/500 [05:56<3:25:52, 25.42s/it]  3%|▎         | 15/500 [06:20<3:23:24, 25.16s/it]  3%|▎         | 16/500 [06:48<3:30:12, 26.06s/it]  3%|▎         | 17/500 [07:13<3:25:13, 25.49s/it]  4%|▎         | 18/500 [07:37<3:21:30, 25.08s/it]  4%|▍         | 19/500 [08:01<3:18:50, 24.80s/it]  4%|▍         | 20/500 [08:29<3:27:10, 25.90s/it]  4%|▍         | 21/500 [08:58<3:32:37, 26.63s/it]  4%|▍         | 22/500 [09:27<3:38:18, 27.40s/it]  5%|▍         | 23/500 [09:56<3:41:21, 27.84s/it]  5%|▍         | 24/500 [10:25<3:44:42, 28.32s/it]  5%|▌         | 25/500 [10:54<3:45:13, 28.45s/it]  5%|▌         | 26/500 [11:18<3:34:41, 27.18s/it]  5%|▌         | 27/500 [11:42<3:26:15, 26.16s/it]  6%|▌         | 28/500 [12:11<3:31:39, 26.91s/it]  6%|▌         | 29/500 [12:34<3:24:06, 26.00s/it]  6%|▌         | 30/500 [12:59<3:19:17, 25.44s/it]  6%|▌         | 30/500 [12:59<3:23:26, 25.97s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.306 MB uploadedwandb: | 0.020 MB of 0.306 MB uploadedwandb: / 0.306 MB of 0.306 MB uploadedwandb: - 0.306 MB of 0.306 MB uploadedwandb: \ 0.306 MB of 0.306 MB uploadedwandb: | 0.306 MB of 0.306 MB uploadedwandb: / 0.306 MB of 0.306 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▃▃▃▅▃▃▆▄▅▇▆▇▇██▇█▇▇███▇▇█▇██▇
wandb:     train_loss ▄▃▇▃▁█▄▁▆▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb:   val_accuracy ▃█▄▂▃▃▃▂▃▂▂▂▃▃▂▂▁▃▃▅▂▃▂▃▃▂▇▂▂▄
wandb:       val_loss ▁▁▂▁▂▄▄▂▆▂▄▇▁▁▅█▂▂▄▁▃▅▃▅▁▂▁▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.93165
wandb:     train_loss 0.23575
wandb:   val_accuracy 0.41556
wandb:       val_loss 3.60908
wandb: 
wandb: 🚀 View run faithful-plant-344 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mddv32r3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_083110-mddv32r3/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_084451-y0ubskb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-breeze-345
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y0ubskb8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:21:53, 31.49s/it]  0%|          | 2/500 [00:55<3:46:17, 27.26s/it]  1%|          | 3/500 [01:25<3:54:47, 28.35s/it]  1%|          | 4/500 [01:49<3:39:00, 26.49s/it]  1%|          | 5/500 [02:12<3:29:36, 25.41s/it]  1%|          | 6/500 [02:35<3:22:23, 24.58s/it]  1%|▏         | 7/500 [02:59<3:20:22, 24.39s/it]  2%|▏         | 8/500 [03:23<3:18:14, 24.18s/it]  2%|▏         | 9/500 [03:51<3:27:12, 25.32s/it]  2%|▏         | 10/500 [04:24<3:46:04, 27.68s/it]  2%|▏         | 11/500 [04:47<3:34:26, 26.31s/it]  2%|▏         | 12/500 [05:10<3:27:23, 25.50s/it]  3%|▎         | 13/500 [05:36<3:26:04, 25.39s/it]  3%|▎         | 14/500 [06:02<3:27:13, 25.58s/it]  3%|▎         | 15/500 [06:26<3:23:46, 25.21s/it]  3%|▎         | 16/500 [06:51<3:22:45, 25.13s/it]  3%|▎         | 17/500 [07:15<3:20:46, 24.94s/it]  4%|▎         | 18/500 [07:47<3:35:47, 26.86s/it]  4%|▍         | 19/500 [08:15<3:39:20, 27.36s/it]  4%|▍         | 20/500 [08:39<3:30:07, 26.27s/it]  4%|▍         | 21/500 [09:08<3:36:42, 27.15s/it]  4%|▍         | 22/500 [09:33<3:30:47, 26.46s/it]  5%|▍         | 23/500 [09:58<3:26:42, 26.00s/it]  5%|▍         | 24/500 [10:22<3:22:01, 25.46s/it]  5%|▌         | 25/500 [10:46<3:17:55, 25.00s/it]  5%|▌         | 26/500 [11:10<3:15:59, 24.81s/it]  5%|▌         | 27/500 [11:34<3:13:29, 24.54s/it]  6%|▌         | 28/500 [12:02<3:20:44, 25.52s/it]  6%|▌         | 29/500 [12:27<3:19:27, 25.41s/it]  6%|▌         | 30/500 [12:52<3:18:02, 25.28s/it]  6%|▌         | 31/500 [13:17<3:16:08, 25.09s/it]  6%|▋         | 32/500 [13:41<3:14:06, 24.89s/it]  7%|▋         | 33/500 [14:06<3:12:30, 24.73s/it]  7%|▋         | 34/500 [14:30<3:10:39, 24.55s/it]  7%|▋         | 35/500 [14:55<3:10:36, 24.59s/it]  7%|▋         | 36/500 [15:19<3:09:41, 24.53s/it]  7%|▋         | 37/500 [15:43<3:09:04, 24.50s/it]  8%|▊         | 38/500 [16:12<3:19:23, 25.90s/it]  8%|▊         | 39/500 [16:42<3:27:39, 27.03s/it]  8%|▊         | 40/500 [17:07<3:21:13, 26.25s/it]  8%|▊         | 41/500 [17:31<3:17:07, 25.77s/it]  8%|▊         | 42/500 [17:56<3:13:38, 25.37s/it]  9%|▊         | 43/500 [18:23<3:17:46, 25.97s/it]  9%|▉         | 44/500 [18:48<3:14:44, 25.62s/it]  9%|▉         | 45/500 [19:20<3:28:44, 27.53s/it]  9%|▉         | 46/500 [19:44<3:20:41, 26.52s/it]  9%|▉         | 47/500 [20:10<3:19:12, 26.39s/it] 10%|▉         | 48/500 [20:35<3:14:45, 25.85s/it] 10%|▉         | 49/500 [21:06<3:27:45, 27.64s/it] 10%|█         | 50/500 [21:39<3:37:40, 29.02s/it] 10%|█         | 51/500 [22:11<3:43:50, 29.91s/it] 10%|█         | 52/500 [22:35<3:30:39, 28.21s/it] 11%|█         | 53/500 [23:07<3:37:40, 29.22s/it] 11%|█         | 54/500 [23:31<3:25:38, 27.66s/it] 11%|█         | 55/500 [23:59<3:27:43, 28.01s/it] 11%|█         | 56/500 [24:24<3:18:51, 26.87s/it] 11%|█▏        | 57/500 [24:52<3:22:08, 27.38s/it] 12%|█▏        | 58/500 [25:16<3:14:47, 26.44s/it] 12%|█▏        | 58/500 [25:16<3:12:40, 26.15s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.019 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▅▅▅▄▅▅▄▆▄▆▆▇▂▅▆▅▆▆▆▇▇▇▇▆▆▇▆▇▇▇▇██▇█▄▆
wandb:     train_loss ▁▂▂▁▁▁▁▂▁▁▁▁▁▁▂█▂▁▁▁▃▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▂
wandb:   val_accuracy ▁▂▄▇█▇▆▇▇▂▆▄▅▅▄▁▃▃▂▃▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃
wandb:       val_loss ▂▂▂▂▂▁▁▂▂▁▂▁▁▁▂█▂▂▁▁▁▃▁▂▂▁▁▁▁▁▂▁▃▃▄▃▁▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 57
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.61664
wandb:     train_loss 1.05477
wandb:   val_accuracy 0.42667
wandb:       val_loss 1.87171
wandb: 
wandb: 🚀 View run atomic-breeze-345 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y0ubskb8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_084451-y0ubskb8/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_091054-ivfwtu2b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-grass-348
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ivfwtu2b
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:32<4:26:31, 32.05s/it]  0%|          | 2/500 [00:55<3:43:36, 26.94s/it]  1%|          | 3/500 [01:18<3:30:04, 25.36s/it]  1%|          | 4/500 [01:42<3:23:19, 24.60s/it]  1%|          | 5/500 [02:05<3:18:39, 24.08s/it]  1%|          | 6/500 [02:29<3:17:29, 23.99s/it]  1%|▏         | 7/500 [02:52<3:15:08, 23.75s/it]  2%|▏         | 8/500 [03:20<3:26:03, 25.13s/it]  2%|▏         | 9/500 [03:48<3:32:28, 25.96s/it]  2%|▏         | 10/500 [04:11<3:24:29, 25.04s/it]  2%|▏         | 11/500 [04:39<3:30:47, 25.86s/it]  2%|▏         | 12/500 [05:02<3:23:54, 25.07s/it]  3%|▎         | 13/500 [05:25<3:18:47, 24.49s/it]  3%|▎         | 14/500 [05:48<3:15:07, 24.09s/it]  3%|▎         | 15/500 [06:11<3:11:46, 23.73s/it]  3%|▎         | 16/500 [06:41<3:25:39, 25.49s/it]  3%|▎         | 17/500 [07:09<3:31:46, 26.31s/it]  4%|▎         | 18/500 [07:32<3:23:11, 25.29s/it]  4%|▍         | 19/500 [08:00<3:29:32, 26.14s/it]  4%|▍         | 20/500 [08:23<3:22:01, 25.25s/it]  4%|▍         | 21/500 [08:46<3:16:23, 24.60s/it]  4%|▍         | 22/500 [09:14<3:24:26, 25.66s/it]  5%|▍         | 23/500 [09:38<3:18:20, 24.95s/it]  5%|▍         | 24/500 [10:01<3:13:20, 24.37s/it]  5%|▌         | 25/500 [10:24<3:10:05, 24.01s/it]  5%|▌         | 26/500 [10:47<3:07:19, 23.71s/it]  5%|▌         | 27/500 [11:10<3:05:12, 23.49s/it]  6%|▌         | 28/500 [11:33<3:03:43, 23.35s/it]  6%|▌         | 29/500 [12:01<3:14:44, 24.81s/it]  6%|▌         | 29/500 [12:01<3:15:20, 24.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.022 MB of 0.316 MB uploadedwandb: | 0.309 MB of 0.316 MB uploadedwandb: / 0.309 MB of 0.316 MB uploadedwandb: - 0.309 MB of 0.316 MB uploadedwandb: \ 0.309 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▂▃▃▂▂▃▅▃▄▃▅▄▆▆▂▆▄▆▆█▆▆▇▇█▃
wandb:     train_loss ▂▂▃▃▁▁▃▁▃▂▁▁▁▂▃▂▁█▁▃▂▁▁▁▁▁▁▁▂
wandb:   val_accuracy ▂█▃▇▅▁▂▂▂▂▂▁▂▁▁▄▂▂▁▁▁▂▂▂▂▂▂▂▃
wandb:       val_loss ▂▂▂▂▃▄▅▂▆▂▅█▁▁▆▄▁▆▄▃▄▇▃▄▃▃▅▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.50817
wandb:     train_loss 0.50401
wandb:   val_accuracy 0.36667
wandb:       val_loss 0.17722
wandb: 
wandb: 🚀 View run magic-grass-348 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ivfwtu2b
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_091054-ivfwtu2b/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_092341-pqiwiid0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-blaze-349
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pqiwiid0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:23, 25.30s/it]  0%|          | 2/500 [00:49<3:24:49, 24.68s/it]  1%|          | 3/500 [01:13<3:19:59, 24.14s/it]  1%|          | 4/500 [01:36<3:16:59, 23.83s/it]  1%|          | 5/500 [02:06<3:33:49, 25.92s/it]  1%|          | 6/500 [02:34<3:39:54, 26.71s/it]  1%|▏         | 7/500 [02:57<3:28:49, 25.42s/it]  2%|▏         | 8/500 [03:19<3:21:50, 24.61s/it]  2%|▏         | 9/500 [03:47<3:30:09, 25.68s/it]  2%|▏         | 10/500 [04:11<3:23:44, 24.95s/it]  2%|▏         | 11/500 [04:34<3:18:44, 24.39s/it]  2%|▏         | 12/500 [04:57<3:15:18, 24.01s/it]  3%|▎         | 13/500 [05:20<3:12:07, 23.67s/it]  3%|▎         | 14/500 [05:44<3:13:02, 23.83s/it]  3%|▎         | 15/500 [06:07<3:11:03, 23.64s/it]  3%|▎         | 16/500 [06:33<3:16:16, 24.33s/it]  3%|▎         | 17/500 [06:56<3:13:16, 24.01s/it]  4%|▎         | 18/500 [07:25<3:23:31, 25.33s/it]  4%|▍         | 19/500 [07:48<3:17:13, 24.60s/it]  4%|▍         | 20/500 [08:11<3:13:07, 24.14s/it]  4%|▍         | 21/500 [08:34<3:10:11, 23.82s/it]  4%|▍         | 22/500 [08:57<3:08:06, 23.61s/it]  5%|▍         | 23/500 [09:23<3:13:09, 24.30s/it]  5%|▍         | 24/500 [09:46<3:10:02, 23.95s/it]  5%|▌         | 25/500 [10:09<3:07:53, 23.73s/it]  5%|▌         | 26/500 [10:33<3:06:15, 23.58s/it]  5%|▌         | 27/500 [10:56<3:05:04, 23.48s/it]  6%|▌         | 28/500 [11:19<3:04:25, 23.44s/it]  6%|▌         | 29/500 [11:42<3:03:11, 23.34s/it]  6%|▌         | 30/500 [12:10<3:13:56, 24.76s/it]  6%|▌         | 31/500 [12:33<3:09:39, 24.26s/it]  6%|▋         | 32/500 [13:03<3:21:40, 25.86s/it]  7%|▋         | 33/500 [13:32<3:28:57, 26.85s/it]  7%|▋         | 34/500 [14:01<3:32:04, 27.31s/it]  7%|▋         | 35/500 [14:24<3:21:51, 26.05s/it]  7%|▋         | 36/500 [14:47<3:14:26, 25.14s/it]  7%|▋         | 37/500 [15:10<3:09:00, 24.49s/it]  8%|▊         | 38/500 [15:35<3:11:23, 24.86s/it]  8%|▊         | 39/500 [15:59<3:07:08, 24.36s/it]  8%|▊         | 40/500 [16:21<3:03:22, 23.92s/it]  8%|▊         | 41/500 [16:45<3:01:18, 23.70s/it]  8%|▊         | 42/500 [17:08<2:59:13, 23.48s/it]  9%|▊         | 43/500 [17:31<2:58:02, 23.37s/it]  9%|▉         | 44/500 [17:54<2:56:45, 23.26s/it]  9%|▉         | 45/500 [18:17<2:56:03, 23.22s/it]  9%|▉         | 46/500 [18:46<3:09:35, 25.06s/it]  9%|▉         | 47/500 [19:09<3:04:23, 24.42s/it] 10%|▉         | 48/500 [19:32<3:00:53, 24.01s/it] 10%|▉         | 49/500 [19:55<2:58:13, 23.71s/it] 10%|█         | 50/500 [20:18<2:56:35, 23.55s/it] 10%|█         | 51/500 [20:42<2:55:23, 23.44s/it] 10%|█         | 52/500 [21:05<2:55:00, 23.44s/it] 11%|█         | 53/500 [21:28<2:53:45, 23.32s/it] 11%|█         | 54/500 [21:56<3:03:32, 24.69s/it] 11%|█         | 55/500 [22:19<2:59:16, 24.17s/it] 11%|█         | 56/500 [22:42<2:56:18, 23.82s/it] 11%|█▏        | 57/500 [23:10<3:05:33, 25.13s/it] 12%|█▏        | 58/500 [23:33<3:00:39, 24.52s/it] 12%|█▏        | 59/500 [24:01<3:07:11, 25.47s/it] 12%|█▏        | 59/500 [24:01<2:59:33, 24.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.025 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.029 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▁▂▂▂▃█▂▇▅▄▄▄▄▄▄▃▂▄▄▄▄▃▅▃▃▃▄▃▃▄▄▄▄▄▁▃▃▃
wandb:     train_loss ▁▁▂▁▃▁▂▁▁▂▁▁▁▃▁▁▄▁▃▄▄▄▄▁▅▄▁▃▄▄▃▁▄▁▅▃▁▁█▄
wandb:   val_accuracy ▂█▂▂▂▂▂▁▂▃▃▃▁▃▄▃▄▃▂▄▃▃▄▄▂▃▃▃▄▃▄▃▃▄▃▄▂▄▃▃
wandb:       val_loss ▁▁▂▂▂▁▄▂█▁▃▃▃▃▂▂▄▁▂▄▃▃▁▁▃▁▂▁▂▄▃▃▄▃▃▃▃▂▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 58
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.45617
wandb:     train_loss 7.4323
wandb:   val_accuracy 0.38444
wandb:       val_loss 4.66056
wandb: 
wandb: 🚀 View run silvery-blaze-349 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pqiwiid0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_092341-pqiwiid0/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_094830-k9ip3a22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-armadillo-351
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k9ip3a22
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:09:07, 29.96s/it]  0%|          | 2/500 [00:54<3:41:11, 26.65s/it]  1%|          | 3/500 [01:29<4:13:44, 30.63s/it]  1%|          | 4/500 [01:58<4:05:44, 29.73s/it]  1%|          | 5/500 [02:21<3:46:12, 27.42s/it]  1%|          | 6/500 [02:51<3:53:43, 28.39s/it]  1%|▏         | 7/500 [03:25<4:07:31, 30.12s/it]  2%|▏         | 8/500 [03:53<4:02:07, 29.53s/it]  2%|▏         | 9/500 [04:16<3:45:34, 27.57s/it]  2%|▏         | 10/500 [04:45<3:47:14, 27.82s/it]  2%|▏         | 11/500 [05:15<3:51:59, 28.47s/it]  2%|▏         | 12/500 [05:49<4:07:06, 30.38s/it]  3%|▎         | 13/500 [06:20<4:06:55, 30.42s/it]  3%|▎         | 14/500 [06:43<3:49:28, 28.33s/it]  3%|▎         | 15/500 [07:07<3:36:23, 26.77s/it]  3%|▎         | 16/500 [07:30<3:27:07, 25.68s/it]  3%|▎         | 17/500 [07:53<3:21:42, 25.06s/it]  4%|▎         | 18/500 [08:17<3:17:38, 24.60s/it]  4%|▍         | 19/500 [08:41<3:15:05, 24.33s/it]  4%|▍         | 20/500 [09:04<3:11:34, 23.95s/it]  4%|▍         | 21/500 [09:32<3:20:52, 25.16s/it]  4%|▍         | 22/500 [10:00<3:28:11, 26.13s/it]  5%|▍         | 23/500 [10:30<3:36:40, 27.25s/it]  5%|▍         | 24/500 [11:00<3:43:07, 28.12s/it]  5%|▌         | 25/500 [11:23<3:30:54, 26.64s/it]  5%|▌         | 26/500 [11:51<3:33:07, 26.98s/it]  5%|▌         | 27/500 [12:19<3:36:15, 27.43s/it]  6%|▌         | 28/500 [12:43<3:25:57, 26.18s/it]  6%|▌         | 29/500 [13:06<3:18:18, 25.26s/it]  6%|▌         | 30/500 [13:34<3:24:49, 26.15s/it]  6%|▌         | 31/500 [14:04<3:32:11, 27.15s/it]  6%|▋         | 32/500 [14:31<3:33:13, 27.34s/it]  7%|▋         | 33/500 [14:55<3:23:23, 26.13s/it]  7%|▋         | 34/500 [15:18<3:15:40, 25.19s/it]  7%|▋         | 35/500 [15:41<3:10:53, 24.63s/it]  7%|▋         | 36/500 [16:04<3:07:32, 24.25s/it]  7%|▋         | 37/500 [16:27<3:04:22, 23.89s/it]  8%|▊         | 38/500 [16:59<3:22:41, 26.32s/it]  8%|▊         | 39/500 [17:28<3:26:54, 26.93s/it]  8%|▊         | 40/500 [17:51<3:17:51, 25.81s/it]  8%|▊         | 41/500 [18:14<3:11:01, 24.97s/it]  8%|▊         | 42/500 [18:37<3:06:44, 24.46s/it]  9%|▊         | 43/500 [19:00<3:02:53, 24.01s/it]  9%|▉         | 44/500 [19:23<3:00:21, 23.73s/it]  9%|▉         | 45/500 [19:46<2:58:14, 23.50s/it]  9%|▉         | 46/500 [20:09<2:56:24, 23.31s/it]  9%|▉         | 47/500 [20:32<2:56:06, 23.33s/it] 10%|▉         | 48/500 [21:00<3:05:31, 24.63s/it] 10%|▉         | 49/500 [21:28<3:13:05, 25.69s/it] 10%|█         | 50/500 [21:51<3:06:53, 24.92s/it] 10%|█         | 51/500 [22:14<3:01:52, 24.30s/it] 10%|█         | 52/500 [22:37<2:59:05, 23.98s/it] 11%|█         | 53/500 [23:01<2:56:42, 23.72s/it] 11%|█         | 54/500 [23:29<3:06:45, 25.12s/it] 11%|█         | 55/500 [24:03<3:26:00, 27.78s/it] 11%|█         | 56/500 [24:27<3:16:18, 26.53s/it] 11%|█▏        | 57/500 [24:50<3:08:40, 25.55s/it] 12%|█▏        | 58/500 [25:13<3:02:57, 24.84s/it] 12%|█▏        | 59/500 [25:36<2:58:29, 24.29s/it] 12%|█▏        | 60/500 [25:59<2:55:30, 23.93s/it] 12%|█▏        | 61/500 [26:26<3:00:36, 24.68s/it] 12%|█▏        | 62/500 [26:49<2:56:53, 24.23s/it] 13%|█▎        | 63/500 [27:12<2:53:53, 23.88s/it] 13%|█▎        | 64/500 [27:35<2:51:46, 23.64s/it] 13%|█▎        | 65/500 [28:03<3:01:08, 24.99s/it] 13%|█▎        | 66/500 [28:36<3:18:35, 27.45s/it] 13%|█▎        | 67/500 [28:59<3:08:54, 26.18s/it] 14%|█▎        | 68/500 [29:23<3:01:53, 25.26s/it] 14%|█▍        | 69/500 [29:46<2:57:32, 24.71s/it] 14%|█▍        | 70/500 [30:09<2:53:42, 24.24s/it] 14%|█▍        | 71/500 [30:37<3:00:46, 25.28s/it] 14%|█▍        | 72/500 [31:05<3:06:24, 26.13s/it] 15%|█▍        | 73/500 [31:28<2:59:15, 25.19s/it] 15%|█▍        | 74/500 [31:51<2:54:29, 24.58s/it] 15%|█▌        | 75/500 [32:14<2:50:58, 24.14s/it] 15%|█▌        | 76/500 [32:38<2:49:00, 23.92s/it] 15%|█▌        | 77/500 [33:01<2:46:54, 23.68s/it] 16%|█▌        | 78/500 [33:24<2:45:17, 23.50s/it] 16%|█▌        | 79/500 [33:52<2:54:19, 24.84s/it] 16%|█▌        | 80/500 [34:20<3:01:13, 25.89s/it] 16%|█▌        | 81/500 [34:43<2:54:11, 24.94s/it] 16%|█▋        | 82/500 [35:06<2:50:29, 24.47s/it] 16%|█▋        | 82/500 [35:06<2:58:59, 25.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.234 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆▆▅▆▅▆▄▅▆▅▇▇▄▅▅▅▃▄▆▆▆▅▇▇▇▆▇▇▇▆▇▇▇████▇
wandb:     train_loss ▃▄▂▄▄▅▂▃▃▆▃▃▃▂█▃▁▅▆▄▄▄▄▃▂▁▄▂▃▃▁▄▂▂▆▃▂▂▃▄
wandb:   val_accuracy ▁▂██▇▇▄▇▃▅▆▃▆▅▄▄▄▄▁▄█▇█▅▇▇▇▇▆▆▅▆▆▅▄▄▃▃▃▅
wandb:       val_loss ▂▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▂▂▁▂▁▂▂▂▂▁▁▁▁▁▂▁▂▁▂▂█▁▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 81
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.65825
wandb:     train_loss 1.21795
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.65264
wandb: 
wandb: 🚀 View run clear-armadillo-351 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k9ip3a22
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_094830-k9ip3a22/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_102426-z86hu55w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-hill-355
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z86hu55w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:51, 26.44s/it]  0%|          | 2/500 [00:51<3:30:39, 25.38s/it]  1%|          | 3/500 [01:15<3:27:41, 25.07s/it]  1%|          | 4/500 [01:40<3:26:07, 24.93s/it]  1%|          | 5/500 [02:05<3:24:47, 24.82s/it]  1%|          | 6/500 [02:31<3:30:00, 25.51s/it]  1%|▏         | 7/500 [02:57<3:30:29, 25.62s/it]  2%|▏         | 8/500 [03:22<3:27:17, 25.28s/it]  2%|▏         | 9/500 [03:47<3:27:02, 25.30s/it]  2%|▏         | 10/500 [04:12<3:25:22, 25.15s/it]  2%|▏         | 11/500 [04:37<3:23:52, 25.01s/it]  2%|▏         | 12/500 [05:02<3:23:49, 25.06s/it]  3%|▎         | 13/500 [05:27<3:22:57, 25.00s/it]  3%|▎         | 14/500 [05:52<3:23:44, 25.15s/it]  3%|▎         | 15/500 [06:17<3:21:26, 24.92s/it]  3%|▎         | 16/500 [06:40<3:18:05, 24.56s/it]  3%|▎         | 17/500 [07:05<3:17:13, 24.50s/it]  4%|▎         | 18/500 [07:29<3:15:17, 24.31s/it]  4%|▍         | 19/500 [07:52<3:13:00, 24.08s/it]  4%|▍         | 20/500 [08:16<3:11:21, 23.92s/it]  4%|▍         | 21/500 [08:41<3:13:12, 24.20s/it]  4%|▍         | 22/500 [09:05<3:12:23, 24.15s/it]  5%|▍         | 23/500 [09:29<3:11:38, 24.11s/it]  5%|▍         | 24/500 [09:53<3:11:19, 24.12s/it]  5%|▌         | 25/500 [10:16<3:09:20, 23.92s/it]  5%|▌         | 26/500 [10:40<3:09:19, 23.97s/it]  5%|▌         | 27/500 [11:05<3:09:53, 24.09s/it]  6%|▌         | 28/500 [11:32<3:16:38, 25.00s/it]  6%|▌         | 29/500 [11:55<3:12:31, 24.53s/it]  6%|▌         | 30/500 [12:24<3:23:13, 25.94s/it]  6%|▌         | 31/500 [12:48<3:16:39, 25.16s/it]  6%|▋         | 32/500 [13:16<3:24:08, 26.17s/it]  7%|▋         | 33/500 [13:40<3:18:16, 25.47s/it]  7%|▋         | 33/500 [13:40<3:13:33, 24.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.020 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁
wandb: train_accuracy ▁▃▃▃▃▂▁▅▄▂▂▃▇▅▆▅▅▅▆▁▆▆▄▆▅▆▇▇▄▇▇██
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▄▄▃▃▃▃▄▂▃▃▃▃▂▅▂▅▂▁▃▅▅█▂▂▂▄▂▂▁▂▂▁
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 32
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.79792
wandb:     train_loss 0.14185
wandb:   val_accuracy 0.28
wandb:       val_loss 1.25567
wandb: 
wandb: 🚀 View run vibrant-hill-355 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z86hu55w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_102426-z86hu55w/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_103852-ngzc9hdl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-spaceship-356
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ngzc9hdl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:21:39, 17.03s/it]  0%|          | 2/500 [00:32<2:15:57, 16.38s/it]  1%|          | 3/500 [00:49<2:15:47, 16.39s/it]  1%|          | 4/500 [01:05<2:13:47, 16.18s/it]  1%|          | 5/500 [01:20<2:12:03, 16.01s/it]  1%|          | 6/500 [01:36<2:11:27, 15.97s/it]  1%|▏         | 7/500 [01:52<2:10:43, 15.91s/it]  2%|▏         | 8/500 [02:09<2:11:45, 16.07s/it]  2%|▏         | 9/500 [02:25<2:11:25, 16.06s/it]  2%|▏         | 10/500 [02:40<2:10:09, 15.94s/it]  2%|▏         | 11/500 [02:56<2:10:32, 16.02s/it]  2%|▏         | 12/500 [03:17<2:20:40, 17.30s/it]  3%|▎         | 13/500 [03:32<2:16:13, 16.78s/it]  3%|▎         | 14/500 [03:48<2:13:46, 16.51s/it]  3%|▎         | 15/500 [04:04<2:11:41, 16.29s/it]  3%|▎         | 16/500 [04:20<2:11:25, 16.29s/it]  3%|▎         | 17/500 [04:36<2:10:30, 16.21s/it]  4%|▎         | 18/500 [04:52<2:08:41, 16.02s/it]  4%|▍         | 19/500 [05:07<2:07:22, 15.89s/it]  4%|▍         | 20/500 [05:23<2:06:12, 15.78s/it]  4%|▍         | 21/500 [05:38<2:03:41, 15.49s/it]  4%|▍         | 22/500 [05:52<2:01:29, 15.25s/it]  5%|▍         | 23/500 [06:08<2:01:52, 15.33s/it]  5%|▍         | 24/500 [06:24<2:02:11, 15.40s/it]  5%|▌         | 25/500 [06:39<2:02:38, 15.49s/it]  5%|▌         | 26/500 [06:55<2:02:27, 15.50s/it]  5%|▌         | 27/500 [07:10<2:01:55, 15.47s/it]  6%|▌         | 28/500 [07:26<2:02:32, 15.58s/it]  6%|▌         | 28/500 [07:26<2:05:26, 15.95s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.309 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.103 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▃▄▃▃▄▄▁▇▁▁▆▇▃▁▃▂▁▅▇▆██▇▇▇
wandb:     train_loss ▂▃▃▃▂▄▂▁▂▄▁▁▃▁▁▁▇▁▁█▁▁▁▃▂▁▁▁
wandb:   val_accuracy ▂█▅▆▂▃▃▂▄▂▂▂▂▂▂▁▂▂▂▂▂▂▁▃▃▂▃▂
wandb:       val_loss ▂▂▂▁▂▂▁▂▃▄▃▁▄▂▃▃▄▃▄▄▆█▅▇▇▇▇▃
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.87816
wandb:     train_loss 0.00129
wandb:   val_accuracy 0.34889
wandb:       val_loss 3.52517
wandb: 
wandb: 🚀 View run clear-spaceship-356 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ngzc9hdl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_103852-ngzc9hdl/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_104703-13u3eobm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-tree-358
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/13u3eobm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:23:20, 17.24s/it]  0%|          | 2/500 [00:32<2:15:30, 16.33s/it]  1%|          | 3/500 [00:53<2:30:49, 18.21s/it]  1%|          | 4/500 [01:09<2:22:53, 17.28s/it]  1%|          | 5/500 [01:24<2:17:57, 16.72s/it]  1%|          | 6/500 [01:40<2:15:28, 16.46s/it]  1%|▏         | 7/500 [01:56<2:14:11, 16.33s/it]  2%|▏         | 8/500 [02:12<2:12:35, 16.17s/it]  2%|▏         | 9/500 [02:29<2:12:37, 16.21s/it]  2%|▏         | 10/500 [02:45<2:12:24, 16.21s/it]  2%|▏         | 11/500 [03:01<2:11:28, 16.13s/it]  2%|▏         | 12/500 [03:17<2:11:31, 16.17s/it]  3%|▎         | 13/500 [03:33<2:11:10, 16.16s/it]  3%|▎         | 14/500 [03:49<2:09:12, 15.95s/it]  3%|▎         | 15/500 [04:04<2:07:53, 15.82s/it]  3%|▎         | 16/500 [04:20<2:07:44, 15.84s/it]  3%|▎         | 17/500 [04:36<2:06:52, 15.76s/it]  4%|▎         | 18/500 [04:51<2:06:22, 15.73s/it]  4%|▍         | 19/500 [05:07<2:05:33, 15.66s/it]  4%|▍         | 20/500 [05:22<2:04:57, 15.62s/it]  4%|▍         | 21/500 [05:37<2:03:41, 15.49s/it]  4%|▍         | 22/500 [05:53<2:03:13, 15.47s/it]  5%|▍         | 23/500 [06:08<2:03:12, 15.50s/it]  5%|▍         | 24/500 [06:24<2:03:32, 15.57s/it]  5%|▌         | 25/500 [06:39<2:02:30, 15.48s/it]  5%|▌         | 26/500 [06:55<2:02:13, 15.47s/it]  5%|▌         | 27/500 [07:10<2:01:41, 15.44s/it]  6%|▌         | 28/500 [07:26<2:01:20, 15.42s/it]  6%|▌         | 29/500 [07:41<2:00:53, 15.40s/it]  6%|▌         | 29/500 [07:41<2:04:56, 15.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.231 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▃▃▃▅▅▆▆▆▇▇▇▇▇▇▇▇█████████▇
wandb:     train_loss ▇▆▆▆▆▅▆▅▆▅▃▇▄▂█▄▃▄▄▃▅▅▃▆▁▅▁▂▆
wandb:   val_accuracy ▂▂▂▂▂▄█▇▆▆▅▂▄▃▂▃▂▁▂▁▁▂▂▁▁▁▁▁▂
wandb:       val_loss ▃▃▃▃▃▃▃▃▂▂▃▃▁▃▂▂▄▄▃▃█▄▄▄▃▄▃▂▇
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.73403
wandb:     train_loss 1.11699
wandb:   val_accuracy 0.34444
wandb:       val_loss 2.40314
wandb: 
wandb: 🚀 View run summer-tree-358 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/13u3eobm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_104703-13u3eobm/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_105527-n9ufvxy8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-water-359
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n9ufvxy8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:03, 18.64s/it]  0%|          | 2/500 [00:34<2:23:17, 17.26s/it]  1%|          | 3/500 [00:50<2:17:08, 16.56s/it]  1%|          | 4/500 [01:06<2:14:04, 16.22s/it]  1%|          | 5/500 [01:22<2:12:44, 16.09s/it]  1%|          | 6/500 [01:37<2:11:08, 15.93s/it]  1%|▏         | 7/500 [01:53<2:10:39, 15.90s/it]  2%|▏         | 8/500 [02:09<2:10:25, 15.91s/it]  2%|▏         | 9/500 [02:25<2:09:04, 15.77s/it]  2%|▏         | 10/500 [02:40<2:08:01, 15.68s/it]  2%|▏         | 11/500 [02:56<2:08:17, 15.74s/it]  2%|▏         | 12/500 [03:11<2:07:09, 15.63s/it]  3%|▎         | 13/500 [03:27<2:07:34, 15.72s/it]  3%|▎         | 14/500 [03:43<2:06:47, 15.65s/it]  3%|▎         | 15/500 [03:58<2:06:20, 15.63s/it]  3%|▎         | 16/500 [04:14<2:05:54, 15.61s/it]  3%|▎         | 17/500 [04:30<2:06:03, 15.66s/it]  4%|▎         | 18/500 [04:45<2:05:36, 15.63s/it]  4%|▍         | 19/500 [05:01<2:05:20, 15.64s/it]  4%|▍         | 20/500 [05:17<2:05:09, 15.65s/it]  4%|▍         | 21/500 [05:33<2:07:01, 15.91s/it]  4%|▍         | 22/500 [05:49<2:07:10, 15.96s/it]  5%|▍         | 23/500 [06:05<2:07:08, 15.99s/it]  5%|▍         | 24/500 [06:21<2:06:00, 15.88s/it]  5%|▌         | 25/500 [06:37<2:06:38, 16.00s/it]  5%|▌         | 26/500 [06:53<2:07:10, 16.10s/it]  5%|▌         | 27/500 [07:09<2:06:19, 16.02s/it]  6%|▌         | 28/500 [07:25<2:05:47, 15.99s/it]  6%|▌         | 29/500 [07:42<2:06:44, 16.15s/it]  6%|▌         | 29/500 [07:46<2:06:19, 16.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.022 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▃▄▂▂▄▂▄▃▃▅▄▄▃▇▆▄▅██▆▇▇█▆▅▇
wandb:     train_loss ▃▂▃▄▄▁▃▄█▂▁▆▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▄
wandb:   val_accuracy ▅▅▅▅█▅▅▄▅▅▄▂▂▃▃▄▂▄▂▂▄▁▁▂▂▃▂▃▂
wandb:       val_loss ▂▂▂▂▃▂▁▃▃▁▅▄▁▅▃▄▅▆▄▄█▃▅▆▄▄▅▂█
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.80684
wandb:     train_loss 1.9989
wandb:   val_accuracy 0.29333
wandb:       val_loss 6.76102
wandb: 
wandb: 🚀 View run dry-water-359 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n9ufvxy8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_105527-n9ufvxy8/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_110403-bcdl00fl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-smoke-361
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bcdl00fl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:25, 17.37s/it]  0%|          | 2/500 [00:37<2:39:12, 19.18s/it]  1%|          | 3/500 [00:53<2:26:40, 17.71s/it]  1%|          | 4/500 [01:09<2:19:24, 16.86s/it]  1%|          | 5/500 [01:29<2:30:16, 18.22s/it]  1%|          | 6/500 [01:51<2:39:18, 19.35s/it]  1%|▏         | 7/500 [02:07<2:28:45, 18.10s/it]  2%|▏         | 8/500 [02:22<2:22:00, 17.32s/it]  2%|▏         | 9/500 [02:37<2:16:30, 16.68s/it]  2%|▏         | 10/500 [02:53<2:13:38, 16.36s/it]  2%|▏         | 11/500 [03:09<2:11:33, 16.14s/it]  2%|▏         | 12/500 [03:25<2:10:28, 16.04s/it]  3%|▎         | 13/500 [03:40<2:09:00, 15.89s/it]  3%|▎         | 14/500 [03:56<2:07:30, 15.74s/it]  3%|▎         | 15/500 [04:11<2:07:08, 15.73s/it]  3%|▎         | 16/500 [04:27<2:06:59, 15.74s/it]  3%|▎         | 17/500 [04:43<2:06:16, 15.69s/it]  4%|▎         | 18/500 [04:58<2:06:24, 15.74s/it]  4%|▍         | 19/500 [05:14<2:06:36, 15.79s/it]  4%|▍         | 20/500 [05:30<2:05:30, 15.69s/it]  4%|▍         | 21/500 [05:45<2:04:33, 15.60s/it]  4%|▍         | 22/500 [06:01<2:04:23, 15.61s/it]  5%|▍         | 23/500 [06:16<2:04:01, 15.60s/it]  5%|▍         | 24/500 [06:32<2:03:52, 15.61s/it]  5%|▌         | 25/500 [06:48<2:03:36, 15.61s/it]  5%|▌         | 26/500 [07:03<2:02:37, 15.52s/it]  5%|▌         | 27/500 [07:18<2:01:50, 15.46s/it]  6%|▌         | 28/500 [07:34<2:01:51, 15.49s/it]  6%|▌         | 29/500 [07:49<2:01:56, 15.53s/it]  6%|▌         | 29/500 [07:49<2:07:12, 16.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.011 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▂▁▂▃▁▄▄▂▆▅▂▂▄▄▅▆▅█▄▇▅▅█▅█▇▆
wandb:     train_loss ▄▅▇▂▃▆▄▂▃▄▁▁▄▁▅▄█▂▁▁▃▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃█▃▃▂▃▃▃▃▃▃▃▄▃▃▃▃▃▃▄▁▄▃▅▄▄▄▄▅
wandb:       val_loss ▂▁▃▂▂▁▃▃▃▂▅▁▁▂▃▃▄▃▂▂█▆▆▅▅▄▂▁▆
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.75483
wandb:     train_loss 0.02771
wandb:   val_accuracy 0.44667
wandb:       val_loss 8.2589
wandb: 
wandb: 🚀 View run lemon-smoke-361 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bcdl00fl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_110403-bcdl00fl/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_111240-td17jqek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-yogurt-362
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/td17jqek
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:07:29, 22.54s/it]  0%|          | 2/500 [00:38<2:34:02, 18.56s/it]  1%|          | 3/500 [00:54<2:23:17, 17.30s/it]  1%|          | 4/500 [01:10<2:18:44, 16.78s/it]  1%|          | 5/500 [01:25<2:14:50, 16.34s/it]  1%|          | 6/500 [01:41<2:12:34, 16.10s/it]  1%|▏         | 7/500 [01:57<2:11:25, 15.99s/it]  2%|▏         | 8/500 [02:12<2:09:54, 15.84s/it]  2%|▏         | 9/500 [02:28<2:09:00, 15.76s/it]  2%|▏         | 10/500 [02:44<2:09:24, 15.85s/it]  2%|▏         | 11/500 [02:59<2:08:52, 15.81s/it]  2%|▏         | 12/500 [03:20<2:20:39, 17.29s/it]  3%|▎         | 13/500 [03:41<2:28:01, 18.24s/it]  3%|▎         | 14/500 [04:01<2:33:05, 18.90s/it]  3%|▎         | 15/500 [04:17<2:25:18, 17.98s/it]  3%|▎         | 16/500 [04:32<2:19:21, 17.28s/it]  3%|▎         | 17/500 [04:48<2:15:08, 16.79s/it]  4%|▎         | 18/500 [05:04<2:12:31, 16.50s/it]  4%|▍         | 19/500 [05:20<2:10:09, 16.24s/it]  4%|▍         | 20/500 [05:35<2:08:14, 16.03s/it]  4%|▍         | 21/500 [05:51<2:06:37, 15.86s/it]  4%|▍         | 22/500 [06:06<2:06:07, 15.83s/it]  5%|▍         | 23/500 [06:22<2:05:01, 15.73s/it]  5%|▍         | 24/500 [06:37<2:04:36, 15.71s/it]  5%|▌         | 25/500 [06:53<2:04:46, 15.76s/it]  5%|▌         | 26/500 [07:09<2:04:27, 15.76s/it]  5%|▌         | 27/500 [07:25<2:03:51, 15.71s/it]  6%|▌         | 28/500 [07:40<2:03:02, 15.64s/it]  6%|▌         | 29/500 [07:56<2:02:32, 15.61s/it]  6%|▌         | 30/500 [08:11<2:02:06, 15.59s/it]  6%|▌         | 31/500 [08:27<2:02:15, 15.64s/it]  6%|▋         | 32/500 [08:42<2:00:58, 15.51s/it]  7%|▋         | 33/500 [08:57<1:59:25, 15.34s/it]  7%|▋         | 34/500 [09:16<2:06:47, 16.33s/it]  7%|▋         | 35/500 [09:32<2:05:19, 16.17s/it]  7%|▋         | 36/500 [09:51<2:12:32, 17.14s/it]  7%|▋         | 37/500 [10:07<2:08:43, 16.68s/it]  8%|▊         | 38/500 [10:22<2:05:46, 16.33s/it]  8%|▊         | 39/500 [10:39<2:05:34, 16.34s/it]  8%|▊         | 40/500 [10:54<2:03:09, 16.06s/it]  8%|▊         | 41/500 [11:09<2:01:29, 15.88s/it]  8%|▊         | 42/500 [11:25<2:00:22, 15.77s/it]  9%|▊         | 43/500 [11:41<1:59:48, 15.73s/it]  9%|▉         | 44/500 [11:56<1:59:32, 15.73s/it]  9%|▉         | 45/500 [12:17<2:09:50, 17.12s/it]  9%|▉         | 46/500 [12:37<2:16:00, 17.98s/it]  9%|▉         | 47/500 [12:52<2:10:28, 17.28s/it] 10%|▉         | 48/500 [13:08<2:06:18, 16.77s/it] 10%|▉         | 49/500 [13:23<2:03:04, 16.37s/it] 10%|█         | 50/500 [13:44<2:11:38, 17.55s/it] 10%|█         | 51/500 [13:59<2:06:39, 16.93s/it] 10%|█         | 51/500 [13:59<2:03:13, 16.47s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.029 MB uploadedwandb: | 0.020 MB of 0.312 MB uploadedwandb: / 0.029 MB of 0.312 MB uploadedwandb: - 0.201 MB of 0.312 MB uploadedwandb: \ 0.201 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▁▃▃▃▃▃▄▅▅▆▅▇▇▇▆▇▇▆█▇▇▇▆█████████▇█████
wandb:     train_loss ▅▅▄▅▅▅▄▅▄▄▅▄▂▄▂▃▅▆▅▁▅▂▃▄▄▂▄▁▁▁▂▆▆▁█▂▁▃▂▂
wandb:   val_accuracy ▂▂▁▂▂▂▂▂▄▅▆██▆▅▄▅▃▃▅▃▃▃▃▄▂▂▂▃▃▂▃▃▃▃▂▂▂▃▂
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.78752
wandb:     train_loss 0.28074
wandb:   val_accuracy 0.39111
wandb:       val_loss 0.48166
wandb: 
wandb: 🚀 View run neat-yogurt-362 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/td17jqek
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_111240-td17jqek/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_112727-30rhlolw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-elevator-364
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/30rhlolw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:20:59, 16.95s/it]  0%|          | 2/500 [00:36<2:35:36, 18.75s/it]  1%|          | 3/500 [00:53<2:26:11, 17.65s/it]  1%|          | 4/500 [01:13<2:35:27, 18.81s/it]  1%|          | 5/500 [01:29<2:26:01, 17.70s/it]  1%|          | 6/500 [01:45<2:20:07, 17.02s/it]  1%|▏         | 7/500 [02:01<2:18:07, 16.81s/it]  2%|▏         | 8/500 [02:17<2:14:29, 16.40s/it]  2%|▏         | 9/500 [02:35<2:19:16, 17.02s/it]  2%|▏         | 10/500 [02:51<2:15:23, 16.58s/it]  2%|▏         | 11/500 [03:06<2:11:53, 16.18s/it]  2%|▏         | 12/500 [03:22<2:10:48, 16.08s/it]  3%|▎         | 13/500 [03:42<2:20:33, 17.32s/it]  3%|▎         | 14/500 [03:58<2:16:07, 16.81s/it]  3%|▎         | 15/500 [04:13<2:12:46, 16.43s/it]  3%|▎         | 16/500 [04:29<2:10:39, 16.20s/it]  3%|▎         | 17/500 [04:45<2:09:23, 16.07s/it]  4%|▎         | 18/500 [05:00<2:08:35, 16.01s/it]  4%|▍         | 19/500 [05:16<2:07:22, 15.89s/it]  4%|▍         | 20/500 [05:32<2:06:18, 15.79s/it]  4%|▍         | 21/500 [05:47<2:05:06, 15.67s/it]  4%|▍         | 22/500 [06:03<2:04:25, 15.62s/it]  5%|▍         | 23/500 [06:18<2:04:01, 15.60s/it]  5%|▍         | 23/500 [06:18<2:10:51, 16.46s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.308 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.020 MB of 0.308 MB uploadedwandb: / 0.303 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▁▂▄▅▅▂▅▆▆▆▆▇▇▇▇█▇█▇███
wandb:     train_loss ▄▅▄▄▇▂█▅▁▂▂█▂▁▅▁▂▁▁▁▁▂▁
wandb:   val_accuracy ▂▂▃█▅▃▂▄▃▃▄▃▃▃▂▂▁▁▁▃▁▁▁
wandb:       val_loss ▂▂▁▁▁▁▁▂▂▁▃▂▁▂▂▁▄▄▃▃█▅▃
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.96582
wandb:     train_loss 0.09008
wandb:   val_accuracy 0.28667
wandb:       val_loss 3.79075
wandb: 
wandb: 🚀 View run laced-elevator-364 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/30rhlolw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_112727-30rhlolw/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_113435-fkwmo87k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-tree-365
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fkwmo87k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:20<2:51:04, 20.57s/it]  0%|          | 2/500 [00:35<2:25:08, 17.49s/it]  1%|          | 3/500 [00:51<2:17:01, 16.54s/it]  1%|          | 4/500 [01:06<2:11:08, 15.86s/it]  1%|          | 5/500 [01:25<2:22:05, 17.22s/it]  1%|          | 6/500 [01:40<2:15:15, 16.43s/it]  1%|▏         | 7/500 [02:04<2:34:45, 18.83s/it]  2%|▏         | 8/500 [02:19<2:24:49, 17.66s/it]  2%|▏         | 9/500 [02:34<2:17:19, 16.78s/it]  2%|▏         | 10/500 [02:49<2:12:20, 16.20s/it]  2%|▏         | 11/500 [03:04<2:08:36, 15.78s/it]  2%|▏         | 12/500 [03:19<2:06:26, 15.55s/it]  3%|▎         | 13/500 [03:33<2:04:14, 15.31s/it]  3%|▎         | 14/500 [03:53<2:14:29, 16.60s/it]  3%|▎         | 15/500 [04:08<2:09:35, 16.03s/it]  3%|▎         | 16/500 [04:23<2:06:43, 15.71s/it]  3%|▎         | 17/500 [04:38<2:05:13, 15.56s/it]  4%|▎         | 18/500 [04:53<2:03:35, 15.38s/it]  4%|▍         | 19/500 [05:08<2:01:28, 15.15s/it]  4%|▍         | 20/500 [05:22<2:00:28, 15.06s/it]  4%|▍         | 21/500 [05:37<2:00:00, 15.03s/it]  4%|▍         | 22/500 [05:57<2:10:12, 16.34s/it]  5%|▍         | 23/500 [06:12<2:06:57, 15.97s/it]  5%|▍         | 24/500 [06:27<2:05:36, 15.83s/it]  5%|▌         | 25/500 [06:42<2:03:07, 15.55s/it]  5%|▌         | 26/500 [06:57<2:01:21, 15.36s/it]  5%|▌         | 26/500 [06:57<2:06:54, 16.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.137 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▁▂▂▄▂▃▄▇█▂▂▄█▆▆▅▅▇▆█▅▇▇
wandb:     train_loss ▃▃▃▂▂▆▂▁█▃▁▁█▂▃▁▂▄▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂██▁▃▂▃▁▃▂▃▃▂▁▂▂▃▃▅▂▃▇▅▅▂▂
wandb:       val_loss ▁▁▂▂▂▁▁▁▂▁▃▁▃▁▃▁▄▄▂▂█▃▄▃▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.87816
wandb:     train_loss 0.01095
wandb:   val_accuracy 0.30889
wandb:       val_loss 6.46612
wandb: 
wandb: 🚀 View run misunderstood-tree-365 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fkwmo87k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_113435-fkwmo87k/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_114221-d7ubj3kb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-valley-366
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d7ubj3kb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:46, 18.37s/it]  0%|          | 2/500 [00:35<2:27:26, 17.76s/it]  1%|          | 3/500 [00:51<2:20:48, 17.00s/it]  1%|          | 4/500 [01:07<2:15:13, 16.36s/it]  1%|          | 5/500 [01:27<2:26:04, 17.71s/it]  1%|          | 6/500 [01:42<2:19:52, 16.99s/it]  1%|▏         | 7/500 [01:58<2:15:59, 16.55s/it]  2%|▏         | 8/500 [02:18<2:25:30, 17.74s/it]  2%|▏         | 9/500 [02:33<2:18:14, 16.89s/it]  2%|▏         | 10/500 [02:52<2:23:38, 17.59s/it]  2%|▏         | 11/500 [03:07<2:16:21, 16.73s/it]  2%|▏         | 12/500 [03:22<2:11:19, 16.15s/it]  3%|▎         | 13/500 [03:37<2:07:12, 15.67s/it]  3%|▎         | 14/500 [03:52<2:05:47, 15.53s/it]  3%|▎         | 15/500 [04:07<2:03:49, 15.32s/it]  3%|▎         | 16/500 [04:21<2:02:13, 15.15s/it]  3%|▎         | 17/500 [04:37<2:01:51, 15.14s/it]  4%|▎         | 18/500 [04:52<2:01:13, 15.09s/it]  4%|▍         | 19/500 [05:06<2:00:04, 14.98s/it]  4%|▍         | 20/500 [05:21<1:59:51, 14.98s/it]  4%|▍         | 21/500 [05:36<1:59:26, 14.96s/it]  4%|▍         | 22/500 [05:51<1:59:18, 14.98s/it]  5%|▍         | 23/500 [06:06<1:59:40, 15.05s/it]  5%|▍         | 24/500 [06:21<1:59:13, 15.03s/it]  5%|▌         | 25/500 [06:37<1:59:18, 15.07s/it]  5%|▌         | 26/500 [06:52<1:59:07, 15.08s/it]  5%|▌         | 27/500 [07:07<1:58:57, 15.09s/it]  6%|▌         | 28/500 [07:26<2:09:18, 16.44s/it]  6%|▌         | 29/500 [07:41<2:05:48, 16.03s/it]  6%|▌         | 29/500 [07:41<2:05:02, 15.93s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.025 MB of 0.314 MB uploadedwandb: - 0.231 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▅▅▆▅▆▇▇▇█▇▇▆▅▇▇▆█▇▅▆▇▇██▆▇
wandb:     train_loss ▇▆▆▆▆▅▅▆▆▅▄▆▅▃▇▅█▅▄▄▆▅▄▅▁▅▃▃▆
wandb:   val_accuracy ▁▁▂▆▆█▇▇▇▆▅▃▄▄▄▃▄▃▃▃▃▃▃▃▃▃▂▃▃
wandb:       val_loss ▃▃▃▃▃▂▃▃▂▂▃▃▁▃▂▃▄▄▃▄█▃▃▄▂▄▂▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.63596
wandb:     train_loss 1.11221
wandb:   val_accuracy 0.46222
wandb:       val_loss 1.6195
wandb: 
wandb: 🚀 View run clear-valley-366 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d7ubj3kb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_114221-d7ubj3kb/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_115049-eim09nuv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-microwave-367
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/eim09nuv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:29:28, 17.97s/it]  0%|          | 2/500 [00:34<2:22:48, 17.21s/it]  1%|          | 3/500 [00:51<2:19:56, 16.89s/it]  1%|          | 4/500 [01:11<2:29:40, 18.11s/it]  1%|          | 5/500 [01:26<2:20:32, 17.04s/it]  1%|          | 6/500 [01:41<2:14:22, 16.32s/it]  1%|▏         | 7/500 [01:56<2:10:06, 15.83s/it]  2%|▏         | 8/500 [02:10<2:07:18, 15.52s/it]  2%|▏         | 9/500 [02:25<2:05:42, 15.36s/it]  2%|▏         | 10/500 [02:40<2:04:10, 15.20s/it]  2%|▏         | 11/500 [02:56<2:06:15, 15.49s/it]  2%|▏         | 12/500 [03:11<2:04:57, 15.36s/it]  3%|▎         | 13/500 [03:27<2:04:14, 15.31s/it]  3%|▎         | 14/500 [03:46<2:12:40, 16.38s/it]  3%|▎         | 15/500 [04:00<2:08:29, 15.90s/it]  3%|▎         | 16/500 [04:15<2:04:58, 15.49s/it]  3%|▎         | 17/500 [04:30<2:03:09, 15.30s/it]  4%|▎         | 18/500 [04:44<2:00:10, 14.96s/it]  4%|▍         | 19/500 [04:59<1:59:32, 14.91s/it]  4%|▍         | 20/500 [05:13<1:58:53, 14.86s/it]  4%|▍         | 21/500 [05:30<2:02:24, 15.33s/it]  4%|▍         | 22/500 [05:45<2:00:43, 15.15s/it]  4%|▍         | 22/500 [05:45<2:04:58, 15.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.231 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▃▅▅▄▅▆▅▇▄▆▇▃▆█▇▇▅█▆▇▆
wandb:     train_loss ▅▅▆▄▅▂▄▄▃▃▄▂▃▃▂▃▆▄▁█▁▃
wandb:   val_accuracy ▁▁█▆▅▃▃▃▃▁▂▂▂▁▂▂▂▁▂▁▁▁
wandb:       val_loss ▂▂▂▁▁▁▁▁▂▂▄▁▁▂▂▃▅▄▃▃█▂
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.65825
wandb:     train_loss 0.45694
wandb:   val_accuracy 0.34222
wandb:       val_loss 1.6828
wandb: 
wandb: 🚀 View run super-microwave-367 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/eim09nuv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_115049-eim09nuv/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_115721-0uahu5hd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-dream-368
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0uahu5hd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:22, 18.08s/it]  0%|          | 2/500 [00:34<2:19:34, 16.82s/it]  1%|          | 3/500 [00:50<2:17:03, 16.55s/it]  1%|          | 4/500 [01:06<2:14:19, 16.25s/it]  1%|          | 5/500 [01:21<2:12:26, 16.05s/it]  1%|          | 6/500 [01:37<2:12:12, 16.06s/it]  1%|▏         | 7/500 [01:54<2:12:28, 16.12s/it]  2%|▏         | 8/500 [02:10<2:11:46, 16.07s/it]  2%|▏         | 9/500 [02:26<2:13:09, 16.27s/it]  2%|▏         | 10/500 [02:43<2:15:07, 16.55s/it]  2%|▏         | 11/500 [03:00<2:16:03, 16.69s/it]  2%|▏         | 12/500 [03:17<2:16:42, 16.81s/it]  3%|▎         | 13/500 [03:34<2:16:44, 16.85s/it]  3%|▎         | 14/500 [03:51<2:15:41, 16.75s/it]  3%|▎         | 15/500 [04:07<2:13:07, 16.47s/it]  3%|▎         | 16/500 [04:27<2:22:25, 17.66s/it]  3%|▎         | 17/500 [04:43<2:17:48, 17.12s/it]  4%|▎         | 18/500 [04:59<2:15:04, 16.81s/it]  4%|▍         | 19/500 [05:15<2:13:24, 16.64s/it]  4%|▍         | 20/500 [05:32<2:12:38, 16.58s/it]  4%|▍         | 21/500 [05:48<2:10:51, 16.39s/it]  4%|▍         | 22/500 [06:04<2:10:49, 16.42s/it]  5%|▍         | 23/500 [06:20<2:09:31, 16.29s/it]  5%|▍         | 24/500 [06:36<2:08:30, 16.20s/it]  5%|▌         | 25/500 [06:52<2:07:55, 16.16s/it]  5%|▌         | 26/500 [07:08<2:06:55, 16.07s/it]  5%|▌         | 27/500 [07:24<2:05:44, 15.95s/it]  6%|▌         | 28/500 [07:40<2:05:34, 15.96s/it]  6%|▌         | 29/500 [07:56<2:05:10, 15.95s/it]  6%|▌         | 29/500 [07:56<2:08:55, 16.42s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.011 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▁▁▁▂▅▄▆▅▄▆▃█▄█▅▅▆▆█▇█▇█▆▄▆█
wandb:     train_loss ▃▂▅██▁▃▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅
wandb:   val_accuracy █▅▅▅▅▇▆▅▄▃▄▅▅▆▅▅▂▁▄▄▅▃▁▁▅▂▅▂▄
wandb:       val_loss ▂▂▄▃▄▅▁▁▃▂▄▂▁▁▅▃▃▃▃▃▇▆▃▅▄▄█▂█
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.95097
wandb:     train_loss 2.82941
wandb:   val_accuracy 0.28667
wandb:       val_loss 7.70601
wandb: 
wandb: 🚀 View run copper-dream-368 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0uahu5hd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_115721-0uahu5hd/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_120559-m6jmsa81
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-darkness-369
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m6jmsa81
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:39, 17.75s/it]  0%|          | 2/500 [00:33<2:16:39, 16.47s/it]  1%|          | 3/500 [00:53<2:31:28, 18.29s/it]  1%|          | 4/500 [01:09<2:22:54, 17.29s/it]  1%|          | 5/500 [01:25<2:17:57, 16.72s/it]  1%|          | 6/500 [01:41<2:15:29, 16.46s/it]  1%|▏         | 7/500 [01:58<2:16:34, 16.62s/it]  2%|▏         | 8/500 [02:14<2:14:33, 16.41s/it]  2%|▏         | 9/500 [02:29<2:12:23, 16.18s/it]  2%|▏         | 10/500 [02:45<2:11:30, 16.10s/it]  2%|▏         | 11/500 [03:01<2:10:10, 15.97s/it]  2%|▏         | 12/500 [03:17<2:09:36, 15.94s/it]  3%|▎         | 13/500 [03:33<2:09:07, 15.91s/it]  3%|▎         | 14/500 [03:49<2:09:17, 15.96s/it]  3%|▎         | 15/500 [04:06<2:11:19, 16.25s/it]  3%|▎         | 16/500 [04:21<2:09:38, 16.07s/it]  3%|▎         | 17/500 [04:37<2:08:55, 16.01s/it]  4%|▎         | 18/500 [04:53<2:08:35, 16.01s/it]  4%|▍         | 19/500 [05:09<2:07:41, 15.93s/it]  4%|▍         | 20/500 [05:25<2:07:04, 15.88s/it]  4%|▍         | 21/500 [05:40<2:06:27, 15.84s/it]  4%|▍         | 22/500 [05:56<2:05:57, 15.81s/it]  5%|▍         | 23/500 [06:12<2:05:22, 15.77s/it]  5%|▍         | 24/500 [06:28<2:05:15, 15.79s/it]  5%|▌         | 25/500 [06:44<2:05:19, 15.83s/it]  5%|▌         | 26/500 [06:59<2:04:45, 15.79s/it]  5%|▌         | 27/500 [07:15<2:05:32, 15.93s/it]  6%|▌         | 28/500 [07:31<2:05:09, 15.91s/it]  6%|▌         | 29/500 [07:47<2:04:08, 15.82s/it]  6%|▌         | 29/500 [07:51<2:07:44, 16.27s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.021 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▄▆▆▇▇▇█▇▇█▂▁▁▃▃▄▄▄▄▅▅▆▆▆▆▆▆
wandb:     train_loss ▆▆▅▅▅▄▅▅▅▅▄▅▁▆█▅▅▅▃▅▅▆▄▆▄▆▅▅▆
wandb:   val_accuracy ▂▂▂▆▆█▇▇▇▆▆▅▁▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂
wandb:       val_loss ▆▆▆▆▅▅▅▆▅▄▅▆▁▅▇▆▇▆▆▆█▆▆▆▅▆▅▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.53492
wandb:     train_loss 1.18384
wandb:   val_accuracy 0.37333
wandb:       val_loss 0.86298
wandb: 
wandb: 🚀 View run clear-darkness-369 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m6jmsa81
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_120559-m6jmsa81/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_121431-actp97ly
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sound-370
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/actp97ly
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:36:28, 18.81s/it]  0%|          | 2/500 [00:34<2:19:54, 16.86s/it]  1%|          | 3/500 [00:49<2:14:13, 16.20s/it]  1%|          | 4/500 [01:04<2:10:41, 15.81s/it]  1%|          | 5/500 [01:20<2:08:43, 15.60s/it]  1%|          | 6/500 [01:35<2:07:33, 15.49s/it]  1%|▏         | 7/500 [01:50<2:06:01, 15.34s/it]  2%|▏         | 8/500 [02:10<2:17:47, 16.80s/it]  2%|▏         | 9/500 [02:25<2:13:47, 16.35s/it]  2%|▏         | 10/500 [02:40<2:10:24, 15.97s/it]  2%|▏         | 11/500 [02:56<2:08:09, 15.72s/it]  2%|▏         | 12/500 [03:11<2:06:28, 15.55s/it]  3%|▎         | 13/500 [03:26<2:05:45, 15.49s/it]  3%|▎         | 14/500 [03:41<2:05:20, 15.47s/it]  3%|▎         | 15/500 [03:57<2:04:19, 15.38s/it]  3%|▎         | 16/500 [04:12<2:03:44, 15.34s/it]  3%|▎         | 17/500 [04:27<2:03:20, 15.32s/it]  4%|▎         | 18/500 [04:43<2:03:17, 15.35s/it]  4%|▍         | 19/500 [04:58<2:02:47, 15.32s/it]  4%|▍         | 20/500 [05:13<2:02:04, 15.26s/it]  4%|▍         | 21/500 [05:28<2:02:19, 15.32s/it]  4%|▍         | 22/500 [05:44<2:01:51, 15.30s/it]  5%|▍         | 23/500 [06:00<2:03:00, 15.47s/it]  5%|▍         | 24/500 [06:15<2:02:49, 15.48s/it]  5%|▌         | 25/500 [06:35<2:13:55, 16.92s/it]  5%|▌         | 26/500 [06:51<2:10:25, 16.51s/it]  5%|▌         | 27/500 [07:11<2:18:05, 17.52s/it]  6%|▌         | 28/500 [07:27<2:13:53, 17.02s/it]  6%|▌         | 29/500 [07:46<2:20:22, 17.88s/it]  6%|▌         | 30/500 [08:02<2:13:56, 17.10s/it]  6%|▌         | 31/500 [08:17<2:10:02, 16.64s/it]  6%|▋         | 32/500 [08:32<2:06:19, 16.20s/it]  7%|▋         | 33/500 [08:48<2:03:51, 15.91s/it]  7%|▋         | 34/500 [09:03<2:02:19, 15.75s/it]  7%|▋         | 35/500 [09:18<2:00:39, 15.57s/it]  7%|▋         | 36/500 [09:38<2:10:02, 16.81s/it]  7%|▋         | 37/500 [09:53<2:05:39, 16.28s/it]  8%|▊         | 38/500 [10:08<2:03:19, 16.02s/it]  8%|▊         | 39/500 [10:24<2:01:16, 15.78s/it]  8%|▊         | 40/500 [10:44<2:11:00, 17.09s/it]  8%|▊         | 41/500 [10:59<2:07:28, 16.66s/it]  8%|▊         | 42/500 [11:15<2:04:02, 16.25s/it]  9%|▊         | 43/500 [11:30<2:01:02, 15.89s/it]  9%|▉         | 44/500 [11:49<2:09:26, 17.03s/it]  9%|▉         | 45/500 [12:05<2:05:07, 16.50s/it]  9%|▉         | 46/500 [12:20<2:02:13, 16.15s/it]  9%|▉         | 47/500 [12:35<1:59:59, 15.89s/it] 10%|▉         | 48/500 [12:51<1:58:11, 15.69s/it] 10%|▉         | 49/500 [13:11<2:07:54, 17.02s/it] 10%|█         | 50/500 [13:26<2:03:38, 16.48s/it] 10%|█         | 51/500 [13:46<2:11:00, 17.51s/it] 10%|█         | 52/500 [14:01<2:06:23, 16.93s/it] 11%|█         | 53/500 [14:17<2:02:08, 16.39s/it] 11%|█         | 54/500 [14:33<2:01:27, 16.34s/it] 11%|█         | 55/500 [14:48<1:58:46, 16.02s/it] 11%|█         | 56/500 [15:03<1:57:03, 15.82s/it] 11%|█▏        | 57/500 [15:19<1:56:03, 15.72s/it] 12%|█▏        | 58/500 [15:39<2:06:26, 17.16s/it] 12%|█▏        | 59/500 [15:55<2:03:34, 16.81s/it] 12%|█▏        | 60/500 [16:15<2:09:28, 17.66s/it] 12%|█▏        | 61/500 [16:31<2:06:21, 17.27s/it] 12%|█▏        | 62/500 [16:52<2:12:47, 18.19s/it] 13%|█▎        | 63/500 [17:07<2:05:36, 17.25s/it] 13%|█▎        | 64/500 [17:23<2:02:17, 16.83s/it] 13%|█▎        | 65/500 [17:38<1:59:17, 16.45s/it] 13%|█▎        | 66/500 [17:54<1:57:42, 16.27s/it] 13%|█▎        | 67/500 [18:10<1:55:59, 16.07s/it] 14%|█▎        | 68/500 [18:30<2:04:42, 17.32s/it] 14%|█▍        | 69/500 [18:45<1:59:17, 16.61s/it] 14%|█▍        | 70/500 [19:00<1:56:08, 16.21s/it] 14%|█▍        | 70/500 [19:00<1:56:46, 16.29s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.025 MB of 0.318 MB uploadedwandb: - 0.025 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▂▂▄▃▃▄▃▅▆▄▄▃▇█▄█▆▇▅▅▇▆▅▄▂▆▄▆▃▅▄▃▄▆▅▅▇
wandb:     train_loss ▂▂▂▂▂▂▃▃▁▁▁▁▂▁▁▁▃▁▁▁▁▁▁▁▁▁██▁▂▁▂▇█▁▁▁▁▁▁
wandb:   val_accuracy ▂█▆▅▂▄▃▃▃▃▂▃▂▃▃▃▂▂▁▂▂▃▂▄▂▃▃▂▂▃▄▃▃▃▃▄▂▃▄▄
wandb:       val_loss ▂▂▁▁▁▁▂▂▂▂▃▂▁▃▁▃▄▂▃▄▃▃▄▂▃▄▂█▂▂▁▃▃▅▃▃▆▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 69
wandb:  learning_rate 0.0001
wandb: train_accuracy 0.81129
wandb:     train_loss 0.07689
wandb:   val_accuracy 0.43556
wandb:       val_loss 0.47934
wandb: 
wandb: 🚀 View run visionary-sound-370 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/actp97ly
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_121431-actp97ly/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_123421-jzzswazm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-water-372
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jzzswazm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:10, 18.42s/it]  0%|          | 2/500 [00:34<2:23:34, 17.30s/it]  1%|          | 3/500 [00:51<2:20:37, 16.98s/it]  1%|          | 4/500 [01:07<2:17:12, 16.60s/it]  1%|          | 5/500 [01:23<2:13:55, 16.23s/it]  1%|          | 6/500 [01:39<2:15:09, 16.42s/it]  1%|▏         | 7/500 [01:55<2:11:29, 16.00s/it]  2%|▏         | 8/500 [02:11<2:12:53, 16.21s/it]  2%|▏         | 9/500 [02:27<2:12:52, 16.24s/it]  2%|▏         | 10/500 [02:48<2:24:08, 17.65s/it]  2%|▏         | 11/500 [03:04<2:19:39, 17.14s/it]  2%|▏         | 12/500 [03:20<2:15:28, 16.66s/it]  3%|▎         | 13/500 [03:40<2:24:24, 17.79s/it]  3%|▎         | 14/500 [03:56<2:19:07, 17.18s/it]  3%|▎         | 15/500 [04:11<2:14:35, 16.65s/it]  3%|▎         | 16/500 [04:27<2:11:58, 16.36s/it]  3%|▎         | 17/500 [04:43<2:09:54, 16.14s/it]  4%|▎         | 18/500 [04:58<2:07:30, 15.87s/it]  4%|▍         | 19/500 [05:13<2:06:09, 15.74s/it]  4%|▍         | 20/500 [05:29<2:05:59, 15.75s/it]  4%|▍         | 21/500 [05:45<2:06:49, 15.89s/it]  4%|▍         | 22/500 [06:01<2:06:11, 15.84s/it]  5%|▍         | 23/500 [06:16<2:04:31, 15.66s/it]  5%|▍         | 24/500 [06:32<2:04:33, 15.70s/it]  5%|▌         | 25/500 [06:52<2:14:27, 16.98s/it]  5%|▌         | 26/500 [07:08<2:11:26, 16.64s/it]  5%|▌         | 27/500 [07:24<2:09:41, 16.45s/it]  6%|▌         | 28/500 [07:44<2:17:59, 17.54s/it]  6%|▌         | 29/500 [08:00<2:14:22, 17.12s/it]  6%|▌         | 30/500 [08:16<2:10:35, 16.67s/it]  6%|▌         | 31/500 [08:33<2:12:06, 16.90s/it]  6%|▋         | 32/500 [08:49<2:09:15, 16.57s/it]  7%|▋         | 33/500 [09:10<2:18:34, 17.81s/it]  7%|▋         | 34/500 [09:25<2:13:12, 17.15s/it]  7%|▋         | 35/500 [09:41<2:08:54, 16.63s/it]  7%|▋         | 36/500 [09:56<2:06:20, 16.34s/it]  7%|▋         | 37/500 [10:13<2:05:43, 16.29s/it]  8%|▊         | 38/500 [10:29<2:05:06, 16.25s/it]  8%|▊         | 39/500 [10:44<2:03:20, 16.05s/it]  8%|▊         | 40/500 [11:00<2:02:59, 16.04s/it]  8%|▊         | 41/500 [11:17<2:03:14, 16.11s/it]  8%|▊         | 42/500 [11:35<2:09:04, 16.91s/it]  9%|▊         | 43/500 [11:53<2:09:37, 17.02s/it]  9%|▉         | 44/500 [12:09<2:07:58, 16.84s/it]  9%|▉         | 45/500 [12:25<2:06:24, 16.67s/it]  9%|▉         | 45/500 [12:25<2:05:42, 16.58s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.010 MB of 0.315 MB uploadedwandb: - 0.020 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▁▄▁▅▁▄▃▆▆▇▃▇█▅▇▆▃▆▆▇▇█▃▆▃▅▇▇█▇▆▇▇▇▇▆▆▇
wandb:     train_loss ▂▁▂▁▃▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▂▃▁▃▄▁▁▁▁▁▁▃▁▂▂▁▁▁█
wandb:   val_accuracy ▃▄▃▆▃▃▄▃▇▆▃▃▃▆▄▃▆▄▃▁▆▃▆▃▃█▃█▇▆▆▆█▃▆▇██▄▃
wandb:       val_loss ▂▃▄▂▅▂█▂▂▂▇▁█▂▁▃▂▄▆▂▃▄▁▁▃▁▅▁▁▁▁▁▁▄▄▁▂▁▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.82021
wandb:     train_loss 13.79254
wandb:   val_accuracy 0.32444
wandb:       val_loss 1.46376
wandb: 
wandb: 🚀 View run fragrant-water-372 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jzzswazm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_123421-jzzswazm/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_124741-chyx4r85
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-night-373
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/chyx4r85
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:23<3:16:20, 23.61s/it]  0%|          | 2/500 [00:38<2:35:03, 18.68s/it]  1%|          | 3/500 [00:54<2:22:35, 17.21s/it]  1%|          | 4/500 [01:13<2:29:23, 18.07s/it]  1%|          | 5/500 [01:29<2:21:00, 17.09s/it]  1%|          | 6/500 [01:48<2:26:12, 17.76s/it]  1%|▏         | 7/500 [02:03<2:18:57, 16.91s/it]  2%|▏         | 8/500 [02:18<2:13:07, 16.23s/it]  2%|▏         | 9/500 [02:38<2:22:40, 17.43s/it]  2%|▏         | 10/500 [02:53<2:17:24, 16.83s/it]  2%|▏         | 11/500 [03:08<2:12:14, 16.23s/it]  2%|▏         | 12/500 [03:24<2:11:54, 16.22s/it]  3%|▎         | 13/500 [03:40<2:10:25, 16.07s/it]  3%|▎         | 14/500 [03:55<2:08:07, 15.82s/it]  3%|▎         | 15/500 [04:10<2:06:47, 15.68s/it]  3%|▎         | 16/500 [04:26<2:05:23, 15.55s/it]  3%|▎         | 17/500 [04:41<2:04:59, 15.53s/it]  4%|▎         | 18/500 [04:56<2:04:08, 15.45s/it]  4%|▍         | 19/500 [05:12<2:04:26, 15.52s/it]  4%|▍         | 20/500 [05:28<2:05:16, 15.66s/it]  4%|▍         | 21/500 [05:43<2:04:02, 15.54s/it]  4%|▍         | 22/500 [05:59<2:02:57, 15.43s/it]  5%|▍         | 23/500 [06:14<2:02:08, 15.36s/it]  5%|▍         | 24/500 [06:29<2:01:14, 15.28s/it]  5%|▌         | 25/500 [06:45<2:02:59, 15.54s/it]  5%|▌         | 26/500 [07:00<2:02:28, 15.50s/it]  5%|▌         | 27/500 [07:16<2:02:16, 15.51s/it]  6%|▌         | 28/500 [07:32<2:02:20, 15.55s/it]  6%|▌         | 29/500 [07:46<2:00:25, 15.34s/it]  6%|▌         | 29/500 [07:46<2:06:24, 16.10s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.021 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▃▆▇█▇█▆▆▇▇▆▆▃▆▇▆▆▇▆▆▆▃▂▁▂▃▄
wandb:     train_loss ▄▃▃▃▃▂▃▃▃▂▁▃▅▁█▂▅▂▂▁▃▃▂▁▂▃▄▃▃
wandb:   val_accuracy ▂▂▂▆▇█▇▇▇▇▇█▆▆▂▅▆▆▆▇▅▆▅▂▂▁▂▂▂
wandb:       val_loss ▆▆▆▅▃▃▄▆▂▃▃▅▁▁▃▃▆▄▂▂▄▅▆█▁▇▄▇▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.39376
wandb:     train_loss 1.18895
wandb:   val_accuracy 0.34889
wandb:       val_loss 0.86715
wandb: 
wandb: 🚀 View run iconic-night-373 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/chyx4r85
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_124741-chyx4r85/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_125627-zn0angs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-star-374
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zn0angs9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:10, 28.40s/it]  0%|          | 2/500 [00:44<2:57:46, 21.42s/it]  1%|          | 3/500 [01:00<2:34:47, 18.69s/it]  1%|          | 4/500 [01:16<2:25:24, 17.59s/it]  1%|          | 5/500 [01:31<2:19:33, 16.92s/it]  1%|          | 6/500 [01:47<2:15:38, 16.48s/it]  1%|▏         | 7/500 [02:03<2:13:21, 16.23s/it]  2%|▏         | 8/500 [02:19<2:13:40, 16.30s/it]  2%|▏         | 9/500 [02:35<2:12:16, 16.16s/it]  2%|▏         | 10/500 [02:50<2:09:20, 15.84s/it]  2%|▏         | 11/500 [03:05<2:07:01, 15.59s/it]  2%|▏         | 12/500 [03:22<2:09:07, 15.88s/it]  3%|▎         | 13/500 [03:38<2:08:40, 15.85s/it]  3%|▎         | 14/500 [03:54<2:09:24, 15.98s/it]  3%|▎         | 15/500 [04:11<2:11:52, 16.31s/it]  3%|▎         | 16/500 [04:27<2:10:32, 16.18s/it]  3%|▎         | 17/500 [04:45<2:16:08, 16.91s/it]  4%|▎         | 18/500 [05:02<2:13:56, 16.67s/it]  4%|▍         | 19/500 [05:18<2:13:34, 16.66s/it]  4%|▍         | 20/500 [05:34<2:11:03, 16.38s/it]  4%|▍         | 21/500 [05:54<2:20:07, 17.55s/it]  4%|▍         | 22/500 [06:10<2:16:07, 17.09s/it]  5%|▍         | 23/500 [06:26<2:12:17, 16.64s/it]  5%|▍         | 24/500 [06:47<2:22:54, 18.01s/it]  5%|▌         | 25/500 [07:03<2:18:05, 17.44s/it]  5%|▌         | 26/500 [07:24<2:25:22, 18.40s/it]  5%|▌         | 27/500 [07:39<2:18:10, 17.53s/it]  6%|▌         | 28/500 [07:55<2:12:42, 16.87s/it]  6%|▌         | 29/500 [08:10<2:10:00, 16.56s/it]  6%|▌         | 30/500 [08:27<2:08:59, 16.47s/it]  6%|▌         | 31/500 [08:47<2:18:26, 17.71s/it]  6%|▋         | 32/500 [09:03<2:12:50, 17.03s/it]  7%|▋         | 33/500 [09:19<2:11:05, 16.84s/it]  7%|▋         | 34/500 [09:35<2:09:14, 16.64s/it]  7%|▋         | 35/500 [09:51<2:06:08, 16.28s/it]  7%|▋         | 36/500 [10:07<2:05:03, 16.17s/it]  7%|▋         | 37/500 [10:24<2:06:41, 16.42s/it]  7%|▋         | 37/500 [10:24<2:10:12, 16.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▅▁▅▄▆▆▅▆█▅█▄▄▇█▆▆▇▄▂▃▄▅▄▄▅▄▆▄▄▆▅▅▇▇▆
wandb:     train_loss ▂▂▂▂▂▁▂▁▂▂▃▁▂▂▂▁▆▁▁▁▄▂▁▃▁▁██▃▁▁▁▅▃▁▁▁
wandb:   val_accuracy ▁█▁▁▃▄▂▄▇▃▆▂▁▅▄▂▆▆▆▂▃▂▅▁▁▁▄▃▃▂▂▅▄▂▇▇▃
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▁█▁▂▂▂▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.6523
wandb:     train_loss 0.09752
wandb:   val_accuracy 0.37556
wandb:       val_loss 3.46576
wandb: 
wandb: 🚀 View run northern-star-374 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zn0angs9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_125627-zn0angs9/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_130741-xdwqar09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-leaf-376
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xdwqar09
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:26:56, 24.88s/it]  0%|          | 2/500 [00:52<3:41:49, 26.73s/it]  1%|          | 3/500 [01:16<3:29:50, 25.33s/it]  1%|          | 4/500 [01:40<3:24:14, 24.71s/it]  1%|          | 5/500 [02:05<3:24:25, 24.78s/it]  1%|          | 6/500 [02:29<3:21:15, 24.44s/it]  1%|▏         | 7/500 [02:59<3:38:04, 26.54s/it]  2%|▏         | 8/500 [03:23<3:30:11, 25.63s/it]  2%|▏         | 9/500 [03:51<3:36:53, 26.50s/it]  2%|▏         | 10/500 [04:20<3:41:51, 27.17s/it]  2%|▏         | 11/500 [04:53<3:56:39, 29.04s/it]  2%|▏         | 12/500 [05:26<4:04:09, 30.02s/it]  3%|▎         | 13/500 [05:50<3:49:03, 28.22s/it]  3%|▎         | 14/500 [06:19<3:52:13, 28.67s/it]  3%|▎         | 15/500 [06:43<3:40:01, 27.22s/it]  3%|▎         | 16/500 [07:14<3:48:24, 28.32s/it]  3%|▎         | 17/500 [07:38<3:36:21, 26.88s/it]  4%|▎         | 18/500 [08:06<3:39:21, 27.31s/it]  4%|▍         | 19/500 [08:29<3:29:38, 26.15s/it]  4%|▍         | 20/500 [08:53<3:23:03, 25.38s/it]  4%|▍         | 21/500 [09:16<3:17:47, 24.78s/it]  4%|▍         | 22/500 [09:40<3:14:21, 24.40s/it]  5%|▍         | 23/500 [10:04<3:12:01, 24.15s/it]  5%|▍         | 24/500 [10:27<3:09:43, 23.91s/it]  5%|▌         | 25/500 [10:55<3:18:30, 25.08s/it]  5%|▌         | 26/500 [11:24<3:28:22, 26.38s/it]  5%|▌         | 27/500 [11:48<3:21:54, 25.61s/it]  6%|▌         | 28/500 [12:12<3:17:30, 25.11s/it]  6%|▌         | 29/500 [12:35<3:13:24, 24.64s/it]  6%|▌         | 30/500 [12:59<3:10:23, 24.31s/it]  6%|▌         | 30/500 [12:59<3:23:31, 25.98s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.019 MB of 0.308 MB uploadedwandb: \ 0.244 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▃▃▃▃▃▁▅▄▁▅▆▄█▇▇▅██▆▇███████▇█
wandb:     train_loss ▂▂▁▂▂▁█▁▁▇▁▁█▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▇▅▆▅▇▃█▆▃▆█▆▆▄▄▆▅▅▃▃▄▇▆▆▄▅▁▂▆
wandb:       val_loss ▁▁▁▁▁▁▂▂▂▃▁▁▄▁▁█▆▁▂▂▂▂▁▇▂▂▁▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.99703
wandb:     train_loss 0.00014
wandb:   val_accuracy 0.53778
wandb:       val_loss 4.68316
wandb: 
wandb: 🚀 View run cool-leaf-376 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xdwqar09
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_130741-xdwqar09/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_132124-ien2d2kf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-serenity-377
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ien2d2kf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:04:33, 36.62s/it]  0%|          | 2/500 [01:00<4:02:11, 29.18s/it]  1%|          | 3/500 [01:29<4:01:47, 29.19s/it]  1%|          | 4/500 [01:59<4:01:39, 29.23s/it]  1%|          | 5/500 [02:22<3:45:14, 27.30s/it]  1%|          | 6/500 [02:51<3:48:22, 27.74s/it]  1%|▏         | 7/500 [03:20<3:50:41, 28.08s/it]  2%|▏         | 8/500 [03:49<3:52:46, 28.39s/it]  2%|▏         | 9/500 [04:13<3:40:56, 27.00s/it]  2%|▏         | 10/500 [04:41<3:44:23, 27.48s/it]  2%|▏         | 11/500 [05:05<3:34:45, 26.35s/it]  2%|▏         | 12/500 [05:33<3:39:02, 26.93s/it]  3%|▎         | 13/500 [06:06<3:53:06, 28.72s/it]  3%|▎         | 14/500 [06:30<3:40:05, 27.17s/it]  3%|▎         | 15/500 [06:53<3:30:51, 26.08s/it]  3%|▎         | 16/500 [07:22<3:36:31, 26.84s/it]  3%|▎         | 17/500 [07:46<3:29:02, 25.97s/it]  4%|▎         | 18/500 [08:10<3:22:56, 25.26s/it]  4%|▍         | 19/500 [08:33<3:18:22, 24.75s/it]  4%|▍         | 20/500 [08:57<3:15:26, 24.43s/it]  4%|▍         | 21/500 [09:26<3:25:33, 25.75s/it]  4%|▍         | 22/500 [09:49<3:20:12, 25.13s/it]  5%|▍         | 23/500 [10:13<3:16:20, 24.70s/it]  5%|▍         | 24/500 [10:37<3:13:22, 24.38s/it]  5%|▌         | 25/500 [11:00<3:11:41, 24.21s/it]  5%|▌         | 26/500 [11:25<3:13:04, 24.44s/it]  5%|▌         | 27/500 [11:49<3:10:55, 24.22s/it]  6%|▌         | 28/500 [12:17<3:19:43, 25.39s/it]  6%|▌         | 29/500 [12:52<3:42:16, 28.32s/it]  6%|▌         | 30/500 [13:16<3:30:59, 26.93s/it]  6%|▌         | 31/500 [13:47<3:40:01, 28.15s/it]  6%|▋         | 32/500 [14:16<3:40:39, 28.29s/it]  7%|▋         | 33/500 [14:47<3:47:29, 29.23s/it]  7%|▋         | 34/500 [15:15<3:44:38, 28.92s/it]  7%|▋         | 35/500 [15:44<3:43:22, 28.82s/it]  7%|▋         | 36/500 [16:08<3:31:17, 27.32s/it]  7%|▋         | 37/500 [16:37<3:34:15, 27.77s/it]  8%|▊         | 38/500 [17:06<3:36:41, 28.14s/it]  8%|▊         | 39/500 [17:34<3:36:19, 28.15s/it]  8%|▊         | 40/500 [17:58<3:25:47, 26.84s/it]  8%|▊         | 41/500 [18:21<3:17:38, 25.84s/it]  8%|▊         | 42/500 [18:53<3:31:42, 27.73s/it]  9%|▊         | 43/500 [19:17<3:21:34, 26.47s/it]  9%|▉         | 44/500 [19:40<3:13:53, 25.51s/it]  9%|▉         | 45/500 [20:03<3:07:45, 24.76s/it]  9%|▉         | 45/500 [20:03<3:22:49, 26.75s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.029 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▃▄▅▅▅▅▆▆▆▆▆▆▆▇▇▆▅▇▇▇▇▇▇▇▇█▇▇▇▇▇███████
wandb:     train_loss ▅▅▇▆▄▆▆▂▄▅▂▂▆▅█▅▃▃█▃▃▂▃▇▂▁▂▃▂▂▂█▃▂▅▄▃▄▅▁
wandb:   val_accuracy ▃▄▆▃█▅▃▅▄▄▄▆▆▂▂▅▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▃▁▁
wandb:       val_loss ▃▃▄▃▃▃▃▃▃▄▅▂▂▅▄▂▄▃▆█▅▆▄▅▆▁▃▆▄▃▆▂▆▇▄▂▁▁▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.80981
wandb:     train_loss 0.048
wandb:   val_accuracy 0.28444
wandb:       val_loss 1.60644
wandb: 
wandb: 🚀 View run skilled-serenity-377 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ien2d2kf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_132124-ien2d2kf/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_134208-j72t5u32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-galaxy-380
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j72t5u32
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:22, 25.66s/it]  0%|          | 2/500 [00:50<3:29:41, 25.26s/it]  1%|          | 3/500 [01:15<3:26:39, 24.95s/it]  1%|          | 4/500 [01:39<3:23:05, 24.57s/it]  1%|          | 5/500 [02:03<3:22:34, 24.55s/it]  1%|          | 6/500 [02:27<3:20:38, 24.37s/it]  1%|▏         | 7/500 [02:51<3:19:33, 24.29s/it]  2%|▏         | 8/500 [03:15<3:18:18, 24.18s/it]  2%|▏         | 9/500 [03:39<3:17:50, 24.18s/it]  2%|▏         | 10/500 [04:04<3:18:02, 24.25s/it]  2%|▏         | 11/500 [04:28<3:17:24, 24.22s/it]  2%|▏         | 12/500 [04:52<3:16:18, 24.14s/it]  3%|▎         | 13/500 [05:16<3:15:17, 24.06s/it]  3%|▎         | 14/500 [05:39<3:13:46, 23.92s/it]  3%|▎         | 15/500 [06:03<3:12:33, 23.82s/it]  3%|▎         | 16/500 [06:32<3:25:01, 25.42s/it]  3%|▎         | 17/500 [06:56<3:20:13, 24.87s/it]  4%|▎         | 18/500 [07:20<3:16:59, 24.52s/it]  4%|▍         | 19/500 [07:43<3:14:20, 24.24s/it]  4%|▍         | 20/500 [08:07<3:12:22, 24.05s/it]  4%|▍         | 21/500 [08:30<3:10:26, 23.85s/it]  4%|▍         | 22/500 [08:54<3:09:02, 23.73s/it]  5%|▍         | 23/500 [09:17<3:07:33, 23.59s/it]  5%|▍         | 24/500 [09:40<3:06:28, 23.50s/it]  5%|▌         | 25/500 [10:08<3:16:42, 24.85s/it]  5%|▌         | 26/500 [10:32<3:13:14, 24.46s/it]  5%|▌         | 27/500 [11:00<3:21:55, 25.61s/it]  6%|▌         | 28/500 [11:23<3:16:17, 24.95s/it]  6%|▌         | 29/500 [11:47<3:12:16, 24.49s/it]  6%|▌         | 30/500 [12:10<3:09:26, 24.18s/it]  6%|▌         | 30/500 [12:10<3:10:48, 24.36s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.231 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▂▂▃▃▁▁▂▂▄▄▄▆▇▆▇█▇█▂▃▆▇████████
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▃▄▆▃▃▄▂▂▇▆▅▃▂▂▁▅▄▃▅█▆▄▄▄▆▅▅▅▅
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▃▂▁█▂▁▁▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.96137
wandb:     train_loss 0.02989
wandb:   val_accuracy 0.42667
wandb:       val_loss 2.94882
wandb: 
wandb: 🚀 View run astral-galaxy-380 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j72t5u32
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_134208-j72t5u32/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_135510-2u0jddu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-plant-382
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2u0jddu5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:06:59, 29.70s/it]  0%|          | 2/500 [00:54<3:40:17, 26.54s/it]  1%|          | 3/500 [01:18<3:31:56, 25.59s/it]  1%|          | 4/500 [01:42<3:26:42, 25.00s/it]  1%|          | 5/500 [02:10<3:33:39, 25.90s/it]  1%|          | 6/500 [02:34<3:28:14, 25.29s/it]  1%|▏         | 7/500 [02:58<3:24:03, 24.83s/it]  2%|▏         | 8/500 [03:22<3:21:19, 24.55s/it]  2%|▏         | 9/500 [03:45<3:19:17, 24.35s/it]  2%|▏         | 10/500 [04:09<3:17:31, 24.19s/it]  2%|▏         | 11/500 [04:33<3:15:45, 24.02s/it]  2%|▏         | 12/500 [04:57<3:14:46, 23.95s/it]  3%|▎         | 13/500 [05:21<3:15:12, 24.05s/it]  3%|▎         | 14/500 [05:45<3:15:11, 24.10s/it]  3%|▎         | 15/500 [06:09<3:14:12, 24.03s/it]  3%|▎         | 16/500 [06:34<3:16:07, 24.31s/it]  3%|▎         | 17/500 [06:58<3:15:14, 24.25s/it]  4%|▎         | 18/500 [07:22<3:14:48, 24.25s/it]  4%|▍         | 19/500 [07:46<3:13:45, 24.17s/it]  4%|▍         | 20/500 [08:18<3:31:42, 26.46s/it]  4%|▍         | 21/500 [08:47<3:37:32, 27.25s/it]  4%|▍         | 22/500 [09:12<3:30:20, 26.40s/it]  5%|▍         | 23/500 [09:36<3:24:16, 25.70s/it]  5%|▍         | 24/500 [10:00<3:20:53, 25.32s/it]  5%|▌         | 25/500 [10:24<3:16:31, 24.82s/it]  5%|▌         | 26/500 [10:48<3:14:10, 24.58s/it]  5%|▌         | 27/500 [11:12<3:12:38, 24.44s/it]  6%|▌         | 28/500 [11:36<3:10:54, 24.27s/it]  6%|▌         | 29/500 [12:00<3:09:13, 24.10s/it]  6%|▌         | 30/500 [12:23<3:07:58, 24.00s/it]  6%|▌         | 31/500 [12:47<3:07:37, 24.00s/it]  6%|▋         | 32/500 [13:16<3:17:27, 25.31s/it]  7%|▋         | 33/500 [13:44<3:24:20, 26.25s/it]  7%|▋         | 34/500 [14:08<3:19:00, 25.62s/it]  7%|▋         | 35/500 [14:36<3:23:55, 26.31s/it]  7%|▋         | 36/500 [15:00<3:18:34, 25.68s/it]  7%|▋         | 36/500 [15:01<3:13:34, 25.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.020 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▁▁▃▂▄▂▁▃▇▄▆▇▅▆▇█▆▇▇█████▇█▇▇▇▇▅██
wandb:     train_loss ▃▃▂▂█▃▃▁▆▇▅▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁█▁▂▁▂
wandb:   val_accuracy ▃█▅▄▃▅▄▄▃▄▆▂▆▄▃▃▃▅▂▆▂▃▁▂▃▃▃▂▂▆▂▄▃▇▃▂
wandb:       val_loss ▁▁▁▁▁▂▂▂▅▁▂▅▃▁▄█▄▂▄▁▂▃▄▇▂▂▃▂▄▂▄▃▁▄▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 35
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.97325
wandb:     train_loss 0.39683
wandb:   val_accuracy 0.28222
wandb:       val_loss 6.72403
wandb: 
wandb: 🚀 View run earthy-plant-382 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2u0jddu5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_135510-2u0jddu5/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_141054-4tmwt9dt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-flower-384
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4tmwt9dt
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:28:43, 25.10s/it]  0%|          | 2/500 [00:48<3:20:32, 24.16s/it]  1%|          | 3/500 [01:12<3:18:47, 24.00s/it]  1%|          | 4/500 [01:36<3:17:26, 23.88s/it]  1%|          | 5/500 [01:59<3:16:07, 23.77s/it]  1%|          | 6/500 [02:22<3:14:18, 23.60s/it]  1%|▏         | 7/500 [02:46<3:13:53, 23.60s/it]  2%|▏         | 8/500 [03:10<3:13:30, 23.60s/it]  2%|▏         | 9/500 [03:33<3:12:42, 23.55s/it]  2%|▏         | 10/500 [04:02<3:25:19, 25.14s/it]  2%|▏         | 11/500 [04:25<3:20:34, 24.61s/it]  2%|▏         | 12/500 [04:49<3:17:57, 24.34s/it]  3%|▎         | 13/500 [05:13<3:16:22, 24.19s/it]  3%|▎         | 14/500 [05:36<3:14:23, 24.00s/it]  3%|▎         | 15/500 [06:00<3:13:05, 23.89s/it]  3%|▎         | 16/500 [06:23<3:11:43, 23.77s/it]  3%|▎         | 17/500 [06:47<3:10:36, 23.68s/it]  4%|▎         | 18/500 [07:10<3:09:36, 23.60s/it]  4%|▍         | 19/500 [07:34<3:09:03, 23.58s/it]  4%|▍         | 20/500 [08:01<3:17:57, 24.75s/it]  4%|▍         | 21/500 [08:25<3:14:51, 24.41s/it]  4%|▍         | 22/500 [08:48<3:12:08, 24.12s/it]  5%|▍         | 23/500 [09:12<3:10:22, 23.95s/it]  5%|▍         | 24/500 [09:35<3:08:57, 23.82s/it]  5%|▌         | 25/500 [09:59<3:08:18, 23.79s/it]  5%|▌         | 26/500 [10:23<3:07:20, 23.71s/it]  5%|▌         | 27/500 [10:46<3:06:53, 23.71s/it]  6%|▌         | 28/500 [11:10<3:06:27, 23.70s/it]  6%|▌         | 29/500 [11:34<3:06:48, 23.80s/it]  6%|▌         | 30/500 [11:58<3:05:43, 23.71s/it]  6%|▌         | 31/500 [12:21<3:04:46, 23.64s/it]  6%|▋         | 32/500 [12:49<3:14:57, 24.99s/it]  7%|▋         | 33/500 [13:13<3:11:55, 24.66s/it]  7%|▋         | 34/500 [13:39<3:13:54, 24.97s/it]  7%|▋         | 35/500 [14:03<3:10:41, 24.60s/it]  7%|▋         | 36/500 [14:26<3:08:13, 24.34s/it]  7%|▋         | 37/500 [14:50<3:05:15, 24.01s/it]  8%|▊         | 38/500 [15:13<3:03:29, 23.83s/it]  8%|▊         | 39/500 [15:42<3:16:14, 25.54s/it]  8%|▊         | 40/500 [16:07<3:13:38, 25.26s/it]  8%|▊         | 41/500 [16:32<3:11:34, 25.04s/it]  8%|▊         | 42/500 [16:56<3:08:36, 24.71s/it]  9%|▊         | 43/500 [17:20<3:06:59, 24.55s/it]  9%|▉         | 44/500 [17:46<3:09:25, 24.92s/it]  9%|▉         | 45/500 [18:09<3:06:47, 24.63s/it]  9%|▉         | 46/500 [18:36<3:09:52, 25.09s/it]  9%|▉         | 47/500 [19:00<3:06:45, 24.74s/it] 10%|▉         | 48/500 [19:23<3:03:08, 24.31s/it] 10%|▉         | 49/500 [19:46<3:00:55, 24.07s/it] 10%|█         | 50/500 [20:10<2:58:39, 23.82s/it] 10%|█         | 51/500 [20:33<2:57:50, 23.76s/it] 10%|█         | 52/500 [20:57<2:56:47, 23.68s/it] 11%|█         | 53/500 [21:20<2:56:15, 23.66s/it] 11%|█         | 54/500 [21:44<2:55:07, 23.56s/it] 11%|█         | 55/500 [22:07<2:54:36, 23.54s/it] 11%|█         | 56/500 [22:31<2:53:48, 23.49s/it] 11%|█▏        | 57/500 [22:54<2:53:27, 23.49s/it] 11%|█▏        | 57/500 [22:54<2:58:02, 24.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.020 MB of 0.316 MB uploadedwandb: - 0.028 MB of 0.316 MB uploadedwandb: \ 0.315 MB of 0.316 MB uploadedwandb: | 0.315 MB of 0.316 MB uploadedwandb: / 0.315 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▄▅▅▅▅▅▆▅▆▆▇▆▆▆▄▆▆▆▆▇▇▇▅▆▇▆▆▇▇▇▇▄▇██▆█▇
wandb:     train_loss ▄▄▄▃▄▂▄▆▂▂▄▅▃▃▆▃▂▃▂▂▂▂▂▂▅▁▃▂▇█▄▁▃█▂▂▂▃▃▂
wandb:   val_accuracy ▁▂▄▆██▆▇▇▆▅▄▃▃▂▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▂▂▃▃▃▂▁▃▁▃
wandb:       val_loss ▂▃▃▂▂▂▂▃▃▂▃▃▂▃▄▆▂▄▃▃▁▄▂▄▂▄▄▁▁▁▄▃▃▁▄█▇▂▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.72363
wandb:     train_loss 0.35737
wandb:   val_accuracy 0.45111
wandb:       val_loss 2.16768
wandb: 
wandb: 🚀 View run sparkling-flower-384 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4tmwt9dt
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_141054-4tmwt9dt/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_143430-gl9ap24x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-fog-386
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gl9ap24x
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:46:53, 27.28s/it]  0%|          | 2/500 [00:50<3:27:53, 25.05s/it]  1%|          | 3/500 [01:14<3:21:26, 24.32s/it]  1%|          | 4/500 [01:37<3:16:47, 23.81s/it]  1%|          | 5/500 [02:00<3:14:29, 23.57s/it]  1%|          | 6/500 [02:23<3:13:15, 23.47s/it]  1%|▏         | 7/500 [02:46<3:11:59, 23.37s/it]  2%|▏         | 8/500 [03:10<3:11:39, 23.37s/it]  2%|▏         | 9/500 [03:32<3:09:45, 23.19s/it]  2%|▏         | 10/500 [03:56<3:09:08, 23.16s/it]  2%|▏         | 11/500 [04:18<3:07:45, 23.04s/it]  2%|▏         | 12/500 [04:42<3:07:45, 23.08s/it]  3%|▎         | 13/500 [05:05<3:07:46, 23.13s/it]  3%|▎         | 14/500 [05:27<3:05:42, 22.93s/it]  3%|▎         | 15/500 [05:50<3:05:04, 22.89s/it]  3%|▎         | 16/500 [06:21<3:23:32, 25.23s/it]  3%|▎         | 17/500 [06:50<3:32:22, 26.38s/it]  4%|▎         | 18/500 [07:14<3:25:58, 25.64s/it]  4%|▍         | 19/500 [07:37<3:20:39, 25.03s/it]  4%|▍         | 20/500 [08:01<3:17:53, 24.74s/it]  4%|▍         | 21/500 [08:25<3:14:53, 24.41s/it]  4%|▍         | 22/500 [08:50<3:15:36, 24.55s/it]  5%|▍         | 23/500 [09:15<3:15:55, 24.64s/it]  5%|▍         | 24/500 [09:39<3:14:33, 24.52s/it]  5%|▌         | 25/500 [10:06<3:19:34, 25.21s/it]  5%|▌         | 26/500 [10:30<3:15:44, 24.78s/it]  5%|▌         | 27/500 [10:54<3:14:38, 24.69s/it]  6%|▌         | 28/500 [11:19<3:14:24, 24.71s/it]  6%|▌         | 29/500 [11:48<3:24:13, 26.02s/it]  6%|▌         | 30/500 [12:12<3:19:52, 25.51s/it]  6%|▌         | 31/500 [12:36<3:16:27, 25.13s/it]  6%|▋         | 32/500 [13:06<3:25:25, 26.34s/it]  7%|▋         | 33/500 [13:30<3:21:21, 25.87s/it]  7%|▋         | 34/500 [13:56<3:19:16, 25.66s/it]  7%|▋         | 35/500 [14:20<3:16:25, 25.34s/it]  7%|▋         | 36/500 [14:45<3:14:38, 25.17s/it]  7%|▋         | 37/500 [15:10<3:13:44, 25.11s/it]  8%|▊         | 38/500 [15:35<3:13:17, 25.10s/it]  8%|▊         | 39/500 [15:59<3:10:17, 24.77s/it]  8%|▊         | 40/500 [16:23<3:07:04, 24.40s/it]  8%|▊         | 41/500 [16:46<3:05:05, 24.20s/it]  8%|▊         | 42/500 [17:10<3:03:48, 24.08s/it]  9%|▊         | 43/500 [17:34<3:03:02, 24.03s/it]  9%|▉         | 44/500 [17:58<3:01:33, 23.89s/it]  9%|▉         | 45/500 [18:21<3:00:39, 23.82s/it]  9%|▉         | 45/500 [18:21<3:05:39, 24.48s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.028 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.024 MB of 0.318 MB uploadedwandb: - 0.122 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▃▂▂▃▁▃▂▄▃▅▆▄▅▅▅▃▆▆▅▅▆▅▄▅▆▇▇▇▇▆█▇████▆█
wandb:     train_loss ▃▂▄▄▁▄▃▁▁▁▁▁▂▃▃▂▁▄▄▁▁▁▁█▁▁▁▁▁▁▁▃▁▂▁▂▁▁▁▁
wandb:   val_accuracy ▂█▆▂▆▄▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▂▂▂▂▂▂▄▂
wandb:       val_loss ▂▂▃▂▃▄▆▂▃▅█▁▁▆▄▂▅▃▆█▅▆▃▆▇▁▂▅▄▂▅▃▄▅▃▁▁▁▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.93908
wandb:     train_loss 0.25131
wandb:   val_accuracy 0.34889
wandb:       val_loss 3.77954
wandb: 
wandb: 🚀 View run fallen-fog-386 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gl9ap24x
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_143430-gl9ap24x/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_145356-ll0syrdk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-energy-388
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ll0syrdk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:29, 25.43s/it]  0%|          | 2/500 [00:49<3:24:11, 24.60s/it]  1%|          | 3/500 [01:13<3:21:09, 24.28s/it]  1%|          | 4/500 [01:36<3:18:09, 23.97s/it]  1%|          | 5/500 [02:00<3:18:00, 24.00s/it]  1%|          | 6/500 [02:24<3:17:03, 23.93s/it]  1%|▏         | 7/500 [02:52<3:27:18, 25.23s/it]  2%|▏         | 8/500 [03:16<3:23:20, 24.80s/it]  2%|▏         | 9/500 [03:39<3:19:16, 24.35s/it]  2%|▏         | 10/500 [04:04<3:19:16, 24.40s/it]  2%|▏         | 11/500 [04:32<3:28:35, 25.59s/it]  2%|▏         | 12/500 [04:56<3:23:13, 24.99s/it]  3%|▎         | 13/500 [05:19<3:19:29, 24.58s/it]  3%|▎         | 14/500 [05:43<3:17:33, 24.39s/it]  3%|▎         | 15/500 [06:07<3:15:16, 24.16s/it]  3%|▎         | 16/500 [06:31<3:14:06, 24.06s/it]  3%|▎         | 17/500 [06:55<3:13:42, 24.06s/it]  4%|▎         | 18/500 [07:19<3:12:24, 23.95s/it]  4%|▍         | 19/500 [07:44<3:15:13, 24.35s/it]  4%|▍         | 20/500 [08:08<3:13:40, 24.21s/it]  4%|▍         | 21/500 [08:31<3:11:29, 23.99s/it]  4%|▍         | 22/500 [09:00<3:21:41, 25.32s/it]  5%|▍         | 23/500 [09:23<3:17:33, 24.85s/it]  5%|▍         | 24/500 [09:47<3:14:10, 24.48s/it]  5%|▌         | 25/500 [10:11<3:11:40, 24.21s/it]  5%|▌         | 26/500 [10:34<3:09:44, 24.02s/it]  5%|▌         | 27/500 [10:58<3:09:13, 24.00s/it]  6%|▌         | 28/500 [11:23<3:11:49, 24.39s/it]  6%|▌         | 29/500 [11:48<3:11:40, 24.42s/it]  6%|▌         | 30/500 [12:16<3:20:42, 25.62s/it]  6%|▌         | 30/500 [12:16<3:12:23, 24.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.137 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▃▁▃▃▄▅▅▆▇▅▇▇▆▇▆▇▇██▆▇█▇▇▇▇█▅
wandb:     train_loss ▅▄▂▃▂█▄▁▂▃▁▁▁▁▁▁▁█▁▁▂▁▁▁▁▃▁▁▁▄
wandb:   val_accuracy ▁▂▇▂▃▄▃▃▄██▇▇▆▅▇▆▇▆▇▇▃▆█▅▄▄▄▇▅
wandb:       val_loss ▂▂▂▂▂▂▃▁▅▂▂▁▃▁▃▇▂▁▂▂▂▃▁█▄▂▂▁▅█
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.75037
wandb:     train_loss 0.89086
wandb:   val_accuracy 0.45333
wandb:       val_loss 11.49598
wandb: 
wandb: 🚀 View run lemon-energy-388 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ll0syrdk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_145356-ll0syrdk/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_150657-lsp2r1si
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-pond-390
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lsp2r1si
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:18, 25.29s/it]  0%|          | 2/500 [00:50<3:27:04, 24.95s/it]  1%|          | 3/500 [01:14<3:23:31, 24.57s/it]  1%|          | 4/500 [01:46<3:49:02, 27.71s/it]  1%|          | 5/500 [02:10<3:37:41, 26.39s/it]  1%|          | 6/500 [02:34<3:30:01, 25.51s/it]  1%|▏         | 7/500 [02:58<3:26:21, 25.12s/it]  2%|▏         | 8/500 [03:23<3:24:18, 24.92s/it]  2%|▏         | 9/500 [03:47<3:22:41, 24.77s/it]  2%|▏         | 10/500 [04:29<4:05:18, 30.04s/it]  2%|▏         | 11/500 [04:54<3:53:04, 28.60s/it]  2%|▏         | 12/500 [05:19<3:42:29, 27.35s/it]  3%|▎         | 13/500 [06:07<4:32:41, 33.60s/it]  3%|▎         | 14/500 [07:01<5:21:30, 39.69s/it]  3%|▎         | 15/500 [07:25<4:43:28, 35.07s/it]  3%|▎         | 16/500 [07:49<4:16:11, 31.76s/it]  3%|▎         | 17/500 [08:38<4:56:48, 36.87s/it]  4%|▎         | 18/500 [09:03<4:29:07, 33.50s/it]  4%|▍         | 19/500 [09:29<4:08:12, 30.96s/it]  4%|▍         | 20/500 [09:54<3:55:38, 29.46s/it]  4%|▍         | 21/500 [10:19<3:42:17, 27.84s/it]  4%|▍         | 22/500 [10:43<3:34:46, 26.96s/it]  5%|▍         | 23/500 [11:08<3:27:22, 26.08s/it]  5%|▍         | 24/500 [11:31<3:20:41, 25.30s/it]  5%|▌         | 25/500 [11:55<3:18:07, 25.03s/it]  5%|▌         | 26/500 [12:19<3:14:52, 24.67s/it]  5%|▌         | 27/500 [12:43<3:12:37, 24.43s/it]  6%|▌         | 28/500 [13:07<3:11:22, 24.33s/it]  6%|▌         | 29/500 [13:32<3:11:08, 24.35s/it]  6%|▌         | 30/500 [13:55<3:09:43, 24.22s/it]  6%|▌         | 31/500 [14:19<3:08:35, 24.13s/it]  6%|▋         | 32/500 [14:43<3:07:41, 24.06s/it]  7%|▋         | 33/500 [15:08<3:07:38, 24.11s/it]  7%|▋         | 34/500 [15:31<3:06:35, 24.02s/it]  7%|▋         | 35/500 [15:55<3:05:36, 23.95s/it]  7%|▋         | 36/500 [16:19<3:04:39, 23.88s/it]  7%|▋         | 37/500 [16:43<3:04:14, 23.88s/it]  8%|▊         | 38/500 [17:06<3:03:18, 23.81s/it]  8%|▊         | 39/500 [17:31<3:03:51, 23.93s/it]  8%|▊         | 40/500 [17:54<3:02:44, 23.84s/it]  8%|▊         | 41/500 [18:17<3:01:00, 23.66s/it]  8%|▊         | 42/500 [18:41<3:00:54, 23.70s/it]  9%|▊         | 43/500 [19:05<3:00:58, 23.76s/it]  9%|▉         | 44/500 [19:30<3:02:11, 23.97s/it]  9%|▉         | 45/500 [19:53<3:01:33, 23.94s/it]  9%|▉         | 46/500 [20:17<3:00:40, 23.88s/it]  9%|▉         | 47/500 [20:42<3:01:47, 24.08s/it] 10%|▉         | 48/500 [21:06<3:00:43, 23.99s/it] 10%|▉         | 49/500 [21:30<3:00:31, 24.02s/it] 10%|█         | 50/500 [21:54<3:00:01, 24.00s/it] 10%|█         | 51/500 [22:18<3:00:41, 24.14s/it] 10%|█         | 52/500 [22:42<2:59:28, 24.04s/it] 11%|█         | 53/500 [23:06<2:58:24, 23.95s/it] 11%|█         | 54/500 [23:30<2:58:25, 24.00s/it] 11%|█         | 55/500 [23:54<2:57:56, 23.99s/it] 11%|█         | 56/500 [24:17<2:56:51, 23.90s/it] 11%|█▏        | 57/500 [24:41<2:56:03, 23.85s/it] 12%|█▏        | 58/500 [25:05<2:55:02, 23.76s/it] 12%|█▏        | 58/500 [25:05<3:11:10, 25.95s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.030 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.021 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▅▅▅▅▆▅▅▆▅▇▅▇▆▄▅▅▅▆▆▇▇▇▇▆▆▆▇▆▆▇▇██▆██▆
wandb:     train_loss ▃▄▄▂▄▂▃▅▁▄▄▃▃▂▅▃▄▃▃▂▆▂▂▁▁▂▂▂█▃▃▃▁▂▃▂▃▄▄▃
wandb:   val_accuracy ▁▁▄▆██▇▇▆▅▃▄▂▂▂▂▂▂▃▃▃▃▂▂▂▂▃▃▃▃▃▃▃▃▂▁▂▂▂▃
wandb:       val_loss ▃▃▃▂▃▂▂▃▃▂▄▂▂▂▄▆▄▄▄▂▂▅▂▄▄▂▁▂▁▂▄▂▅▅█▇▂▂▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 57
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.64933
wandb:     train_loss 0.72442
wandb:   val_accuracy 0.44222
wandb:       val_loss 2.0151
wandb: 
wandb: 🚀 View run divine-pond-390 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lsp2r1si
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_150657-lsp2r1si/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_153246-vcst2b61
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-grass-393
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vcst2b61
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:57:45, 28.59s/it]  0%|          | 2/500 [00:52<3:35:41, 25.99s/it]  1%|          | 3/500 [01:17<3:30:52, 25.46s/it]  1%|          | 4/500 [01:41<3:26:03, 24.93s/it]  1%|          | 5/500 [02:05<3:21:54, 24.47s/it]  1%|          | 6/500 [02:28<3:18:52, 24.15s/it]  1%|▏         | 7/500 [02:52<3:16:26, 23.91s/it]  2%|▏         | 8/500 [03:15<3:14:41, 23.74s/it]  2%|▏         | 9/500 [03:39<3:13:20, 23.63s/it]  2%|▏         | 10/500 [04:02<3:12:26, 23.56s/it]  2%|▏         | 11/500 [04:25<3:11:51, 23.54s/it]  2%|▏         | 12/500 [04:49<3:11:03, 23.49s/it]  3%|▎         | 13/500 [05:12<3:10:41, 23.49s/it]  3%|▎         | 14/500 [05:36<3:11:13, 23.61s/it]  3%|▎         | 15/500 [06:00<3:10:57, 23.62s/it]  3%|▎         | 16/500 [06:23<3:10:26, 23.61s/it]  3%|▎         | 17/500 [06:47<3:09:32, 23.55s/it]  4%|▎         | 18/500 [07:11<3:09:39, 23.61s/it]  4%|▍         | 19/500 [07:35<3:10:42, 23.79s/it]  4%|▍         | 20/500 [08:01<3:16:16, 24.53s/it]  4%|▍         | 21/500 [08:25<3:15:09, 24.45s/it]  4%|▍         | 22/500 [08:49<3:13:25, 24.28s/it]  5%|▍         | 23/500 [09:13<3:11:38, 24.11s/it]  5%|▍         | 24/500 [09:41<3:20:23, 25.26s/it]  5%|▌         | 25/500 [10:05<3:16:38, 24.84s/it]  5%|▌         | 26/500 [10:32<3:22:31, 25.64s/it]  5%|▌         | 27/500 [10:56<3:17:16, 25.02s/it]  6%|▌         | 28/500 [11:19<3:13:10, 24.56s/it]  6%|▌         | 29/500 [11:47<3:20:53, 25.59s/it]  6%|▌         | 30/500 [12:11<3:15:49, 25.00s/it]  6%|▌         | 30/500 [12:16<3:12:12, 24.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.227 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▃▃▃▃▁▃▃▄▄▃▄▆▄▃▃▄▄▆██▆▆▇▇▇▆█▇
wandb:     train_loss ▃▃▄▃▁▄▃▁▅▁▂▁▁▄▃▆▃▁▅▁▂▁▁▁▁█▁▁▁▁
wandb:   val_accuracy ▃▃█▃▄▅▂▂▂▃▃▃▂▂▂▂▂▂▇▂▃▃▄▂▄▁▄▆▁▄
wandb:       val_loss ▂▂▃▂▃▄▅▂█▂▅▇▁▁▇▆▁▆▂▂▅█▄▅▄▄▆▃▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.85884
wandb:     train_loss 0.09727
wandb:   val_accuracy 0.37778
wandb:       val_loss 1.93356
wandb: 
wandb: 🚀 View run resilient-grass-393 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vcst2b61
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_153246-vcst2b61/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_154550-151ehwe8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-blaze-395
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/151ehwe8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:46:24, 27.22s/it]  0%|          | 2/500 [00:51<3:29:18, 25.22s/it]  1%|          | 3/500 [01:15<3:25:12, 24.77s/it]  1%|          | 4/500 [01:38<3:21:16, 24.35s/it]  1%|          | 5/500 [02:06<3:30:59, 25.57s/it]  1%|          | 6/500 [02:30<3:26:06, 25.03s/it]  1%|▏         | 7/500 [02:55<3:23:54, 24.82s/it]  2%|▏         | 8/500 [03:18<3:20:35, 24.46s/it]  2%|▏         | 9/500 [03:43<3:19:56, 24.43s/it]  2%|▏         | 10/500 [04:11<3:29:43, 25.68s/it]  2%|▏         | 11/500 [04:35<3:23:40, 24.99s/it]  2%|▏         | 12/500 [04:58<3:19:44, 24.56s/it]  3%|▎         | 13/500 [05:22<3:17:06, 24.28s/it]  3%|▎         | 14/500 [05:46<3:17:25, 24.37s/it]  3%|▎         | 15/500 [06:10<3:16:06, 24.26s/it]  3%|▎         | 16/500 [06:34<3:14:11, 24.07s/it]  3%|▎         | 17/500 [06:58<3:12:57, 23.97s/it]  4%|▎         | 18/500 [07:22<3:12:37, 23.98s/it]  4%|▍         | 19/500 [07:46<3:12:31, 24.01s/it]  4%|▍         | 20/500 [08:10<3:11:24, 23.93s/it]  4%|▍         | 21/500 [08:34<3:13:24, 24.23s/it]  4%|▍         | 22/500 [08:59<3:13:46, 24.32s/it]  5%|▍         | 23/500 [09:23<3:12:51, 24.26s/it]  5%|▍         | 24/500 [09:47<3:10:52, 24.06s/it]  5%|▌         | 25/500 [10:11<3:11:27, 24.18s/it]  5%|▌         | 26/500 [10:35<3:10:02, 24.06s/it]  5%|▌         | 27/500 [10:59<3:08:56, 23.97s/it]  6%|▌         | 28/500 [11:22<3:08:04, 23.91s/it]  6%|▌         | 29/500 [11:47<3:08:44, 24.04s/it]  6%|▌         | 30/500 [12:10<3:07:21, 23.92s/it]  6%|▌         | 30/500 [12:11<3:10:53, 24.37s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.010 MB of 0.310 MB uploadedwandb: - 0.301 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▃▃▄▃▄▃▇▄▇█▆▄▃▇█▇█▆▇▇▇▇████▇█▇
wandb:     train_loss ▆▅▇▆▁▆▇▁▇▁▁▁▁▇▁▁▁▁▁▇█▁▁▁▁▂▁▁▁▅
wandb:   val_accuracy ▃▆▅▆▃▃▃▃▃▁▂▃▃▃▂▂▂▂▂▂▂▂▁▂▅▂▂▄▂█
wandb:       val_loss ▁▁▂▁▂▃▅▂▇▂▃▇▁▁▃█▃▃▃▁▃▄▃▆▁▂▃▁▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.90342
wandb:     train_loss 0.88374
wandb:   val_accuracy 0.54667
wandb:       val_loss 3.82771
wandb: 
wandb: 🚀 View run eternal-blaze-395 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/151ehwe8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_154550-151ehwe8/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_155846-bfv7g07j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-thunder-396
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bfv7g07j
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:09, 25.27s/it]  0%|          | 2/500 [00:48<3:21:20, 24.26s/it]  1%|          | 3/500 [01:12<3:19:45, 24.12s/it]  1%|          | 4/500 [01:37<3:19:44, 24.16s/it]  1%|          | 5/500 [02:00<3:17:44, 23.97s/it]  1%|          | 6/500 [02:24<3:16:29, 23.87s/it]  1%|▏         | 7/500 [02:47<3:15:18, 23.77s/it]  2%|▏         | 8/500 [03:11<3:14:32, 23.72s/it]  2%|▏         | 9/500 [03:36<3:18:22, 24.24s/it]  2%|▏         | 10/500 [04:13<3:49:19, 28.08s/it]  2%|▏         | 11/500 [04:37<3:39:29, 26.93s/it]  2%|▏         | 12/500 [05:05<3:41:35, 27.25s/it]  3%|▎         | 13/500 [05:30<3:34:38, 26.45s/it]  3%|▎         | 14/500 [05:54<3:28:29, 25.74s/it]  3%|▎         | 15/500 [06:19<3:25:02, 25.37s/it]  3%|▎         | 16/500 [06:43<3:22:31, 25.11s/it]  3%|▎         | 17/500 [07:07<3:19:57, 24.84s/it]  4%|▎         | 18/500 [07:32<3:19:22, 24.82s/it]  4%|▍         | 19/500 [07:57<3:18:18, 24.74s/it]  4%|▍         | 20/500 [08:21<3:16:58, 24.62s/it]  4%|▍         | 21/500 [08:45<3:15:03, 24.43s/it]  4%|▍         | 22/500 [09:09<3:13:19, 24.27s/it]  5%|▍         | 23/500 [09:33<3:13:05, 24.29s/it]  5%|▍         | 24/500 [09:59<3:15:31, 24.64s/it]  5%|▌         | 25/500 [10:28<3:25:27, 25.95s/it]  5%|▌         | 26/500 [10:51<3:18:52, 25.17s/it]  5%|▌         | 27/500 [11:15<3:15:22, 24.78s/it]  6%|▌         | 28/500 [11:47<3:31:37, 26.90s/it]  6%|▌         | 29/500 [12:16<3:36:44, 27.61s/it]  6%|▌         | 30/500 [12:39<3:25:56, 26.29s/it]  6%|▌         | 31/500 [13:03<3:18:54, 25.45s/it]  6%|▋         | 32/500 [13:26<3:14:02, 24.88s/it]  7%|▋         | 33/500 [13:50<3:11:52, 24.65s/it]  7%|▋         | 34/500 [14:14<3:09:27, 24.39s/it]  7%|▋         | 35/500 [14:38<3:08:18, 24.30s/it]  7%|▋         | 36/500 [15:05<3:14:41, 25.18s/it]  7%|▋         | 37/500 [15:31<3:15:38, 25.35s/it]  8%|▊         | 38/500 [15:55<3:12:09, 24.96s/it]  8%|▊         | 39/500 [16:19<3:09:10, 24.62s/it]  8%|▊         | 40/500 [16:44<3:10:07, 24.80s/it]  8%|▊         | 41/500 [17:09<3:08:42, 24.67s/it]  8%|▊         | 42/500 [17:33<3:08:05, 24.64s/it]  9%|▊         | 43/500 [17:57<3:06:10, 24.44s/it]  9%|▉         | 44/500 [18:26<3:15:19, 25.70s/it]  9%|▉         | 45/500 [18:51<3:14:27, 25.64s/it]  9%|▉         | 46/500 [19:16<3:11:06, 25.26s/it]  9%|▉         | 47/500 [19:40<3:08:30, 24.97s/it] 10%|▉         | 48/500 [20:05<3:08:46, 25.06s/it] 10%|▉         | 49/500 [20:29<3:05:32, 24.68s/it] 10%|█         | 50/500 [20:52<3:02:08, 24.29s/it] 10%|█         | 51/500 [21:15<2:59:00, 23.92s/it] 10%|█         | 52/500 [21:46<3:13:40, 25.94s/it] 11%|█         | 53/500 [22:10<3:07:34, 25.18s/it] 11%|█         | 54/500 [22:35<3:07:12, 25.18s/it] 11%|█         | 55/500 [22:58<3:03:29, 24.74s/it] 11%|█         | 56/500 [23:22<3:00:19, 24.37s/it] 11%|█▏        | 57/500 [23:46<2:58:38, 24.19s/it] 11%|█▏        | 57/500 [23:46<3:04:44, 25.02s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.130 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▅▆▆▆▆▇▆▇▇▇█████▇█▇█▂▇▇▇▇▇█▇█▇▇▇██▇█▇
wandb:     train_loss ▅▆▆▃▅▂▆▅▁▁▄█▅▃█▂▃▄▆▂▄▁▃██▁▂▃▄▄▄▁▂▄▄▄▂▂▆▁
wandb:   val_accuracy ▂▂▆▆▃▇█▅▄▇▂▂▂▃▁▃▄▄▁▅▃▆▄▂▄▅▄▆▆▄▅▅▆▆▅▄▂▅▅▆
wandb:       val_loss ▃▃▃▃▃▂▄▃▅▁▄▃▄▄▄▆▃▄▄▃▁▅▂▆▂▄▅▁▁▁▄▃▂▃▅█▆▂▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.72363
wandb:     train_loss 0.28661
wandb:   val_accuracy 0.45333
wandb:       val_loss 1.99678
wandb: 
wandb: 🚀 View run neat-thunder-396 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bfv7g07j
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_155846-bfv7g07j/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_162317-29urlz3v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-cosmos-400
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29urlz3v
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:31, 25.79s/it]  0%|          | 2/500 [00:49<3:22:34, 24.41s/it]  1%|          | 3/500 [01:12<3:19:40, 24.10s/it]  1%|          | 4/500 [01:38<3:25:01, 24.80s/it]  1%|          | 5/500 [02:02<3:22:19, 24.52s/it]  1%|          | 6/500 [02:26<3:19:44, 24.26s/it]  1%|▏         | 7/500 [02:50<3:17:59, 24.10s/it]  2%|▏         | 8/500 [03:14<3:16:29, 23.96s/it]  2%|▏         | 9/500 [03:37<3:14:19, 23.75s/it]  2%|▏         | 10/500 [04:00<3:12:33, 23.58s/it]  2%|▏         | 11/500 [04:29<3:24:47, 25.13s/it]  2%|▏         | 12/500 [04:57<3:33:07, 26.20s/it]  3%|▎         | 13/500 [05:21<3:27:26, 25.56s/it]  3%|▎         | 14/500 [05:45<3:22:29, 25.00s/it]  3%|▎         | 15/500 [06:09<3:20:03, 24.75s/it]  3%|▎         | 16/500 [06:33<3:17:39, 24.50s/it]  3%|▎         | 17/500 [06:58<3:18:25, 24.65s/it]  4%|▎         | 18/500 [07:22<3:16:15, 24.43s/it]  4%|▍         | 19/500 [07:46<3:13:55, 24.19s/it]  4%|▍         | 20/500 [08:09<3:11:48, 23.98s/it]  4%|▍         | 21/500 [08:37<3:20:54, 25.17s/it]  4%|▍         | 22/500 [09:01<3:16:35, 24.68s/it]  5%|▍         | 23/500 [09:29<3:23:54, 25.65s/it]  5%|▍         | 24/500 [09:52<3:17:53, 24.94s/it]  5%|▌         | 25/500 [10:20<3:25:03, 25.90s/it]  5%|▌         | 26/500 [10:44<3:19:20, 25.23s/it]  5%|▌         | 27/500 [11:07<3:15:00, 24.74s/it]  6%|▌         | 28/500 [11:31<3:12:14, 24.44s/it]  6%|▌         | 29/500 [11:55<3:10:55, 24.32s/it]  6%|▌         | 30/500 [12:19<3:08:39, 24.08s/it]  6%|▌         | 31/500 [12:42<3:06:54, 23.91s/it]  6%|▋         | 32/500 [13:06<3:05:44, 23.81s/it]  7%|▋         | 33/500 [13:29<3:04:30, 23.71s/it]  7%|▋         | 34/500 [13:53<3:04:39, 23.77s/it]  7%|▋         | 35/500 [14:17<3:04:02, 23.75s/it]  7%|▋         | 36/500 [14:41<3:03:32, 23.73s/it]  7%|▋         | 37/500 [15:04<3:02:45, 23.68s/it]  8%|▊         | 38/500 [15:28<3:02:11, 23.66s/it]  8%|▊         | 39/500 [15:52<3:02:14, 23.72s/it]  8%|▊         | 40/500 [16:15<3:01:45, 23.71s/it]  8%|▊         | 41/500 [16:42<3:07:44, 24.54s/it]  8%|▊         | 42/500 [17:05<3:05:17, 24.27s/it]  9%|▊         | 43/500 [17:29<3:03:24, 24.08s/it]  9%|▉         | 44/500 [17:53<3:02:48, 24.05s/it]  9%|▉         | 45/500 [18:17<3:01:34, 23.94s/it]  9%|▉         | 46/500 [18:45<3:11:03, 25.25s/it]  9%|▉         | 47/500 [19:09<3:07:04, 24.78s/it] 10%|▉         | 48/500 [19:33<3:05:59, 24.69s/it] 10%|▉         | 49/500 [19:59<3:07:24, 24.93s/it] 10%|█         | 50/500 [20:25<3:09:37, 25.28s/it] 10%|█         | 51/500 [20:48<3:05:26, 24.78s/it] 10%|█         | 52/500 [21:12<3:03:22, 24.56s/it] 11%|█         | 53/500 [21:36<3:00:36, 24.24s/it] 11%|█         | 54/500 [22:00<2:58:55, 24.07s/it] 11%|█         | 55/500 [22:28<3:07:44, 25.31s/it] 11%|█         | 56/500 [22:51<3:03:14, 24.76s/it] 11%|█▏        | 57/500 [23:15<2:59:42, 24.34s/it] 11%|█▏        | 57/500 [23:15<3:00:42, 24.48s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.025 MB of 0.316 MB uploadedwandb: \ 0.123 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▃▅▄▃▅▅▄▄▃▇▆▇▇█▇▇██▇██▆█▄▇▆▇▇██▇▇▇█▆██
wandb:     train_loss ▃▃▄▁▂▁▄▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▅▁▆▁██▁▁▁▁▁▂▁▁▁▁
wandb:   val_accuracy ▂▃▃▄█▄▃▁▂▂▁▁▁▃▂▃▃▅▂▂▃▄▁▂▄▃▂▄▄▃▃▃▄▄▃▃▄▅▃▃
wandb:       val_loss ▂▂▂▃▃▁▅▄▆▁▅▄▄▄▄▆▃▂▄▃▁▃▂▄▂▄▂▁▁▁▇▃██▂▅▅▄▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.93759
wandb:     train_loss 0.00809
wandb:   val_accuracy 0.35778
wandb:       val_loss 4.54163
wandb: 
wandb: 🚀 View run stilted-cosmos-400 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/29urlz3v
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_162317-29urlz3v/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_225828-5dlrh09w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-surf-465
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5dlrh09w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:29, 25.79s/it]  0%|          | 2/500 [00:49<3:22:28, 24.40s/it]  1%|          | 3/500 [01:13<3:21:07, 24.28s/it]  1%|          | 4/500 [01:37<3:19:46, 24.17s/it]  1%|          | 5/500 [02:01<3:18:01, 24.00s/it]  1%|          | 6/500 [02:25<3:19:22, 24.22s/it]  1%|▏         | 7/500 [02:50<3:19:42, 24.31s/it]  2%|▏         | 8/500 [03:13<3:17:40, 24.11s/it]  2%|▏         | 9/500 [03:43<3:30:40, 25.75s/it]  2%|▏         | 10/500 [04:07<3:26:13, 25.25s/it]  2%|▏         | 11/500 [04:36<3:35:55, 26.49s/it]  2%|▏         | 12/500 [05:05<3:41:32, 27.24s/it]  3%|▎         | 13/500 [05:29<3:32:20, 26.16s/it]  3%|▎         | 14/500 [05:53<3:25:57, 25.43s/it]  3%|▎         | 15/500 [06:16<3:21:26, 24.92s/it]  3%|▎         | 16/500 [06:40<3:18:20, 24.59s/it]  3%|▎         | 17/500 [07:03<3:14:55, 24.22s/it]  4%|▎         | 18/500 [07:28<3:14:40, 24.23s/it]  4%|▍         | 19/500 [07:51<3:12:28, 24.01s/it]  4%|▍         | 20/500 [08:15<3:10:43, 23.84s/it]  4%|▍         | 21/500 [08:38<3:10:01, 23.80s/it]  4%|▍         | 22/500 [09:08<3:23:15, 25.51s/it]  5%|▍         | 23/500 [09:39<3:35:45, 27.14s/it]  5%|▍         | 24/500 [10:09<3:43:14, 28.14s/it]  5%|▌         | 25/500 [10:33<3:33:10, 26.93s/it]  5%|▌         | 26/500 [10:57<3:25:40, 26.04s/it]  5%|▌         | 27/500 [11:21<3:19:01, 25.25s/it]  6%|▌         | 28/500 [11:44<3:14:58, 24.78s/it]  6%|▌         | 28/500 [11:44<3:18:03, 25.18s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.308 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.231 MB of 0.308 MB uploadedwandb: / 0.231 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▁▂▁▁▁▁▃▄▃▁▅▂█▆▅▃▅▆▃▄▄▇▄█
wandb:     train_loss ▂▂▁▁▃▁▅▃▄▅▃▂▁▆▃▂▁▁▂▄▁▁▁█▁▁▁▁
wandb:   val_accuracy ▁▂▃▃▃▄▂▃▃▂▇▇▆▃█▆▃█▇▄▆▇▅▅▆▇▆▆
wandb:       val_loss ▃▃▂▂▃▃▄▇▁▇▁▁█▃▁▆▄▁▂▄▂▂▂▄▃▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.77266
wandb:     train_loss 0.00033
wandb:   val_accuracy 0.60889
wandb:       val_loss 0.92488
wandb: 
wandb: 🚀 View run earthy-surf-465 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5dlrh09w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_225828-5dlrh09w/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_231107-4pnz2rqn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-morning-467
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4pnz2rqn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:34, 25.32s/it]  0%|          | 2/500 [00:49<3:22:12, 24.36s/it]  1%|          | 3/500 [01:12<3:19:44, 24.11s/it]  1%|          | 4/500 [01:36<3:19:08, 24.09s/it]  1%|          | 5/500 [02:00<3:17:09, 23.90s/it]  1%|          | 6/500 [02:24<3:16:54, 23.92s/it]  1%|▏         | 7/500 [02:48<3:15:45, 23.83s/it]  2%|▏         | 8/500 [03:22<3:43:35, 27.27s/it]  2%|▏         | 9/500 [03:47<3:36:12, 26.42s/it]  2%|▏         | 10/500 [04:17<3:45:27, 27.61s/it]  2%|▏         | 11/500 [04:41<3:35:37, 26.46s/it]  2%|▏         | 12/500 [05:04<3:27:45, 25.54s/it]  3%|▎         | 13/500 [05:28<3:22:37, 24.96s/it]  3%|▎         | 14/500 [05:51<3:18:48, 24.54s/it]  3%|▎         | 15/500 [06:15<3:16:05, 24.26s/it]  3%|▎         | 16/500 [06:38<3:13:31, 23.99s/it]  3%|▎         | 17/500 [07:02<3:12:22, 23.90s/it]  4%|▎         | 18/500 [07:26<3:11:44, 23.87s/it]  4%|▍         | 19/500 [07:49<3:10:09, 23.72s/it]  4%|▍         | 20/500 [08:13<3:09:13, 23.65s/it]  4%|▍         | 21/500 [08:41<3:18:49, 24.91s/it]  4%|▍         | 22/500 [09:04<3:15:09, 24.50s/it]  5%|▍         | 23/500 [09:28<3:12:08, 24.17s/it]  5%|▍         | 24/500 [09:51<3:09:37, 23.90s/it]  5%|▌         | 25/500 [10:15<3:09:24, 23.93s/it]  5%|▌         | 26/500 [10:40<3:11:49, 24.28s/it]  5%|▌         | 27/500 [11:04<3:09:49, 24.08s/it]  6%|▌         | 28/500 [11:34<3:25:32, 26.13s/it]  6%|▌         | 29/500 [12:14<3:56:16, 30.10s/it]  6%|▌         | 30/500 [12:37<3:40:21, 28.13s/it]  6%|▌         | 31/500 [13:01<3:28:55, 26.73s/it]  6%|▋         | 32/500 [13:24<3:20:41, 25.73s/it]  7%|▋         | 33/500 [13:47<3:14:16, 24.96s/it]  7%|▋         | 34/500 [14:11<3:09:57, 24.46s/it]  7%|▋         | 35/500 [14:34<3:07:50, 24.24s/it]  7%|▋         | 36/500 [14:58<3:06:09, 24.07s/it]  7%|▋         | 37/500 [15:22<3:04:41, 23.93s/it]  8%|▊         | 38/500 [15:45<3:03:20, 23.81s/it]  8%|▊         | 39/500 [16:09<3:01:54, 23.68s/it]  8%|▊         | 40/500 [16:39<3:17:30, 25.76s/it]  8%|▊         | 41/500 [17:07<3:22:41, 26.50s/it]  8%|▊         | 42/500 [17:39<3:33:38, 27.99s/it]  9%|▊         | 43/500 [18:08<3:35:31, 28.30s/it]  9%|▉         | 44/500 [18:32<3:24:25, 26.90s/it]  9%|▉         | 45/500 [19:00<3:27:55, 27.42s/it]  9%|▉         | 46/500 [19:29<3:30:18, 27.79s/it]  9%|▉         | 47/500 [19:53<3:20:42, 26.58s/it] 10%|▉         | 48/500 [20:22<3:25:43, 27.31s/it] 10%|▉         | 49/500 [20:50<3:27:53, 27.66s/it] 10%|█         | 50/500 [21:13<3:17:26, 26.32s/it] 10%|█         | 51/500 [21:41<3:20:52, 26.84s/it] 10%|█         | 52/500 [22:10<3:25:11, 27.48s/it] 11%|█         | 53/500 [22:34<3:16:21, 26.36s/it] 11%|█         | 54/500 [22:58<3:09:49, 25.54s/it] 11%|█         | 55/500 [23:21<3:04:48, 24.92s/it] 11%|█         | 56/500 [23:45<3:01:05, 24.47s/it] 11%|█▏        | 57/500 [24:08<2:58:55, 24.23s/it] 12%|█▏        | 58/500 [24:32<2:56:55, 24.02s/it] 12%|█▏        | 59/500 [24:55<2:55:45, 23.91s/it] 12%|█▏        | 59/500 [24:55<3:06:21, 25.35s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.020 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇███▇█▇█
wandb:     train_loss ▄▅▅▃▄▁▅▄▁▄▄▄▃▄▅▂▃▃▂▄▇▂▂▄▃▃▃▄█▄▄▁▄▃▂▃▁▄▂▃
wandb:   val_accuracy ▂▁▁▂▂▂▇▂▄█▂▆▁▄▃▇▇▆▇▅▇▄▅▆▆▆▆▆▅▄▄▄▅▄▄▅▅▅▅▄
wandb:       val_loss ▃▃▃▃▃▂▄▄▅▂▄▁▃▂▄▂▃▂▂▁▂▂▁▂▄▄▁▁▁▁▂▆▁█▇▂▂▁▄▂
wandb: 
wandb: Run summary:
wandb:          epoch 58
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.65379
wandb:     train_loss 0.78048
wandb:   val_accuracy 0.46889
wandb:       val_loss 0.70819
wandb: 
wandb: 🚀 View run iconic-morning-467 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4pnz2rqn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_231107-4pnz2rqn/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_233654-on7u8nhb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-cloud-469
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/on7u8nhb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:27:29, 24.95s/it]  0%|          | 2/500 [00:48<3:21:10, 24.24s/it]  1%|          | 3/500 [01:12<3:19:05, 24.03s/it]  1%|          | 4/500 [01:36<3:19:54, 24.18s/it]  1%|          | 5/500 [02:01<3:21:12, 24.39s/it]  1%|          | 6/500 [02:25<3:18:58, 24.17s/it]  1%|▏         | 7/500 [02:48<3:16:49, 23.95s/it]  2%|▏         | 8/500 [03:12<3:15:36, 23.85s/it]  2%|▏         | 9/500 [03:36<3:15:30, 23.89s/it]  2%|▏         | 10/500 [04:00<3:14:27, 23.81s/it]  2%|▏         | 11/500 [04:23<3:12:41, 23.64s/it]  2%|▏         | 12/500 [04:47<3:12:35, 23.68s/it]  3%|▎         | 13/500 [05:11<3:12:40, 23.74s/it]  3%|▎         | 14/500 [05:34<3:12:02, 23.71s/it]  3%|▎         | 15/500 [05:57<3:10:39, 23.59s/it]  3%|▎         | 16/500 [06:22<3:11:48, 23.78s/it]  3%|▎         | 17/500 [06:45<3:11:18, 23.76s/it]  4%|▎         | 18/500 [07:15<3:25:07, 25.53s/it]  4%|▍         | 19/500 [07:39<3:20:30, 25.01s/it]  4%|▍         | 20/500 [08:03<3:16:56, 24.62s/it]  4%|▍         | 21/500 [08:26<3:14:38, 24.38s/it]  4%|▍         | 22/500 [08:50<3:11:55, 24.09s/it]  5%|▍         | 23/500 [09:14<3:10:37, 23.98s/it]  5%|▍         | 24/500 [09:41<3:18:44, 25.05s/it]  5%|▌         | 25/500 [10:06<3:18:48, 25.11s/it]  5%|▌         | 26/500 [10:31<3:16:20, 24.85s/it]  5%|▌         | 27/500 [10:55<3:13:45, 24.58s/it]  6%|▌         | 28/500 [11:18<3:10:57, 24.27s/it]  6%|▌         | 29/500 [11:51<3:31:35, 26.95s/it]  6%|▌         | 30/500 [12:15<3:22:52, 25.90s/it]  6%|▌         | 30/500 [12:15<3:11:58, 24.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.020 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▂▁▃▄▅▁▄▂▃▁▄▃▅▄▄▅▆▅▅▆▇█▄▆▅▅▅▅██
wandb:     train_loss ▁▁▂▁▁▂▁▁▂▁▁▁▁▂▂▁▂█▁▁▁▁▁▁▂▄▅▁▁▁
wandb:   val_accuracy ▂▃▅▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▆▅▅▄▅▅▅▁▂
wandb:       val_loss ▂▂▂▂▂▃▃▂▅▂▄▇▁▁▄▄▁▅▄▃▃▅▃▃█▃▁▂▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.77415
wandb:     train_loss 0.00477
wandb:   val_accuracy 0.30667
wandb:       val_loss 0.83368
wandb: 
wandb: 🚀 View run eternal-cloud-469 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/on7u8nhb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_233654-on7u8nhb/logs
Successfully processed 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_234949-nueyvit7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sun-470
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nueyvit7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.017 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: \ 0.023 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: | 0.038 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: / 0.038 MB of 0.038 MB uploaded (0.004 MB deduped)wandb: 🚀 View run polar-sun-470 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nueyvit7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_234949-nueyvit7/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235035-xh1123s8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-frost-471
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xh1123s8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.026 MB uploadedwandb: - 0.022 MB of 0.026 MB uploadedwandb: \ 0.026 MB of 0.026 MB uploadedwandb: 🚀 View run balmy-frost-471 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xh1123s8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235035-xh1123s8/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235118-or9dnaxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sky-472
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/or9dnaxe
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run noble-sky-472 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/or9dnaxe
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235118-or9dnaxe/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235158-gd05m7ha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-hill-473
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gd05m7ha
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: \ 0.026 MB of 0.031 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fanciful-hill-473 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gd05m7ha
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235158-gd05m7ha/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235232-ildc4q1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-wave-474
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ildc4q1n
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.020 MB of 0.031 MB uploadedwandb: \ 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run still-wave-474 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ildc4q1n
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235232-ildc4q1n/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235305-uz216of9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-firefly-475
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/uz216of9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run bright-firefly-475 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/uz216of9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235305-uz216of9/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235342-m3foz2l9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-snowball-476
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m3foz2l9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run radiant-snowball-476 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m3foz2l9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235342-m3foz2l9/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235417-cwq399yh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-surf-477
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cwq399yh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run crimson-surf-477 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/cwq399yh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235417-cwq399yh/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235500-3y2b4vry
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-shape-478
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3y2b4vry
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: \ 0.026 MB of 0.031 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run ethereal-shape-478 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3y2b4vry
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235500-3y2b4vry/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235540-n6bfped5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-pyramid-479
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n6bfped5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.025 MB of 0.031 MB uploadedwandb: 🚀 View run polished-pyramid-479 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/n6bfped5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235540-n6bfped5/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235624-amfc91ab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-haze-480
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/amfc91ab
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run true-haze-480 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/amfc91ab
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235624-amfc91ab/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235704-btcrujyd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-wind-481
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/btcrujyd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fine-wind-481 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/btcrujyd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235704-btcrujyd/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235743-kti9lu33
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-dream-482
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kti9lu33
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run twilight-dream-482 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kti9lu33
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235743-kti9lu33/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235818-fqxmmtv6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-wood-483
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fqxmmtv6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run dry-wood-483 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fqxmmtv6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235818-fqxmmtv6/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235848-8si2s1uz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-gorge-484
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8si2s1uz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run summer-gorge-484 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8si2s1uz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235848-8si2s1uz/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140404
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240921_235920-5mrxbf9m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-darkness-485
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5mrxbf9m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<1:57:47, 14.16s/it]  0%|          | 2/500 [00:28<1:59:55, 14.45s/it]  1%|          | 3/500 [00:41<1:54:27, 13.82s/it]  1%|          | 4/500 [00:54<1:50:11, 13.33s/it]  1%|          | 5/500 [01:07<1:49:24, 13.26s/it]  1%|          | 6/500 [01:19<1:45:23, 12.80s/it]  1%|▏         | 7/500 [01:31<1:43:28, 12.59s/it]  2%|▏         | 8/500 [01:44<1:42:59, 12.56s/it]  2%|▏         | 9/500 [01:56<1:41:52, 12.45s/it]  2%|▏         | 10/500 [02:08<1:41:15, 12.40s/it]  2%|▏         | 11/500 [02:20<1:40:33, 12.34s/it]  2%|▏         | 12/500 [02:32<1:39:26, 12.23s/it]  3%|▎         | 13/500 [02:44<1:39:05, 12.21s/it]  3%|▎         | 14/500 [02:57<1:38:34, 12.17s/it]  3%|▎         | 15/500 [03:08<1:37:46, 12.10s/it]  3%|▎         | 16/500 [03:20<1:36:38, 11.98s/it]  3%|▎         | 17/500 [03:32<1:36:07, 11.94s/it]  4%|▎         | 18/500 [03:44<1:35:37, 11.90s/it]  4%|▍         | 19/500 [04:02<1:49:43, 13.69s/it]  4%|▍         | 20/500 [04:15<1:47:45, 13.47s/it]  4%|▍         | 21/500 [04:27<1:45:12, 13.18s/it]  4%|▍         | 22/500 [04:39<1:42:34, 12.88s/it]  5%|▍         | 23/500 [04:52<1:42:34, 12.90s/it]  5%|▍         | 24/500 [05:05<1:41:20, 12.77s/it]  5%|▌         | 25/500 [05:17<1:39:44, 12.60s/it]  5%|▌         | 26/500 [05:30<1:39:31, 12.60s/it]  5%|▌         | 27/500 [05:42<1:39:22, 12.61s/it]  6%|▌         | 28/500 [05:55<1:40:22, 12.76s/it]  6%|▌         | 29/500 [06:07<1:38:08, 12.50s/it]  6%|▌         | 30/500 [06:19<1:37:14, 12.41s/it]  6%|▌         | 31/500 [06:33<1:40:28, 12.85s/it]  6%|▋         | 32/500 [06:46<1:40:11, 12.85s/it]  7%|▋         | 33/500 [06:59<1:39:15, 12.75s/it]  7%|▋         | 34/500 [07:11<1:37:54, 12.61s/it]  7%|▋         | 35/500 [07:24<1:38:09, 12.67s/it]  7%|▋         | 36/500 [07:36<1:37:51, 12.65s/it]  7%|▋         | 37/500 [07:49<1:37:10, 12.59s/it]  8%|▊         | 38/500 [08:01<1:36:41, 12.56s/it]  8%|▊         | 39/500 [08:14<1:37:39, 12.71s/it]  8%|▊         | 40/500 [08:27<1:37:28, 12.71s/it]  8%|▊         | 41/500 [08:39<1:36:07, 12.57s/it]  8%|▊         | 42/500 [08:52<1:35:39, 12.53s/it]  9%|▊         | 43/500 [09:05<1:36:57, 12.73s/it]  9%|▉         | 44/500 [09:20<1:42:30, 13.49s/it]  9%|▉         | 45/500 [09:34<1:42:46, 13.55s/it]  9%|▉         | 46/500 [09:49<1:45:39, 13.96s/it]  9%|▉         | 47/500 [10:02<1:44:32, 13.85s/it] 10%|▉         | 48/500 [10:17<1:45:37, 14.02s/it] 10%|▉         | 49/500 [10:29<1:41:48, 13.55s/it] 10%|█         | 50/500 [10:44<1:43:47, 13.84s/it] 10%|█         | 51/500 [10:57<1:42:29, 13.70s/it] 10%|█         | 52/500 [11:11<1:42:01, 13.66s/it] 11%|█         | 53/500 [11:24<1:40:42, 13.52s/it] 11%|█         | 54/500 [11:38<1:40:49, 13.56s/it] 11%|█         | 55/500 [11:51<1:40:59, 13.62s/it] 11%|█         | 56/500 [12:04<1:39:11, 13.40s/it] 11%|█         | 56/500 [12:04<1:35:47, 12.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.030 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.310 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▄▄▄▃▃▇▆▇▇▅▇▇▇▇█▆▇▇▇██▅▇▅▆▆█▇███▇██▇██
wandb:     train_loss ▄▂▂▃▄▁▄▁▂▅█▇▁▁▁▁▁▁▁▁▁▃▁▁▁▁▂▄▆▂▁▁▁▁▁▁▁▁▂▁
wandb:   val_accuracy ▂▁▅▅▄▄▄▃▃▄▅▅▅▄▅▆▇▅▅▆▇▅▇▇▆▄▇▆▆█▇▇▇█▅▇▇█▇█
wandb:       val_loss ▁▁▁▁▁▁▁▁▂▁▂▁▁▁█▂▁▁▂▁▂▁▁▃▁▁▂▂▁▁▃▁▂▂▂▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.95542
wandb:     train_loss 0.06228
wandb:   val_accuracy 0.59333
wandb:       val_loss 0.01224
wandb: 
wandb: 🚀 View run graceful-darkness-485 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5mrxbf9m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235920-5mrxbf9m/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_001215-c2bux3kx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-dew-487
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c2bux3kx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:07:10, 15.29s/it]  0%|          | 2/500 [00:28<1:54:49, 13.83s/it]  1%|          | 3/500 [00:40<1:49:00, 13.16s/it]  1%|          | 4/500 [00:52<1:45:12, 12.73s/it]  1%|          | 5/500 [01:04<1:41:53, 12.35s/it]  1%|          | 6/500 [01:15<1:39:43, 12.11s/it]  1%|▏         | 7/500 [01:28<1:39:53, 12.16s/it]  2%|▏         | 8/500 [01:40<1:40:34, 12.27s/it]  2%|▏         | 9/500 [01:52<1:40:25, 12.27s/it]  2%|▏         | 10/500 [02:04<1:38:39, 12.08s/it]  2%|▏         | 11/500 [02:16<1:37:19, 11.94s/it]  2%|▏         | 12/500 [02:28<1:37:10, 11.95s/it]  3%|▎         | 13/500 [02:40<1:38:26, 12.13s/it]  3%|▎         | 14/500 [02:52<1:38:23, 12.15s/it]  3%|▎         | 15/500 [03:04<1:38:03, 12.13s/it]  3%|▎         | 16/500 [03:16<1:37:11, 12.05s/it]  3%|▎         | 17/500 [03:29<1:37:38, 12.13s/it]  4%|▎         | 18/500 [03:40<1:36:45, 12.04s/it]  4%|▍         | 19/500 [03:57<1:47:38, 13.43s/it]  4%|▍         | 20/500 [04:10<1:45:20, 13.17s/it]  4%|▍         | 21/500 [04:21<1:41:28, 12.71s/it]  4%|▍         | 22/500 [04:34<1:41:00, 12.68s/it]  5%|▍         | 23/500 [04:46<1:39:03, 12.46s/it]  5%|▍         | 24/500 [04:59<1:39:43, 12.57s/it]  5%|▌         | 25/500 [05:12<1:40:47, 12.73s/it]  5%|▌         | 26/500 [05:24<1:38:38, 12.49s/it]  5%|▌         | 27/500 [05:36<1:38:29, 12.49s/it]  6%|▌         | 28/500 [05:48<1:37:13, 12.36s/it]  6%|▌         | 29/500 [06:00<1:36:20, 12.27s/it]  6%|▌         | 30/500 [06:13<1:37:40, 12.47s/it]  6%|▌         | 31/500 [06:26<1:38:36, 12.62s/it]  6%|▋         | 32/500 [06:40<1:40:36, 12.90s/it]  6%|▋         | 32/500 [06:40<1:37:34, 12.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.231 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁
wandb: train_accuracy ▁▁▂▂▅▂▆▅▁▂▂▂▂▂▁▅▅▅▂▂▃▂▆█▂▁▃▃▃▂▆▃
wandb:     train_loss ▂▂▂▂▂▃▂▁▂▁▂▃▄▂▂▁▁▂▃█▂▁▁▁▅▂▃▃▂▁▂▁
wandb:   val_accuracy ▂▃▂▂█▂▄▅▁▂▁▂▂▂▁▄▅▅▂▂▁▂▆▅▂▁▂▂▂▂▅▂
wandb:       val_loss ▂▁▁▂▁▄▁▁▁▂▁▂▃▂▂▁▂▂▄▆▁▃▁▁▆▁▂▂▂█▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 31
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.42496
wandb:     train_loss 0.053
wandb:   val_accuracy 0.31778
wandb:       val_loss 2.62648
wandb: 
wandb: 🚀 View run bumbling-dew-487 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c2bux3kx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_001215-c2bux3kx/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_001944-8pvfs9gc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-wood-488
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8pvfs9gc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:02, 13.71s/it]  0%|          | 2/500 [00:25<1:44:06, 12.54s/it]  1%|          | 3/500 [00:38<1:44:18, 12.59s/it]  1%|          | 4/500 [00:50<1:42:22, 12.38s/it]  1%|          | 5/500 [01:02<1:41:11, 12.27s/it]  1%|          | 6/500 [01:15<1:43:19, 12.55s/it]  1%|▏         | 7/500 [01:26<1:40:31, 12.23s/it]  2%|▏         | 8/500 [01:38<1:39:29, 12.13s/it]  2%|▏         | 9/500 [01:50<1:38:07, 11.99s/it]  2%|▏         | 10/500 [02:03<1:41:42, 12.45s/it]  2%|▏         | 11/500 [02:16<1:40:31, 12.33s/it]  2%|▏         | 12/500 [02:29<1:42:14, 12.57s/it]  3%|▎         | 13/500 [02:40<1:39:15, 12.23s/it]  3%|▎         | 14/500 [02:52<1:37:58, 12.10s/it]  3%|▎         | 15/500 [03:04<1:38:13, 12.15s/it]  3%|▎         | 16/500 [03:16<1:37:38, 12.10s/it]  3%|▎         | 17/500 [03:28<1:37:19, 12.09s/it]  4%|▎         | 18/500 [03:40<1:36:14, 11.98s/it]  4%|▍         | 19/500 [03:52<1:35:09, 11.87s/it]  4%|▍         | 20/500 [04:03<1:33:49, 11.73s/it]  4%|▍         | 21/500 [04:15<1:34:39, 11.86s/it]  4%|▍         | 22/500 [04:28<1:36:14, 12.08s/it]  5%|▍         | 23/500 [04:40<1:35:35, 12.02s/it]  5%|▍         | 24/500 [04:51<1:34:39, 11.93s/it]  5%|▌         | 25/500 [05:03<1:33:45, 11.84s/it]  5%|▌         | 26/500 [05:15<1:33:18, 11.81s/it]  5%|▌         | 27/500 [05:27<1:33:16, 11.83s/it]  6%|▌         | 28/500 [05:38<1:32:02, 11.70s/it]  6%|▌         | 29/500 [05:50<1:31:46, 11.69s/it]  6%|▌         | 30/500 [06:01<1:31:33, 11.69s/it]  6%|▌         | 31/500 [06:14<1:33:04, 11.91s/it]  6%|▋         | 32/500 [06:26<1:33:08, 11.94s/it]  6%|▋         | 32/500 [06:26<1:34:09, 12.07s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.231 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁
wandb: train_accuracy ▁▂▄▅▅▅▅▅▅▆▅▇▅▆▆▆▆█▅▇▆▆▇█▇▆▇▇▇▆██
wandb:     train_loss ▄▂▃▃▅▃▁▁▃▁▁▂▅▂█▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▇▁
wandb:   val_accuracy ▂▂▆▇██▇▄▃▃▄▂▃▄▁▄▄▄▃▄▄▃▃▃▄▃▃▂▃▃▃▄
wandb:       val_loss ▂▂▁▂▂▂▂▂▁▂▁▂█▂▃▁▂▃▁▄▃▃▁▃▄▄▂▂▃▄▅▂
wandb: 
wandb: Run summary:
wandb:          epoch 31
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.95097
wandb:     train_loss 0.06467
wandb:   val_accuracy 0.42444
wandb:       val_loss 1.33099
wandb: 
wandb: 🚀 View run solar-wood-488 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8pvfs9gc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_001944-8pvfs9gc/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_002655-svnxnqjz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-rain-489
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/svnxnqjz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:52:55, 13.58s/it]  0%|          | 2/500 [00:25<1:43:47, 12.50s/it]  1%|          | 3/500 [00:37<1:41:40, 12.27s/it]  1%|          | 4/500 [00:48<1:38:42, 11.94s/it]  1%|          | 5/500 [01:00<1:37:43, 11.84s/it]  1%|          | 6/500 [01:11<1:36:38, 11.74s/it]  1%|▏         | 7/500 [01:23<1:34:53, 11.55s/it]  2%|▏         | 8/500 [01:35<1:36:17, 11.74s/it]  2%|▏         | 9/500 [01:47<1:36:07, 11.75s/it]  2%|▏         | 10/500 [01:58<1:35:36, 11.71s/it]  2%|▏         | 11/500 [02:10<1:34:48, 11.63s/it]  2%|▏         | 12/500 [02:21<1:34:29, 11.62s/it]  3%|▎         | 13/500 [02:33<1:34:47, 11.68s/it]  3%|▎         | 14/500 [02:45<1:34:19, 11.64s/it]  3%|▎         | 15/500 [02:56<1:34:40, 11.71s/it]  3%|▎         | 16/500 [03:09<1:35:24, 11.83s/it]  3%|▎         | 17/500 [03:20<1:34:49, 11.78s/it]  4%|▎         | 18/500 [03:33<1:36:06, 11.96s/it]  4%|▍         | 19/500 [03:44<1:34:43, 11.82s/it]  4%|▍         | 20/500 [03:56<1:33:40, 11.71s/it]  4%|▍         | 21/500 [04:07<1:33:21, 11.69s/it]  4%|▍         | 22/500 [04:19<1:33:00, 11.67s/it]  5%|▍         | 23/500 [04:30<1:32:41, 11.66s/it]  5%|▍         | 24/500 [04:42<1:31:37, 11.55s/it]  5%|▌         | 25/500 [04:54<1:33:20, 11.79s/it]  5%|▌         | 26/500 [05:06<1:34:01, 11.90s/it]  5%|▌         | 27/500 [05:18<1:33:12, 11.82s/it]  6%|▌         | 28/500 [05:31<1:35:15, 12.11s/it]  6%|▌         | 29/500 [05:42<1:33:33, 11.92s/it]  6%|▌         | 29/500 [05:42<1:32:45, 11.82s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.135 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▃▃▅▅▄▆▄▆▆▆▅▆▇▇▇▇▇▇▇█▇█▇███
wandb:     train_loss ▄▁▂▂▂▃▆▁▃▁▁▂▂▁█▁▁▁▁▂▁▁▁▁▁▁▁▁▃
wandb:   val_accuracy ▁▁▅█▇▄▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▅▅
wandb:       val_loss ▂▂▁▂▂▂▃▂▁▂▂▄▁▂▅▁▃▆▁▅▂▂▂▂▂▃▂▅█
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.96582
wandb:     train_loss 0.63815
wandb:   val_accuracy 0.48667
wandb:       val_loss 6.24303
wandb: 
wandb: 🚀 View run earthy-rain-489 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/svnxnqjz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_002655-svnxnqjz/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_003321-f9j2obd3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-forest-490
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f9j2obd3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:06:37, 15.22s/it]  0%|          | 2/500 [00:26<1:48:46, 13.11s/it]  1%|          | 3/500 [00:39<1:47:14, 12.95s/it]  1%|          | 4/500 [00:50<1:41:46, 12.31s/it]  1%|          | 5/500 [01:02<1:39:37, 12.07s/it]  1%|          | 6/500 [01:14<1:38:27, 11.96s/it]  1%|▏         | 7/500 [01:26<1:38:28, 11.98s/it]  2%|▏         | 8/500 [01:37<1:36:43, 11.80s/it]  2%|▏         | 9/500 [01:49<1:35:54, 11.72s/it]  2%|▏         | 10/500 [02:00<1:35:18, 11.67s/it]  2%|▏         | 11/500 [02:13<1:36:23, 11.83s/it]  2%|▏         | 12/500 [02:26<1:39:43, 12.26s/it]  3%|▎         | 13/500 [02:37<1:37:59, 12.07s/it]  3%|▎         | 14/500 [02:49<1:37:05, 11.99s/it]  3%|▎         | 15/500 [03:01<1:35:36, 11.83s/it]  3%|▎         | 16/500 [03:12<1:33:59, 11.65s/it]  3%|▎         | 17/500 [03:23<1:31:40, 11.39s/it]  4%|▎         | 18/500 [03:33<1:29:31, 11.14s/it]  4%|▍         | 19/500 [03:51<1:44:37, 13.05s/it]  4%|▍         | 20/500 [04:03<1:42:43, 12.84s/it]  4%|▍         | 21/500 [04:15<1:40:25, 12.58s/it]  4%|▍         | 22/500 [04:27<1:39:05, 12.44s/it]  5%|▍         | 23/500 [04:39<1:38:00, 12.33s/it]  5%|▍         | 24/500 [04:52<1:38:45, 12.45s/it]  5%|▌         | 25/500 [05:04<1:38:34, 12.45s/it]  5%|▌         | 26/500 [05:17<1:37:38, 12.36s/it]  5%|▌         | 27/500 [05:29<1:37:55, 12.42s/it]  6%|▌         | 28/500 [05:41<1:36:32, 12.27s/it]  6%|▌         | 29/500 [05:53<1:34:41, 12.06s/it]  6%|▌         | 30/500 [06:05<1:35:09, 12.15s/it]  6%|▌         | 31/500 [06:19<1:38:57, 12.66s/it]  6%|▋         | 32/500 [06:32<1:39:12, 12.72s/it]  7%|▋         | 33/500 [06:45<1:40:55, 12.97s/it]  7%|▋         | 34/500 [06:57<1:38:02, 12.62s/it]  7%|▋         | 35/500 [07:10<1:37:24, 12.57s/it]  7%|▋         | 36/500 [07:22<1:35:53, 12.40s/it]  7%|▋         | 37/500 [07:34<1:35:56, 12.43s/it]  8%|▊         | 38/500 [07:47<1:36:18, 12.51s/it]  8%|▊         | 39/500 [07:58<1:33:57, 12.23s/it]  8%|▊         | 40/500 [08:11<1:34:28, 12.32s/it]  8%|▊         | 41/500 [08:22<1:32:24, 12.08s/it]  8%|▊         | 42/500 [08:34<1:30:29, 11.85s/it]  9%|▊         | 43/500 [08:45<1:29:40, 11.77s/it]  9%|▉         | 44/500 [08:57<1:29:13, 11.74s/it]  9%|▉         | 45/500 [09:09<1:29:51, 11.85s/it]  9%|▉         | 46/500 [09:22<1:31:36, 12.11s/it]  9%|▉         | 47/500 [09:34<1:30:41, 12.01s/it] 10%|▉         | 48/500 [09:46<1:32:11, 12.24s/it] 10%|▉         | 49/500 [09:58<1:31:15, 12.14s/it] 10%|█         | 50/500 [10:10<1:31:14, 12.16s/it] 10%|█         | 51/500 [10:22<1:30:00, 12.03s/it] 10%|█         | 52/500 [10:34<1:28:52, 11.90s/it] 11%|█         | 53/500 [10:46<1:28:24, 11.87s/it] 11%|█         | 54/500 [10:57<1:28:03, 11.85s/it] 11%|█         | 55/500 [11:09<1:27:21, 11.78s/it] 11%|█         | 56/500 [11:21<1:26:57, 11.75s/it] 11%|█         | 56/500 [11:21<1:30:01, 12.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.318 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.021 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▆▆▆▅▇▆▇▆▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇█████
wandb:     train_loss ▆▆▆▆▆▃▅▁▅▅▇▇▅▅▅▁▅▁▂▅▅▂▃▅▆█▅▄▄▃▃▅▄▃▃▄▁▆▄▁
wandb:   val_accuracy ▁▁▂█▆▇▆▇██▇▇▇▇█▇██▆▇▇▆▇▇▇▇▇▆▆▇▇▇▇▆▆▇▇▆▇▇
wandb:       val_loss ▄▃▃▃▄▃▃▃▃▃▃▃▄▃▄▃▃▃▃▂▃▃▃▄▃▂█▅▁▂▅▄▂▂▅▄▄▂▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.75929
wandb:     train_loss 0.04803
wandb:   val_accuracy 0.55778
wandb:       val_loss 0.80498
wandb: 
wandb: 🚀 View run fancy-forest-490 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f9j2obd3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_003321-f9j2obd3/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_004528-kmvi49bi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-galaxy-492
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kmvi49bi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<1:57:19, 14.11s/it]  0%|          | 2/500 [00:26<1:49:07, 13.15s/it]  1%|          | 3/500 [00:39<1:48:06, 13.05s/it]  1%|          | 4/500 [00:51<1:43:10, 12.48s/it]  1%|          | 5/500 [01:04<1:44:49, 12.71s/it]  1%|          | 6/500 [01:15<1:41:53, 12.38s/it]  1%|▏         | 7/500 [01:28<1:41:52, 12.40s/it]  2%|▏         | 8/500 [01:40<1:41:47, 12.41s/it]  2%|▏         | 9/500 [01:52<1:40:23, 12.27s/it]  2%|▏         | 10/500 [02:04<1:39:26, 12.18s/it]  2%|▏         | 11/500 [02:18<1:42:55, 12.63s/it]  2%|▏         | 12/500 [02:30<1:42:13, 12.57s/it]  3%|▎         | 13/500 [02:44<1:44:22, 12.86s/it]  3%|▎         | 14/500 [02:56<1:41:28, 12.53s/it]  3%|▎         | 15/500 [03:07<1:39:11, 12.27s/it]  3%|▎         | 16/500 [03:19<1:37:36, 12.10s/it]  3%|▎         | 17/500 [03:31<1:37:55, 12.16s/it]  4%|▎         | 18/500 [03:43<1:37:11, 12.10s/it]  4%|▍         | 19/500 [03:56<1:37:45, 12.19s/it]  4%|▍         | 20/500 [04:08<1:37:18, 12.16s/it]  4%|▍         | 21/500 [04:20<1:36:04, 12.03s/it]  4%|▍         | 22/500 [04:32<1:35:54, 12.04s/it]  5%|▍         | 23/500 [04:44<1:35:35, 12.02s/it]  5%|▍         | 24/500 [04:55<1:35:04, 11.98s/it]  5%|▌         | 25/500 [05:07<1:34:22, 11.92s/it]  5%|▌         | 26/500 [05:19<1:33:05, 11.78s/it]  5%|▌         | 27/500 [05:31<1:34:14, 11.95s/it]  6%|▌         | 28/500 [05:43<1:33:26, 11.88s/it]  6%|▌         | 29/500 [05:54<1:32:27, 11.78s/it]  6%|▌         | 30/500 [06:06<1:32:13, 11.77s/it]  6%|▌         | 31/500 [06:18<1:32:33, 11.84s/it]  6%|▋         | 32/500 [06:30<1:31:43, 11.76s/it]  7%|▋         | 33/500 [06:42<1:33:08, 11.97s/it]  7%|▋         | 34/500 [06:54<1:32:51, 11.96s/it]  7%|▋         | 35/500 [07:06<1:33:26, 12.06s/it]  7%|▋         | 35/500 [07:06<1:34:30, 12.19s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.231 MB of 0.312 MB uploadedwandb: \ 0.231 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▅▄▆▄▄▅▄▆▅▅▄▅▅▅▄▇▄▆▂█▂▆▅▅█▄██▆▇▄
wandb:     train_loss ▄▃▃▂▄▃▂▂▄▂▁▃▄▄█▅▁▄▁▁▁▁▁▁▁▁▁▂▁▆▇▁▂▁▁
wandb:   val_accuracy ▂▁▅▆█▇█▃▅▃▅▃▁▅▄▄▄▃▃▄▅▄▅▄▅▃▅▃▄▄▅▆▃▄▅
wandb:       val_loss ▂▂▂▂▁▂▂▂▁▂▂▃▂▅▆▁▅▆▁▅▅▃▂▁▃▂▂▁▂▇█▂▅▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 34
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.58247
wandb:     train_loss 0.13331
wandb:   val_accuracy 0.43333
wandb:       val_loss 1.30294
wandb: 
wandb: 🚀 View run glorious-galaxy-492 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kmvi49bi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_004528-kmvi49bi/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_005315-jns4rqo9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sunset-493
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jns4rqo9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:50:58, 13.34s/it]  0%|          | 2/500 [00:25<1:43:02, 12.41s/it]  1%|          | 3/500 [00:36<1:40:17, 12.11s/it]  1%|          | 4/500 [00:49<1:40:23, 12.14s/it]  1%|          | 5/500 [01:00<1:39:16, 12.03s/it]  1%|          | 6/500 [01:12<1:38:09, 11.92s/it]  1%|▏         | 7/500 [01:23<1:36:29, 11.74s/it]  2%|▏         | 8/500 [01:35<1:35:15, 11.62s/it]  2%|▏         | 9/500 [01:47<1:36:14, 11.76s/it]  2%|▏         | 10/500 [02:00<1:38:50, 12.10s/it]  2%|▏         | 11/500 [02:12<1:38:30, 12.09s/it]  2%|▏         | 12/500 [02:23<1:36:59, 11.92s/it]  3%|▎         | 13/500 [02:35<1:36:16, 11.86s/it]  3%|▎         | 14/500 [02:47<1:36:56, 11.97s/it]  3%|▎         | 15/500 [02:59<1:37:14, 12.03s/it]  3%|▎         | 16/500 [03:12<1:37:08, 12.04s/it]  3%|▎         | 17/500 [03:23<1:35:42, 11.89s/it]  4%|▎         | 18/500 [03:35<1:34:46, 11.80s/it]  4%|▍         | 19/500 [03:46<1:34:05, 11.74s/it]  4%|▍         | 20/500 [03:58<1:34:19, 11.79s/it]  4%|▍         | 21/500 [04:10<1:35:03, 11.91s/it]  4%|▍         | 22/500 [04:22<1:34:37, 11.88s/it]  5%|▍         | 23/500 [04:35<1:35:56, 12.07s/it]  5%|▍         | 24/500 [04:46<1:34:53, 11.96s/it]  5%|▌         | 25/500 [04:59<1:36:18, 12.17s/it]  5%|▌         | 26/500 [05:12<1:37:36, 12.36s/it]  5%|▌         | 27/500 [05:24<1:36:17, 12.21s/it]  6%|▌         | 28/500 [05:36<1:36:20, 12.25s/it]  6%|▌         | 29/500 [05:48<1:34:44, 12.07s/it]  6%|▌         | 30/500 [06:00<1:36:09, 12.28s/it]  6%|▌         | 31/500 [06:12<1:34:26, 12.08s/it]  6%|▋         | 32/500 [06:24<1:35:01, 12.18s/it]  7%|▋         | 33/500 [06:36<1:33:13, 11.98s/it]  7%|▋         | 34/500 [06:48<1:32:28, 11.91s/it]  7%|▋         | 35/500 [07:01<1:34:53, 12.24s/it]  7%|▋         | 36/500 [07:13<1:33:30, 12.09s/it]  7%|▋         | 37/500 [07:25<1:33:37, 12.13s/it]  8%|▊         | 38/500 [07:36<1:32:24, 12.00s/it]  8%|▊         | 39/500 [07:49<1:33:18, 12.14s/it]  8%|▊         | 40/500 [08:02<1:34:30, 12.33s/it]  8%|▊         | 41/500 [08:14<1:33:19, 12.20s/it]  8%|▊         | 42/500 [08:25<1:32:16, 12.09s/it]  9%|▊         | 43/500 [08:37<1:31:13, 11.98s/it]  9%|▉         | 44/500 [08:50<1:32:31, 12.17s/it]  9%|▉         | 45/500 [09:03<1:34:53, 12.51s/it]  9%|▉         | 46/500 [09:16<1:35:02, 12.56s/it]  9%|▉         | 47/500 [09:28<1:34:06, 12.46s/it] 10%|▉         | 48/500 [09:40<1:33:27, 12.41s/it] 10%|▉         | 49/500 [09:52<1:30:55, 12.10s/it] 10%|█         | 50/500 [10:03<1:29:33, 11.94s/it] 10%|█         | 51/500 [10:15<1:28:39, 11.85s/it] 10%|█         | 52/500 [10:27<1:29:36, 12.00s/it] 11%|█         | 53/500 [10:39<1:29:05, 11.96s/it] 11%|█         | 54/500 [10:51<1:29:57, 12.10s/it] 11%|█         | 55/500 [11:04<1:30:41, 12.23s/it] 11%|█         | 56/500 [11:16<1:30:55, 12.29s/it] 11%|█         | 56/500 [11:17<1:29:28, 12.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.028 MB of 0.312 MB uploadedwandb: / 0.028 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▅▄▂▅▆▃▇▅▇▅▆▇▇▇▆▇█▆▇▇▇▇▆█▆████▇▇█▇███
wandb:     train_loss ▄▂▃▆▄▁▃▁▅▆█▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▅▁▃▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▂█▅▃▃▃▃▄▃▃▄▃▄▄▅▅▃▄▆▃▇▅▆▆▄▅▃▅▅▅▅▄▃▅▅█▅▅
wandb:       val_loss ▂▂▂▁▂▂▁▂▃▁▄▂▄▁▄▄▂▃▂▁▄▃▁▅▄▁▆▅▁▁▁▆▅▃▄▆█▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.95097
wandb:     train_loss 0.0012
wandb:   val_accuracy 0.49333
wandb:       val_loss 0.457
wandb: 
wandb: 🚀 View run tough-sunset-493 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jns4rqo9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_005315-jns4rqo9/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_010515-mqp020g4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-wind-495
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mqp020g4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:20:00, 24.05s/it]  0%|          | 2/500 [00:36<2:21:56, 17.10s/it]  1%|          | 3/500 [00:47<2:01:16, 14.64s/it]  1%|          | 4/500 [00:59<1:51:01, 13.43s/it]  1%|          | 5/500 [01:11<1:45:01, 12.73s/it]  1%|          | 6/500 [01:22<1:42:02, 12.39s/it]  1%|▏         | 7/500 [01:34<1:39:06, 12.06s/it]  2%|▏         | 8/500 [01:45<1:37:34, 11.90s/it]  2%|▏         | 9/500 [01:56<1:35:02, 11.61s/it]  2%|▏         | 10/500 [02:08<1:34:36, 11.59s/it]  2%|▏         | 11/500 [02:20<1:35:00, 11.66s/it]  2%|▏         | 12/500 [02:31<1:34:08, 11.57s/it]  3%|▎         | 13/500 [02:43<1:34:15, 11.61s/it]  3%|▎         | 14/500 [02:54<1:33:59, 11.60s/it]  3%|▎         | 15/500 [03:05<1:32:48, 11.48s/it]  3%|▎         | 16/500 [03:17<1:32:58, 11.53s/it]  3%|▎         | 17/500 [03:29<1:33:19, 11.59s/it]  4%|▎         | 18/500 [03:41<1:33:55, 11.69s/it]  4%|▍         | 19/500 [03:53<1:34:11, 11.75s/it]  4%|▍         | 20/500 [04:04<1:33:30, 11.69s/it]  4%|▍         | 21/500 [04:16<1:32:44, 11.62s/it]  4%|▍         | 22/500 [04:28<1:34:21, 11.84s/it]  5%|▍         | 23/500 [04:39<1:33:08, 11.72s/it]  5%|▍         | 24/500 [04:52<1:34:53, 11.96s/it]  5%|▌         | 25/500 [05:08<1:45:17, 13.30s/it]  5%|▌         | 26/500 [05:21<1:43:55, 13.15s/it]  5%|▌         | 27/500 [05:34<1:43:35, 13.14s/it]  6%|▌         | 28/500 [05:46<1:40:17, 12.75s/it]  6%|▌         | 29/500 [05:58<1:37:31, 12.42s/it]  6%|▌         | 30/500 [06:09<1:35:07, 12.14s/it]  6%|▌         | 31/500 [06:21<1:33:46, 12.00s/it]  6%|▋         | 32/500 [06:33<1:32:51, 11.91s/it]  7%|▋         | 33/500 [06:45<1:33:40, 12.04s/it]  7%|▋         | 34/500 [06:57<1:32:20, 11.89s/it]  7%|▋         | 35/500 [07:08<1:30:57, 11.74s/it]  7%|▋         | 36/500 [07:20<1:30:32, 11.71s/it]  7%|▋         | 37/500 [07:32<1:31:37, 11.87s/it]  8%|▊         | 38/500 [07:44<1:32:03, 11.96s/it]  8%|▊         | 39/500 [07:56<1:32:41, 12.06s/it]  8%|▊         | 40/500 [08:08<1:31:40, 11.96s/it]  8%|▊         | 41/500 [08:20<1:31:11, 11.92s/it]  8%|▊         | 42/500 [08:31<1:30:24, 11.84s/it]  9%|▊         | 43/500 [08:43<1:30:06, 11.83s/it]  9%|▉         | 44/500 [08:56<1:31:29, 12.04s/it]  9%|▉         | 45/500 [09:08<1:31:56, 12.12s/it]  9%|▉         | 46/500 [09:20<1:30:33, 11.97s/it]  9%|▉         | 47/500 [09:32<1:31:21, 12.10s/it] 10%|▉         | 48/500 [09:44<1:30:19, 11.99s/it] 10%|▉         | 49/500 [09:55<1:29:07, 11.86s/it] 10%|█         | 50/500 [10:07<1:28:51, 11.85s/it] 10%|█         | 51/500 [10:19<1:27:58, 11.76s/it] 10%|█         | 52/500 [10:30<1:27:43, 11.75s/it] 11%|█         | 53/500 [10:43<1:29:13, 11.98s/it] 11%|█         | 54/500 [10:55<1:29:22, 12.02s/it] 11%|█         | 55/500 [11:07<1:29:37, 12.08s/it] 11%|█         | 56/500 [11:19<1:28:36, 11.97s/it] 11%|█         | 56/500 [11:19<1:29:48, 12.14s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.010 MB of 0.317 MB uploadedwandb: - 0.020 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▂▂▂▆▄▂▁▂▂▂▂▅▂▂▂▃▂▁▂▃▇▃▂▁▂▂▂▂▂▅▂▂▄▅▆▂█▄▂▂
wandb:     train_loss ▂▂▂▂▂▁▂▁▂▂▁▂▄▂▂▂▅▂▁▂▁▂▂▁▃▆▄█▂▂▇▅▂▁▁▂▁▂▄▁
wandb:   val_accuracy ▄▄▃█▄▃▂▃▃▃▃▇▃▃▃▃▃▁▃▃▆▂▂▂▃▃▃▃▃▅▃▃▅▆▅▃▅▃▃▃
wandb:       val_loss ▂▂▂▂▂▅▂▄▂▂▃▁▃▂▂▂▆▂▇▂▂▂▂▂▂▄▃▅▁▂█▄▃▁▃▄▂▂▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.3581
wandb:     train_loss 0.00427
wandb:   val_accuracy 0.29556
wandb:       val_loss 3.33533
wandb: 
wandb: 🚀 View run golden-wind-495 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mqp020g4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_010515-mqp020g4/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_011716-st109nh0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-monkey-496
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/st109nh0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:11:09, 15.77s/it]  0%|          | 2/500 [00:30<2:05:25, 15.11s/it]  1%|          | 3/500 [00:43<1:57:35, 14.20s/it]  1%|          | 4/500 [00:57<1:56:33, 14.10s/it]  1%|          | 5/500 [01:10<1:52:29, 13.64s/it]  1%|          | 6/500 [01:26<1:58:14, 14.36s/it]  1%|▏         | 7/500 [01:38<1:51:37, 13.58s/it]  2%|▏         | 8/500 [01:49<1:45:15, 12.84s/it]  2%|▏         | 9/500 [02:01<1:42:14, 12.49s/it]  2%|▏         | 10/500 [02:12<1:40:27, 12.30s/it]  2%|▏         | 11/500 [02:24<1:38:36, 12.10s/it]  2%|▏         | 12/500 [02:37<1:39:36, 12.25s/it]  3%|▎         | 13/500 [02:49<1:39:03, 12.20s/it]  3%|▎         | 14/500 [03:00<1:37:27, 12.03s/it]  3%|▎         | 15/500 [03:12<1:36:20, 11.92s/it]  3%|▎         | 16/500 [03:23<1:34:26, 11.71s/it]  3%|▎         | 17/500 [03:34<1:32:16, 11.46s/it]  4%|▎         | 18/500 [03:45<1:31:16, 11.36s/it]  4%|▍         | 19/500 [03:57<1:32:19, 11.52s/it]  4%|▍         | 20/500 [04:09<1:33:24, 11.68s/it]  4%|▍         | 21/500 [04:21<1:34:00, 11.78s/it]  4%|▍         | 22/500 [04:33<1:33:32, 11.74s/it]  5%|▍         | 23/500 [04:45<1:33:49, 11.80s/it]  5%|▍         | 24/500 [04:57<1:33:59, 11.85s/it]  5%|▌         | 25/500 [05:09<1:34:44, 11.97s/it]  5%|▌         | 26/500 [05:20<1:33:08, 11.79s/it]  5%|▌         | 27/500 [05:32<1:33:28, 11.86s/it]  6%|▌         | 28/500 [05:45<1:35:48, 12.18s/it]  6%|▌         | 29/500 [05:58<1:36:26, 12.29s/it]  6%|▌         | 30/500 [06:10<1:34:51, 12.11s/it]  6%|▌         | 31/500 [06:22<1:35:14, 12.18s/it]  6%|▋         | 32/500 [06:34<1:35:40, 12.27s/it]  7%|▋         | 33/500 [06:47<1:35:56, 12.33s/it]  7%|▋         | 34/500 [07:01<1:39:16, 12.78s/it]  7%|▋         | 35/500 [07:14<1:39:23, 12.83s/it]  7%|▋         | 35/500 [07:14<1:36:07, 12.40s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.137 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▄▅▆▆▅▃▆▅▆▂▆▄▅▆▇▅▆▄▅▄█▆▅▆▇█▆▇█▆▇▇
wandb:     train_loss ▄▃▄▃▄▃▂▁▆▁▂▅▅▃▇▇▁▁▁▁▁▁█▁▁▁▁▆▁▁█▁▄▁▁
wandb:   val_accuracy ▁▁▆▆█▇▇▃▃▄▃▃▃▄▃▄▅▄▄▄▄▃▄▄▃▄▄▃▄▃▃▄▄▃▄
wandb:       val_loss ▂▁▁▁▁▁▂▁▁▁▁▂▁▂▂▁▁▃▁▃▂▂▂▂▃▂▁▁▂▂█▂▃▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 34
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.84101
wandb:     train_loss 0.01166
wandb:   val_accuracy 0.44
wandb:       val_loss 1.55434
wandb: 
wandb: 🚀 View run frosty-monkey-496 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/st109nh0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_011716-st109nh0/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_012514-2azwmrwg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-donkey-498
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2azwmrwg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:00:27, 14.48s/it]  0%|          | 2/500 [00:27<1:53:05, 13.63s/it]  1%|          | 3/500 [00:41<1:52:29, 13.58s/it]  1%|          | 4/500 [00:53<1:49:19, 13.22s/it]  1%|          | 5/500 [01:06<1:48:50, 13.19s/it]  1%|          | 6/500 [01:19<1:48:29, 13.18s/it]  1%|▏         | 7/500 [01:32<1:47:20, 13.06s/it]  2%|▏         | 8/500 [01:45<1:45:10, 12.83s/it]  2%|▏         | 9/500 [01:58<1:45:18, 12.87s/it]  2%|▏         | 10/500 [02:10<1:43:37, 12.69s/it]  2%|▏         | 11/500 [02:22<1:41:37, 12.47s/it]  2%|▏         | 12/500 [02:34<1:39:43, 12.26s/it]  3%|▎         | 13/500 [02:47<1:42:55, 12.68s/it]  3%|▎         | 14/500 [02:59<1:40:50, 12.45s/it]  3%|▎         | 15/500 [03:11<1:38:46, 12.22s/it]  3%|▎         | 16/500 [03:24<1:40:48, 12.50s/it]  3%|▎         | 17/500 [03:37<1:41:49, 12.65s/it]  4%|▎         | 18/500 [03:49<1:40:57, 12.57s/it]  4%|▍         | 19/500 [04:02<1:40:18, 12.51s/it]  4%|▍         | 20/500 [04:14<1:40:00, 12.50s/it]  4%|▍         | 21/500 [04:27<1:41:04, 12.66s/it]  4%|▍         | 22/500 [04:39<1:39:01, 12.43s/it]  5%|▍         | 23/500 [04:52<1:39:09, 12.47s/it]  5%|▍         | 24/500 [05:04<1:38:57, 12.47s/it]  5%|▌         | 25/500 [05:17<1:38:49, 12.48s/it]  5%|▌         | 26/500 [05:29<1:37:26, 12.33s/it]  5%|▌         | 27/500 [05:41<1:36:42, 12.27s/it]  6%|▌         | 28/500 [05:54<1:38:25, 12.51s/it]  6%|▌         | 29/500 [06:06<1:36:55, 12.35s/it]  6%|▌         | 29/500 [06:06<1:39:12, 12.64s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.231 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▅▄▆▅▆▅▆▅▇▇▆▅▅▅▇▇█▇█▇███▇██▇
wandb:     train_loss ▃▁▂▂▃▃▁▁▃▁▂▁▁▂▄▄▁▁▁▁▁▁▁▁▂▁▁▁█
wandb:   val_accuracy ▂▁▆▆█▄▅▅▅▅▆▆▅▄▄▂▅▆▆▆▆▆▆▆▆▅▅▅▅
wandb:       val_loss ▃▂▂▂▂▂▃▂▂▂▂▂▁▂▄▂▂█▃▅▁▂▃▂▅▄▄▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.84993
wandb:     train_loss 3.66325
wandb:   val_accuracy 0.52667
wandb:       val_loss 0.31795
wandb: 
wandb: 🚀 View run fluent-donkey-498 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2azwmrwg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_012514-2azwmrwg/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_013203-yt1va4oy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-glade-499
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yt1va4oy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<1:58:23, 14.24s/it]  0%|          | 2/500 [00:26<1:48:54, 13.12s/it]  1%|          | 3/500 [00:39<1:49:34, 13.23s/it]  1%|          | 4/500 [00:51<1:43:32, 12.52s/it]  1%|          | 5/500 [01:05<1:47:17, 13.01s/it]  1%|          | 6/500 [01:17<1:45:25, 12.81s/it]  1%|▏         | 7/500 [01:30<1:44:47, 12.75s/it]  2%|▏         | 8/500 [01:42<1:42:47, 12.54s/it]  2%|▏         | 9/500 [01:54<1:42:48, 12.56s/it]  2%|▏         | 10/500 [02:07<1:43:02, 12.62s/it]  2%|▏         | 11/500 [02:20<1:42:08, 12.53s/it]  2%|▏         | 12/500 [02:32<1:40:40, 12.38s/it]  3%|▎         | 13/500 [02:44<1:40:00, 12.32s/it]  3%|▎         | 14/500 [02:56<1:39:38, 12.30s/it]  3%|▎         | 15/500 [03:09<1:40:34, 12.44s/it]  3%|▎         | 16/500 [03:22<1:40:58, 12.52s/it]  3%|▎         | 17/500 [03:33<1:38:24, 12.22s/it]  4%|▎         | 18/500 [03:46<1:39:51, 12.43s/it]  4%|▍         | 19/500 [03:58<1:38:54, 12.34s/it]  4%|▍         | 20/500 [04:10<1:38:12, 12.28s/it]  4%|▍         | 21/500 [04:22<1:37:57, 12.27s/it]  4%|▍         | 22/500 [04:34<1:36:10, 12.07s/it]  5%|▍         | 23/500 [04:46<1:35:25, 12.00s/it]  5%|▍         | 24/500 [04:58<1:34:34, 11.92s/it]  5%|▌         | 25/500 [05:09<1:34:09, 11.89s/it]  5%|▌         | 26/500 [05:22<1:35:26, 12.08s/it]  5%|▌         | 27/500 [05:34<1:34:23, 11.97s/it]  6%|▌         | 28/500 [05:46<1:34:04, 11.96s/it]  6%|▌         | 28/500 [05:46<1:37:15, 12.36s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.021 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▂▂▃▂█▅▆▇▁▂▂▃▂▄▂▇▃▂▃▂▃▂▂▅▃▇
wandb:     train_loss ▃▃▃▃▃█▃▂▃▁▃▅▇▄▃▃▃▂▃▅▂▁▃▁█▄▂▂
wandb:   val_accuracy ▃▃▃▂▃▃▆▅██▁▃▃▂▃▅▁▇▂▃▃▃▄▃▃▅▃█
wandb:       val_loss ▂▁▂▁▁▇▁▁▁▁▁▁▃▂▂▁▂▃▁▂▃█▁▄▅▁▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.56018
wandb:     train_loss 0.85262
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.91928
wandb: 
wandb: 🚀 View run generous-glade-499 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yt1va4oy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_013203-yt1va4oy/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_013833-eqd3uih3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-pond-500
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/eqd3uih3
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:24, 17.73s/it]  0%|          | 2/500 [00:31<2:08:40, 15.50s/it]  1%|          | 3/500 [00:45<2:00:18, 14.52s/it]  1%|          | 4/500 [00:57<1:53:07, 13.68s/it]  1%|          | 5/500 [01:09<1:48:58, 13.21s/it]  1%|          | 6/500 [01:22<1:48:36, 13.19s/it]  1%|▏         | 7/500 [01:35<1:45:26, 12.83s/it]  2%|▏         | 8/500 [01:46<1:41:20, 12.36s/it]  2%|▏         | 9/500 [01:58<1:39:44, 12.19s/it]  2%|▏         | 10/500 [02:09<1:37:44, 11.97s/it]  2%|▏         | 11/500 [02:21<1:37:58, 12.02s/it]  2%|▏         | 12/500 [02:34<1:40:32, 12.36s/it]  3%|▎         | 13/500 [02:46<1:38:26, 12.13s/it]  3%|▎         | 14/500 [02:58<1:37:44, 12.07s/it]  3%|▎         | 15/500 [03:12<1:41:20, 12.54s/it]  3%|▎         | 16/500 [03:24<1:41:05, 12.53s/it]  3%|▎         | 17/500 [03:37<1:41:14, 12.58s/it]  4%|▎         | 18/500 [03:49<1:40:49, 12.55s/it]  4%|▍         | 19/500 [04:02<1:41:30, 12.66s/it]  4%|▍         | 20/500 [04:15<1:41:26, 12.68s/it]  4%|▍         | 21/500 [04:28<1:42:31, 12.84s/it]  4%|▍         | 22/500 [04:41<1:41:37, 12.76s/it]  5%|▍         | 23/500 [04:53<1:39:33, 12.52s/it]  5%|▍         | 24/500 [05:05<1:39:31, 12.54s/it]  5%|▌         | 25/500 [05:17<1:38:26, 12.44s/it]  5%|▌         | 26/500 [05:31<1:39:58, 12.66s/it]  5%|▌         | 27/500 [05:43<1:38:30, 12.50s/it]  6%|▌         | 28/500 [05:55<1:38:46, 12.56s/it]  6%|▌         | 29/500 [06:08<1:38:21, 12.53s/it]  6%|▌         | 29/500 [06:08<1:39:43, 12.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.021 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▃▅▅▄▅▆▆▇▇▇▆▆▇▇▆▇█▇▇▇█▇▆█▇▇
wandb:     train_loss ▄▄▄▂▄▄▂▁▂▁▂▁▅▂█▆▁▁▁▁▁▁▂▁▁▁▁▁▃
wandb:   val_accuracy ▁▁▁▄▇███▆▆▄▆▅▄▄▅▄▅▃▅▄▄▆▅▄▃▅▄▄
wandb:       val_loss ▄▄▃▄▃▃▃▄▄▅▅▄▁▇▅▃▃▅▅▇▁▇▃▇▆▇▃▆█
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.82021
wandb:     train_loss 0.72722
wandb:   val_accuracy 0.43111
wandb:       val_loss 2.47091
wandb: 
wandb: 🚀 View run avid-pond-500 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/eqd3uih3
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_013833-eqd3uih3/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_014535-oi1ge0dg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-paper-502
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oi1ge0dg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:12:00, 15.87s/it]  0%|          | 2/500 [00:29<1:58:28, 14.27s/it]  1%|          | 3/500 [00:41<1:52:06, 13.53s/it]  1%|          | 4/500 [00:53<1:46:23, 12.87s/it]  1%|          | 5/500 [01:05<1:43:42, 12.57s/it]  1%|          | 6/500 [01:17<1:41:46, 12.36s/it]  1%|▏         | 7/500 [01:30<1:44:02, 12.66s/it]  2%|▏         | 8/500 [01:43<1:44:33, 12.75s/it]  2%|▏         | 9/500 [01:56<1:44:42, 12.79s/it]  2%|▏         | 10/500 [02:09<1:44:26, 12.79s/it]  2%|▏         | 11/500 [02:22<1:43:52, 12.75s/it]  2%|▏         | 12/500 [02:33<1:39:41, 12.26s/it]  3%|▎         | 13/500 [02:45<1:38:48, 12.17s/it]  3%|▎         | 14/500 [03:01<1:48:40, 13.42s/it]  3%|▎         | 15/500 [03:13<1:44:59, 12.99s/it]  3%|▎         | 16/500 [03:25<1:42:56, 12.76s/it]  3%|▎         | 17/500 [03:37<1:41:10, 12.57s/it]  4%|▎         | 18/500 [03:50<1:41:58, 12.69s/it]  4%|▍         | 19/500 [04:04<1:43:03, 12.86s/it]  4%|▍         | 20/500 [04:16<1:42:06, 12.76s/it]  4%|▍         | 21/500 [04:28<1:40:09, 12.55s/it]  4%|▍         | 22/500 [04:40<1:39:01, 12.43s/it]  5%|▍         | 23/500 [04:52<1:36:50, 12.18s/it]  5%|▍         | 24/500 [05:04<1:36:16, 12.14s/it]  5%|▌         | 25/500 [05:16<1:36:20, 12.17s/it]  5%|▌         | 26/500 [05:29<1:37:56, 12.40s/it]  5%|▌         | 27/500 [05:42<1:38:02, 12.44s/it]  5%|▌         | 27/500 [05:42<1:39:55, 12.68s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.021 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▅▆▅▁▁▂▅▂▄▅▄▄▂▅▅▇▅▃▂▁▁▁▂▅█
wandb:     train_loss ▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁█▁▃▁▁▁
wandb:   val_accuracy ▁▅▇█▆▂▂▄▄▄▂▃▄▂▃▃▃▄▂▄▂▃▂▂▂▃▅
wandb:       val_loss ▁▁▁▁▁▁▂▁▁▁▁▂▁▁▂▁▁▂▁▃▆▁▅▄█▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.7162
wandb:     train_loss 0.00051
wandb:   val_accuracy 0.5
wandb:       val_loss 2.14336
wandb: 
wandb: 🚀 View run lyric-paper-502 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oi1ge0dg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_014535-oi1ge0dg/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_015200-gnlubv4i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-rain-503
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gnlubv4i
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:53:10, 13.61s/it]  0%|          | 2/500 [00:26<1:51:10, 13.40s/it]  1%|          | 3/500 [00:40<1:53:13, 13.67s/it]  1%|          | 4/500 [00:53<1:51:17, 13.46s/it]  1%|          | 5/500 [01:05<1:46:19, 12.89s/it]  1%|          | 6/500 [01:19<1:47:39, 13.08s/it]  1%|▏         | 7/500 [01:32<1:47:33, 13.09s/it]  2%|▏         | 8/500 [01:44<1:43:27, 12.62s/it]  2%|▏         | 9/500 [01:58<1:47:51, 13.18s/it]  2%|▏         | 10/500 [02:11<1:46:42, 13.07s/it]  2%|▏         | 11/500 [02:23<1:45:40, 12.97s/it]  2%|▏         | 12/500 [02:36<1:44:13, 12.81s/it]  3%|▎         | 13/500 [02:48<1:42:28, 12.63s/it]  3%|▎         | 14/500 [03:02<1:45:36, 13.04s/it]  3%|▎         | 15/500 [03:16<1:47:04, 13.25s/it]  3%|▎         | 16/500 [03:28<1:44:38, 12.97s/it]  3%|▎         | 17/500 [03:40<1:41:48, 12.65s/it]  4%|▎         | 18/500 [03:52<1:39:45, 12.42s/it]  4%|▍         | 19/500 [04:04<1:38:12, 12.25s/it]  4%|▍         | 20/500 [04:16<1:37:29, 12.19s/it]  4%|▍         | 21/500 [04:28<1:37:02, 12.16s/it]  4%|▍         | 22/500 [04:40<1:35:41, 12.01s/it]  5%|▍         | 23/500 [04:53<1:38:00, 12.33s/it]  5%|▍         | 24/500 [05:05<1:36:38, 12.18s/it]  5%|▌         | 25/500 [05:17<1:36:47, 12.23s/it]  5%|▌         | 26/500 [05:28<1:35:00, 12.03s/it]  5%|▌         | 27/500 [05:40<1:34:28, 11.98s/it]  6%|▌         | 28/500 [05:53<1:35:02, 12.08s/it]  6%|▌         | 29/500 [06:04<1:34:11, 12.00s/it]  6%|▌         | 30/500 [06:16<1:33:33, 11.94s/it]  6%|▌         | 31/500 [06:28<1:31:57, 11.76s/it]  6%|▋         | 32/500 [06:39<1:31:16, 11.70s/it]  7%|▋         | 33/500 [06:51<1:31:12, 11.72s/it]  7%|▋         | 34/500 [07:03<1:31:12, 11.74s/it]  7%|▋         | 35/500 [07:14<1:30:39, 11.70s/it]  7%|▋         | 36/500 [07:26<1:31:01, 11.77s/it]  7%|▋         | 37/500 [07:39<1:33:08, 12.07s/it]  8%|▊         | 38/500 [07:51<1:33:38, 12.16s/it]  8%|▊         | 39/500 [08:03<1:31:39, 11.93s/it]  8%|▊         | 40/500 [08:15<1:31:56, 11.99s/it]  8%|▊         | 41/500 [08:27<1:31:07, 11.91s/it]  8%|▊         | 42/500 [08:39<1:30:55, 11.91s/it]  9%|▊         | 43/500 [08:50<1:29:00, 11.69s/it]  9%|▉         | 44/500 [09:02<1:29:32, 11.78s/it]  9%|▉         | 45/500 [09:13<1:28:24, 11.66s/it]  9%|▉         | 46/500 [09:24<1:27:07, 11.51s/it]  9%|▉         | 47/500 [09:36<1:26:46, 11.49s/it] 10%|▉         | 48/500 [09:49<1:29:56, 11.94s/it] 10%|▉         | 49/500 [10:02<1:33:04, 12.38s/it] 10%|█         | 50/500 [10:14<1:31:19, 12.18s/it] 10%|█         | 51/500 [10:25<1:29:53, 12.01s/it] 10%|█         | 52/500 [10:37<1:29:47, 12.02s/it] 11%|█         | 53/500 [10:50<1:29:50, 12.06s/it] 11%|█         | 54/500 [11:01<1:28:48, 11.95s/it] 11%|█         | 55/500 [11:13<1:28:37, 11.95s/it] 11%|█         | 56/500 [11:27<1:31:22, 12.35s/it] 11%|█         | 56/500 [11:27<1:30:48, 12.27s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.030 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▂▂▅▇▆▇▇▆█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:     train_loss ▆▅▆▆▆▃▅▁▅▅▇█▅▅▅▁▅▁▂▆▅▃▇▅▆▆▄▄▄▃▃▅▅▅▄▅▂▆▄▁
wandb:   val_accuracy ▂▁▂▆▇▇█▇█▇▆▆▆▆▇▆▆▆▅▆▆▆▆▆▆▆▆▅▅▆▆▆▆▆▅▆▆▆▆▆
wandb:       val_loss ▆▆▅▅▅▅▅▄▅▄▅▅▆▄▆▄▄▄▄▂▃▄▄▇▄▂▆█▁▂▂█▄▁▄▅▇▄▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.55126
wandb:     train_loss 0.14928
wandb:   val_accuracy 0.54889
wandb:       val_loss 0.58277
wandb: 
wandb: 🚀 View run pretty-rain-503 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gnlubv4i
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_015200-gnlubv4i/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_020416-wralpdc4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-snowball-504
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wralpdc4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:03:15, 14.82s/it]  0%|          | 2/500 [00:28<1:58:25, 14.27s/it]  1%|          | 3/500 [00:41<1:52:13, 13.55s/it]  1%|          | 4/500 [00:54<1:50:25, 13.36s/it]  1%|          | 5/500 [01:06<1:46:09, 12.87s/it]  1%|          | 6/500 [01:19<1:46:25, 12.93s/it]  1%|▏         | 7/500 [01:31<1:44:08, 12.68s/it]  2%|▏         | 8/500 [01:44<1:43:19, 12.60s/it]  2%|▏         | 9/500 [01:56<1:41:45, 12.43s/it]  2%|▏         | 10/500 [02:08<1:40:16, 12.28s/it]  2%|▏         | 11/500 [02:20<1:39:13, 12.17s/it]  2%|▏         | 12/500 [02:32<1:40:29, 12.36s/it]  3%|▎         | 13/500 [02:46<1:42:45, 12.66s/it]  3%|▎         | 14/500 [02:59<1:43:24, 12.77s/it]  3%|▎         | 15/500 [03:11<1:42:56, 12.74s/it]  3%|▎         | 16/500 [03:25<1:44:04, 12.90s/it]  3%|▎         | 17/500 [03:42<1:55:08, 14.30s/it]  4%|▎         | 18/500 [03:55<1:51:25, 13.87s/it]  4%|▍         | 19/500 [04:08<1:50:02, 13.73s/it]  4%|▍         | 20/500 [04:21<1:46:28, 13.31s/it]  4%|▍         | 21/500 [04:33<1:44:48, 13.13s/it]  4%|▍         | 22/500 [04:46<1:43:58, 13.05s/it]  5%|▍         | 23/500 [04:59<1:43:38, 13.04s/it]  5%|▍         | 24/500 [05:12<1:42:51, 12.96s/it]  5%|▌         | 25/500 [05:25<1:41:21, 12.80s/it]  5%|▌         | 26/500 [05:37<1:39:31, 12.60s/it]  5%|▌         | 27/500 [05:50<1:39:48, 12.66s/it]  6%|▌         | 28/500 [06:02<1:39:34, 12.66s/it]  6%|▌         | 29/500 [06:14<1:37:35, 12.43s/it]  6%|▌         | 30/500 [06:26<1:36:34, 12.33s/it]  6%|▌         | 31/500 [06:38<1:36:15, 12.31s/it]  6%|▋         | 32/500 [06:51<1:36:42, 12.40s/it]  7%|▋         | 33/500 [07:03<1:35:07, 12.22s/it]  7%|▋         | 34/500 [07:16<1:37:26, 12.55s/it]  7%|▋         | 35/500 [07:29<1:37:30, 12.58s/it]  7%|▋         | 36/500 [07:42<1:38:07, 12.69s/it]  7%|▋         | 37/500 [07:54<1:37:02, 12.58s/it]  8%|▊         | 38/500 [08:06<1:35:22, 12.39s/it]  8%|▊         | 39/500 [08:18<1:34:23, 12.28s/it]  8%|▊         | 40/500 [08:30<1:34:27, 12.32s/it]  8%|▊         | 41/500 [08:43<1:34:16, 12.32s/it]  8%|▊         | 42/500 [08:54<1:32:39, 12.14s/it]  9%|▊         | 43/500 [09:06<1:31:47, 12.05s/it]  9%|▉         | 44/500 [09:18<1:30:33, 11.91s/it]  9%|▉         | 45/500 [09:31<1:33:32, 12.34s/it]  9%|▉         | 46/500 [09:44<1:33:32, 12.36s/it]  9%|▉         | 47/500 [09:56<1:34:07, 12.47s/it] 10%|▉         | 48/500 [10:09<1:35:19, 12.65s/it] 10%|▉         | 48/500 [10:10<1:35:44, 12.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.138 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▂▅▅█▇▇▅▂▁▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃
wandb:     train_loss ▃▂▃▂▄▂▂▃▅▂▃▃▅▅▂▂▂▂▅▂▁▄▁▅▁▆▆▁▁▁▆█▆▅██▁▅▁▆
wandb:   val_accuracy ▁▄▇▇█▆▅▅▁▅▁▃▁▁▁▂▄▄▂▁▁▄▁▁▁▄▃▃▂▂▄▂▄▂▂▄▄▁▃▄
wandb:       val_loss ▃▂▂▃▂▃▂▂▂▂▃▃▃▂▃▁▄▄▂▂▄▃▄▄▄▃▁▅▃▄▆▃▆▂▅█▃▃▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.3997
wandb:     train_loss 2.46078
wandb:   val_accuracy 0.45111
wandb:       val_loss 2.57983
wandb: 
wandb: 🚀 View run feasible-snowball-504 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wralpdc4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_020416-wralpdc4/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_021516-nfg9pyh6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-lion-506
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nfg9pyh6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:20<2:46:46, 20.05s/it]  0%|          | 2/500 [00:36<2:29:00, 17.95s/it]  1%|          | 3/500 [00:53<2:24:01, 17.39s/it]  1%|          | 4/500 [01:09<2:20:47, 17.03s/it]  1%|          | 5/500 [01:26<2:18:22, 16.77s/it]  1%|          | 6/500 [01:44<2:23:19, 17.41s/it]  1%|▏         | 7/500 [02:01<2:20:38, 17.12s/it]  2%|▏         | 8/500 [02:17<2:17:44, 16.80s/it]  2%|▏         | 9/500 [02:33<2:16:34, 16.69s/it]  2%|▏         | 10/500 [02:55<2:28:29, 18.18s/it]  2%|▏         | 11/500 [03:13<2:27:04, 18.05s/it]  2%|▏         | 12/500 [03:30<2:24:49, 17.81s/it]  3%|▎         | 13/500 [03:52<2:34:13, 19.00s/it]  3%|▎         | 14/500 [04:09<2:29:34, 18.47s/it]  3%|▎         | 15/500 [04:26<2:25:11, 17.96s/it]  3%|▎         | 16/500 [04:42<2:21:56, 17.60s/it]  3%|▎         | 17/500 [04:59<2:18:59, 17.27s/it]  4%|▎         | 18/500 [05:16<2:17:50, 17.16s/it]  4%|▍         | 19/500 [05:33<2:17:14, 17.12s/it]  4%|▍         | 20/500 [05:49<2:14:59, 16.87s/it]  4%|▍         | 21/500 [06:06<2:13:50, 16.76s/it]  4%|▍         | 22/500 [06:22<2:13:45, 16.79s/it]  5%|▍         | 23/500 [06:40<2:15:02, 16.99s/it]  5%|▍         | 24/500 [06:56<2:13:52, 16.87s/it]  5%|▌         | 25/500 [07:13<2:12:14, 16.70s/it]  5%|▌         | 26/500 [07:35<2:25:14, 18.39s/it]  5%|▌         | 27/500 [07:54<2:25:45, 18.49s/it]  6%|▌         | 28/500 [08:10<2:20:54, 17.91s/it]  6%|▌         | 29/500 [08:27<2:17:09, 17.47s/it]  6%|▌         | 30/500 [08:43<2:13:55, 17.10s/it]  6%|▌         | 31/500 [09:00<2:13:50, 17.12s/it]  6%|▋         | 32/500 [09:17<2:12:14, 16.95s/it]  7%|▋         | 33/500 [09:33<2:10:40, 16.79s/it]  7%|▋         | 34/500 [09:50<2:09:52, 16.72s/it]  7%|▋         | 35/500 [10:06<2:09:17, 16.68s/it]  7%|▋         | 36/500 [10:28<2:20:42, 18.20s/it]  7%|▋         | 37/500 [10:44<2:15:55, 17.61s/it]  8%|▊         | 38/500 [11:01<2:13:10, 17.30s/it]  8%|▊         | 39/500 [11:17<2:10:46, 17.02s/it]  8%|▊         | 40/500 [11:33<2:08:09, 16.72s/it]  8%|▊         | 41/500 [11:50<2:07:19, 16.64s/it]  8%|▊         | 42/500 [12:11<2:17:41, 18.04s/it]  9%|▊         | 43/500 [12:27<2:12:59, 17.46s/it]  9%|▊         | 43/500 [12:27<2:12:26, 17.39s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.232 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▅▅▃▄▄▇▂▇▃▇▆▇█▇██▆██▆██▅████████▇███████
wandb:     train_loss ▄▄▄▂▄▅▁█▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▇▇▆▆▆█▂█▃▆▆▇█▆██▆▅▆▆▆▇▅▇▆▆▆▆▇▆▆▆▇▇▆▇▆▆▆
wandb:       val_loss ▂▂▂▃▁▂▃▅▂▄▃▄▁▄▁▆▅▂▂▁▂▁▃▁█▁▃▁▆▁▂▃▇▃▁▃▁▂▁▇
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.00041
wandb: train_accuracy 1.0
wandb:     train_loss 0.00036
wandb:   val_accuracy 0.56667
wandb:       val_loss 5.75122
wandb: 
wandb: 🚀 View run ancient-lion-506 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nfg9pyh6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_021516-nfg9pyh6/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_022835-lgbwz5yn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-jazz-507
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lgbwz5yn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:23:01, 24.41s/it]  0%|          | 2/500 [00:41<2:45:11, 19.90s/it]  1%|          | 3/500 [00:57<2:31:52, 18.34s/it]  1%|          | 4/500 [01:14<2:26:25, 17.71s/it]  1%|          | 5/500 [01:30<2:21:47, 17.19s/it]  1%|          | 6/500 [01:46<2:17:33, 16.71s/it]  1%|▏         | 7/500 [02:03<2:17:11, 16.70s/it]  2%|▏         | 8/500 [02:19<2:14:54, 16.45s/it]  2%|▏         | 9/500 [02:34<2:13:07, 16.27s/it]  2%|▏         | 10/500 [02:50<2:11:48, 16.14s/it]  2%|▏         | 11/500 [03:07<2:13:01, 16.32s/it]  2%|▏         | 12/500 [03:23<2:11:33, 16.17s/it]  3%|▎         | 13/500 [03:44<2:24:48, 17.84s/it]  3%|▎         | 14/500 [04:01<2:21:07, 17.42s/it]  3%|▎         | 15/500 [04:17<2:17:48, 17.05s/it]  3%|▎         | 16/500 [04:37<2:25:24, 18.03s/it]  3%|▎         | 17/500 [04:54<2:20:46, 17.49s/it]  4%|▎         | 18/500 [05:10<2:16:36, 17.00s/it]  4%|▍         | 19/500 [05:26<2:15:31, 16.91s/it]  4%|▍         | 20/500 [05:42<2:13:05, 16.64s/it]  4%|▍         | 21/500 [05:58<2:11:04, 16.42s/it]  4%|▍         | 22/500 [06:14<2:09:53, 16.30s/it]  5%|▍         | 23/500 [06:30<2:08:49, 16.20s/it]  5%|▍         | 24/500 [06:46<2:07:54, 16.12s/it]  5%|▌         | 25/500 [07:02<2:06:31, 15.98s/it]  5%|▌         | 26/500 [07:18<2:05:57, 15.95s/it]  5%|▌         | 26/500 [07:18<2:13:06, 16.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.020 MB of 0.313 MB uploadedwandb: | 0.231 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▄▂▁▂▂▆▂▂▅▃▂▂▃▃▃▃▃▄▃▂▃▃█▃
wandb:     train_loss ▂▁▂▂▂▁▄▂▄▅▂▂▂▁▂▂▂▃▂▃▂█▂▃▂▂
wandb:   val_accuracy ▄▄▅▄▁▄▄█▄▄▆▄▃▄▄▄▄▄▄▅▄▄▄▄▆▄
wandb:       val_loss ▂▂▂▂▂▃▁▂▃▁▂▂▂▇▂▂▂▂▂▂▂█▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.3477
wandb:     train_loss 0.84367
wandb:   val_accuracy 0.33556
wandb:       val_loss 1.28927
wandb: 
wandb: 🚀 View run comfy-jazz-507 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lgbwz5yn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_022835-lgbwz5yn/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_023640-m5kspzn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-deluge-508
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m5kspzn4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:08, 18.05s/it]  0%|          | 2/500 [00:34<2:20:50, 16.97s/it]  1%|          | 3/500 [00:50<2:16:31, 16.48s/it]  1%|          | 4/500 [01:09<2:26:00, 17.66s/it]  1%|          | 5/500 [01:29<2:32:44, 18.51s/it]  1%|          | 6/500 [01:44<2:22:28, 17.30s/it]  1%|▏         | 7/500 [02:03<2:27:32, 17.96s/it]  2%|▏         | 8/500 [02:23<2:32:17, 18.57s/it]  2%|▏         | 9/500 [02:39<2:23:35, 17.55s/it]  2%|▏         | 10/500 [02:53<2:15:56, 16.65s/it]  2%|▏         | 11/500 [03:08<2:11:53, 16.18s/it]  2%|▏         | 12/500 [03:28<2:20:21, 17.26s/it]  3%|▎         | 13/500 [03:43<2:15:20, 16.67s/it]  3%|▎         | 14/500 [03:59<2:11:13, 16.20s/it]  3%|▎         | 15/500 [04:14<2:09:12, 15.99s/it]  3%|▎         | 16/500 [04:30<2:07:51, 15.85s/it]  3%|▎         | 17/500 [04:45<2:06:57, 15.77s/it]  4%|▎         | 18/500 [05:01<2:06:34, 15.76s/it]  4%|▍         | 19/500 [05:16<2:05:13, 15.62s/it]  4%|▍         | 20/500 [05:32<2:04:28, 15.56s/it]  4%|▍         | 21/500 [05:47<2:04:07, 15.55s/it]  4%|▍         | 22/500 [06:03<2:04:35, 15.64s/it]  5%|▍         | 23/500 [06:18<2:03:18, 15.51s/it]  5%|▍         | 24/500 [06:34<2:03:19, 15.54s/it]  5%|▌         | 25/500 [06:50<2:03:59, 15.66s/it]  5%|▌         | 26/500 [07:06<2:05:28, 15.88s/it]  5%|▌         | 27/500 [07:21<2:03:50, 15.71s/it]  6%|▌         | 28/500 [07:38<2:05:55, 16.01s/it]  6%|▌         | 29/500 [07:53<2:04:00, 15.80s/it]  6%|▌         | 30/500 [08:09<2:02:33, 15.65s/it]  6%|▌         | 31/500 [08:24<2:01:38, 15.56s/it]  6%|▋         | 32/500 [08:40<2:02:37, 15.72s/it]  7%|▋         | 33/500 [08:56<2:03:23, 15.85s/it]  7%|▋         | 34/500 [09:13<2:05:13, 16.12s/it]  7%|▋         | 35/500 [09:29<2:04:46, 16.10s/it]  7%|▋         | 36/500 [09:46<2:05:19, 16.21s/it]  7%|▋         | 37/500 [10:01<2:03:49, 16.05s/it]  8%|▊         | 38/500 [10:17<2:03:41, 16.06s/it]  8%|▊         | 39/500 [10:35<2:06:03, 16.41s/it]  8%|▊         | 40/500 [10:51<2:06:05, 16.45s/it]  8%|▊         | 41/500 [11:09<2:09:59, 16.99s/it]  8%|▊         | 42/500 [11:25<2:07:32, 16.71s/it]  9%|▊         | 43/500 [11:41<2:05:34, 16.49s/it]  9%|▉         | 44/500 [11:57<2:03:49, 16.29s/it]  9%|▉         | 45/500 [12:13<2:03:09, 16.24s/it]  9%|▉         | 46/500 [12:30<2:02:54, 16.24s/it]  9%|▉         | 47/500 [12:46<2:03:43, 16.39s/it] 10%|▉         | 48/500 [13:02<2:02:33, 16.27s/it] 10%|▉         | 49/500 [13:18<2:01:30, 16.17s/it] 10%|█         | 50/500 [13:34<2:00:51, 16.11s/it] 10%|█         | 51/500 [13:51<2:00:58, 16.17s/it] 10%|█         | 52/500 [14:07<2:00:17, 16.11s/it] 11%|█         | 53/500 [14:23<1:59:51, 16.09s/it] 11%|█         | 54/500 [14:38<1:58:38, 15.96s/it] 11%|█         | 55/500 [14:54<1:57:31, 15.85s/it] 11%|█         | 56/500 [15:10<1:57:10, 15.83s/it] 11%|█         | 56/500 [15:10<2:00:15, 16.25s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.029 MB uploadedwandb: \ 0.020 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▂▅▅▅▄▆▆▇▇▇▇▇▅█▄▇█▇▅▇█▄▆█▇▇▇▇█▅██▇▆▇▆▅▇
wandb:     train_loss ▂▂▂▂▁▃▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb:   val_accuracy ▁▁▁██▆▅▆▇▆▆▆▇▅▇▅▃▆▅▆▆▅▇▆▃▆▆▄▄▄▅▄▆▆▄▄▄▄▃▄
wandb:       val_loss ▂▂▂▁▁▂▁▁▁█▂▁▃▁▄▁▁▃▃▁▁▁▁▅▂▂▁▃▁▂▂▂▃▃▁▁▃▂▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.83061
wandb:     train_loss 0.50313
wandb:   val_accuracy 0.48222
wandb:       val_loss 0.48457
wandb: 
wandb: 🚀 View run driven-deluge-508 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/m5kspzn4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_023640-m5kspzn4/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_025244-9qwtdgug
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sun-510
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9qwtdgug
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:04, 18.16s/it]  0%|          | 2/500 [00:35<2:25:49, 17.57s/it]  1%|          | 3/500 [00:52<2:23:54, 17.37s/it]  1%|          | 4/500 [01:09<2:22:08, 17.19s/it]  1%|          | 5/500 [01:25<2:19:27, 16.90s/it]  1%|          | 6/500 [01:42<2:18:42, 16.85s/it]  1%|▏         | 7/500 [01:58<2:16:39, 16.63s/it]  2%|▏         | 8/500 [02:14<2:14:18, 16.38s/it]  2%|▏         | 9/500 [02:31<2:15:43, 16.59s/it]  2%|▏         | 10/500 [02:52<2:26:19, 17.92s/it]  2%|▏         | 11/500 [03:09<2:22:44, 17.51s/it]  2%|▏         | 12/500 [03:25<2:20:05, 17.22s/it]  3%|▎         | 13/500 [03:42<2:18:21, 17.05s/it]  3%|▎         | 14/500 [03:59<2:18:12, 17.06s/it]  3%|▎         | 15/500 [04:16<2:17:42, 17.04s/it]  3%|▎         | 16/500 [04:33<2:17:37, 17.06s/it]  3%|▎         | 17/500 [04:55<2:28:45, 18.48s/it]  4%|▎         | 18/500 [05:11<2:23:50, 17.90s/it]  4%|▍         | 19/500 [05:28<2:20:58, 17.59s/it]  4%|▍         | 20/500 [05:45<2:19:48, 17.48s/it]  4%|▍         | 21/500 [06:02<2:17:24, 17.21s/it]  4%|▍         | 21/500 [06:02<2:17:48, 17.26s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.238 MB uploadedwandb: - 0.021 MB of 0.238 MB uploadedwandb: \ 0.238 MB of 0.238 MB uploadedwandb: | 0.238 MB of 0.238 MB uploadedwandb: / 0.238 MB of 0.238 MB uploadedwandb: - 0.238 MB of 0.238 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁
wandb: train_accuracy ▁▇█▆▆▃▁▂▂▃▂▃▄▄▄▃▆▂▄▇▇
wandb:     train_loss ▃▄▃▂▃▅▃▄▄▄▄▂▄▄▄▄▄█▁▄▁
wandb:   val_accuracy ▂██▆▅▄▂▂▂▂▂▂▂▂▁▂▁▄▄▃▃
wandb:       val_loss ▃▂▂▄▁▃▃▃▃▃▃▂▃▃▃▄▃█▇▅▇
wandb: 
wandb: Run summary:
wandb:          epoch 20
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.62407
wandb:     train_loss 0.00861
wandb:   val_accuracy 0.43333
wandb:       val_loss 2.83429
wandb: 
wandb: 🚀 View run wobbly-sun-510 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9qwtdgug
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_025244-9qwtdgug/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_025928-83fhkmtb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-lake-512
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/83fhkmtb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:41:47, 19.45s/it]  0%|          | 2/500 [00:36<2:27:37, 17.79s/it]  1%|          | 3/500 [00:52<2:22:08, 17.16s/it]  1%|          | 4/500 [01:08<2:18:01, 16.70s/it]  1%|          | 5/500 [01:23<2:14:11, 16.27s/it]  1%|          | 6/500 [01:39<2:12:33, 16.10s/it]  1%|▏         | 7/500 [01:54<2:09:45, 15.79s/it]  2%|▏         | 8/500 [02:10<2:09:05, 15.74s/it]  2%|▏         | 9/500 [02:26<2:09:18, 15.80s/it]  2%|▏         | 10/500 [02:41<2:08:15, 15.71s/it]  2%|▏         | 11/500 [02:57<2:07:19, 15.62s/it]  2%|▏         | 12/500 [03:17<2:17:37, 16.92s/it]  3%|▎         | 13/500 [03:33<2:15:21, 16.68s/it]  3%|▎         | 14/500 [03:49<2:13:47, 16.52s/it]  3%|▎         | 15/500 [04:05<2:11:21, 16.25s/it]  3%|▎         | 16/500 [04:20<2:09:33, 16.06s/it]  3%|▎         | 17/500 [04:36<2:08:33, 15.97s/it]  4%|▎         | 18/500 [04:52<2:07:16, 15.84s/it]  4%|▍         | 19/500 [05:07<2:06:12, 15.74s/it]  4%|▍         | 20/500 [05:23<2:07:21, 15.92s/it]  4%|▍         | 21/500 [05:40<2:08:55, 16.15s/it]  4%|▍         | 22/500 [05:57<2:09:21, 16.24s/it]  5%|▍         | 23/500 [06:13<2:09:24, 16.28s/it]  5%|▍         | 24/500 [06:33<2:18:41, 17.48s/it]  5%|▌         | 25/500 [06:49<2:14:44, 17.02s/it]  5%|▌         | 26/500 [07:04<2:10:14, 16.49s/it]  5%|▌         | 27/500 [07:20<2:08:48, 16.34s/it]  6%|▌         | 28/500 [07:36<2:07:14, 16.18s/it]  6%|▌         | 29/500 [07:53<2:07:45, 16.27s/it]  6%|▌         | 29/500 [07:53<2:08:07, 16.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.137 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▅▄▆▇▇▇▇▆█▇▇██▇▁▄▅▆▇▇▇█▇██▄▇
wandb:     train_loss ▇▇▆▇▆▄▅█▅▆▄▄▂▃▆▅▁▅▆▄▅▅▄▆▆▃▂▂▆
wandb:   val_accuracy ▂▂▅▂▇▅█▇█▇█▇▇▇▇▇▁▃▂▅▆▆▇▇▆▇▆▄▇
wandb:       val_loss ▃▃▃▃▂▂▃▃▂▂▂▂▁▂▃▂█▄▂▃▂▂▂▂▃▃▃▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.65973
wandb:     train_loss 1.02707
wandb:   val_accuracy 0.56222
wandb:       val_loss 0.62272
wandb: 
wandb: 🚀 View run stilted-lake-512 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/83fhkmtb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_025928-83fhkmtb/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_030811-taxlkhfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-wood-513
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/taxlkhfv
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:46, 17.89s/it]  0%|          | 2/500 [00:39<2:44:48, 19.86s/it]  1%|          | 3/500 [00:55<2:32:47, 18.45s/it]  1%|          | 4/500 [01:12<2:24:53, 17.53s/it]  1%|          | 5/500 [01:27<2:19:39, 16.93s/it]  1%|          | 6/500 [01:44<2:18:56, 16.88s/it]  1%|▏         | 7/500 [02:01<2:17:47, 16.77s/it]  2%|▏         | 8/500 [02:18<2:17:58, 16.83s/it]  2%|▏         | 9/500 [02:34<2:15:14, 16.53s/it]  2%|▏         | 10/500 [02:50<2:16:06, 16.67s/it]  2%|▏         | 11/500 [03:07<2:16:22, 16.73s/it]  2%|▏         | 12/500 [03:23<2:13:04, 16.36s/it]  3%|▎         | 13/500 [03:39<2:11:53, 16.25s/it]  3%|▎         | 14/500 [03:54<2:09:19, 15.97s/it]  3%|▎         | 15/500 [04:11<2:11:39, 16.29s/it]  3%|▎         | 16/500 [04:27<2:08:59, 15.99s/it]  3%|▎         | 17/500 [04:42<2:06:50, 15.76s/it]  4%|▎         | 18/500 [04:57<2:06:04, 15.69s/it]  4%|▍         | 19/500 [05:13<2:05:51, 15.70s/it]  4%|▍         | 20/500 [05:29<2:05:18, 15.66s/it]  4%|▍         | 21/500 [05:49<2:16:46, 17.13s/it]  4%|▍         | 22/500 [06:05<2:13:53, 16.81s/it]  5%|▍         | 23/500 [06:20<2:08:34, 16.17s/it]  5%|▍         | 24/500 [06:34<2:04:10, 15.65s/it]  5%|▌         | 25/500 [06:54<2:13:18, 16.84s/it]  5%|▌         | 26/500 [07:09<2:09:29, 16.39s/it]  5%|▌         | 27/500 [07:24<2:06:14, 16.01s/it]  6%|▌         | 28/500 [07:40<2:04:58, 15.89s/it]  6%|▌         | 29/500 [07:55<2:03:47, 15.77s/it]  6%|▌         | 30/500 [08:11<2:02:13, 15.60s/it]  6%|▌         | 31/500 [08:26<2:00:53, 15.46s/it]  6%|▋         | 32/500 [08:41<2:00:39, 15.47s/it]  7%|▋         | 33/500 [08:57<1:59:42, 15.38s/it]  7%|▋         | 34/500 [09:12<1:58:45, 15.29s/it]  7%|▋         | 35/500 [09:27<1:58:44, 15.32s/it]  7%|▋         | 36/500 [09:42<1:57:45, 15.23s/it]  7%|▋         | 37/500 [10:02<2:09:02, 16.72s/it]  8%|▊         | 38/500 [10:17<2:05:22, 16.28s/it]  8%|▊         | 38/500 [10:22<2:06:06, 16.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.020 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▆▃▅▃▅▅▅▃▇▄▅▅▄▅▇▅▅▇▆▇▆▆▇▇▇█▇▄▇▇▇▇█▇▇
wandb:     train_loss ▃▃▃▃▂▂▂▂▂▃█▂▂▂▃▄▂▂▂▄▁▁▁▁▁▂▁▁▁▁▃▁█▁▁▁▁▁
wandb:   val_accuracy ▁▁▅█▃█▂▆▅▃▂▅▃▃▄▂▄▄▄▃▄▄▆▄▁▄▃▃▇▄▁▃▅▄▅▇▅▅
wandb:       val_loss ▂▂▂▂▂▁▃▂▁▂▃▂█▁▃▂▃▃▂▄▃▁▁▂▁▃▃▂▁▃▅▂▃▂█▁▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.90045
wandb:     train_loss 0.02174
wandb:   val_accuracy 0.47778
wandb:       val_loss 0.11646
wandb: 
wandb: 🚀 View run efficient-wood-513 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/taxlkhfv
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_030811-taxlkhfv/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_031922-efovfqjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-cloud-515
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/efovfqjr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:09:10, 22.75s/it]  0%|          | 2/500 [00:41<2:48:11, 20.26s/it]  1%|          | 3/500 [00:58<2:36:15, 18.86s/it]  1%|          | 4/500 [01:20<2:44:50, 19.94s/it]  1%|          | 5/500 [01:35<2:32:00, 18.42s/it]  1%|          | 6/500 [01:51<2:24:00, 17.49s/it]  1%|▏         | 7/500 [02:07<2:19:17, 16.95s/it]  2%|▏         | 8/500 [02:28<2:29:34, 18.24s/it]  2%|▏         | 9/500 [02:44<2:24:21, 17.64s/it]  2%|▏         | 10/500 [03:01<2:20:57, 17.26s/it]  2%|▏         | 11/500 [03:18<2:20:17, 17.21s/it]  2%|▏         | 12/500 [03:34<2:18:09, 16.99s/it]  3%|▎         | 13/500 [03:51<2:17:50, 16.98s/it]  3%|▎         | 14/500 [04:08<2:16:15, 16.82s/it]  3%|▎         | 15/500 [04:24<2:15:17, 16.74s/it]  3%|▎         | 16/500 [04:41<2:15:17, 16.77s/it]  3%|▎         | 17/500 [05:02<2:25:40, 18.10s/it]  4%|▎         | 18/500 [05:19<2:22:37, 17.75s/it]  4%|▍         | 19/500 [05:35<2:18:01, 17.22s/it]  4%|▍         | 20/500 [05:51<2:14:36, 16.83s/it]  4%|▍         | 21/500 [06:11<2:22:53, 17.90s/it]  4%|▍         | 22/500 [06:27<2:17:37, 17.28s/it]  5%|▍         | 23/500 [06:43<2:13:43, 16.82s/it]  5%|▍         | 24/500 [06:59<2:10:43, 16.48s/it]  5%|▌         | 25/500 [07:15<2:09:57, 16.42s/it]  5%|▌         | 26/500 [07:31<2:08:42, 16.29s/it]  5%|▌         | 27/500 [07:48<2:09:25, 16.42s/it]  6%|▌         | 28/500 [08:03<2:07:40, 16.23s/it]  6%|▌         | 29/500 [08:24<2:17:11, 17.48s/it]  6%|▌         | 30/500 [08:45<2:24:53, 18.50s/it]  6%|▌         | 31/500 [09:01<2:18:47, 17.76s/it]  6%|▋         | 32/500 [09:17<2:14:20, 17.22s/it]  7%|▋         | 33/500 [09:32<2:10:15, 16.73s/it]  7%|▋         | 34/500 [09:48<2:07:11, 16.38s/it]  7%|▋         | 35/500 [10:04<2:05:32, 16.20s/it]  7%|▋         | 36/500 [10:19<2:04:15, 16.07s/it]  7%|▋         | 37/500 [10:35<2:03:50, 16.05s/it]  8%|▊         | 38/500 [10:51<2:02:54, 15.96s/it]  8%|▊         | 39/500 [11:07<2:02:25, 15.93s/it]  8%|▊         | 40/500 [11:23<2:01:45, 15.88s/it]  8%|▊         | 41/500 [11:39<2:01:18, 15.86s/it]  8%|▊         | 42/500 [11:55<2:01:20, 15.90s/it]  9%|▊         | 43/500 [12:10<2:00:42, 15.85s/it]  9%|▊         | 43/500 [12:10<2:09:27, 17.00s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.308 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▂▁▄▅▅▅▆▃▆▄▅▅▇█▇█▇██████▆█▇███▇███████▇██
wandb:     train_loss ▃▂▃▂▃▂▁▂▁▁█▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:   val_accuracy ▁▁▅█▇█▇▂▅▅▅▅▅▆▅▇▇▆▆▆▆▆▇▃▇▇▆▆▅▅▅▅▆▇▆▇▇▅▇▅
wandb:       val_loss ▁▁▁▁▁▁▂▂▁▂▂▂▁▃▁▃▄▁▃▁▁▂▆▁▄▁▂▁▆▁▂▂█▂▁▂▃▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.98366
wandb:     train_loss 0.00026
wandb:   val_accuracy 0.53111
wandb:       val_loss 0.00938
wandb: 
wandb: 🚀 View run earnest-cloud-515 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/efovfqjr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_031922-efovfqjr/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_033208-9n2oactz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-dust-516
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9n2oactz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:37:19, 18.92s/it]  0%|          | 2/500 [00:34<2:20:37, 16.94s/it]  1%|          | 3/500 [00:50<2:16:22, 16.46s/it]  1%|          | 4/500 [01:10<2:28:23, 17.95s/it]  1%|          | 5/500 [01:26<2:21:31, 17.15s/it]  1%|          | 6/500 [01:41<2:16:35, 16.59s/it]  1%|▏         | 7/500 [01:57<2:14:45, 16.40s/it]  2%|▏         | 8/500 [02:13<2:12:06, 16.11s/it]  2%|▏         | 9/500 [02:29<2:11:08, 16.03s/it]  2%|▏         | 10/500 [02:45<2:10:39, 16.00s/it]  2%|▏         | 11/500 [03:00<2:09:47, 15.92s/it]  2%|▏         | 12/500 [03:16<2:07:43, 15.70s/it]  3%|▎         | 13/500 [03:35<2:17:46, 16.97s/it]  3%|▎         | 14/500 [03:51<2:14:46, 16.64s/it]  3%|▎         | 15/500 [04:07<2:12:04, 16.34s/it]  3%|▎         | 16/500 [04:27<2:21:06, 17.49s/it]  3%|▎         | 17/500 [04:43<2:17:22, 17.07s/it]  4%|▎         | 18/500 [04:59<2:13:46, 16.65s/it]  4%|▍         | 19/500 [05:15<2:10:59, 16.34s/it]  4%|▍         | 20/500 [05:31<2:10:31, 16.32s/it]  4%|▍         | 21/500 [05:46<2:08:26, 16.09s/it]  4%|▍         | 22/500 [06:07<2:18:52, 17.43s/it]  5%|▍         | 23/500 [06:26<2:21:41, 17.82s/it]  5%|▍         | 24/500 [06:42<2:17:46, 17.37s/it]  5%|▌         | 25/500 [06:58<2:15:18, 17.09s/it]  5%|▌         | 26/500 [07:15<2:13:14, 16.87s/it]  5%|▌         | 27/500 [07:31<2:12:32, 16.81s/it]  6%|▌         | 28/500 [07:47<2:10:01, 16.53s/it]  6%|▌         | 29/500 [08:04<2:10:12, 16.59s/it]  6%|▌         | 29/500 [08:04<2:11:09, 16.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▃▅▄▅▆▆▇▇▇▇▇▇▇█▇█▇▇▇██▇████
wandb:     train_loss ▇▆▆▇▆▄▅█▅▅▄▃▂▂▆▅▄▄▄▄▅▃▅▅▇▄▁▁▄
wandb:   val_accuracy ▂▄▄▁▄▂▆▆▅▆▇▆▇█▇▇█▇▇█▇▇▇█▆▇▇▇▇
wandb:       val_loss ▆▇▇▆▄▅▆▆▄▃▅▄▁▆█▃▇▅▂▇▄▆▅▃█▆▆▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.78306
wandb:     train_loss 0.67765
wandb:   val_accuracy 0.57333
wandb:       val_loss 0.69923
wandb: 
wandb: 🚀 View run hopeful-dust-516 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9n2oactz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_033208-9n2oactz/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_034050-d0ljvcgu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-haze-518
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d0ljvcgu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:34:43, 18.61s/it]  0%|          | 2/500 [00:34<2:23:18, 17.27s/it]  1%|          | 3/500 [00:51<2:19:47, 16.88s/it]  1%|          | 4/500 [01:07<2:18:35, 16.77s/it]  1%|          | 5/500 [01:29<2:31:15, 18.33s/it]  1%|          | 6/500 [01:45<2:25:48, 17.71s/it]  1%|▏         | 7/500 [02:01<2:21:31, 17.22s/it]  2%|▏         | 8/500 [02:23<2:33:05, 18.67s/it]  2%|▏         | 9/500 [02:40<2:27:13, 17.99s/it]  2%|▏         | 10/500 [02:56<2:23:30, 17.57s/it]  2%|▏         | 11/500 [03:17<2:32:00, 18.65s/it]  2%|▏         | 12/500 [03:35<2:28:57, 18.31s/it]  3%|▎         | 13/500 [03:52<2:24:44, 17.83s/it]  3%|▎         | 14/500 [04:12<2:30:59, 18.64s/it]  3%|▎         | 15/500 [04:29<2:25:25, 17.99s/it]  3%|▎         | 16/500 [04:45<2:21:00, 17.48s/it]  3%|▎         | 17/500 [05:06<2:28:47, 18.48s/it]  4%|▎         | 18/500 [05:23<2:25:16, 18.08s/it]  4%|▍         | 19/500 [05:39<2:20:19, 17.51s/it]  4%|▍         | 20/500 [06:00<2:29:42, 18.71s/it]  4%|▍         | 21/500 [06:17<2:23:43, 18.00s/it]  4%|▍         | 22/500 [06:33<2:19:27, 17.51s/it]  5%|▍         | 23/500 [06:49<2:14:44, 16.95s/it]  5%|▍         | 24/500 [07:06<2:14:28, 16.95s/it]  5%|▍         | 24/500 [07:06<2:20:54, 17.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.021 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁
wandb: train_accuracy ▁▅▄▆▃▅▅▁▅▅▅▇▇▇█▅█▇▆▆▅▆▇█
wandb:     train_loss ▄▃▃▃▂▂▃▁▂▂▅█▁▂▂▄▂▂▁▄▁▁▁▂
wandb:   val_accuracy ▁▄▅█▃▆▂▁▃▁▂▅▄▄▄▂▅▄▃▃▃▃▄▅
wandb:       val_loss ▂▁▂▂▁▁▂▁▂▂▂▂█▁▂▂▂▄▂▄▂▁▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 23
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.84101
wandb:     train_loss 0.30781
wandb:   val_accuracy 0.52
wandb:       val_loss 3.75481
wandb: 
wandb: 🚀 View run rich-haze-518 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d0ljvcgu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_034050-d0ljvcgu/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_034858-td2tedwi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-firebrand-519
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/td2tedwi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:43, 17.64s/it]  0%|          | 2/500 [00:34<2:22:03, 17.12s/it]  1%|          | 3/500 [00:51<2:21:06, 17.04s/it]  1%|          | 4/500 [01:08<2:19:55, 16.93s/it]  1%|          | 5/500 [01:24<2:17:00, 16.61s/it]  1%|          | 6/500 [01:40<2:15:09, 16.42s/it]  1%|▏         | 7/500 [01:56<2:13:21, 16.23s/it]  2%|▏         | 8/500 [02:12<2:12:46, 16.19s/it]  2%|▏         | 9/500 [02:29<2:14:20, 16.42s/it]  2%|▏         | 10/500 [02:44<2:11:55, 16.15s/it]  2%|▏         | 11/500 [03:01<2:12:26, 16.25s/it]  2%|▏         | 12/500 [03:16<2:11:10, 16.13s/it]  3%|▎         | 13/500 [03:33<2:13:06, 16.40s/it]  3%|▎         | 14/500 [03:53<2:20:57, 17.40s/it]  3%|▎         | 15/500 [04:08<2:14:33, 16.65s/it]  3%|▎         | 16/500 [04:28<2:22:35, 17.68s/it]  3%|▎         | 17/500 [04:44<2:17:58, 17.14s/it]  4%|▎         | 18/500 [05:00<2:14:40, 16.76s/it]  4%|▍         | 19/500 [05:20<2:22:54, 17.83s/it]  4%|▍         | 20/500 [05:36<2:18:07, 17.27s/it]  4%|▍         | 21/500 [05:52<2:13:33, 16.73s/it]  4%|▍         | 22/500 [06:12<2:21:27, 17.76s/it]  5%|▍         | 23/500 [06:28<2:18:00, 17.36s/it]  5%|▍         | 24/500 [06:49<2:26:13, 18.43s/it]  5%|▌         | 25/500 [07:05<2:20:10, 17.71s/it]  5%|▌         | 26/500 [07:21<2:15:38, 17.17s/it]  5%|▌         | 26/500 [07:26<2:15:38, 17.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.021 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▂▁▂▁▇▂▃▃▂▄▇▃▆▄▆▆▆▆██▇▇▇▇▆▅
wandb:     train_loss ▅▂▄█▃▁▃▆█▁▁▃▁▁▁▂▃▁▁▂▄▁▁▃▁▆
wandb:   val_accuracy ▂▁▁▁█▁▅▄▁▃▇▁▆▄▆▅▅▆▆█▆█▄▇▃▅
wandb:       val_loss ▂▂▃▂▁▂▂▂▁▁▃▃█▂▃▂▄▁▂▂▅▁▁▃▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.66865
wandb:     train_loss 1.87666
wandb:   val_accuracy 0.49333
wandb:       val_loss 3.1613
wandb: 
wandb: 🚀 View run worldly-firebrand-519 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/td2tedwi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_034858-td2tedwi/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_035708-frt64scc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-wildflower-521
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/frt64scc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:21<3:01:33, 21.83s/it]  0%|          | 2/500 [00:37<2:33:04, 18.44s/it]  1%|          | 3/500 [00:59<2:44:50, 19.90s/it]  1%|          | 4/500 [01:15<2:30:41, 18.23s/it]  1%|          | 5/500 [01:30<2:22:24, 17.26s/it]  1%|          | 6/500 [01:46<2:18:23, 16.81s/it]  1%|▏         | 7/500 [02:02<2:14:31, 16.37s/it]  2%|▏         | 8/500 [02:17<2:11:56, 16.09s/it]  2%|▏         | 9/500 [02:37<2:22:22, 17.40s/it]  2%|▏         | 10/500 [02:59<2:32:56, 18.73s/it]  2%|▏         | 11/500 [03:16<2:27:54, 18.15s/it]  2%|▏         | 12/500 [03:32<2:21:13, 17.36s/it]  3%|▎         | 13/500 [03:47<2:17:22, 16.93s/it]  3%|▎         | 14/500 [04:03<2:14:33, 16.61s/it]  3%|▎         | 15/500 [04:19<2:11:37, 16.28s/it]  3%|▎         | 16/500 [04:35<2:10:39, 16.20s/it]  3%|▎         | 17/500 [04:50<2:08:54, 16.01s/it]  4%|▎         | 18/500 [05:06<2:08:05, 15.94s/it]  4%|▍         | 19/500 [05:22<2:07:51, 15.95s/it]  4%|▍         | 20/500 [05:38<2:06:23, 15.80s/it]  4%|▍         | 21/500 [05:53<2:05:31, 15.72s/it]  4%|▍         | 22/500 [06:09<2:05:06, 15.70s/it]  5%|▍         | 23/500 [06:24<2:04:36, 15.67s/it]  5%|▍         | 24/500 [06:40<2:04:29, 15.69s/it]  5%|▌         | 25/500 [06:56<2:04:51, 15.77s/it]  5%|▌         | 26/500 [07:12<2:04:01, 15.70s/it]  5%|▌         | 27/500 [07:31<2:12:26, 16.80s/it]  6%|▌         | 28/500 [07:47<2:10:49, 16.63s/it]  6%|▌         | 29/500 [08:03<2:09:29, 16.50s/it]  6%|▌         | 29/500 [08:04<2:11:01, 16.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.137 MB of 0.313 MB uploadedwandb: | 0.137 MB of 0.313 MB uploadedwandb: / 0.137 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▅▄▅▅▆▆▆▆▇▇▇▇▅▇▇▇▇▇▇█▇▇█████
wandb:     train_loss ▇▆▆▆▆▄▅█▅▅▄▄▂▂▆▅▄▅▆▅▆▄▄▇▆▄▁▁▄
wandb:   val_accuracy ▁▅▄▁▃▂▆▇▄▇▇▇▇▇▆▇▇▇▇▇█▇▇█▇█▇▇█
wandb:       val_loss ▆▇▇▆▅▅▇▆▄▃▅▅▁▆▇▃▇▅▃▇▅▆▅▂█▅▅▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.79941
wandb:     train_loss 0.66559
wandb:   val_accuracy 0.59333
wandb:       val_loss 0.77068
wandb: 
wandb: 🚀 View run apricot-wildflower-521 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/frt64scc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_035708-frt64scc/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_040551-4blj5vxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-breeze-522
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4blj5vxh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:37, 17.51s/it]  0%|          | 2/500 [00:36<2:33:45, 18.52s/it]  1%|          | 3/500 [00:52<2:23:49, 17.36s/it]  1%|          | 4/500 [01:08<2:18:26, 16.75s/it]  1%|          | 5/500 [01:29<2:32:03, 18.43s/it]  1%|          | 6/500 [01:45<2:24:05, 17.50s/it]  1%|▏         | 7/500 [02:04<2:28:39, 18.09s/it]  2%|▏         | 8/500 [02:21<2:23:45, 17.53s/it]  2%|▏         | 9/500 [02:37<2:19:20, 17.03s/it]  2%|▏         | 10/500 [02:53<2:17:08, 16.79s/it]  2%|▏         | 11/500 [03:09<2:13:46, 16.41s/it]  2%|▏         | 12/500 [03:24<2:11:54, 16.22s/it]  3%|▎         | 13/500 [03:40<2:09:52, 16.00s/it]  3%|▎         | 14/500 [03:55<2:08:52, 15.91s/it]  3%|▎         | 15/500 [04:12<2:09:29, 16.02s/it]  3%|▎         | 16/500 [04:32<2:20:12, 17.38s/it]  3%|▎         | 17/500 [04:49<2:18:38, 17.22s/it]  4%|▎         | 18/500 [05:05<2:15:59, 16.93s/it]  4%|▍         | 19/500 [05:21<2:12:14, 16.50s/it]  4%|▍         | 20/500 [05:36<2:08:19, 16.04s/it]  4%|▍         | 21/500 [05:51<2:05:31, 15.72s/it]  4%|▍         | 22/500 [06:07<2:05:06, 15.70s/it]  5%|▍         | 23/500 [06:22<2:04:41, 15.68s/it]  5%|▍         | 24/500 [06:38<2:04:05, 15.64s/it]  5%|▌         | 25/500 [06:53<2:03:12, 15.56s/it]  5%|▌         | 26/500 [07:08<2:02:28, 15.50s/it]  5%|▌         | 27/500 [07:24<2:02:10, 15.50s/it]  6%|▌         | 28/500 [07:39<2:02:04, 15.52s/it]  6%|▌         | 29/500 [07:55<2:02:04, 15.55s/it]  6%|▌         | 30/500 [08:11<2:01:38, 15.53s/it]  6%|▌         | 31/500 [08:26<2:01:03, 15.49s/it]  6%|▋         | 32/500 [08:46<2:10:53, 16.78s/it]  7%|▋         | 33/500 [09:01<2:07:48, 16.42s/it]  7%|▋         | 34/500 [09:22<2:17:08, 17.66s/it]  7%|▋         | 35/500 [09:37<2:11:06, 16.92s/it]  7%|▋         | 36/500 [09:52<2:07:02, 16.43s/it]  7%|▋         | 37/500 [10:08<2:04:28, 16.13s/it]  8%|▊         | 38/500 [10:23<2:02:50, 15.95s/it]  8%|▊         | 39/500 [10:39<2:01:09, 15.77s/it]  8%|▊         | 40/500 [10:55<2:01:19, 15.83s/it]  8%|▊         | 41/500 [11:10<1:59:52, 15.67s/it]  8%|▊         | 42/500 [11:25<1:59:00, 15.59s/it]  9%|▊         | 43/500 [11:41<1:59:14, 15.65s/it]  9%|▉         | 44/500 [11:57<1:58:31, 15.60s/it]  9%|▉         | 45/500 [12:12<1:58:52, 15.68s/it]  9%|▉         | 46/500 [12:28<1:58:24, 15.65s/it]  9%|▉         | 47/500 [12:44<1:57:48, 15.60s/it] 10%|▉         | 48/500 [12:59<1:56:50, 15.51s/it] 10%|▉         | 49/500 [13:14<1:56:32, 15.50s/it] 10%|█         | 50/500 [13:30<1:56:14, 15.50s/it] 10%|█         | 51/500 [13:45<1:55:50, 15.48s/it] 10%|█         | 52/500 [14:01<1:55:19, 15.45s/it] 11%|█         | 53/500 [14:16<1:54:31, 15.37s/it] 11%|█         | 53/500 [14:16<2:00:22, 16.16s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.029 MB uploadedwandb: | 0.011 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▅▆▂▅▄▃▃▃▃▂▅▅▇▃▄▇▆▆▇▆▇▆▅▇▆██▆▇▆▂▅▆▆▅▆▇▇▆
wandb:     train_loss ▂▂▂▂▁▁▂▂▄▂▂▂▄▁▂▂▁▁▁▁▁▅▅▄█▁▁▃▁▁▁▁▁▄▁▅▁▁▇▁
wandb:   val_accuracy ▂▇█▄█▅▃▂▂▄▁▃▃▅▃▁▅▃▂▄▃▅▄▂▆▂▅█▃▂▃▃▂▂▂▂▃▂▅▃
wandb:       val_loss ▂▂▂▂▁▄▁▃▃▁▃▄▅▄▂▂▁▂▁▄▂▂▃▇▆▁█▁▂▅▁▁▃▄█▇▆▅▄▄
wandb: 
wandb: Run summary:
wandb:          epoch 52
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.70877
wandb:     train_loss 0.00983
wandb:   val_accuracy 0.37111
wandb:       val_loss 2.76276
wandb: 
wandb: 🚀 View run northern-breeze-522 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4blj5vxh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_040551-4blj5vxh/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_042052-jm28u3nu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-forest-524
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jm28u3nu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:23:43, 17.28s/it]  0%|          | 2/500 [00:37<2:36:35, 18.87s/it]  1%|          | 3/500 [00:53<2:24:42, 17.47s/it]  1%|          | 4/500 [01:08<2:17:12, 16.60s/it]  1%|          | 5/500 [01:28<2:27:43, 17.91s/it]  1%|          | 6/500 [01:44<2:21:12, 17.15s/it]  1%|▏         | 7/500 [01:59<2:16:36, 16.63s/it]  2%|▏         | 8/500 [02:20<2:26:27, 17.86s/it]  2%|▏         | 9/500 [02:35<2:19:40, 17.07s/it]  2%|▏         | 10/500 [02:51<2:16:19, 16.69s/it]  2%|▏         | 11/500 [03:07<2:13:30, 16.38s/it]  2%|▏         | 12/500 [03:22<2:11:28, 16.16s/it]  3%|▎         | 13/500 [03:38<2:10:35, 16.09s/it]  3%|▎         | 14/500 [03:54<2:09:10, 15.95s/it]  3%|▎         | 15/500 [04:10<2:09:26, 16.01s/it]  3%|▎         | 16/500 [04:26<2:08:14, 15.90s/it]  3%|▎         | 17/500 [04:41<2:07:23, 15.83s/it]  4%|▎         | 18/500 [04:57<2:06:38, 15.76s/it]  4%|▍         | 19/500 [05:15<2:11:22, 16.39s/it]  4%|▍         | 20/500 [05:31<2:09:35, 16.20s/it]  4%|▍         | 21/500 [05:47<2:09:20, 16.20s/it]  4%|▍         | 21/500 [05:47<2:12:00, 16.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.021 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁
wandb: train_accuracy ▁▄▆▅▅▅▆▅▆▅▆▆▆▇▆▇▅▄▄██
wandb:     train_loss ▄▄▄▂█▂▄▃▁▂▂▃▂▁▃▁▁▁▃▄▄
wandb:   val_accuracy ▁█▆▇▆▅▆▅▆▄▄▆▆▇▆▆▄▂▂▆▆
wandb:       val_loss ▃▂▂▃▁▂▂▂▂▁▂█▂▁▆▃▁▄▅▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 20
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.88707
wandb:     train_loss 0.96233
wandb:   val_accuracy 0.54889
wandb:       val_loss 1.35836
wandb: 
wandb: 🚀 View run glowing-forest-524 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jm28u3nu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_042052-jm28u3nu/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_042724-rgymgzn7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-shadow-525
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rgymgzn7
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:18, 17.59s/it]  0%|          | 2/500 [00:37<2:35:46, 18.77s/it]  1%|          | 3/500 [00:52<2:24:06, 17.40s/it]  1%|          | 4/500 [01:08<2:16:25, 16.50s/it]  1%|          | 5/500 [01:23<2:12:43, 16.09s/it]  1%|          | 6/500 [01:39<2:11:08, 15.93s/it]  1%|▏         | 7/500 [01:54<2:09:15, 15.73s/it]  2%|▏         | 8/500 [02:10<2:08:47, 15.71s/it]  2%|▏         | 9/500 [02:25<2:08:38, 15.72s/it]  2%|▏         | 10/500 [02:41<2:07:40, 15.63s/it]  2%|▏         | 11/500 [03:01<2:18:12, 16.96s/it]  2%|▏         | 12/500 [03:16<2:14:10, 16.50s/it]  3%|▎         | 13/500 [03:36<2:23:22, 17.66s/it]  3%|▎         | 14/500 [03:52<2:18:16, 17.07s/it]  3%|▎         | 15/500 [04:07<2:13:37, 16.53s/it]  3%|▎         | 16/500 [04:23<2:10:37, 16.19s/it]  3%|▎         | 17/500 [04:38<2:08:13, 15.93s/it]  4%|▎         | 18/500 [04:54<2:06:36, 15.76s/it]  4%|▍         | 19/500 [05:09<2:05:34, 15.66s/it]  4%|▍         | 20/500 [05:29<2:16:34, 17.07s/it]  4%|▍         | 21/500 [05:48<2:20:36, 17.61s/it]  4%|▍         | 22/500 [06:04<2:14:57, 16.94s/it]  5%|▍         | 23/500 [06:19<2:10:39, 16.44s/it]  5%|▍         | 24/500 [06:34<2:07:32, 16.08s/it]  5%|▌         | 25/500 [06:50<2:07:18, 16.08s/it]  5%|▌         | 26/500 [07:06<2:05:27, 15.88s/it]  5%|▌         | 27/500 [07:26<2:14:54, 17.11s/it]  6%|▌         | 28/500 [07:41<2:10:19, 16.57s/it]  6%|▌         | 29/500 [07:57<2:08:33, 16.38s/it]  6%|▌         | 29/500 [08:01<2:10:27, 16.62s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▂▄▅▅▆▇▅▆▆▆▆▇▆▇▇▆▇▇▇▆▇▅▅▇██
wandb:     train_loss █▄▇▇▇▆▆█▅▄▆▇▂▂▅▄▇▄▆▄▆▆▃▇▇▂▁▁▄
wandb:   val_accuracy ▂▁▁▁▃▆▆▇▇▆▇▇▇██▇▇▆█▇█▇██▇▅██▇
wandb:       val_loss ▅▇▅▆▅▅▇▅▄▄▆▅▁▄▇▃▆▅▃▇▄▅▅▃▅█▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.73105
wandb:     train_loss 0.66578
wandb:   val_accuracy 0.62222
wandb:       val_loss 0.55709
wandb: 
wandb: 🚀 View run rural-shadow-525 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rgymgzn7
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_042724-rgymgzn7/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_043611-9je5xrp0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-breeze-526
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9je5xrp0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:22<3:10:41, 22.93s/it]  0%|          | 2/500 [00:38<2:35:02, 18.68s/it]  1%|          | 3/500 [00:54<2:24:53, 17.49s/it]  1%|          | 4/500 [01:10<2:18:30, 16.75s/it]  1%|          | 5/500 [01:26<2:15:09, 16.38s/it]  1%|          | 6/500 [01:41<2:13:12, 16.18s/it]  1%|▏         | 7/500 [01:57<2:11:02, 15.95s/it]  2%|▏         | 8/500 [02:12<2:08:50, 15.71s/it]  2%|▏         | 9/500 [02:27<2:06:54, 15.51s/it]  2%|▏         | 10/500 [02:43<2:06:49, 15.53s/it]  2%|▏         | 11/500 [02:58<2:07:00, 15.58s/it]  2%|▏         | 12/500 [03:14<2:07:04, 15.62s/it]  3%|▎         | 13/500 [03:30<2:07:15, 15.68s/it]  3%|▎         | 14/500 [03:46<2:07:24, 15.73s/it]  3%|▎         | 15/500 [04:01<2:06:44, 15.68s/it]  3%|▎         | 16/500 [04:17<2:06:51, 15.73s/it]  3%|▎         | 17/500 [04:34<2:09:37, 16.10s/it]  4%|▎         | 18/500 [04:50<2:07:54, 15.92s/it]  4%|▍         | 19/500 [05:06<2:07:50, 15.95s/it]  4%|▍         | 20/500 [05:22<2:08:46, 16.10s/it]  4%|▍         | 21/500 [05:38<2:07:49, 16.01s/it]  4%|▍         | 22/500 [05:54<2:08:42, 16.16s/it]  5%|▍         | 23/500 [06:10<2:07:53, 16.09s/it]  5%|▍         | 24/500 [06:26<2:06:57, 16.00s/it]  5%|▌         | 25/500 [06:42<2:06:24, 15.97s/it]  5%|▌         | 26/500 [06:58<2:06:20, 15.99s/it]  5%|▌         | 27/500 [07:14<2:06:28, 16.04s/it]  6%|▌         | 28/500 [07:30<2:05:42, 15.98s/it]  6%|▌         | 29/500 [07:46<2:04:43, 15.89s/it]  6%|▌         | 29/500 [07:46<2:06:15, 16.08s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.027 MB uploadedwandb: | 0.011 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆▆▃▅▂▁▄▃▃▄▃▄█▃▄▆▆▇▇▆▆▆▅▇▇▅▇
wandb:     train_loss ▄▅▄▃▃▂▃▃▃▃█▅▁▂▃▅█▄▂▂▂▂▂▄▄▄▁▁▃
wandb:   val_accuracy ▁▅██▂▇▄▁▄▃▃▃▃▂▇▁▂▄▄▄▄▅▅▄▃▅▅▃▅
wandb:       val_loss ▄▂▃▅▃▂█▄▂▃▅▃▁▄▅▄▄▆▂█▁▃▆▂▃▇▁▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.72214
wandb:     train_loss 0.55272
wandb:   val_accuracy 0.53111
wandb:       val_loss 1.20264
wandb: 
wandb: 🚀 View run fearless-breeze-526 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9je5xrp0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_043611-9je5xrp0/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_044452-c5pl9r16
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-snowflake-528
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c5pl9r16
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:44:48, 27.03s/it]  0%|          | 2/500 [00:50<3:26:33, 24.89s/it]  1%|          | 3/500 [01:13<3:19:43, 24.11s/it]  1%|          | 4/500 [01:39<3:24:39, 24.76s/it]  1%|          | 5/500 [02:02<3:19:40, 24.20s/it]  1%|          | 6/500 [02:25<3:16:35, 23.88s/it]  1%|▏         | 7/500 [02:49<3:14:34, 23.68s/it]  2%|▏         | 8/500 [03:12<3:13:15, 23.57s/it]  2%|▏         | 9/500 [03:35<3:12:52, 23.57s/it]  2%|▏         | 10/500 [03:59<3:11:54, 23.50s/it]  2%|▏         | 11/500 [04:24<3:14:31, 23.87s/it]  2%|▏         | 12/500 [04:47<3:13:23, 23.78s/it]  3%|▎         | 13/500 [05:10<3:11:33, 23.60s/it]  3%|▎         | 14/500 [05:38<3:21:50, 24.92s/it]  3%|▎         | 15/500 [06:02<3:17:52, 24.48s/it]  3%|▎         | 16/500 [06:30<3:25:30, 25.48s/it]  3%|▎         | 17/500 [06:53<3:19:41, 24.81s/it]  4%|▎         | 18/500 [07:16<3:16:32, 24.47s/it]  4%|▍         | 19/500 [07:39<3:12:34, 24.02s/it]  4%|▍         | 20/500 [08:02<3:09:42, 23.71s/it]  4%|▍         | 21/500 [08:26<3:08:39, 23.63s/it]  4%|▍         | 22/500 [08:53<3:16:51, 24.71s/it]  5%|▍         | 23/500 [09:16<3:12:57, 24.27s/it]  5%|▍         | 24/500 [09:39<3:09:21, 23.87s/it]  5%|▌         | 25/500 [10:02<3:06:51, 23.60s/it]  5%|▌         | 26/500 [10:25<3:05:36, 23.50s/it]  5%|▌         | 27/500 [10:49<3:04:11, 23.37s/it]  6%|▌         | 28/500 [11:12<3:03:11, 23.29s/it]  6%|▌         | 29/500 [11:35<3:02:54, 23.30s/it]  6%|▌         | 30/500 [11:58<3:02:16, 23.27s/it]  6%|▌         | 31/500 [12:21<3:01:33, 23.23s/it]  6%|▋         | 32/500 [12:44<3:00:32, 23.15s/it]  7%|▋         | 33/500 [13:07<2:59:55, 23.12s/it]  7%|▋         | 34/500 [13:30<2:58:56, 23.04s/it]  7%|▋         | 35/500 [13:53<2:58:25, 23.02s/it]  7%|▋         | 36/500 [14:16<2:57:34, 22.96s/it]  7%|▋         | 37/500 [14:39<2:56:47, 22.91s/it]  8%|▊         | 38/500 [15:02<2:57:55, 23.11s/it]  8%|▊         | 39/500 [15:26<2:57:53, 23.15s/it]  8%|▊         | 40/500 [15:49<2:57:22, 23.13s/it]  8%|▊         | 41/500 [16:16<3:06:39, 24.40s/it]  8%|▊         | 41/500 [16:16<3:02:13, 23.82s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.134 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▄▄▅▆▆▆▄▆▇▇▇████████████████████▃▅▅▆▆▆▆▇
wandb:     train_loss ▄▄▃▂▁▂█▁▁▂▄▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▂▁▁▁▁
wandb:   val_accuracy ▁▇▇▇▇▆█▆█▅▆█▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇█▇▇▆▇▅▆▆▅▅▅▄▆
wandb:       val_loss ▂▂▂▁▂▂▂▃▁▃▁▁▄▁▁▁▆▃▃▂▃▁▁▁▁▄▁▄█▅▁▃▂▃▁▅▃▃▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.9049
wandb:     train_loss 0.07082
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.24626
wandb: 
wandb: 🚀 View run glad-snowflake-528 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c5pl9r16
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_044452-c5pl9r16/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_050203-5keyw4os
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-mountain-530
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5keyw4os
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:11:40, 30.26s/it]  0%|          | 2/500 [00:54<3:41:50, 26.73s/it]  1%|          | 3/500 [01:19<3:33:34, 25.78s/it]  1%|          | 4/500 [01:43<3:28:01, 25.17s/it]  1%|          | 5/500 [02:07<3:24:04, 24.74s/it]  1%|          | 6/500 [02:31<3:22:34, 24.61s/it]  1%|▏         | 7/500 [02:56<3:22:02, 24.59s/it]  2%|▏         | 8/500 [03:20<3:19:50, 24.37s/it]  2%|▏         | 9/500 [03:44<3:19:09, 24.34s/it]  2%|▏         | 10/500 [04:08<3:17:26, 24.18s/it]  2%|▏         | 11/500 [04:31<3:15:45, 24.02s/it]  2%|▏         | 12/500 [05:00<3:25:49, 25.31s/it]  3%|▎         | 13/500 [05:24<3:21:59, 24.89s/it]  3%|▎         | 14/500 [05:47<3:19:00, 24.57s/it]  3%|▎         | 15/500 [06:11<3:16:43, 24.34s/it]  3%|▎         | 16/500 [06:35<3:14:58, 24.17s/it]  3%|▎         | 17/500 [06:59<3:13:26, 24.03s/it]  4%|▎         | 18/500 [07:26<3:20:30, 24.96s/it]  4%|▍         | 19/500 [07:50<3:17:14, 24.60s/it]  4%|▍         | 20/500 [08:13<3:14:09, 24.27s/it]  4%|▍         | 21/500 [08:37<3:12:19, 24.09s/it]  4%|▍         | 22/500 [09:00<3:10:47, 23.95s/it]  5%|▍         | 23/500 [09:24<3:09:40, 23.86s/it]  5%|▍         | 24/500 [09:48<3:10:20, 23.99s/it]  5%|▌         | 25/500 [10:12<3:09:36, 23.95s/it]  5%|▌         | 26/500 [10:36<3:09:32, 23.99s/it]  5%|▌         | 27/500 [11:01<3:10:16, 24.14s/it]  6%|▌         | 28/500 [11:25<3:09:41, 24.11s/it]  6%|▌         | 29/500 [11:51<3:15:06, 24.85s/it]  6%|▌         | 30/500 [12:15<3:12:23, 24.56s/it]  6%|▌         | 30/500 [12:15<3:12:07, 24.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.020 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▃▅▅▆▆▆▆▆▆▆▆▇▇▆▆▇▇▇▇▇▇▇██████
wandb:     train_loss ▅▆▆▆▄▆▆▂▅▃▇▂▂▆▆▄▂▆▂▅█▂▄▁▁▆▃▄▇█
wandb:   val_accuracy ▁▁▂▇▅▆▆▇█▇▇▇▇▇▇▇▇███▇█▇▇██▇█▇▇
wandb:       val_loss ▄▄▃▃▄▄▃▄▄▂▄▄▂▁▃▃▆▃▂▃▂▃▃▃▂▃▃▃█▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.79941
wandb:     train_loss 1.44311
wandb:   val_accuracy 0.53778
wandb:       val_loss 1.68901
wandb: 
wandb: 🚀 View run still-mountain-530 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5keyw4os
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_050203-5keyw4os/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_051500-yitthjl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-valley-532
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yitthjl8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:15, 25.40s/it]  0%|          | 2/500 [00:49<3:26:32, 24.88s/it]  1%|          | 3/500 [01:14<3:24:45, 24.72s/it]  1%|          | 4/500 [01:38<3:21:44, 24.40s/it]  1%|          | 5/500 [02:02<3:19:22, 24.17s/it]  1%|          | 6/500 [02:26<3:18:33, 24.12s/it]  1%|▏         | 7/500 [02:49<3:17:14, 24.01s/it]  2%|▏         | 8/500 [03:13<3:16:28, 23.96s/it]  2%|▏         | 9/500 [03:40<3:23:17, 24.84s/it]  2%|▏         | 10/500 [04:04<3:20:27, 24.55s/it]  2%|▏         | 11/500 [04:28<3:17:40, 24.25s/it]  2%|▏         | 12/500 [04:57<3:29:14, 25.73s/it]  3%|▎         | 13/500 [05:21<3:24:30, 25.20s/it]  3%|▎         | 14/500 [05:44<3:20:33, 24.76s/it]  3%|▎         | 15/500 [06:09<3:18:46, 24.59s/it]  3%|▎         | 16/500 [06:33<3:17:24, 24.47s/it]  3%|▎         | 17/500 [06:57<3:15:39, 24.31s/it]  4%|▎         | 18/500 [07:20<3:13:32, 24.09s/it]  4%|▍         | 19/500 [07:44<3:12:21, 24.00s/it]  4%|▍         | 20/500 [08:08<3:12:26, 24.05s/it]  4%|▍         | 21/500 [08:32<3:10:56, 23.92s/it]  4%|▍         | 22/500 [08:56<3:10:24, 23.90s/it]  5%|▍         | 23/500 [09:20<3:10:09, 23.92s/it]  5%|▍         | 24/500 [09:44<3:09:53, 23.94s/it]  5%|▌         | 25/500 [10:11<3:16:59, 24.88s/it]  5%|▌         | 26/500 [10:35<3:14:07, 24.57s/it]  5%|▌         | 27/500 [10:59<3:12:23, 24.40s/it]  6%|▌         | 28/500 [11:25<3:15:40, 24.87s/it]  6%|▌         | 29/500 [11:49<3:13:16, 24.62s/it]  6%|▌         | 30/500 [12:13<3:11:39, 24.47s/it]  6%|▌         | 31/500 [12:37<3:10:14, 24.34s/it]  6%|▋         | 32/500 [13:01<3:09:06, 24.24s/it]  7%|▋         | 33/500 [13:24<3:07:22, 24.07s/it]  7%|▋         | 34/500 [13:48<3:06:27, 24.01s/it]  7%|▋         | 35/500 [14:12<3:05:19, 23.91s/it]  7%|▋         | 36/500 [14:36<3:04:11, 23.82s/it]  7%|▋         | 37/500 [15:00<3:04:32, 23.92s/it]  8%|▊         | 38/500 [15:24<3:04:25, 23.95s/it]  8%|▊         | 39/500 [15:50<3:10:17, 24.77s/it]  8%|▊         | 40/500 [16:14<3:08:04, 24.53s/it]  8%|▊         | 41/500 [16:38<3:06:08, 24.33s/it]  8%|▊         | 42/500 [17:02<3:04:11, 24.13s/it]  9%|▊         | 43/500 [17:29<3:10:41, 25.04s/it]  9%|▉         | 44/500 [17:53<3:07:59, 24.73s/it]  9%|▉         | 45/500 [18:18<3:07:04, 24.67s/it]  9%|▉         | 46/500 [18:42<3:05:31, 24.52s/it]  9%|▉         | 47/500 [19:06<3:03:38, 24.32s/it] 10%|▉         | 48/500 [19:30<3:02:18, 24.20s/it] 10%|▉         | 49/500 [19:53<3:01:04, 24.09s/it] 10%|█         | 50/500 [20:17<3:00:03, 24.01s/it] 10%|█         | 51/500 [20:42<3:00:39, 24.14s/it] 10%|█         | 52/500 [21:05<2:59:02, 23.98s/it] 11%|█         | 53/500 [21:29<2:58:17, 23.93s/it] 11%|█         | 54/500 [21:53<2:57:59, 23.95s/it] 11%|█         | 55/500 [22:21<3:06:48, 25.19s/it] 11%|█         | 56/500 [22:45<3:02:43, 24.69s/it] 11%|█▏        | 57/500 [23:08<3:00:08, 24.40s/it] 12%|█▏        | 58/500 [23:32<2:58:53, 24.28s/it] 12%|█▏        | 59/500 [23:56<2:57:28, 24.15s/it] 12%|█▏        | 60/500 [24:21<2:57:24, 24.19s/it] 12%|█▏        | 61/500 [24:44<2:55:26, 23.98s/it] 12%|█▏        | 61/500 [24:44<2:58:03, 24.34s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.030 MB uploadedwandb: \ 0.020 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▄▅▄▅▆▇▇▇▇▇█████████████████▆██████████
wandb:     train_loss █▇▅▁▆▁▁▇▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▇█▅▇▄▅▅▄▃▄▅▅▅▅▆▃▆▄▄▅▄▅▅▂▄▄▄▄▅▃▄▄▅▄▄▃▄▄
wandb:       val_loss ▂▂▁▁▁▂▂▁▃█▃▄▃▁▁▁▁▁▅▄▁▃▃▃▁▄▄▅▁▁▂▁▁▂▂▄▁▄▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 0.00013
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb:   val_accuracy 0.45111
wandb:       val_loss 0.17312
wandb: 
wandb: 🚀 View run brisk-valley-532 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yitthjl8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_051500-yitthjl8/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_054033-ngh1w0ob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-voice-535
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ngh1w0ob
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:09, 25.15s/it]  0%|          | 2/500 [00:48<3:21:50, 24.32s/it]  1%|          | 3/500 [01:13<3:21:18, 24.30s/it]  1%|          | 4/500 [01:37<3:20:28, 24.25s/it]  1%|          | 5/500 [02:01<3:19:28, 24.18s/it]  1%|          | 6/500 [02:27<3:23:21, 24.70s/it]  1%|▏         | 7/500 [02:51<3:21:36, 24.54s/it]  2%|▏         | 8/500 [03:15<3:19:41, 24.35s/it]  2%|▏         | 9/500 [03:39<3:17:52, 24.18s/it]  2%|▏         | 10/500 [04:08<3:30:01, 25.72s/it]  2%|▏         | 11/500 [04:32<3:25:40, 25.24s/it]  2%|▏         | 12/500 [05:00<3:32:35, 26.14s/it]  3%|▎         | 13/500 [05:24<3:27:11, 25.53s/it]  3%|▎         | 14/500 [05:48<3:22:47, 25.04s/it]  3%|▎         | 15/500 [06:12<3:19:30, 24.68s/it]  3%|▎         | 16/500 [06:36<3:17:36, 24.50s/it]  3%|▎         | 17/500 [07:01<3:17:10, 24.49s/it]  4%|▎         | 18/500 [07:24<3:14:47, 24.25s/it]  4%|▍         | 19/500 [07:48<3:12:37, 24.03s/it]  4%|▍         | 20/500 [08:11<3:11:29, 23.94s/it]  4%|▍         | 21/500 [08:35<3:09:49, 23.78s/it]  4%|▍         | 22/500 [08:59<3:09:27, 23.78s/it]  5%|▍         | 23/500 [09:22<3:08:50, 23.75s/it]  5%|▍         | 24/500 [09:46<3:08:40, 23.78s/it]  5%|▌         | 25/500 [10:10<3:09:28, 23.93s/it]  5%|▌         | 26/500 [10:34<3:08:28, 23.86s/it]  5%|▌         | 27/500 [10:58<3:07:52, 23.83s/it]  6%|▌         | 28/500 [11:22<3:07:23, 23.82s/it]  6%|▌         | 29/500 [11:46<3:08:38, 24.03s/it]  6%|▌         | 30/500 [12:10<3:07:36, 23.95s/it]  6%|▌         | 31/500 [12:34<3:06:32, 23.86s/it]  6%|▋         | 32/500 [12:57<3:05:52, 23.83s/it]  7%|▋         | 33/500 [13:22<3:06:21, 23.94s/it]  7%|▋         | 34/500 [13:45<3:05:38, 23.90s/it]  7%|▋         | 35/500 [14:09<3:05:25, 23.93s/it]  7%|▋         | 36/500 [14:33<3:05:07, 23.94s/it]  7%|▋         | 37/500 [14:57<3:04:57, 23.97s/it]  8%|▊         | 38/500 [15:25<3:12:27, 24.99s/it]  8%|▊         | 39/500 [15:49<3:09:40, 24.69s/it]  8%|▊         | 40/500 [16:13<3:07:41, 24.48s/it]  8%|▊         | 41/500 [16:41<3:16:09, 25.64s/it]  8%|▊         | 41/500 [16:41<3:06:53, 24.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.307 MB uploadedwandb: | 0.020 MB of 0.307 MB uploadedwandb: / 0.307 MB of 0.307 MB uploadedwandb: - 0.307 MB of 0.307 MB uploadedwandb: \ 0.307 MB of 0.307 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▄▃▆▅▇▇▇███▇██▇███████████████████████
wandb:     train_loss ▄▄█▄▁▂▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▂▁█▆▇▇▆▇▅▅▆▅▄▇▇▅▅▇▇▇▅▆▆▆▆▅▆▄▄▅▅▅▅▅▅▅▅▅▆
wandb:       val_loss ▂▁▂▁▂▂▁▃▁▁▂▁▂▆▂▁▆▃▂▂▂▂▂▁▁▅▃▅▄▂▁▂▃▃▄▂▁▃█▂
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.99851
wandb:     train_loss 0.00098
wandb:   val_accuracy 0.43333
wandb:       val_loss 1.96078
wandb: 
wandb: 🚀 View run restful-voice-535 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ngh1w0ob
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_054033-ngh1w0ob/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_055808-zb396js0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-capybara-537
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zb396js0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:43, 25.46s/it]  0%|          | 2/500 [00:49<3:24:24, 24.63s/it]  1%|          | 3/500 [01:13<3:21:42, 24.35s/it]  1%|          | 4/500 [01:37<3:20:04, 24.20s/it]  1%|          | 5/500 [02:01<3:18:43, 24.09s/it]  1%|          | 6/500 [02:25<3:18:30, 24.11s/it]  1%|▏         | 7/500 [02:49<3:17:21, 24.02s/it]  2%|▏         | 8/500 [03:13<3:16:49, 24.00s/it]  2%|▏         | 9/500 [03:36<3:15:17, 23.86s/it]  2%|▏         | 10/500 [04:00<3:15:16, 23.91s/it]  2%|▏         | 11/500 [04:24<3:14:39, 23.88s/it]  2%|▏         | 12/500 [04:48<3:13:54, 23.84s/it]  3%|▎         | 13/500 [05:12<3:14:01, 23.90s/it]  3%|▎         | 14/500 [05:36<3:14:23, 24.00s/it]  3%|▎         | 15/500 [06:00<3:13:16, 23.91s/it]  3%|▎         | 16/500 [06:24<3:12:56, 23.92s/it]  3%|▎         | 17/500 [06:48<3:12:18, 23.89s/it]  4%|▎         | 18/500 [07:16<3:23:10, 25.29s/it]  4%|▍         | 19/500 [07:40<3:19:04, 24.83s/it]  4%|▍         | 20/500 [08:08<3:27:12, 25.90s/it]  4%|▍         | 21/500 [08:32<3:20:33, 25.12s/it]  4%|▍         | 22/500 [09:01<3:29:38, 26.31s/it]  5%|▍         | 23/500 [09:25<3:24:04, 25.67s/it]  5%|▍         | 24/500 [09:49<3:20:01, 25.21s/it]  5%|▌         | 25/500 [10:13<3:16:35, 24.83s/it]  5%|▌         | 26/500 [10:37<3:14:25, 24.61s/it]  5%|▌         | 26/500 [10:37<3:13:46, 24.53s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.231 MB of 0.312 MB uploadedwandb: | 0.231 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▆▆▇▇▆▆▇▇▇▁▁▂▂▃▅▆▇▇▇▇▇▇▇█
wandb:     train_loss ▅▅▅▅▂▆▅▁▄▂█▂▇▄▅▄▄█▄▅▇▅▃▂▃▆
wandb:   val_accuracy ▂▆▆▆▆█▇▇███▁▁▁▁▂▃▄▄▄▅▄▅▅▅▅
wandb:       val_loss ▃▂▃▂▃▃▂▄▂▁▂█▅▅▂▄▄▄▂▂▃▃▃▃▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.76077
wandb:     train_loss 1.17774
wandb:   val_accuracy 0.47778
wandb:       val_loss 1.2671
wandb: 
wandb: 🚀 View run hearty-capybara-537 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zb396js0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_055808-zb396js0/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_060929-3hkl10ia
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-meadow-539
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3hkl10ia
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:36:40, 26.05s/it]  0%|          | 2/500 [00:50<3:27:53, 25.05s/it]  1%|          | 3/500 [01:14<3:24:16, 24.66s/it]  1%|          | 4/500 [01:40<3:29:16, 25.32s/it]  1%|          | 5/500 [02:05<3:25:56, 24.96s/it]  1%|          | 6/500 [02:29<3:22:43, 24.62s/it]  1%|▏         | 7/500 [02:53<3:21:09, 24.48s/it]  2%|▏         | 8/500 [03:17<3:19:49, 24.37s/it]  2%|▏         | 9/500 [03:41<3:19:16, 24.35s/it]  2%|▏         | 10/500 [04:05<3:17:35, 24.19s/it]  2%|▏         | 11/500 [04:30<3:18:01, 24.30s/it]  2%|▏         | 12/500 [04:54<3:16:50, 24.20s/it]  3%|▎         | 13/500 [05:18<3:15:59, 24.15s/it]  3%|▎         | 14/500 [05:42<3:14:47, 24.05s/it]  3%|▎         | 15/500 [06:05<3:13:57, 24.00s/it]  3%|▎         | 16/500 [06:29<3:13:29, 23.99s/it]  3%|▎         | 17/500 [06:54<3:13:27, 24.03s/it]  4%|▎         | 18/500 [07:18<3:13:08, 24.04s/it]  4%|▍         | 19/500 [07:46<3:23:55, 25.44s/it]  4%|▍         | 20/500 [08:10<3:20:10, 25.02s/it]  4%|▍         | 20/500 [08:10<3:16:20, 24.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.306 MB uploadedwandb: | 0.021 MB of 0.306 MB uploadedwandb: / 0.021 MB of 0.306 MB uploadedwandb: - 0.306 MB of 0.306 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁
wandb: train_accuracy ▁▁▄▄▄▄▅▁▄▅▆▇▇▇▇█████
wandb:     train_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁█▇█▇█▂▃▃▃▄▄▄▄▄▄▄▅▅
wandb:       val_loss ▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 19
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.98811
wandb:     train_loss 0.11102
wandb:   val_accuracy 0.47333
wandb:       val_loss 2.48811
wandb: 
wandb: 🚀 View run playful-meadow-539 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3hkl10ia
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_060929-3hkl10ia/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_061822-i5j7baub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-brook-540
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i5j7baub
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:32<4:33:58, 32.94s/it]  0%|          | 2/500 [00:57<3:50:20, 27.75s/it]  1%|          | 3/500 [01:21<3:38:31, 26.38s/it]  1%|          | 4/500 [01:47<3:34:16, 25.92s/it]  1%|          | 5/500 [02:12<3:32:16, 25.73s/it]  1%|          | 6/500 [02:38<3:31:44, 25.72s/it]  1%|▏         | 7/500 [03:04<3:31:49, 25.78s/it]  2%|▏         | 8/500 [03:28<3:28:23, 25.41s/it]  2%|▏         | 9/500 [03:52<3:24:46, 25.02s/it]  2%|▏         | 10/500 [04:16<3:21:45, 24.71s/it]  2%|▏         | 11/500 [04:40<3:19:16, 24.45s/it]  2%|▏         | 12/500 [05:05<3:19:39, 24.55s/it]  3%|▎         | 13/500 [05:31<3:22:47, 24.98s/it]  3%|▎         | 14/500 [05:55<3:20:33, 24.76s/it]  3%|▎         | 15/500 [06:23<3:28:32, 25.80s/it]  3%|▎         | 16/500 [06:48<3:25:45, 25.51s/it]  3%|▎         | 17/500 [07:12<3:20:30, 24.91s/it]  4%|▎         | 18/500 [07:35<3:16:54, 24.51s/it]  4%|▍         | 19/500 [08:00<3:15:55, 24.44s/it]  4%|▍         | 20/500 [08:25<3:16:54, 24.61s/it]  4%|▍         | 21/500 [08:49<3:14:47, 24.40s/it]  4%|▍         | 22/500 [09:12<3:13:17, 24.26s/it]  5%|▍         | 23/500 [09:36<3:12:11, 24.17s/it]  5%|▍         | 24/500 [10:00<3:11:14, 24.11s/it]  5%|▌         | 25/500 [10:24<3:10:33, 24.07s/it]  5%|▌         | 26/500 [10:53<3:19:55, 25.31s/it]  5%|▌         | 27/500 [11:17<3:17:03, 25.00s/it]  6%|▌         | 28/500 [11:41<3:14:15, 24.69s/it]  6%|▌         | 29/500 [12:05<3:12:06, 24.47s/it]  6%|▌         | 30/500 [12:29<3:10:05, 24.27s/it]  6%|▌         | 31/500 [12:53<3:09:19, 24.22s/it]  6%|▋         | 32/500 [13:17<3:10:18, 24.40s/it]  7%|▋         | 33/500 [13:42<3:09:18, 24.32s/it]  7%|▋         | 34/500 [14:08<3:12:42, 24.81s/it]  7%|▋         | 35/500 [14:32<3:11:35, 24.72s/it]  7%|▋         | 36/500 [14:56<3:09:34, 24.52s/it]  7%|▋         | 37/500 [15:20<3:08:00, 24.36s/it]  8%|▊         | 38/500 [15:47<3:14:15, 25.23s/it]  8%|▊         | 38/500 [15:47<3:12:05, 24.95s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.308 MB uploadedwandb: | 0.138 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▄▃▃▃▃▄▄▃▃▇▇▅▆▆██▆███████▇▇██████████
wandb:     train_loss ▇▅▅▃▁▄▄▁█▂█▁▁▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂█▇▃▇▅▁▅▂▁▁▅▅▂▄▄▅▇▄▅▇██▅▇▇▃█▇▅▄▅▆▅▆▇▅▆
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00051
wandb: train_accuracy 1.0
wandb:     train_loss 0.0
wandb:   val_accuracy 0.53556
wandb:       val_loss 2.8201
wandb: 
wandb: 🚀 View run dry-brook-540 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i5j7baub
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_061822-i5j7baub/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_063500-snxpso7f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-eon-543
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/snxpso7f
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:46:42, 27.26s/it]  0%|          | 2/500 [00:51<3:32:43, 25.63s/it]  1%|          | 3/500 [01:16<3:29:48, 25.33s/it]  1%|          | 4/500 [01:41<3:27:08, 25.06s/it]  1%|          | 5/500 [02:05<3:23:32, 24.67s/it]  1%|          | 6/500 [02:29<3:21:03, 24.42s/it]  1%|▏         | 7/500 [02:58<3:32:47, 25.90s/it]  2%|▏         | 8/500 [03:22<3:28:38, 25.44s/it]  2%|▏         | 9/500 [03:46<3:23:57, 24.92s/it]  2%|▏         | 10/500 [04:10<3:21:10, 24.63s/it]  2%|▏         | 11/500 [04:34<3:18:41, 24.38s/it]  2%|▏         | 12/500 [05:02<3:28:53, 25.68s/it]  3%|▎         | 13/500 [05:26<3:24:14, 25.16s/it]  3%|▎         | 14/500 [05:51<3:21:41, 24.90s/it]  3%|▎         | 15/500 [06:18<3:27:53, 25.72s/it]  3%|▎         | 16/500 [06:43<3:24:30, 25.35s/it]  3%|▎         | 17/500 [07:10<3:29:11, 25.99s/it]  4%|▎         | 18/500 [07:35<3:26:53, 25.76s/it]  4%|▍         | 19/500 [08:00<3:24:24, 25.50s/it]  4%|▍         | 20/500 [08:25<3:20:55, 25.12s/it]  4%|▍         | 21/500 [08:48<3:17:19, 24.72s/it]  4%|▍         | 22/500 [09:12<3:14:34, 24.42s/it]  5%|▍         | 23/500 [09:39<3:18:56, 25.02s/it]  5%|▍         | 24/500 [10:03<3:16:19, 24.75s/it]  5%|▌         | 25/500 [10:27<3:14:44, 24.60s/it]  5%|▌         | 26/500 [10:51<3:11:58, 24.30s/it]  5%|▌         | 27/500 [11:20<3:23:34, 25.82s/it]  6%|▌         | 28/500 [11:44<3:18:56, 25.29s/it]  6%|▌         | 29/500 [12:13<3:26:37, 26.32s/it]  6%|▌         | 30/500 [12:37<3:21:28, 25.72s/it]  6%|▌         | 30/500 [12:37<3:17:47, 25.25s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▃▅▆▆▇▇▆▅▆▇▇▇▇▇▇▇▇▇█▇▇████████
wandb:     train_loss ▄▅▄▅▂▅▅▁▄▃▇▂▂▅▅▃▃▃▁▃▆▂▂▁▂▂▃▂█▄
wandb:   val_accuracy ▁▁▇▆▆▆▇▇▃▇███▆█▇▇█▇▇▇█▇▇█▇▇▇▆▇
wandb:       val_loss ▄▅▄▄▄▄▄▅▅▃▄▅▃▁▄▃█▃▃▄▄▂▃▅▃▅▂▄█▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.76077
wandb:     train_loss 0.97455
wandb:   val_accuracy 0.56667
wandb:       val_loss 1.34065
wandb: 
wandb: 🚀 View run devoted-eon-543 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/snxpso7f
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_063500-snxpso7f/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_064819-yw4f3y6f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-dawn-545
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yw4f3y6f
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:12:35, 30.37s/it]  0%|          | 2/500 [00:54<3:40:42, 26.59s/it]  1%|          | 3/500 [01:19<3:34:07, 25.85s/it]  1%|          | 4/500 [01:43<3:28:12, 25.19s/it]  1%|          | 5/500 [02:10<3:34:16, 25.97s/it]  1%|          | 6/500 [02:35<3:29:14, 25.41s/it]  1%|▏         | 7/500 [03:03<3:35:51, 26.27s/it]  2%|▏         | 8/500 [03:27<3:29:19, 25.53s/it]  2%|▏         | 9/500 [03:54<3:34:26, 26.20s/it]  2%|▏         | 10/500 [04:18<3:28:09, 25.49s/it]  2%|▏         | 11/500 [04:42<3:23:16, 24.94s/it]  2%|▏         | 12/500 [05:06<3:20:14, 24.62s/it]  3%|▎         | 13/500 [05:34<3:29:50, 25.85s/it]  3%|▎         | 14/500 [05:59<3:26:23, 25.48s/it]  3%|▎         | 15/500 [06:23<3:22:07, 25.00s/it]  3%|▎         | 16/500 [06:47<3:19:30, 24.73s/it]  3%|▎         | 17/500 [07:14<3:23:58, 25.34s/it]  4%|▎         | 18/500 [07:39<3:23:18, 25.31s/it]  4%|▍         | 19/500 [08:03<3:19:53, 24.93s/it]  4%|▍         | 20/500 [08:27<3:17:08, 24.64s/it]  4%|▍         | 20/500 [08:27<3:23:04, 25.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.021 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁
wandb: train_accuracy ▁▃▄▄▁▃▄▇▅▅▆█▇▅██▅█▇▇
wandb:     train_loss ▂▂▂▂█▂▂▁▂▂▂▁▁▁▁▁▁▂▁▁
wandb:   val_accuracy ▂▂▆▅▂▁▂█▃▁▃▅▆▂▆▆▂▇▅▄
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 19
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.80238
wandb:     train_loss 0.04862
wandb:   val_accuracy 0.39556
wandb:       val_loss 1.49259
wandb: 
wandb: 🚀 View run fragrant-dawn-545 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/yw4f3y6f
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_064819-yw4f3y6f/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_065734-nontwak4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-shadow-546
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nontwak4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:38<5:16:45, 38.09s/it]  0%|          | 2/500 [01:02<4:09:08, 30.02s/it]  1%|          | 3/500 [01:27<3:48:11, 27.55s/it]  1%|          | 4/500 [01:54<3:47:15, 27.49s/it]  1%|          | 5/500 [02:18<3:37:48, 26.40s/it]  1%|          | 6/500 [02:46<3:41:12, 26.87s/it]  1%|▏         | 7/500 [03:11<3:36:17, 26.32s/it]  2%|▏         | 8/500 [03:37<3:33:07, 25.99s/it]  2%|▏         | 9/500 [04:07<3:42:48, 27.23s/it]  2%|▏         | 10/500 [04:37<3:49:06, 28.05s/it]  2%|▏         | 11/500 [05:01<3:40:43, 27.08s/it]  2%|▏         | 12/500 [05:26<3:35:08, 26.45s/it]  3%|▎         | 13/500 [05:51<3:31:11, 26.02s/it]  3%|▎         | 14/500 [06:17<3:29:30, 25.86s/it]  3%|▎         | 15/500 [06:42<3:26:21, 25.53s/it]  3%|▎         | 16/500 [07:10<3:32:06, 26.29s/it]  3%|▎         | 17/500 [07:34<3:27:06, 25.73s/it]  4%|▎         | 18/500 [07:59<3:24:30, 25.46s/it]  4%|▍         | 19/500 [08:24<3:22:23, 25.25s/it]  4%|▍         | 20/500 [08:48<3:20:06, 25.01s/it]  4%|▍         | 21/500 [09:18<3:30:18, 26.34s/it]  4%|▍         | 22/500 [09:43<3:26:47, 25.96s/it]  5%|▍         | 23/500 [10:07<3:22:39, 25.49s/it]  5%|▍         | 24/500 [10:32<3:19:33, 25.16s/it]  5%|▌         | 25/500 [10:56<3:18:02, 25.02s/it]  5%|▌         | 26/500 [11:20<3:14:42, 24.65s/it]  5%|▌         | 27/500 [11:44<3:12:57, 24.48s/it]  6%|▌         | 28/500 [12:13<3:23:04, 25.81s/it]  6%|▌         | 29/500 [12:38<3:19:39, 25.43s/it]  6%|▌         | 30/500 [13:02<3:15:43, 24.99s/it]  6%|▌         | 30/500 [13:06<3:25:20, 26.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.019 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▄▃▃▃▂▃▄▃▄▅▆▆▇▆▇▅███▅██▆███▇█▇
wandb:     train_loss ▅▆▂▃▂▂▆▁▂▂█▁▁▂▁▂▇▁▁▁▁▁▁▁▁▃▁▁▁▁
wandb:   val_accuracy ▁▆▆▆▅▆▆▇▆▇██▅▅█▆▆▆▅▆▇▆▆▇▅▅▆▅▆▇
wandb:       val_loss ▂▂▂▂▂▂▂▃▂▁▂▂▁▁▁▂█▁▁▁▂▃▁▁▂▂▃▂▃▇
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.93314
wandb:     train_loss 0.07084
wandb:   val_accuracy 0.61111
wandb:       val_loss 6.73351
wandb: 
wandb: 🚀 View run glorious-shadow-546 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nontwak4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_065734-nontwak4/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_071121-exwlwpjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-paper-549
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/exwlwpjb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:42:25, 26.74s/it]  0%|          | 2/500 [00:51<3:32:13, 25.57s/it]  1%|          | 3/500 [01:16<3:30:48, 25.45s/it]  1%|          | 4/500 [01:41<3:28:22, 25.21s/it]  1%|          | 5/500 [02:05<3:24:48, 24.83s/it]  1%|          | 6/500 [02:30<3:22:57, 24.65s/it]  1%|▏         | 7/500 [02:54<3:20:46, 24.43s/it]  2%|▏         | 8/500 [03:18<3:19:03, 24.28s/it]  2%|▏         | 9/500 [03:42<3:18:11, 24.22s/it]  2%|▏         | 10/500 [04:06<3:18:11, 24.27s/it]  2%|▏         | 11/500 [04:30<3:17:08, 24.19s/it]  2%|▏         | 12/500 [04:58<3:27:23, 25.50s/it]  3%|▎         | 13/500 [05:23<3:25:00, 25.26s/it]  3%|▎         | 14/500 [05:47<3:22:08, 24.96s/it]  3%|▎         | 15/500 [06:12<3:19:33, 24.69s/it]  3%|▎         | 16/500 [06:36<3:18:48, 24.65s/it]  3%|▎         | 17/500 [07:01<3:17:57, 24.59s/it]  4%|▎         | 18/500 [07:24<3:15:27, 24.33s/it]  4%|▍         | 19/500 [07:48<3:13:25, 24.13s/it]  4%|▍         | 20/500 [08:12<3:12:29, 24.06s/it]  4%|▍         | 21/500 [08:35<3:10:41, 23.89s/it]  4%|▍         | 22/500 [09:02<3:16:34, 24.67s/it]  5%|▍         | 23/500 [09:25<3:13:23, 24.33s/it]  5%|▍         | 24/500 [09:49<3:11:00, 24.08s/it]  5%|▌         | 25/500 [10:13<3:10:12, 24.03s/it]  5%|▌         | 26/500 [10:36<3:09:11, 23.95s/it]  5%|▌         | 27/500 [11:00<3:08:38, 23.93s/it]  6%|▌         | 28/500 [11:24<3:07:54, 23.89s/it]  6%|▌         | 29/500 [11:56<3:27:09, 26.39s/it]  6%|▌         | 30/500 [12:21<3:21:45, 25.76s/it]  6%|▌         | 30/500 [12:21<3:13:31, 24.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▁▆▆▇▆▇▆▇▇▇▇▇▆▇▇▇██▃▁▃▆▆▆▆▇▇██
wandb:     train_loss ▅▆▅▅▂▅▆▁▅▃█▁▁▄▅▅▃▄▁▃▄▇▄▃▄▇▄▆▅▅
wandb:   val_accuracy ▃▃▆▆▇▇▇▇▇▇▇██▇██▇██▃▁▂▃▃▄▄▄▄▄▅
wandb:       val_loss ▄▅▄▃▄▄▄▅▄▃▃▃▂▁▃▃█▄▂▅▆▅▄▅▄▅▃▆█▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 5e-05
wandb: train_accuracy 0.74889
wandb:     train_loss 0.92216
wandb:   val_accuracy 0.43111
wandb:       val_loss 1.36215
wandb: 
wandb: 🚀 View run smart-paper-549 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/exwlwpjb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_071121-exwlwpjb/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_072425-w1l6mqpl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-brook-551
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/w1l6mqpl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:11:15, 30.21s/it]  0%|          | 2/500 [00:54<3:40:21, 26.55s/it]  1%|          | 3/500 [01:24<3:53:15, 28.16s/it]  1%|          | 4/500 [01:49<3:41:59, 26.85s/it]  1%|          | 5/500 [02:13<3:33:21, 25.86s/it]  1%|          | 6/500 [02:37<3:28:04, 25.27s/it]  1%|▏         | 7/500 [03:01<3:24:15, 24.86s/it]  2%|▏         | 8/500 [03:25<3:21:41, 24.60s/it]  2%|▏         | 9/500 [03:53<3:29:51, 25.65s/it]  2%|▏         | 10/500 [04:17<3:25:31, 25.17s/it]  2%|▏         | 11/500 [04:41<3:23:04, 24.92s/it]  2%|▏         | 12/500 [05:05<3:20:45, 24.68s/it]  3%|▎         | 13/500 [05:29<3:18:31, 24.46s/it]  3%|▎         | 14/500 [05:53<3:16:34, 24.27s/it]  3%|▎         | 15/500 [06:17<3:15:36, 24.20s/it]  3%|▎         | 16/500 [06:41<3:14:41, 24.14s/it]  3%|▎         | 17/500 [07:07<3:19:09, 24.74s/it]  4%|▎         | 18/500 [07:36<3:27:49, 25.87s/it]  4%|▍         | 19/500 [08:00<3:23:45, 25.42s/it]  4%|▍         | 20/500 [08:24<3:19:48, 24.98s/it]  4%|▍         | 21/500 [08:48<3:16:53, 24.66s/it]  4%|▍         | 22/500 [09:16<3:23:02, 25.49s/it]  5%|▍         | 23/500 [09:40<3:19:00, 25.03s/it]  5%|▍         | 24/500 [10:08<3:25:41, 25.93s/it]  5%|▌         | 25/500 [10:32<3:20:52, 25.37s/it]  5%|▌         | 26/500 [11:00<3:26:56, 26.20s/it]  5%|▌         | 27/500 [11:24<3:21:59, 25.62s/it]  6%|▌         | 28/500 [11:48<3:17:28, 25.10s/it]  6%|▌         | 29/500 [12:12<3:14:48, 24.82s/it]  6%|▌         | 30/500 [12:36<3:12:45, 24.61s/it]  6%|▌         | 31/500 [13:00<3:10:24, 24.36s/it]  6%|▋         | 32/500 [13:24<3:10:01, 24.36s/it]  7%|▋         | 33/500 [13:48<3:08:28, 24.22s/it]  7%|▋         | 34/500 [14:12<3:07:27, 24.14s/it]  7%|▋         | 35/500 [14:36<3:06:49, 24.11s/it]  7%|▋         | 36/500 [15:01<3:06:56, 24.17s/it]  7%|▋         | 37/500 [15:25<3:07:55, 24.35s/it]  8%|▊         | 38/500 [15:52<3:12:35, 25.01s/it]  8%|▊         | 38/500 [15:52<3:12:58, 25.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.020 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▄▄▄▅▆▆▇▆▇▆██▇▇█▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:     train_loss ▃▂▂▂▁▂▂▁▁▁▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁█▁▁▁
wandb:   val_accuracy ▁▁▃▆█▆▆▆▅▄▄▆▇▃▇▇▄▇▆▆▇▆▇▆█▇▄▇▆▆▆▆▆▅▆▇▇▅
wandb:       val_loss ▂▂▂▂▂▂▂▃▁▂▂▁▄▃▁▃█▁▄▅▃▁▁▃▂▃▁▁█▆▁▄▆▄▅▃▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.8737
wandb:     train_loss 0.00038
wandb:   val_accuracy 0.47778
wandb:       val_loss 1.28668
wandb: 
wandb: 🚀 View run flowing-brook-551 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/w1l6mqpl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_072425-w1l6mqpl/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_074102-bu4mq0hc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-brook-553
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bu4mq0hc
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:48:53, 27.52s/it]  0%|          | 2/500 [00:52<3:38:09, 26.28s/it]  1%|          | 3/500 [01:18<3:35:34, 26.02s/it]  1%|          | 4/500 [01:43<3:30:24, 25.45s/it]  1%|          | 5/500 [02:11<3:38:56, 26.54s/it]  1%|          | 6/500 [02:35<3:30:35, 25.58s/it]  1%|▏         | 7/500 [02:59<3:25:59, 25.07s/it]  2%|▏         | 8/500 [03:31<3:42:33, 27.14s/it]  2%|▏         | 9/500 [03:55<3:36:16, 26.43s/it]  2%|▏         | 10/500 [04:21<3:34:20, 26.25s/it]  2%|▏         | 11/500 [04:46<3:31:15, 25.92s/it]  2%|▏         | 12/500 [05:14<3:34:33, 26.38s/it]  3%|▎         | 13/500 [05:40<3:33:55, 26.36s/it]  3%|▎         | 14/500 [06:07<3:35:29, 26.60s/it]  3%|▎         | 15/500 [06:32<3:31:06, 26.12s/it]  3%|▎         | 16/500 [06:58<3:30:30, 26.10s/it]  3%|▎         | 17/500 [07:24<3:28:05, 25.85s/it]  4%|▎         | 18/500 [07:48<3:24:33, 25.46s/it]  4%|▍         | 19/500 [08:14<3:24:58, 25.57s/it]  4%|▍         | 20/500 [08:39<3:23:27, 25.43s/it]  4%|▍         | 21/500 [09:04<3:21:08, 25.19s/it]  4%|▍         | 22/500 [09:35<3:34:06, 26.88s/it]  5%|▍         | 23/500 [10:03<3:36:48, 27.27s/it]  5%|▍         | 24/500 [10:28<3:31:36, 26.67s/it]  5%|▌         | 25/500 [10:53<3:27:58, 26.27s/it]  5%|▌         | 26/500 [11:19<3:25:36, 26.03s/it]  5%|▌         | 27/500 [11:44<3:23:30, 25.81s/it]  6%|▌         | 28/500 [12:09<3:21:38, 25.63s/it]  6%|▌         | 29/500 [12:34<3:19:26, 25.41s/it]  6%|▌         | 30/500 [13:00<3:20:46, 25.63s/it]  6%|▌         | 30/500 [13:05<3:25:02, 26.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.020 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▁▄▄▅▄▃▅▅▅▆▅▆▆▇▇▇▆██▇▇█▇▆▇█▆▆▇
wandb:     train_loss ▃▃▂▃▁▂▆▁▁▁█▁▁▂▁▁▁▃▁▂▂▂▁▁▁▂▁▆▁▁
wandb:   val_accuracy ▁▁▅▇█▇▆█▇▆█▇▇▇█▇▆▅▇▇▆▇▇▇▅▇▇▇▆▇
wandb:       val_loss ▃▂▂▂▂▂▂▄▂▂▁▁▂▁▂▁▁█▃▁▃▆▃▄▂▅▇█▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.79643
wandb:     train_loss 0.00953
wandb:   val_accuracy 0.56667
wandb:       val_loss 0.08733
wandb: 
wandb: 🚀 View run northern-brook-553 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/bu4mq0hc
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_074102-bu4mq0hc/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_075457-b5u2jj4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-capybara-555
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/b5u2jj4w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:42:13, 26.72s/it]  0%|          | 2/500 [00:52<3:35:45, 26.00s/it]  1%|          | 3/500 [01:16<3:30:02, 25.36s/it]  1%|          | 4/500 [01:45<3:39:24, 26.54s/it]  1%|          | 5/500 [02:08<3:30:48, 25.55s/it]  1%|          | 6/500 [02:32<3:26:01, 25.02s/it]  1%|▏         | 7/500 [02:57<3:23:38, 24.78s/it]  2%|▏         | 8/500 [03:21<3:20:52, 24.50s/it]  2%|▏         | 9/500 [03:45<3:20:19, 24.48s/it]  2%|▏         | 10/500 [04:10<3:21:58, 24.73s/it]  2%|▏         | 11/500 [04:35<3:21:18, 24.70s/it]  2%|▏         | 12/500 [04:59<3:19:34, 24.54s/it]  3%|▎         | 13/500 [05:24<3:19:12, 24.54s/it]  3%|▎         | 14/500 [05:48<3:18:39, 24.53s/it]  3%|▎         | 15/500 [06:13<3:19:28, 24.68s/it]  3%|▎         | 16/500 [06:37<3:17:49, 24.52s/it]  3%|▎         | 17/500 [07:02<3:17:43, 24.56s/it]  4%|▎         | 18/500 [07:26<3:16:37, 24.48s/it]  4%|▍         | 19/500 [07:50<3:15:21, 24.37s/it]  4%|▍         | 20/500 [08:17<3:19:56, 24.99s/it]  4%|▍         | 21/500 [08:48<3:33:36, 26.76s/it]  4%|▍         | 22/500 [09:12<3:27:58, 26.11s/it]  5%|▍         | 23/500 [09:38<3:26:17, 25.95s/it]  5%|▍         | 24/500 [10:02<3:22:11, 25.49s/it]  5%|▌         | 25/500 [10:27<3:19:58, 25.26s/it]  5%|▌         | 26/500 [10:52<3:17:41, 25.02s/it]  5%|▌         | 27/500 [11:17<3:18:10, 25.14s/it]  6%|▌         | 28/500 [11:41<3:16:11, 24.94s/it]  6%|▌         | 29/500 [12:07<3:16:44, 25.06s/it]  6%|▌         | 30/500 [12:33<3:17:59, 25.27s/it]  6%|▌         | 31/500 [12:57<3:14:51, 24.93s/it]  6%|▋         | 32/500 [13:21<3:13:12, 24.77s/it]  7%|▋         | 33/500 [13:45<3:10:55, 24.53s/it]  7%|▋         | 34/500 [14:10<3:10:32, 24.53s/it]  7%|▋         | 35/500 [14:33<3:08:25, 24.31s/it]  7%|▋         | 36/500 [14:57<3:06:52, 24.16s/it]  7%|▋         | 37/500 [15:21<3:06:12, 24.13s/it]  8%|▊         | 38/500 [15:45<3:05:20, 24.07s/it]  8%|▊         | 39/500 [16:09<3:04:27, 24.01s/it]  8%|▊         | 40/500 [16:33<3:03:57, 23.99s/it]  8%|▊         | 41/500 [16:58<3:05:31, 24.25s/it]  8%|▊         | 41/500 [16:59<3:10:10, 24.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.020 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▄▅▆▆▆▆▆▅▆▇▇▅▆▆▅▇▆▅▆▇▇▆█▂▂▂▂▂▂▂▃▃▃▄▄▅▅▆
wandb:     train_loss ▄▄▄▄▂▄▄▁▄▃▆▁▁▄▄▃▃▃▂▄▃▂▃▁▁▅█▄▄▆▆▄▆▄▄▄▂▄▃▄
wandb:   val_accuracy ▁▁▅▆▆▆▆▆▆▆▇▇▇▆█▇▆▇█▇█▇█▇▇▁▂▁▂▂▁▂▃▂▂▃▃▄▄▅
wandb:       val_loss ▃▃▃▂▃▂▂▃▃▁▂▂▁▃▂▂▆▂▂▃▂▁▂▂▁▂▁▃▆▄▂▃▄▃█▃▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.68202
wandb:     train_loss 0.9473
wandb:   val_accuracy 0.53778
wandb:       val_loss 0.87914
wandb: 
wandb: 🚀 View run hardy-capybara-555 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/b5u2jj4w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_075457-b5u2jj4w/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_081241-vltt22dy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-silence-557
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vltt22dy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:15:55, 30.77s/it]  0%|          | 2/500 [00:54<3:42:08, 26.77s/it]  1%|          | 3/500 [01:23<3:50:42, 27.85s/it]  1%|          | 4/500 [01:47<3:37:46, 26.34s/it]  1%|          | 5/500 [02:12<3:31:00, 25.58s/it]  1%|          | 6/500 [02:35<3:25:24, 24.95s/it]  1%|▏         | 7/500 [03:04<3:34:29, 26.11s/it]  2%|▏         | 8/500 [03:33<3:42:34, 27.14s/it]  2%|▏         | 9/500 [03:58<3:34:55, 26.26s/it]  2%|▏         | 10/500 [04:22<3:29:10, 25.61s/it]  2%|▏         | 11/500 [04:47<3:26:50, 25.38s/it]  2%|▏         | 12/500 [05:11<3:23:28, 25.02s/it]  3%|▎         | 13/500 [05:35<3:20:20, 24.68s/it]  3%|▎         | 14/500 [05:59<3:18:59, 24.57s/it]  3%|▎         | 15/500 [06:23<3:17:06, 24.39s/it]  3%|▎         | 16/500 [06:46<3:14:45, 24.14s/it]  3%|▎         | 17/500 [07:10<3:12:54, 23.96s/it]  4%|▎         | 18/500 [07:34<3:11:33, 23.85s/it]  4%|▍         | 19/500 [07:57<3:10:37, 23.78s/it]  4%|▍         | 20/500 [08:21<3:09:38, 23.71s/it]  4%|▍         | 21/500 [08:45<3:09:26, 23.73s/it]  4%|▍         | 22/500 [09:08<3:09:10, 23.75s/it]  5%|▍         | 23/500 [09:32<3:08:49, 23.75s/it]  5%|▍         | 24/500 [10:00<3:19:05, 25.10s/it]  5%|▌         | 25/500 [10:24<3:15:13, 24.66s/it]  5%|▌         | 26/500 [10:48<3:12:45, 24.40s/it]  5%|▌         | 27/500 [11:12<3:10:57, 24.22s/it]  6%|▌         | 28/500 [11:35<3:09:28, 24.09s/it]  6%|▌         | 29/500 [12:00<3:09:19, 24.12s/it]  6%|▌         | 30/500 [12:23<3:07:35, 23.95s/it]  6%|▌         | 31/500 [12:47<3:06:06, 23.81s/it]  6%|▋         | 32/500 [13:10<3:05:21, 23.76s/it]  7%|▋         | 33/500 [13:34<3:04:33, 23.71s/it]  7%|▋         | 34/500 [13:58<3:04:24, 23.74s/it]  7%|▋         | 35/500 [14:21<3:03:50, 23.72s/it]  7%|▋         | 36/500 [14:45<3:03:20, 23.71s/it]  7%|▋         | 37/500 [15:09<3:02:45, 23.68s/it]  8%|▊         | 38/500 [15:33<3:02:56, 23.76s/it]  8%|▊         | 39/500 [15:56<3:02:28, 23.75s/it]  8%|▊         | 40/500 [16:20<3:02:28, 23.80s/it]  8%|▊         | 41/500 [16:44<3:01:24, 23.71s/it]  8%|▊         | 42/500 [17:07<3:00:41, 23.67s/it]  9%|▊         | 43/500 [17:31<3:00:46, 23.73s/it]  9%|▉         | 44/500 [17:55<3:00:49, 23.79s/it]  9%|▉         | 45/500 [18:19<3:00:27, 23.80s/it]  9%|▉         | 46/500 [18:43<3:00:09, 23.81s/it]  9%|▉         | 47/500 [19:08<3:02:32, 24.18s/it] 10%|▉         | 48/500 [19:33<3:03:35, 24.37s/it] 10%|▉         | 48/500 [19:33<3:04:06, 24.44s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.020 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▂▂▅▅▄▄▃▆▂▆▅▃▆▄▂▇▄█▆▄▆▇██████▇█▆█▇███▇
wandb:     train_loss ▄▃▃▃█▃▁▄▁▃▁▂▄▂█▁▂▄▁▄▁▅▁▃▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁
wandb:   val_accuracy ▂▂▆▂▄▇▅▄▄▂▅▁▆▆▃█▇▂█▃█▅▂▄█▆▇▆▆▇▇▇▅▄▇▄▇▇▇▃
wandb:       val_loss ▃▄▃▂▂▃▄▁▄▃▇▃▂▅▇▅▅▃▁▄▂▄▄▂█▃▁▇▇▅▄▁▅▅▂▄▅█▅▇
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.79643
wandb:     train_loss 0.01385
wandb:   val_accuracy 0.37111
wandb:       val_loss 3.26474
wandb: 
wandb: 🚀 View run easy-silence-557 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vltt22dy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_081241-vltt22dy/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_083257-vvac3hix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-universe-559
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vvac3hix
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:07, 18.29s/it]  0%|          | 2/500 [00:35<2:25:09, 17.49s/it]  1%|          | 3/500 [00:52<2:22:34, 17.21s/it]  1%|          | 4/500 [01:08<2:18:53, 16.80s/it]  1%|          | 5/500 [01:24<2:17:05, 16.62s/it]  1%|          | 6/500 [01:40<2:14:39, 16.36s/it]  1%|▏         | 7/500 [01:57<2:15:11, 16.45s/it]  2%|▏         | 8/500 [02:13<2:15:49, 16.56s/it]  2%|▏         | 9/500 [02:30<2:15:26, 16.55s/it]  2%|▏         | 10/500 [02:46<2:15:09, 16.55s/it]  2%|▏         | 11/500 [03:03<2:15:27, 16.62s/it]  2%|▏         | 12/500 [03:22<2:19:56, 17.21s/it]  3%|▎         | 13/500 [03:38<2:17:47, 16.98s/it]  3%|▎         | 14/500 [03:55<2:15:57, 16.79s/it]  3%|▎         | 15/500 [04:11<2:15:50, 16.80s/it]  3%|▎         | 16/500 [04:29<2:16:52, 16.97s/it]  3%|▎         | 17/500 [04:45<2:16:02, 16.90s/it]  4%|▎         | 18/500 [05:07<2:27:25, 18.35s/it]  4%|▍         | 19/500 [05:26<2:28:32, 18.53s/it]  4%|▍         | 20/500 [05:43<2:25:14, 18.16s/it]  4%|▍         | 21/500 [06:01<2:23:11, 17.94s/it]  4%|▍         | 22/500 [06:18<2:20:59, 17.70s/it]  5%|▍         | 23/500 [06:35<2:18:36, 17.43s/it]  5%|▍         | 24/500 [06:53<2:20:37, 17.73s/it]  5%|▌         | 25/500 [07:10<2:19:07, 17.57s/it]  5%|▌         | 26/500 [07:27<2:15:58, 17.21s/it]  5%|▌         | 27/500 [07:44<2:14:30, 17.06s/it]  6%|▌         | 28/500 [08:00<2:13:50, 17.01s/it]  6%|▌         | 29/500 [08:22<2:24:55, 18.46s/it]  6%|▌         | 30/500 [08:40<2:22:29, 18.19s/it]  6%|▌         | 31/500 [08:57<2:19:10, 17.81s/it]  6%|▋         | 32/500 [09:14<2:16:53, 17.55s/it]  7%|▋         | 33/500 [09:32<2:17:11, 17.63s/it]  7%|▋         | 34/500 [09:50<2:18:24, 17.82s/it]  7%|▋         | 35/500 [10:08<2:18:44, 17.90s/it]  7%|▋         | 36/500 [10:25<2:16:45, 17.68s/it]  7%|▋         | 37/500 [10:43<2:17:14, 17.79s/it]  8%|▊         | 38/500 [11:00<2:15:37, 17.61s/it]  8%|▊         | 39/500 [11:17<2:14:07, 17.46s/it]  8%|▊         | 40/500 [11:34<2:13:03, 17.36s/it]  8%|▊         | 41/500 [11:57<2:23:47, 18.80s/it]  8%|▊         | 41/500 [11:57<2:13:48, 17.49s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.309 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.028 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▄▄▄▄▅▆▂▃▄▅▆▆▇▇▆▇█▆█████▇█████▇█████████
wandb:     train_loss ▅▅▅▃▄▃▁▄▁▁█▁▃▁▂▃▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁███▆▆▅▂▃▂▃▅▄▅▅▄▆▆▄▆▆▆▆▅▅▆▅▅▆▅▅▅▆▅▆▆▅▅▅▆
wandb:       val_loss ▁▁▁▁▁▁▁▁▂▂▂▁█▁▂▁▂▂▁▂▁▁▁▃▁▁▁▁▁▁▂▁▂▂▂▂▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.55556
wandb:       val_loss 4.56855
wandb: 
wandb: 🚀 View run magic-universe-559 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vvac3hix
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_083257-vvac3hix/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_084532-nyfj2bne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-capybara-561
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nyfj2bne
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:20<2:53:29, 20.86s/it]  0%|          | 2/500 [00:36<2:26:21, 17.63s/it]  1%|          | 3/500 [00:51<2:18:48, 16.76s/it]  1%|          | 4/500 [01:06<2:11:48, 15.94s/it]  1%|          | 5/500 [01:21<2:09:20, 15.68s/it]  1%|          | 6/500 [01:36<2:07:10, 15.45s/it]  1%|▏         | 7/500 [01:51<2:05:29, 15.27s/it]  2%|▏         | 8/500 [02:06<2:04:44, 15.21s/it]  2%|▏         | 9/500 [02:26<2:15:23, 16.54s/it]  2%|▏         | 10/500 [02:41<2:10:46, 16.01s/it]  2%|▏         | 11/500 [02:56<2:07:41, 15.67s/it]  2%|▏         | 12/500 [03:15<2:16:58, 16.84s/it]  3%|▎         | 13/500 [03:30<2:11:56, 16.26s/it]  3%|▎         | 14/500 [03:45<2:07:39, 15.76s/it]  3%|▎         | 15/500 [03:59<2:05:01, 15.47s/it]  3%|▎         | 16/500 [04:14<2:02:36, 15.20s/it]  3%|▎         | 17/500 [04:29<2:01:00, 15.03s/it]  4%|▎         | 18/500 [04:43<1:59:43, 14.90s/it]  4%|▍         | 19/500 [04:58<1:58:33, 14.79s/it]  4%|▍         | 20/500 [05:13<1:59:39, 14.96s/it]  4%|▍         | 21/500 [05:28<2:00:22, 15.08s/it]  4%|▍         | 22/500 [05:44<2:00:18, 15.10s/it]  5%|▍         | 23/500 [05:58<1:59:20, 15.01s/it]  5%|▍         | 24/500 [06:13<1:58:34, 14.95s/it]  5%|▌         | 25/500 [06:28<1:57:25, 14.83s/it]  5%|▌         | 26/500 [06:43<1:57:04, 14.82s/it]  5%|▌         | 27/500 [06:57<1:57:09, 14.86s/it]  6%|▌         | 28/500 [07:12<1:56:11, 14.77s/it]  6%|▌         | 29/500 [07:27<1:55:34, 14.72s/it]  6%|▌         | 29/500 [07:27<2:01:02, 15.42s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.021 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▆▆▆▇▇▆▇▇▆▇▇▆▇██▇████▇██▆█
wandb:     train_loss ▇▇▆▇▆▄▅█▅▅▄▄▂▂▅▅▅▄▅▄▆▅▅▆▆▄▁▂▅
wandb:   val_accuracy ▁▁▁▃▇▄▇██▇▇▇▇██▆█▇▇█▇▇▇█▆▇▆▇▇
wandb:       val_loss ▆▆▆▆▅▄▇▆▃▄▅▅▁▅█▃▇▆▂▆▄▆▅▂▆█▆▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.7474
wandb:     train_loss 0.91189
wandb:   val_accuracy 0.53333
wandb:       val_loss 0.78559
wandb: 
wandb: 🚀 View run dazzling-capybara-561 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nyfj2bne
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_084532-nyfj2bne/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_085344-r8hkormn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-field-562
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r8hkormn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:24:57, 24.64s/it]  0%|          | 2/500 [00:45<3:03:53, 22.15s/it]  1%|          | 3/500 [01:00<2:38:12, 19.10s/it]  1%|          | 4/500 [01:15<2:25:11, 17.56s/it]  1%|          | 5/500 [01:30<2:17:27, 16.66s/it]  1%|          | 6/500 [01:45<2:12:40, 16.11s/it]  1%|▏         | 7/500 [02:01<2:09:57, 15.82s/it]  2%|▏         | 8/500 [02:16<2:07:39, 15.57s/it]  2%|▏         | 9/500 [02:30<2:05:25, 15.33s/it]  2%|▏         | 10/500 [02:46<2:05:30, 15.37s/it]  2%|▏         | 11/500 [03:01<2:04:54, 15.33s/it]  2%|▏         | 12/500 [03:16<2:04:06, 15.26s/it]  3%|▎         | 13/500 [03:36<2:14:39, 16.59s/it]  3%|▎         | 14/500 [03:51<2:11:22, 16.22s/it]  3%|▎         | 15/500 [04:07<2:09:44, 16.05s/it]  3%|▎         | 16/500 [04:22<2:07:20, 15.79s/it]  3%|▎         | 17/500 [04:37<2:04:09, 15.42s/it]  4%|▎         | 18/500 [04:51<2:02:34, 15.26s/it]  4%|▍         | 19/500 [05:07<2:02:29, 15.28s/it]  4%|▍         | 20/500 [05:22<2:01:48, 15.23s/it]  4%|▍         | 21/500 [05:37<2:01:40, 15.24s/it]  4%|▍         | 22/500 [05:57<2:11:45, 16.54s/it]  5%|▍         | 23/500 [06:12<2:07:41, 16.06s/it]  5%|▍         | 24/500 [06:27<2:05:22, 15.80s/it]  5%|▌         | 25/500 [06:42<2:03:28, 15.60s/it]  5%|▌         | 26/500 [06:57<2:01:47, 15.42s/it]  5%|▌         | 27/500 [07:17<2:12:51, 16.85s/it]  6%|▌         | 28/500 [07:37<2:19:44, 17.76s/it]  6%|▌         | 29/500 [07:52<2:12:50, 16.92s/it]  6%|▌         | 30/500 [08:07<2:08:11, 16.36s/it]  6%|▌         | 31/500 [08:22<2:05:01, 15.99s/it]  6%|▋         | 32/500 [08:37<2:02:13, 15.67s/it]  7%|▋         | 33/500 [08:52<2:00:37, 15.50s/it]  7%|▋         | 34/500 [09:07<1:59:31, 15.39s/it]  7%|▋         | 35/500 [09:22<1:57:53, 15.21s/it]  7%|▋         | 36/500 [09:37<1:57:28, 15.19s/it]  7%|▋         | 37/500 [09:52<1:56:23, 15.08s/it]  8%|▊         | 38/500 [10:07<1:56:34, 15.14s/it]  8%|▊         | 39/500 [10:23<1:56:18, 15.14s/it]  8%|▊         | 40/500 [10:37<1:55:10, 15.02s/it]  8%|▊         | 41/500 [10:53<1:56:50, 15.27s/it]  8%|▊         | 42/500 [11:09<1:57:59, 15.46s/it]  9%|▊         | 43/500 [11:24<1:57:02, 15.37s/it]  9%|▉         | 44/500 [11:40<1:57:34, 15.47s/it]  9%|▉         | 44/500 [11:40<2:00:59, 15.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.311 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.135 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb: train_accuracy ▂▁▁▁▅▂▃▃▂▂▆▇▇▆▇▅▆▇▄▆█▇██▇█▇▇▇██▇██▇█▇█▇█
wandb:     train_loss ▃▂▄▇▃▁▃▂▂▇▃▁▂▃▁▁█▁▁▃▁▄▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▁▁▁█▂▄▅▃▂▇▅▆▄▅▂▃▅▁▄▅▅▆▆▅▆▅▅▄▄▆▄▅▅▃▅▄▄▃▅
wandb:       val_loss ▃▃▅▃▂▂▂▄▂▁▃▁▃▅▁▅▇▄█▇▁▃▆▂▂▁▁▄▆▂▂▄▅▁▅█▁▂▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 43
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.95988
wandb:     train_loss 0.0042
wandb:   val_accuracy 0.50889
wandb:       val_loss 0.07453
wandb: 
wandb: 🚀 View run azure-field-562 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r8hkormn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_085344-r8hkormn/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_090604-s08l4ynn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sky-564
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s08l4ynn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:29:42, 18.00s/it]  0%|          | 2/500 [00:34<2:21:27, 17.04s/it]  1%|          | 3/500 [00:51<2:21:57, 17.14s/it]  1%|          | 4/500 [01:08<2:19:51, 16.92s/it]  1%|          | 5/500 [01:24<2:17:52, 16.71s/it]  1%|          | 6/500 [01:41<2:18:51, 16.87s/it]  1%|▏         | 7/500 [01:58<2:18:51, 16.90s/it]  2%|▏         | 8/500 [02:14<2:16:33, 16.65s/it]  2%|▏         | 9/500 [02:30<2:14:56, 16.49s/it]  2%|▏         | 10/500 [02:47<2:14:43, 16.50s/it]  2%|▏         | 11/500 [03:03<2:14:07, 16.46s/it]  2%|▏         | 12/500 [03:19<2:13:07, 16.37s/it]  3%|▎         | 13/500 [03:36<2:12:51, 16.37s/it]  3%|▎         | 14/500 [03:52<2:13:12, 16.45s/it]  3%|▎         | 15/500 [04:09<2:13:45, 16.55s/it]  3%|▎         | 16/500 [04:30<2:24:35, 17.92s/it]  3%|▎         | 17/500 [04:47<2:20:07, 17.41s/it]  4%|▎         | 18/500 [05:02<2:15:39, 16.89s/it]  4%|▍         | 19/500 [05:19<2:14:01, 16.72s/it]  4%|▍         | 20/500 [05:35<2:12:29, 16.56s/it]  4%|▍         | 21/500 [05:52<2:13:29, 16.72s/it]  4%|▍         | 22/500 [06:08<2:11:29, 16.51s/it]  5%|▍         | 23/500 [06:24<2:11:12, 16.50s/it]  5%|▍         | 24/500 [06:40<2:09:56, 16.38s/it]  5%|▌         | 25/500 [06:57<2:09:56, 16.41s/it]  5%|▌         | 26/500 [07:13<2:08:57, 16.32s/it]  5%|▌         | 27/500 [07:29<2:08:35, 16.31s/it]  6%|▌         | 28/500 [07:46<2:08:04, 16.28s/it]  6%|▌         | 29/500 [08:02<2:07:07, 16.20s/it]  6%|▌         | 30/500 [08:18<2:07:14, 16.24s/it]  6%|▌         | 31/500 [08:34<2:06:53, 16.23s/it]  6%|▋         | 32/500 [08:50<2:06:41, 16.24s/it]  7%|▋         | 33/500 [09:06<2:05:46, 16.16s/it]  7%|▋         | 34/500 [09:23<2:07:20, 16.40s/it]  7%|▋         | 35/500 [09:39<2:06:29, 16.32s/it]  7%|▋         | 36/500 [09:56<2:06:41, 16.38s/it]  7%|▋         | 37/500 [10:13<2:06:54, 16.45s/it]  8%|▊         | 38/500 [10:29<2:07:04, 16.50s/it]  8%|▊         | 39/500 [10:46<2:07:24, 16.58s/it]  8%|▊         | 40/500 [11:02<2:06:29, 16.50s/it]  8%|▊         | 41/500 [11:19<2:05:42, 16.43s/it]  8%|▊         | 42/500 [11:35<2:05:36, 16.45s/it]  9%|▊         | 43/500 [11:51<2:04:47, 16.38s/it]  9%|▉         | 44/500 [12:08<2:04:37, 16.40s/it]  9%|▉         | 45/500 [12:24<2:04:23, 16.40s/it]  9%|▉         | 46/500 [12:41<2:04:15, 16.42s/it]  9%|▉         | 47/500 [12:57<2:02:48, 16.27s/it] 10%|▉         | 48/500 [13:13<2:02:34, 16.27s/it] 10%|▉         | 49/500 [13:29<2:01:25, 16.15s/it] 10%|█         | 50/500 [13:45<2:00:56, 16.13s/it] 10%|█         | 50/500 [13:45<2:03:48, 16.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.138 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁
wandb: train_accuracy ▂▁▁▂▃▂▆▄▆▆▇▄▇█▆▄█▇███▆▅▆▇▅████▄▆▆▆████▇█
wandb:     train_loss ▃▁▄▇▁▂▂▅▁▁▁▁█▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▃▁▁▁▁
wandb:   val_accuracy ▂▁▁▁▃▃█▄▆▄▇▃▆▇█▂▅▃▆▆▄▃▂▅▅▃▆▇▆▆▅▄▃▄▆▅▆▆▅▆
wandb:       val_loss ▁▂▂▁▂▁▁▁▂▁█▄▁▂▂▂▂▁▁▃▂▂▁▃▂▂▁▁▂▁▂▃▁▂▂▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 49
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.96731
wandb:     train_loss 0.00211
wandb:   val_accuracy 0.52889
wandb:       val_loss 0.14178
wandb: 
wandb: 🚀 View run drawn-sky-564 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s08l4ynn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_090604-s08l4ynn/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_092031-0zltjsa9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-dust-565
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0zltjsa9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:23:19, 17.23s/it]  0%|          | 2/500 [00:32<2:14:05, 16.16s/it]  1%|          | 3/500 [00:48<2:12:33, 16.00s/it]  1%|          | 4/500 [01:03<2:10:40, 15.81s/it]  1%|          | 5/500 [01:19<2:09:54, 15.75s/it]  1%|          | 6/500 [01:34<2:08:09, 15.57s/it]  1%|▏         | 7/500 [01:49<2:06:19, 15.37s/it]  2%|▏         | 8/500 [02:05<2:06:07, 15.38s/it]  2%|▏         | 9/500 [02:20<2:06:44, 15.49s/it]  2%|▏         | 10/500 [02:36<2:07:05, 15.56s/it]  2%|▏         | 11/500 [02:52<2:06:38, 15.54s/it]  2%|▏         | 12/500 [03:07<2:06:05, 15.50s/it]  3%|▎         | 13/500 [03:23<2:06:03, 15.53s/it]  3%|▎         | 14/500 [03:38<2:05:49, 15.53s/it]  3%|▎         | 15/500 [03:54<2:07:04, 15.72s/it]  3%|▎         | 16/500 [04:11<2:08:36, 15.94s/it]  3%|▎         | 17/500 [04:26<2:07:11, 15.80s/it]  4%|▎         | 18/500 [04:46<2:17:06, 17.07s/it]  4%|▍         | 19/500 [05:02<2:14:07, 16.73s/it]  4%|▍         | 20/500 [05:18<2:10:47, 16.35s/it]  4%|▍         | 21/500 [05:33<2:08:23, 16.08s/it]  4%|▍         | 22/500 [05:49<2:06:48, 15.92s/it]  5%|▍         | 23/500 [06:04<2:05:47, 15.82s/it]  5%|▍         | 24/500 [06:20<2:04:23, 15.68s/it]  5%|▌         | 25/500 [06:35<2:03:23, 15.59s/it]  5%|▌         | 26/500 [06:51<2:03:06, 15.58s/it]  5%|▌         | 27/500 [07:06<2:03:25, 15.66s/it]  6%|▌         | 28/500 [07:27<2:14:04, 17.04s/it]  6%|▌         | 29/500 [07:42<2:09:53, 16.55s/it]  6%|▌         | 30/500 [07:58<2:07:21, 16.26s/it]  6%|▌         | 31/500 [08:14<2:06:19, 16.16s/it]  6%|▋         | 32/500 [08:29<2:05:22, 16.07s/it]  7%|▋         | 33/500 [08:45<2:03:48, 15.91s/it]  7%|▋         | 34/500 [09:00<2:02:33, 15.78s/it]  7%|▋         | 35/500 [09:16<2:01:33, 15.68s/it]  7%|▋         | 36/500 [09:32<2:01:19, 15.69s/it]  7%|▋         | 37/500 [09:47<2:01:13, 15.71s/it]  8%|▊         | 38/500 [10:03<2:01:43, 15.81s/it]  8%|▊         | 39/500 [10:19<2:00:20, 15.66s/it]  8%|▊         | 40/500 [10:34<2:00:06, 15.67s/it]  8%|▊         | 41/500 [10:50<2:00:00, 15.69s/it]  8%|▊         | 42/500 [11:06<1:59:33, 15.66s/it]  9%|▊         | 43/500 [11:21<1:58:58, 15.62s/it]  9%|▉         | 44/500 [11:37<1:58:05, 15.54s/it]  9%|▉         | 45/500 [11:52<1:57:46, 15.53s/it]  9%|▉         | 46/500 [12:08<1:57:29, 15.53s/it]  9%|▉         | 47/500 [12:23<1:57:04, 15.51s/it] 10%|▉         | 48/500 [12:39<1:57:29, 15.60s/it] 10%|▉         | 49/500 [12:55<1:57:20, 15.61s/it] 10%|█         | 50/500 [13:10<1:57:32, 15.67s/it] 10%|█         | 51/500 [13:26<1:57:05, 15.65s/it] 10%|█         | 52/500 [13:46<2:06:38, 16.96s/it] 10%|█         | 52/500 [13:46<1:58:40, 15.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.019 MB of 0.318 MB uploadedwandb: - 0.264 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▁▃▄▅▆▆▅▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇████
wandb:     train_loss █▇▆▇▄▄█▆▅▆▂▄▅▄▄▄▄▄▆▅▁▂▂▇▃▇▂▂▅▃▇▂▁▃▃▅▅▁▃▁
wandb:   val_accuracy ▁▁▁▂▃▇▆▇▆█▇█▆▇▇▇▆▇▆▆▆▇▅▄▆▆▆▆▅▆▅▅▄▄▅▅▆▄▄▅
wandb:       val_loss ▄▄▄▃▃▄▃▂▅▄▃▅▂▄▂▅▂▃▂▃▄▂▃▆▄▂█▁▃▃▃▄▃▁▃▂▃▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 51
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.80981
wandb:     train_loss 0.15712
wandb:   val_accuracy 0.51111
wandb:       val_loss 0.75688
wandb: 
wandb: 🚀 View run morning-dust-565 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0zltjsa9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_092031-0zltjsa9/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_093503-s7tyczvs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-cherry-567
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s7tyczvs
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:23:37, 17.27s/it]  0%|          | 2/500 [00:32<2:15:27, 16.32s/it]  1%|          | 3/500 [00:48<2:12:48, 16.03s/it]  1%|          | 4/500 [01:04<2:12:43, 16.05s/it]  1%|          | 5/500 [01:20<2:11:19, 15.92s/it]  1%|          | 6/500 [01:35<2:09:12, 15.69s/it]  1%|▏         | 7/500 [01:55<2:20:57, 17.16s/it]  2%|▏         | 8/500 [02:11<2:17:26, 16.76s/it]  2%|▏         | 9/500 [02:27<2:15:18, 16.54s/it]  2%|▏         | 10/500 [02:48<2:24:52, 17.74s/it]  2%|▏         | 11/500 [03:03<2:19:12, 17.08s/it]  2%|▏         | 12/500 [03:19<2:15:28, 16.66s/it]  3%|▎         | 13/500 [03:35<2:12:32, 16.33s/it]  3%|▎         | 14/500 [03:50<2:10:28, 16.11s/it]  3%|▎         | 15/500 [04:06<2:08:58, 15.96s/it]  3%|▎         | 16/500 [04:22<2:08:15, 15.90s/it]  3%|▎         | 17/500 [04:37<2:06:59, 15.77s/it]  4%|▎         | 18/500 [04:53<2:06:51, 15.79s/it]  4%|▍         | 19/500 [05:08<2:06:17, 15.75s/it]  4%|▍         | 20/500 [05:23<2:03:45, 15.47s/it]  4%|▍         | 21/500 [05:38<2:02:08, 15.30s/it]  4%|▍         | 22/500 [05:54<2:02:19, 15.35s/it]  5%|▍         | 23/500 [06:09<2:02:28, 15.41s/it]  5%|▍         | 24/500 [06:25<2:03:09, 15.52s/it]  5%|▌         | 25/500 [06:41<2:03:07, 15.55s/it]  5%|▌         | 26/500 [06:56<2:02:13, 15.47s/it]  5%|▌         | 26/500 [06:56<2:06:31, 16.02s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.135 MB of 0.312 MB uploadedwandb: - 0.310 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▄▅▂▄▆▃▄▇▇▇▇▆▇▆▇█▇▆██▇██
wandb:     train_loss ▃▃▃▆▂▁▂▃▄▃▁▁▁▁█▁▁▁▁▁▅▁▁▂▁▁
wandb:   val_accuracy ▂▆▁▄█▂▅▇▄▅▇▇▇█▅▇▅▇█▇▆▇█▆▆▇
wandb:       val_loss ▃▄▅▂▁▂▃▃▁▁▃▁▃▁▄▁▆▆▄▇█▂▁▇▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.93908
wandb:     train_loss 0.20745
wandb:   val_accuracy 0.55111
wandb:       val_loss 0.75408
wandb: 
wandb: 🚀 View run apricot-cherry-567 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s7tyczvs
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_093503-s7tyczvs/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_094242-s8lgy57u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-puddle-569
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s8lgy57u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:16, 18.67s/it]  0%|          | 2/500 [00:34<2:18:55, 16.74s/it]  1%|          | 3/500 [00:50<2:15:48, 16.40s/it]  1%|          | 4/500 [01:05<2:13:33, 16.16s/it]  1%|          | 5/500 [01:21<2:11:27, 15.93s/it]  1%|          | 6/500 [01:37<2:10:23, 15.84s/it]  1%|▏         | 7/500 [01:52<2:09:49, 15.80s/it]  2%|▏         | 8/500 [02:08<2:08:53, 15.72s/it]  2%|▏         | 9/500 [02:24<2:08:38, 15.72s/it]  2%|▏         | 10/500 [02:39<2:07:39, 15.63s/it]  2%|▏         | 11/500 [02:55<2:07:19, 15.62s/it]  2%|▏         | 12/500 [03:10<2:07:19, 15.66s/it]  3%|▎         | 13/500 [03:26<2:06:59, 15.65s/it]  3%|▎         | 14/500 [03:42<2:07:01, 15.68s/it]  3%|▎         | 15/500 [03:57<2:06:44, 15.68s/it]  3%|▎         | 16/500 [04:18<2:18:14, 17.14s/it]  3%|▎         | 17/500 [04:33<2:14:05, 16.66s/it]  4%|▎         | 18/500 [04:49<2:11:09, 16.33s/it]  4%|▍         | 19/500 [05:05<2:09:08, 16.11s/it]  4%|▍         | 20/500 [05:20<2:07:54, 15.99s/it]  4%|▍         | 21/500 [05:36<2:06:35, 15.86s/it]  4%|▍         | 22/500 [05:51<2:05:49, 15.79s/it]  5%|▍         | 23/500 [06:07<2:05:48, 15.83s/it]  5%|▍         | 24/500 [06:23<2:04:41, 15.72s/it]  5%|▌         | 25/500 [06:39<2:04:20, 15.71s/it]  5%|▌         | 26/500 [06:54<2:03:53, 15.68s/it]  5%|▌         | 27/500 [07:10<2:03:55, 15.72s/it]  6%|▌         | 28/500 [07:25<2:03:11, 15.66s/it]  6%|▌         | 29/500 [07:41<2:02:49, 15.65s/it]  6%|▌         | 30/500 [07:57<2:03:04, 15.71s/it]  6%|▌         | 31/500 [08:16<2:10:06, 16.64s/it]  6%|▋         | 32/500 [08:31<2:07:35, 16.36s/it]  7%|▋         | 33/500 [08:47<2:05:38, 16.14s/it]  7%|▋         | 34/500 [09:03<2:03:55, 15.96s/it]  7%|▋         | 35/500 [09:18<2:02:46, 15.84s/it]  7%|▋         | 36/500 [09:34<2:01:41, 15.74s/it]  7%|▋         | 37/500 [09:49<2:00:23, 15.60s/it]  8%|▊         | 38/500 [10:05<2:00:02, 15.59s/it]  8%|▊         | 39/500 [10:20<1:59:26, 15.55s/it]  8%|▊         | 40/500 [10:35<1:58:55, 15.51s/it]  8%|▊         | 41/500 [10:51<1:58:26, 15.48s/it]  8%|▊         | 42/500 [11:06<1:58:19, 15.50s/it]  9%|▊         | 43/500 [11:22<1:58:03, 15.50s/it]  9%|▊         | 43/500 [11:22<2:00:52, 15.87s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.019 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▁▄▅▅▆▆▄▆▆▆▆▇▇▇█▇██▆█▇▅▆██▇█████████████
wandb:     train_loss ▄█▃▃▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▄█▆▇▆▄█▆▆▆▆▇▆▇▇▇▇▇▆▇▆▅▇▆▆▆▆▇▆▇▆▇▆▆▆▆▇▆
wandb:       val_loss ▁▂▁▁▁▁▁▁▁▁▂▂▅▃▁▃▂▁▃▁▂▃▅▁▃▁▁▁▅▁▁▁█▁▁▂▄▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.98068
wandb:     train_loss 6e-05
wandb:   val_accuracy 0.6
wandb:       val_loss 0.12216
wandb: 
wandb: 🚀 View run stilted-puddle-569 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/s8lgy57u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_094242-s8lgy57u/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_095455-ya3mt2s9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-disco-570
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ya3mt2s9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:30, 18.10s/it]  0%|          | 2/500 [00:34<2:20:27, 16.92s/it]  1%|          | 3/500 [00:50<2:17:24, 16.59s/it]  1%|          | 4/500 [01:05<2:13:53, 16.20s/it]  1%|          | 5/500 [01:21<2:11:03, 15.89s/it]  1%|          | 6/500 [01:36<2:09:01, 15.67s/it]  1%|▏         | 7/500 [01:51<2:07:19, 15.49s/it]  2%|▏         | 8/500 [02:06<2:06:15, 15.40s/it]  2%|▏         | 9/500 [02:22<2:05:25, 15.33s/it]  2%|▏         | 10/500 [02:37<2:05:46, 15.40s/it]  2%|▏         | 11/500 [02:52<2:05:21, 15.38s/it]  2%|▏         | 12/500 [03:08<2:04:21, 15.29s/it]  3%|▎         | 13/500 [03:23<2:05:06, 15.41s/it]  3%|▎         | 14/500 [03:38<2:04:20, 15.35s/it]  3%|▎         | 15/500 [03:54<2:04:03, 15.35s/it]  3%|▎         | 16/500 [04:09<2:03:43, 15.34s/it]  3%|▎         | 17/500 [04:24<2:03:26, 15.34s/it]  4%|▎         | 18/500 [04:40<2:02:43, 15.28s/it]  4%|▍         | 19/500 [04:55<2:02:30, 15.28s/it]  4%|▍         | 20/500 [05:09<2:00:26, 15.06s/it]  4%|▍         | 21/500 [05:24<1:58:59, 14.91s/it]  4%|▍         | 22/500 [05:39<1:59:15, 14.97s/it]  5%|▍         | 23/500 [05:54<1:59:29, 15.03s/it]  5%|▍         | 24/500 [06:10<1:59:59, 15.13s/it]  5%|▌         | 25/500 [06:25<2:00:22, 15.21s/it]  5%|▌         | 26/500 [06:40<2:00:18, 15.23s/it]  5%|▌         | 27/500 [06:56<2:01:41, 15.44s/it]  6%|▌         | 28/500 [07:12<2:01:43, 15.47s/it]  6%|▌         | 29/500 [07:27<1:59:55, 15.28s/it]  6%|▌         | 29/500 [07:27<2:01:02, 15.42s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.028 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.133 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▄▆▆▆▆▇▆▇▇▆▇▇▆▇▇▇█████▇████
wandb:     train_loss ▇▅▆▇▆▄▅█▅▆▄▅▂▃▅▅▇▄▆▄▆▆▄▆▆▄▁▁▂
wandb:   val_accuracy ▂▁▁▅▇▄▇▆▇▇██▇▇█▆█▇▇██▇▇█▇▇▇▇▇
wandb:       val_loss ▅▆▆▅▄▄▆▅▃▃▄▄▁▅▆▃▆▇▃▇▃▅▃▂▅▆█▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.73105
wandb:     train_loss 0.20908
wandb:   val_accuracy 0.56222
wandb:       val_loss 1.11925
wandb: 
wandb: 🚀 View run toasty-disco-570 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ya3mt2s9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_095455-ya3mt2s9/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_100306-2rghm9wq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-puddle-572
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2rghm9wq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:30:51, 18.14s/it]  0%|          | 2/500 [00:34<2:20:03, 16.87s/it]  1%|          | 3/500 [00:49<2:15:41, 16.38s/it]  1%|          | 4/500 [01:04<2:10:25, 15.78s/it]  1%|          | 5/500 [01:19<2:07:57, 15.51s/it]  1%|          | 6/500 [01:34<2:06:23, 15.35s/it]  1%|▏         | 7/500 [01:50<2:06:02, 15.34s/it]  2%|▏         | 8/500 [02:05<2:05:07, 15.26s/it]  2%|▏         | 9/500 [02:20<2:05:25, 15.33s/it]  2%|▏         | 10/500 [02:35<2:04:34, 15.25s/it]  2%|▏         | 11/500 [02:50<2:02:51, 15.07s/it]  2%|▏         | 12/500 [03:05<2:02:22, 15.05s/it]  3%|▎         | 13/500 [03:20<2:01:59, 15.03s/it]  3%|▎         | 14/500 [03:35<2:02:03, 15.07s/it]  3%|▎         | 15/500 [03:50<2:02:09, 15.11s/it]  3%|▎         | 16/500 [04:05<2:00:44, 14.97s/it]  3%|▎         | 17/500 [04:20<1:59:54, 14.90s/it]  4%|▎         | 18/500 [04:35<2:00:14, 14.97s/it]  4%|▍         | 19/500 [04:50<2:00:10, 14.99s/it]  4%|▍         | 20/500 [05:05<1:59:29, 14.94s/it]  4%|▍         | 21/500 [05:19<1:58:41, 14.87s/it]  4%|▍         | 22/500 [05:34<1:58:57, 14.93s/it]  5%|▍         | 23/500 [05:49<1:58:14, 14.87s/it]  5%|▍         | 24/500 [06:04<1:58:10, 14.90s/it]  5%|▌         | 25/500 [06:19<1:58:25, 14.96s/it]  5%|▌         | 26/500 [06:34<1:58:04, 14.95s/it]  5%|▌         | 27/500 [06:54<2:09:12, 16.39s/it]  6%|▌         | 28/500 [07:09<2:06:27, 16.07s/it]  6%|▌         | 29/500 [07:24<2:03:42, 15.76s/it]  6%|▌         | 30/500 [07:39<2:01:56, 15.57s/it]  6%|▌         | 31/500 [07:54<2:00:20, 15.40s/it]  6%|▋         | 32/500 [08:09<1:58:23, 15.18s/it]  7%|▋         | 33/500 [08:24<1:57:33, 15.10s/it]  7%|▋         | 34/500 [08:39<1:56:48, 15.04s/it]  7%|▋         | 35/500 [08:54<1:56:33, 15.04s/it]  7%|▋         | 36/500 [09:09<1:55:59, 15.00s/it]  7%|▋         | 37/500 [09:24<1:55:48, 15.01s/it]  8%|▊         | 38/500 [09:39<1:55:11, 14.96s/it]  8%|▊         | 39/500 [09:54<1:54:42, 14.93s/it]  8%|▊         | 40/500 [10:09<1:55:01, 15.00s/it]  8%|▊         | 41/500 [10:24<1:54:24, 14.95s/it]  8%|▊         | 42/500 [10:39<1:54:03, 14.94s/it]  9%|▊         | 43/500 [10:54<1:54:39, 15.05s/it]  9%|▉         | 44/500 [11:09<1:54:37, 15.08s/it]  9%|▉         | 44/500 [11:13<1:56:24, 15.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.311 MB uploadedwandb: \ 0.020 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb: train_accuracy ▁▅▅▆▄▅▄▅▅▆▆▄▄▄▄▄▃█▆██▇▆█▅█▇▆▆▇▇██▇█▇▇▇▇▆
wandb:     train_loss ▂▂▂▂▂▁▂▅▂▃▄▂▂▂▂▂█▁▂▂▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▅▆▇▃▅▂▂▃▅▄▃▂▃▂▄▃█▅█▆▆▅▆▃▅▆▄▄▄▅▇▇▅▆▄▃▅▃▃
wandb:       val_loss ▃▂▃▃▂▂▅▁▁▂▂▃▂▄▂▆▅▄▄▃▁▄▁▄▆▁▂▄▇▃▂█▁▄▁▇▃▂█▅
wandb: 
wandb: Run summary:
wandb:          epoch 43
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.72957
wandb:     train_loss 0.00313
wandb:   val_accuracy 0.42667
wandb:       val_loss 2.37185
wandb: 
wandb: 🚀 View run playful-puddle-572 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2rghm9wq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_100306-2rghm9wq/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_101501-npoq6jmk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-violet-573
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/npoq6jmk
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:25, 17.49s/it]  0%|          | 2/500 [00:33<2:17:17, 16.54s/it]  1%|          | 3/500 [00:50<2:17:41, 16.62s/it]  1%|          | 4/500 [01:06<2:18:06, 16.71s/it]  1%|          | 5/500 [01:22<2:15:31, 16.43s/it]  1%|          | 6/500 [01:38<2:13:27, 16.21s/it]  1%|▏         | 7/500 [01:54<2:11:44, 16.03s/it]  2%|▏         | 8/500 [02:10<2:12:35, 16.17s/it]  2%|▏         | 9/500 [02:26<2:12:15, 16.16s/it]  2%|▏         | 10/500 [02:43<2:11:47, 16.14s/it]  2%|▏         | 11/500 [02:59<2:13:07, 16.33s/it]  2%|▏         | 12/500 [03:15<2:12:06, 16.24s/it]  3%|▎         | 13/500 [03:31<2:11:24, 16.19s/it]  3%|▎         | 14/500 [03:48<2:13:02, 16.42s/it]  3%|▎         | 15/500 [04:05<2:12:17, 16.37s/it]  3%|▎         | 16/500 [04:21<2:12:32, 16.43s/it]  3%|▎         | 17/500 [04:38<2:12:20, 16.44s/it]  4%|▎         | 18/500 [04:53<2:10:35, 16.26s/it]  4%|▍         | 19/500 [05:10<2:10:47, 16.31s/it]  4%|▍         | 20/500 [05:26<2:08:58, 16.12s/it]  4%|▍         | 21/500 [05:42<2:08:23, 16.08s/it]  4%|▍         | 22/500 [05:58<2:08:23, 16.12s/it]  5%|▍         | 23/500 [06:14<2:09:35, 16.30s/it]  5%|▍         | 24/500 [06:31<2:08:54, 16.25s/it]  5%|▌         | 25/500 [06:47<2:09:35, 16.37s/it]  5%|▌         | 26/500 [07:04<2:10:34, 16.53s/it]  5%|▌         | 27/500 [07:21<2:10:45, 16.59s/it]  6%|▌         | 28/500 [07:37<2:09:19, 16.44s/it]  6%|▌         | 29/500 [07:53<2:09:02, 16.44s/it]  6%|▌         | 30/500 [08:10<2:08:05, 16.35s/it]  6%|▌         | 31/500 [08:26<2:07:27, 16.31s/it]  6%|▋         | 32/500 [08:42<2:06:44, 16.25s/it]  7%|▋         | 33/500 [08:58<2:05:58, 16.19s/it]  7%|▋         | 34/500 [09:14<2:05:20, 16.14s/it]  7%|▋         | 35/500 [09:30<2:04:40, 16.09s/it]  7%|▋         | 36/500 [09:47<2:06:09, 16.31s/it]  7%|▋         | 37/500 [10:03<2:05:55, 16.32s/it]  8%|▊         | 38/500 [10:24<2:15:32, 17.60s/it]  8%|▊         | 39/500 [10:40<2:12:21, 17.23s/it]  8%|▊         | 40/500 [10:57<2:10:42, 17.05s/it]  8%|▊         | 41/500 [11:17<2:18:45, 18.14s/it]  8%|▊         | 42/500 [11:33<2:13:12, 17.45s/it]  9%|▊         | 43/500 [11:49<2:09:53, 17.05s/it]  9%|▉         | 44/500 [12:05<2:07:11, 16.74s/it]  9%|▉         | 45/500 [12:21<2:05:11, 16.51s/it]  9%|▉         | 45/500 [12:21<2:05:02, 16.49s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.302 MB uploadedwandb: / 0.010 MB of 0.302 MB uploadedwandb: - 0.025 MB of 0.302 MB uploadedwandb: \ 0.232 MB of 0.302 MB uploadedwandb: | 0.302 MB of 0.302 MB uploadedwandb: / 0.302 MB of 0.302 MB uploadedwandb: - 0.302 MB of 0.302 MB uploadedwandb: \ 0.302 MB of 0.302 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▂▃▆▂▃▆▇▃▄▅▇▄▄▆▇▇▇█▇▅▇▇▇█▆▂▇▆▇▆█▃▇█▅▆█▇
wandb:     train_loss ▄▂▅▆▂▁▄▃▁▅▁▁▁▇▂▁▁▂▁▂▁▆▁▁▁▁▁▂█▁▁▁▁▁▁▁▄▁▁▁
wandb:   val_accuracy ▂▁▂▃█▃▄▆█▂▄▄█▂▅▇▅█▇▆▄▅▄▄▇▇▄▄▇▅▅▄▆▅▄▆▃▃▇▆
wandb:       val_loss ▁▁▂▁▁▁▁▁▁▂▁█▁▂▁▂▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 44
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.8529
wandb:     train_loss 0.00248
wandb:   val_accuracy 0.52
wandb:       val_loss 1.83113
wandb: 
wandb: 🚀 View run unique-violet-573 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/npoq6jmk
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_101501-npoq6jmk/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_102810-d2fk7csn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-oath-575
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d2fk7csn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:20:57, 16.95s/it]  0%|          | 2/500 [00:32<2:13:45, 16.12s/it]  1%|          | 3/500 [00:48<2:12:44, 16.03s/it]  1%|          | 4/500 [01:08<2:26:53, 17.77s/it]  1%|          | 5/500 [01:24<2:20:34, 17.04s/it]  1%|          | 6/500 [01:45<2:29:59, 18.22s/it]  1%|▏         | 7/500 [02:00<2:23:10, 17.43s/it]  2%|▏         | 8/500 [02:16<2:18:12, 16.86s/it]  2%|▏         | 9/500 [02:32<2:14:57, 16.49s/it]  2%|▏         | 10/500 [02:47<2:12:37, 16.24s/it]  2%|▏         | 11/500 [03:03<2:10:37, 16.03s/it]  2%|▏         | 12/500 [03:18<2:09:07, 15.88s/it]  3%|▎         | 13/500 [03:35<2:09:40, 15.98s/it]  3%|▎         | 14/500 [03:50<2:09:01, 15.93s/it]  3%|▎         | 15/500 [04:07<2:09:12, 15.98s/it]  3%|▎         | 16/500 [04:22<2:08:09, 15.89s/it]  3%|▎         | 17/500 [04:38<2:07:50, 15.88s/it]  4%|▎         | 18/500 [04:54<2:06:55, 15.80s/it]  4%|▍         | 19/500 [05:09<2:06:25, 15.77s/it]  4%|▍         | 20/500 [05:25<2:05:40, 15.71s/it]  4%|▍         | 21/500 [05:41<2:06:07, 15.80s/it]  4%|▍         | 22/500 [05:57<2:05:41, 15.78s/it]  5%|▍         | 23/500 [06:12<2:04:37, 15.68s/it]  5%|▍         | 24/500 [06:28<2:03:59, 15.63s/it]  5%|▌         | 25/500 [06:43<2:03:44, 15.63s/it]  5%|▌         | 26/500 [06:59<2:03:06, 15.58s/it]  5%|▌         | 27/500 [07:14<2:03:02, 15.61s/it]  6%|▌         | 28/500 [07:30<2:03:13, 15.66s/it]  6%|▌         | 29/500 [07:46<2:03:05, 15.68s/it]  6%|▌         | 29/500 [07:46<2:06:16, 16.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.137 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▂▂▃▂▃▄▆▇▅▇▆▇▇█▆█▇▇████▇▆██
wandb:     train_loss █▅▇█▇▇▅▇▆▇▇▆▄▃▅▅▇▆▆▅▇▇▄▇▇▇▁▁▄
wandb:   val_accuracy ▂▁▁▁▂▁▁▁▂▃▆▆▇█▇▇▇▆▇▇█▇▇▇▇██▇▇
wandb:       val_loss ▆█▇▆▇▆▅▆▆▆▆▅▁▅█▂▆▅▂▅▃█▃▁▆▂▄▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.69391
wandb:     train_loss 0.64775
wandb:   val_accuracy 0.55556
wandb:       val_loss 0.69056
wandb: 
wandb: 🚀 View run lyric-oath-575 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d2fk7csn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_102810-d2fk7csn/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_103641-elacww4p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-dust-576
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/elacww4p
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:38, 18.71s/it]  0%|          | 2/500 [00:34<2:21:15, 17.02s/it]  1%|          | 3/500 [00:55<2:36:21, 18.88s/it]  1%|          | 4/500 [01:10<2:21:39, 17.14s/it]  1%|          | 5/500 [01:29<2:27:22, 17.86s/it]  1%|          | 6/500 [01:44<2:19:52, 16.99s/it]  1%|▏         | 7/500 [01:59<2:14:24, 16.36s/it]  2%|▏         | 8/500 [02:14<2:11:01, 15.98s/it]  2%|▏         | 9/500 [02:29<2:08:26, 15.70s/it]  2%|▏         | 10/500 [02:49<2:18:35, 16.97s/it]  2%|▏         | 11/500 [03:05<2:14:33, 16.51s/it]  2%|▏         | 12/500 [03:25<2:24:41, 17.79s/it]  3%|▎         | 13/500 [03:40<2:17:35, 16.95s/it]  3%|▎         | 14/500 [03:56<2:14:55, 16.66s/it]  3%|▎         | 15/500 [04:13<2:14:35, 16.65s/it]  3%|▎         | 16/500 [04:30<2:14:03, 16.62s/it]  3%|▎         | 17/500 [04:47<2:15:01, 16.77s/it]  4%|▎         | 18/500 [05:04<2:15:33, 16.87s/it]  4%|▍         | 19/500 [05:20<2:14:34, 16.79s/it]  4%|▍         | 20/500 [05:36<2:12:29, 16.56s/it]  4%|▍         | 21/500 [05:53<2:11:43, 16.50s/it]  4%|▍         | 22/500 [06:09<2:11:12, 16.47s/it]  5%|▍         | 23/500 [06:26<2:10:39, 16.44s/it]  5%|▍         | 24/500 [06:42<2:10:32, 16.45s/it]  5%|▌         | 25/500 [07:03<2:20:59, 17.81s/it]  5%|▌         | 26/500 [07:20<2:18:10, 17.49s/it]  5%|▌         | 27/500 [07:36<2:14:03, 17.01s/it]  6%|▌         | 28/500 [07:57<2:23:40, 18.26s/it]  6%|▌         | 29/500 [08:13<2:18:47, 17.68s/it]  6%|▌         | 30/500 [08:29<2:14:49, 17.21s/it]  6%|▌         | 31/500 [08:45<2:12:05, 16.90s/it]  6%|▋         | 32/500 [09:02<2:10:02, 16.67s/it]  7%|▋         | 33/500 [09:18<2:08:19, 16.49s/it]  7%|▋         | 34/500 [09:35<2:09:15, 16.64s/it]  7%|▋         | 35/500 [09:51<2:07:41, 16.48s/it]  7%|▋         | 36/500 [10:07<2:07:42, 16.51s/it]  7%|▋         | 37/500 [10:24<2:08:09, 16.61s/it]  8%|▊         | 38/500 [10:45<2:17:46, 17.89s/it]  8%|▊         | 39/500 [11:02<2:14:54, 17.56s/it]  8%|▊         | 40/500 [11:18<2:12:02, 17.22s/it]  8%|▊         | 41/500 [11:39<2:18:49, 18.15s/it]  8%|▊         | 41/500 [11:39<2:10:25, 17.05s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.216 MB of 0.313 MB uploadedwandb: \ 0.216 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▅▅▂▄▂▂▄▂▃▃▂▂▄▃▅▅▄▄▅▄▆▆▅▄▄▄▆▆▄▇▇▅▇▇█▆▅▇
wandb:     train_loss ▄▄▃▃▃▂▂▁▃▃█▃▁▂▄▄▅▂▂▃▂▂▂▃▂▁▁▁▃▁▇▁▆▁▁▁▁▂▁▁
wandb:   val_accuracy ▁▆█▇▄▆▄▁▆▃▁▄▃▄▅▂▆▅▄▂▃▃▃▃▂▂▂▁▄▄▂▄▅▂▆▄▆▃▂▄
wandb:       val_loss ▂▂▂▃▂▁▅▂▂▂▃▁█▂▃▂▃▂▁▆▂▁▂▃▁▄▂▂▃▃▃▂▂▂▆▁▂▄▅▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.82169
wandb:     train_loss 0.07918
wandb:   val_accuracy 0.43111
wandb:       val_loss 0.13475
wandb: 
wandb: 🚀 View run hardy-dust-576 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/elacww4p
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_103641-elacww4p/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_104916-63w8b1we
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-smoke-578
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/63w8b1we
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:43:14, 19.63s/it]  0%|          | 2/500 [00:35<2:26:03, 17.60s/it]  1%|          | 3/500 [00:51<2:19:45, 16.87s/it]  1%|          | 4/500 [01:07<2:16:44, 16.54s/it]  1%|          | 5/500 [01:23<2:13:21, 16.17s/it]  1%|          | 6/500 [01:39<2:12:00, 16.03s/it]  1%|▏         | 7/500 [01:54<2:10:43, 15.91s/it]  2%|▏         | 8/500 [02:11<2:11:51, 16.08s/it]  2%|▏         | 9/500 [02:27<2:11:00, 16.01s/it]  2%|▏         | 10/500 [02:42<2:09:28, 15.85s/it]  2%|▏         | 11/500 [02:58<2:08:09, 15.73s/it]  2%|▏         | 12/500 [03:13<2:07:00, 15.62s/it]  3%|▎         | 13/500 [03:29<2:08:02, 15.77s/it]  3%|▎         | 14/500 [03:45<2:08:02, 15.81s/it]  3%|▎         | 15/500 [04:00<2:07:04, 15.72s/it]  3%|▎         | 16/500 [04:16<2:06:34, 15.69s/it]  3%|▎         | 17/500 [04:32<2:06:04, 15.66s/it]  4%|▎         | 18/500 [04:47<2:05:30, 15.62s/it]  4%|▍         | 19/500 [05:03<2:04:51, 15.58s/it]  4%|▍         | 20/500 [05:18<2:04:40, 15.58s/it]  4%|▍         | 21/500 [05:34<2:04:26, 15.59s/it]  4%|▍         | 22/500 [05:49<2:04:02, 15.57s/it]  4%|▍         | 22/500 [05:49<2:06:43, 15.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.027 MB of 0.316 MB uploadedwandb: \ 0.230 MB of 0.316 MB uploadedwandb: | 0.230 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▄▄▄▃▃▂▂▁▄▂▂▃▂▃▂▃▃▄█▅▅
wandb:     train_loss ▃▅▃▄▂▃▁▂▄▇█▂▄▇▆▆█▆▁▁▁▂
wandb:   val_accuracy ▁▆▅▆▃█▂▅▂▇▆▇▆▄▆▅▆▆▆▇▇█
wandb:       val_loss ▃▂▂▃▃▁▅▁▂▃▆▁▂▂▃▂▅▅▃▃█▁
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.52006
wandb:     train_loss 0.40295
wandb:   val_accuracy 0.51778
wandb:       val_loss 0.43894
wandb: 
wandb: 🚀 View run dainty-smoke-578 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/63w8b1we
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_104916-63w8b1we/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_105547-ffmcalae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-moon-579
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ffmcalae
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:02, 17.56s/it]  0%|          | 2/500 [00:33<2:18:25, 16.68s/it]  1%|          | 3/500 [00:49<2:16:17, 16.45s/it]  1%|          | 4/500 [01:05<2:15:04, 16.34s/it]  1%|          | 5/500 [01:22<2:15:19, 16.40s/it]  1%|          | 6/500 [01:38<2:14:19, 16.31s/it]  1%|▏         | 7/500 [01:54<2:11:48, 16.04s/it]  2%|▏         | 8/500 [02:09<2:09:06, 15.74s/it]  2%|▏         | 9/500 [02:24<2:08:18, 15.68s/it]  2%|▏         | 10/500 [02:40<2:07:42, 15.64s/it]  2%|▏         | 11/500 [02:55<2:07:02, 15.59s/it]  2%|▏         | 12/500 [03:10<2:05:17, 15.40s/it]  3%|▎         | 13/500 [03:25<2:04:10, 15.30s/it]  3%|▎         | 14/500 [03:42<2:07:21, 15.72s/it]  3%|▎         | 15/500 [03:58<2:08:11, 15.86s/it]  3%|▎         | 16/500 [04:19<2:19:37, 17.31s/it]  3%|▎         | 17/500 [04:35<2:16:07, 16.91s/it]  4%|▎         | 18/500 [04:50<2:12:48, 16.53s/it]  4%|▍         | 19/500 [05:07<2:13:26, 16.65s/it]  4%|▍         | 20/500 [05:24<2:13:58, 16.75s/it]  4%|▍         | 21/500 [05:40<2:11:15, 16.44s/it]  4%|▍         | 22/500 [05:56<2:09:32, 16.26s/it]  5%|▍         | 23/500 [06:11<2:07:09, 16.00s/it]  5%|▍         | 23/500 [06:11<2:08:32, 16.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.132 MB of 0.315 MB uploadedwandb: / 0.310 MB of 0.315 MB uploadedwandb: - 0.310 MB of 0.315 MB uploadedwandb: \ 0.310 MB of 0.315 MB uploadedwandb: | 0.310 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▅▅▅▅▁▅▅▅▅▄▆▅▅▅▃▅▅▆▅▇█▆▆
wandb:     train_loss ▄▃▄▇▄▁▄▄▅▄▄█▁▁▅█▁▄▃▃▃▃▄
wandb:   val_accuracy █▇▇▇▁▇▆▆▄▂▇▆▆▇▂▆▆▇▄▅▇▇▇
wandb:       val_loss ▁▁▂▁▁▂▁▁▁▁▁▃▂█▁▁▂▂▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.35364
wandb:     train_loss 1.13148
wandb:   val_accuracy 0.33111
wandb:       val_loss 0.82244
wandb: 
wandb: 🚀 View run drawn-moon-579 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ffmcalae
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_105547-ffmcalae/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_110237-f3mxb015
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-capybara-581
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f3mxb015
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:19, 17.71s/it]  0%|          | 2/500 [00:35<2:24:58, 17.47s/it]  1%|          | 3/500 [00:51<2:20:06, 16.92s/it]  1%|          | 4/500 [01:08<2:19:29, 16.87s/it]  1%|          | 5/500 [01:28<2:31:07, 18.32s/it]  1%|          | 6/500 [01:45<2:25:01, 17.61s/it]  1%|▏         | 7/500 [02:01<2:20:30, 17.10s/it]  2%|▏         | 8/500 [02:17<2:18:04, 16.84s/it]  2%|▏         | 9/500 [02:34<2:17:16, 16.77s/it]  2%|▏         | 10/500 [02:49<2:14:35, 16.48s/it]  2%|▏         | 11/500 [03:05<2:12:25, 16.25s/it]  2%|▏         | 12/500 [03:21<2:11:11, 16.13s/it]  3%|▎         | 13/500 [03:37<2:10:23, 16.06s/it]  3%|▎         | 14/500 [03:53<2:10:22, 16.10s/it]  3%|▎         | 15/500 [04:09<2:10:27, 16.14s/it]  3%|▎         | 16/500 [04:31<2:22:22, 17.65s/it]  3%|▎         | 17/500 [04:46<2:17:36, 17.10s/it]  4%|▎         | 18/500 [05:02<2:14:49, 16.78s/it]  4%|▍         | 19/500 [05:23<2:23:36, 17.91s/it]  4%|▍         | 20/500 [05:39<2:19:07, 17.39s/it]  4%|▍         | 21/500 [05:55<2:15:49, 17.01s/it]  4%|▍         | 22/500 [06:11<2:12:27, 16.63s/it]  5%|▍         | 23/500 [06:27<2:09:58, 16.35s/it]  5%|▍         | 24/500 [06:42<2:08:14, 16.16s/it]  5%|▌         | 25/500 [07:03<2:18:27, 17.49s/it]  5%|▌         | 26/500 [07:23<2:23:28, 18.16s/it]  5%|▌         | 27/500 [07:39<2:18:20, 17.55s/it]  6%|▌         | 28/500 [07:55<2:14:15, 17.07s/it]  6%|▌         | 29/500 [08:11<2:11:05, 16.70s/it]  6%|▌         | 30/500 [08:27<2:09:46, 16.57s/it]  6%|▌         | 31/500 [08:43<2:07:29, 16.31s/it]  6%|▋         | 32/500 [08:59<2:08:33, 16.48s/it]  7%|▋         | 33/500 [09:20<2:17:13, 17.63s/it]  7%|▋         | 34/500 [09:36<2:12:48, 17.10s/it]  7%|▋         | 35/500 [09:52<2:11:33, 16.98s/it]  7%|▋         | 36/500 [10:08<2:08:56, 16.67s/it]  7%|▋         | 37/500 [10:24<2:07:09, 16.48s/it]  8%|▊         | 38/500 [10:40<2:05:38, 16.32s/it]  8%|▊         | 39/500 [10:57<2:06:20, 16.44s/it]  8%|▊         | 39/500 [10:57<2:09:32, 16.86s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.137 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▃▄▃▄▄▄▄█▂▃▃▂▄▃▃▅▅▃▄▃▄▅▄▃▅▅▆▂▂▄▂▁▄▂▃▁▁
wandb:     train_loss ▃▄▃▃▃▃▂▁▃▂▇▂▂▅▄▄▇▄▂▄▁▄▂▂▂▂▁▁▂▂▅▁█▃█▂▁▃▁
wandb:   val_accuracy ▃██▄▂▂▃▄▃▇▁▂▃▂▅▃▄▅▄▁▂▁▃▄▂▂▄▃▇▂▃▅▃▂▄▃▄▂▃
wandb:       val_loss ▂▂▂▃▂▂▄▁▁▁▃▂▂▃▅▂▄▄▃▅▂▂▁▆▁▆▃▁▂▂█▂▄▄▇▃▇▅▆
wandb: 
wandb: Run summary:
wandb:          epoch 38
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.38336
wandb:     train_loss 0.11388
wandb:   val_accuracy 0.32889
wandb:       val_loss 3.47037
wandb: 
wandb: 🚀 View run valiant-capybara-581 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f3mxb015
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_110237-f3mxb015/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_111419-6ao5ydvn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-wood-582
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6ao5ydvn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:04, 25.74s/it]  0%|          | 2/500 [00:55<3:51:10, 27.85s/it]  1%|          | 3/500 [01:19<3:38:20, 26.36s/it]  1%|          | 4/500 [01:43<3:30:19, 25.44s/it]  1%|          | 5/500 [02:08<3:28:15, 25.24s/it]  1%|          | 6/500 [02:32<3:24:37, 24.85s/it]  1%|▏         | 7/500 [02:56<3:22:06, 24.60s/it]  2%|▏         | 8/500 [03:20<3:20:22, 24.44s/it]  2%|▏         | 9/500 [03:44<3:19:03, 24.32s/it]  2%|▏         | 10/500 [04:08<3:17:57, 24.24s/it]  2%|▏         | 11/500 [04:33<3:17:05, 24.18s/it]  2%|▏         | 12/500 [04:57<3:16:12, 24.12s/it]  3%|▎         | 13/500 [05:21<3:16:41, 24.23s/it]  3%|▎         | 14/500 [05:45<3:16:45, 24.29s/it]  3%|▎         | 15/500 [06:10<3:16:07, 24.26s/it]  3%|▎         | 16/500 [06:33<3:14:30, 24.11s/it]  3%|▎         | 17/500 [06:57<3:13:31, 24.04s/it]  4%|▎         | 18/500 [07:22<3:14:14, 24.18s/it]  4%|▍         | 19/500 [07:46<3:13:40, 24.16s/it]  4%|▍         | 20/500 [08:10<3:13:14, 24.15s/it]  4%|▍         | 21/500 [08:34<3:12:49, 24.15s/it]  4%|▍         | 22/500 [08:59<3:12:56, 24.22s/it]  5%|▍         | 23/500 [09:23<3:12:01, 24.15s/it]  5%|▍         | 24/500 [09:47<3:11:10, 24.10s/it]  5%|▌         | 25/500 [10:11<3:11:31, 24.19s/it]  5%|▌         | 26/500 [10:35<3:10:31, 24.12s/it]  5%|▌         | 27/500 [10:59<3:10:18, 24.14s/it]  6%|▌         | 28/500 [11:23<3:09:57, 24.15s/it]  6%|▌         | 29/500 [11:47<3:09:49, 24.18s/it]  6%|▌         | 30/500 [12:12<3:09:05, 24.14s/it]  6%|▌         | 31/500 [12:36<3:08:21, 24.10s/it]  6%|▋         | 32/500 [13:04<3:19:11, 25.54s/it]  7%|▋         | 33/500 [13:29<3:15:22, 25.10s/it]  7%|▋         | 34/500 [13:53<3:12:34, 24.80s/it]  7%|▋         | 35/500 [14:17<3:10:38, 24.60s/it]  7%|▋         | 36/500 [14:41<3:09:10, 24.46s/it]  7%|▋         | 37/500 [15:05<3:07:27, 24.29s/it]  8%|▊         | 38/500 [15:29<3:06:02, 24.16s/it]  8%|▊         | 39/500 [15:53<3:06:23, 24.26s/it]  8%|▊         | 40/500 [16:17<3:05:24, 24.18s/it]  8%|▊         | 41/500 [16:46<3:16:43, 25.72s/it]  8%|▊         | 41/500 [16:46<3:07:52, 24.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.028 MB uploadedwandb: \ 0.019 MB of 0.307 MB uploadedwandb: | 0.303 MB of 0.307 MB uploadedwandb: / 0.303 MB of 0.307 MB uploadedwandb: - 0.303 MB of 0.307 MB uploadedwandb: \ 0.303 MB of 0.307 MB uploadedwandb: | 0.303 MB of 0.307 MB uploadedwandb: / 0.307 MB of 0.307 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▃▆▅▆▆▆▇▇▇▇▇▇▇█▇█▇▇█▇████▆█████████████
wandb:     train_loss ▆▃▆▃▁▃▅▁▁▁█▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▄█▇▇▇▇▆▅▆▇▆▄▅▅▅▆▅▅▅▆▆▆▇▆▄█▇▇▆▇▇▆▇▇▇▆▆█
wandb:       val_loss ▂▂▁▁▁▁▂▂▁▁▂▁▃▁▁▁█▁▄▂▂▄▁▂▁▂▂▁▇▄▁▂▃▃▃▂▂▅▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 1.0
wandb:     train_loss 0.00085
wandb:   val_accuracy 0.57333
wandb:       val_loss 0.19453
wandb: 
wandb: 🚀 View run copper-wood-582 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/6ao5ydvn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_111419-6ao5ydvn/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_113147-20xz0aoi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-haze-584
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/20xz0aoi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:28:28, 25.07s/it]  0%|          | 2/500 [00:48<3:21:12, 24.24s/it]  1%|          | 3/500 [01:12<3:19:58, 24.14s/it]  1%|          | 4/500 [01:36<3:17:58, 23.95s/it]  1%|          | 5/500 [01:59<3:16:16, 23.79s/it]  1%|          | 6/500 [02:23<3:15:38, 23.76s/it]  1%|▏         | 7/500 [02:47<3:15:12, 23.76s/it]  2%|▏         | 8/500 [03:11<3:14:42, 23.75s/it]  2%|▏         | 9/500 [03:34<3:13:31, 23.65s/it]  2%|▏         | 10/500 [03:58<3:14:00, 23.76s/it]  2%|▏         | 11/500 [04:22<3:13:14, 23.71s/it]  2%|▏         | 12/500 [04:46<3:13:20, 23.77s/it]  3%|▎         | 13/500 [05:14<3:25:39, 25.34s/it]  3%|▎         | 14/500 [05:38<3:20:59, 24.81s/it]  3%|▎         | 15/500 [06:02<3:17:36, 24.45s/it]  3%|▎         | 16/500 [06:25<3:15:19, 24.21s/it]  3%|▎         | 17/500 [06:49<3:13:56, 24.09s/it]  4%|▎         | 18/500 [07:13<3:12:17, 23.94s/it]  4%|▍         | 19/500 [07:36<3:10:30, 23.76s/it]  4%|▍         | 20/500 [08:00<3:09:30, 23.69s/it]  4%|▍         | 21/500 [08:23<3:08:52, 23.66s/it]  4%|▍         | 22/500 [08:47<3:07:40, 23.56s/it]  5%|▍         | 23/500 [09:10<3:07:16, 23.56s/it]  5%|▍         | 24/500 [09:34<3:06:42, 23.54s/it]  5%|▌         | 25/500 [09:58<3:07:33, 23.69s/it]  5%|▌         | 26/500 [10:21<3:07:01, 23.67s/it]  5%|▌         | 27/500 [10:45<3:06:53, 23.71s/it]  6%|▌         | 28/500 [11:08<3:05:48, 23.62s/it]  6%|▌         | 29/500 [11:37<3:17:57, 25.22s/it]  6%|▌         | 30/500 [12:01<3:13:36, 24.72s/it]  6%|▌         | 31/500 [12:25<3:11:08, 24.45s/it]  6%|▋         | 32/500 [12:48<3:08:16, 24.14s/it]  7%|▋         | 33/500 [13:12<3:06:40, 23.98s/it]  7%|▋         | 34/500 [13:36<3:05:44, 23.92s/it]  7%|▋         | 35/500 [13:59<3:04:31, 23.81s/it]  7%|▋         | 36/500 [14:23<3:03:22, 23.71s/it]  7%|▋         | 37/500 [14:46<3:02:20, 23.63s/it]  8%|▊         | 38/500 [15:13<3:09:33, 24.62s/it]  8%|▊         | 39/500 [15:37<3:07:54, 24.46s/it]  8%|▊         | 40/500 [16:01<3:06:14, 24.29s/it]  8%|▊         | 41/500 [16:24<3:04:00, 24.05s/it]  8%|▊         | 42/500 [16:48<3:02:10, 23.87s/it]  9%|▊         | 43/500 [17:12<3:01:18, 23.80s/it]  9%|▉         | 44/500 [17:35<3:00:53, 23.80s/it]  9%|▉         | 45/500 [17:59<3:00:07, 23.75s/it]  9%|▉         | 46/500 [18:23<2:59:37, 23.74s/it]  9%|▉         | 47/500 [18:52<3:11:25, 25.35s/it] 10%|▉         | 48/500 [19:15<3:06:45, 24.79s/it] 10%|▉         | 49/500 [19:39<3:03:26, 24.40s/it] 10%|█         | 50/500 [20:03<3:01:36, 24.21s/it] 10%|█         | 51/500 [20:27<3:00:43, 24.15s/it] 10%|█         | 51/500 [20:27<3:00:02, 24.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.138 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▄▄▅▄▅▅▆▆▅▆▇▆▇▅▇▇▆▇▇▇▇█▇▇█▇█▆▇▇███████
wandb:     train_loss ▄▅▄▅█▅▁▄▆▁▁▄▂▄▄▃▆▂▁▁▂▂▄▅▁▂▁▂▁▁▂▁▂▃▁▃▄▁▁▁
wandb:   val_accuracy ▂▂▂▅▁▃▃▄▅▅▆▆▇▇▆▇▆▇█▇▇█▇▇▇▇█▇▇█▅▇▆▇▇▇▇▇▇▇
wandb:       val_loss ▅▅▄▄▅▅▅▆▅▅▂▄▄█▄▃▄▄▅▂█▅█▆▂▄▄▁▃▃█▁▆▃▃▂▅▂▁▆
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.83358
wandb:     train_loss 0.15262
wandb:   val_accuracy 0.54222
wandb:       val_loss 1.38318
wandb: 
wandb: 🚀 View run unique-haze-584 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/20xz0aoi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_113147-20xz0aoi/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_115252-e4be8kdd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-field-585
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e4be8kdd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:35<4:57:47, 35.81s/it]  0%|          | 2/500 [01:00<4:01:33, 29.10s/it]  1%|          | 3/500 [01:31<4:10:38, 30.26s/it]  1%|          | 4/500 [01:56<3:53:06, 28.20s/it]  1%|          | 5/500 [02:20<3:40:04, 26.68s/it]  1%|          | 6/500 [02:45<3:32:48, 25.85s/it]  1%|▏         | 7/500 [03:14<3:41:29, 26.96s/it]  2%|▏         | 8/500 [03:47<3:56:27, 28.84s/it]  2%|▏         | 9/500 [04:16<3:58:07, 29.10s/it]  2%|▏         | 10/500 [04:47<4:00:28, 29.45s/it]  2%|▏         | 11/500 [05:11<3:46:59, 27.85s/it]  2%|▏         | 12/500 [05:35<3:37:25, 26.73s/it]  3%|▎         | 13/500 [05:59<3:30:45, 25.97s/it]  3%|▎         | 14/500 [06:23<3:25:08, 25.33s/it]  3%|▎         | 15/500 [06:47<3:21:56, 24.98s/it]  3%|▎         | 16/500 [07:12<3:20:04, 24.80s/it]  3%|▎         | 17/500 [07:36<3:18:00, 24.60s/it]  4%|▎         | 18/500 [08:00<3:16:32, 24.47s/it]  4%|▍         | 19/500 [08:24<3:14:31, 24.26s/it]  4%|▍         | 20/500 [08:48<3:12:58, 24.12s/it]  4%|▍         | 21/500 [09:12<3:12:39, 24.13s/it]  4%|▍         | 22/500 [09:36<3:12:30, 24.16s/it]  5%|▍         | 23/500 [10:00<3:11:41, 24.11s/it]  5%|▍         | 24/500 [10:25<3:12:30, 24.27s/it]  5%|▌         | 25/500 [10:49<3:12:36, 24.33s/it]  5%|▌         | 26/500 [11:18<3:22:34, 25.64s/it]  5%|▌         | 27/500 [11:42<3:18:41, 25.20s/it]  6%|▌         | 28/500 [12:12<3:29:45, 26.66s/it]  6%|▌         | 29/500 [12:36<3:22:49, 25.84s/it]  6%|▌         | 30/500 [13:00<3:18:09, 25.30s/it]  6%|▌         | 31/500 [13:29<3:26:20, 26.40s/it]  6%|▋         | 32/500 [13:53<3:21:30, 25.84s/it]  7%|▋         | 33/500 [14:18<3:18:16, 25.47s/it]  7%|▋         | 34/500 [14:42<3:14:45, 25.08s/it]  7%|▋         | 35/500 [15:06<3:12:10, 24.80s/it]  7%|▋         | 36/500 [15:30<3:09:45, 24.54s/it]  7%|▋         | 37/500 [15:55<3:09:02, 24.50s/it]  8%|▊         | 38/500 [16:19<3:07:35, 24.36s/it]  8%|▊         | 39/500 [16:43<3:06:18, 24.25s/it]  8%|▊         | 40/500 [17:07<3:05:44, 24.23s/it]  8%|▊         | 41/500 [17:31<3:05:44, 24.28s/it]  8%|▊         | 41/500 [17:31<3:16:14, 25.65s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.103 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▄▅▄▄▆▅▆▆▇▇▇▇▇▇▇███████████████████████
wandb:     train_loss ██▆▅▁▄▆▁▂▅▇▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▇▆██▇▅▅▃▄▄▄▃▃▃▂▄▃▃▃▅▄▃▅▅▂▅▄▃▃▃▄▄▅▅▆▄▂▄
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.0002
wandb: train_accuracy 1.0
wandb:     train_loss 0.00091
wandb:   val_accuracy 0.47111
wandb:       val_loss 2.16077
wandb: 
wandb: 🚀 View run happy-field-585 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e4be8kdd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_115252-e4be8kdd/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_121108-r7pc243b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-bird-587
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r7pc243b
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:30<4:16:53, 30.89s/it]  0%|          | 2/500 [00:54<3:42:05, 26.76s/it]  1%|          | 3/500 [01:19<3:32:32, 25.66s/it]  1%|          | 4/500 [01:42<3:25:59, 24.92s/it]  1%|          | 5/500 [02:06<3:21:28, 24.42s/it]  1%|          | 6/500 [02:35<3:32:58, 25.87s/it]  1%|▏         | 7/500 [02:58<3:26:29, 25.13s/it]  2%|▏         | 8/500 [03:27<3:34:46, 26.19s/it]  2%|▏         | 9/500 [03:50<3:27:38, 25.37s/it]  2%|▏         | 10/500 [04:14<3:22:32, 24.80s/it]  2%|▏         | 11/500 [04:37<3:19:04, 24.43s/it]  2%|▏         | 12/500 [05:02<3:19:38, 24.55s/it]  3%|▎         | 13/500 [05:26<3:17:29, 24.33s/it]  3%|▎         | 14/500 [05:50<3:15:43, 24.16s/it]  3%|▎         | 15/500 [06:18<3:26:17, 25.52s/it]  3%|▎         | 16/500 [06:42<3:20:47, 24.89s/it]  3%|▎         | 17/500 [07:10<3:27:55, 25.83s/it]  4%|▎         | 18/500 [07:39<3:34:25, 26.69s/it]  4%|▍         | 19/500 [08:08<3:41:02, 27.57s/it]  4%|▍         | 20/500 [08:32<3:31:06, 26.39s/it]  4%|▍         | 21/500 [09:01<3:36:53, 27.17s/it]  4%|▍         | 22/500 [09:30<3:41:24, 27.79s/it]  5%|▍         | 23/500 [10:00<3:45:20, 28.35s/it]  5%|▍         | 24/500 [10:23<3:33:41, 26.93s/it]  5%|▌         | 25/500 [10:48<3:26:38, 26.10s/it]  5%|▌         | 26/500 [11:11<3:20:37, 25.40s/it]  5%|▌         | 27/500 [11:35<3:16:29, 24.93s/it]  6%|▌         | 28/500 [11:59<3:13:10, 24.56s/it]  6%|▌         | 29/500 [12:22<3:10:17, 24.24s/it]  6%|▌         | 30/500 [12:46<3:07:51, 23.98s/it]  6%|▌         | 31/500 [13:10<3:08:55, 24.17s/it]  6%|▋         | 32/500 [13:34<3:08:37, 24.18s/it]  7%|▋         | 33/500 [13:58<3:06:55, 24.02s/it]  7%|▋         | 34/500 [14:22<3:05:40, 23.91s/it]  7%|▋         | 35/500 [14:45<3:04:25, 23.80s/it]  7%|▋         | 36/500 [15:08<3:02:33, 23.61s/it]  7%|▋         | 37/500 [15:31<3:00:44, 23.42s/it]  8%|▊         | 38/500 [16:01<3:14:45, 25.29s/it]  8%|▊         | 39/500 [16:25<3:10:29, 24.79s/it]  8%|▊         | 40/500 [16:48<3:06:59, 24.39s/it]  8%|▊         | 41/500 [17:12<3:04:33, 24.12s/it]  8%|▊         | 41/500 [17:12<3:12:36, 25.18s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▄▃▃▃▃▂▆▇▃▄▇▅▅▅▆▆▆▇▇▆█▇▆█▇▅█▆█▇██▆███▅██
wandb:     train_loss ▄▂▃▄▁▄▄▁▃▆▆▁▁▁▁▁▁▃▁▁▁▁▂▁▁▂█▁▁▂▁▁▂▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▆▅▂▂▁▂▄▄▃▃▆▄▃▃▄▅▃█▄▅▅▅▅▇▇▅▆▄▆▆▆▆▂▇▆▇▅▆▅
wandb:       val_loss ▂▁▁▂▂▂▂▂▁▂▂▁▃█▃▂▄▂▂▃▂▂▄▁▁▂▂▂▆▃▁▁▂▄▁▁▂▂▆▂
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.97325
wandb:     train_loss 0.01117
wandb:   val_accuracy 0.47778
wandb:       val_loss 1.18895
wandb: 
wandb: 🚀 View run iconic-bird-587 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/r7pc243b
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_121108-r7pc243b/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_122906-u9wz2wmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-leaf-589
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/u9wz2wmu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:37:08, 26.11s/it]  0%|          | 2/500 [00:50<3:26:23, 24.87s/it]  1%|          | 3/500 [01:19<3:43:22, 26.97s/it]  1%|          | 4/500 [01:49<3:51:58, 28.06s/it]  1%|          | 5/500 [02:18<3:55:58, 28.60s/it]  1%|          | 6/500 [02:43<3:44:08, 27.22s/it]  1%|▏         | 7/500 [03:08<3:37:58, 26.53s/it]  2%|▏         | 8/500 [03:37<3:43:44, 27.28s/it]  2%|▏         | 9/500 [04:01<3:36:00, 26.40s/it]  2%|▏         | 10/500 [04:26<3:30:33, 25.78s/it]  2%|▏         | 11/500 [04:50<3:25:22, 25.20s/it]  2%|▏         | 12/500 [05:13<3:19:28, 24.53s/it]  3%|▎         | 13/500 [05:35<3:14:51, 24.01s/it]  3%|▎         | 14/500 [05:58<3:11:55, 23.69s/it]  3%|▎         | 15/500 [06:21<3:09:06, 23.39s/it]  3%|▎         | 16/500 [06:44<3:08:01, 23.31s/it]  3%|▎         | 17/500 [07:07<3:06:57, 23.22s/it]  4%|▎         | 18/500 [07:30<3:05:59, 23.15s/it]  4%|▍         | 19/500 [07:53<3:05:00, 23.08s/it]  4%|▍         | 20/500 [08:16<3:04:48, 23.10s/it]  4%|▍         | 21/500 [08:39<3:04:00, 23.05s/it]  4%|▍         | 22/500 [09:02<3:03:13, 23.00s/it]  5%|▍         | 23/500 [09:31<3:16:28, 24.71s/it]  5%|▍         | 24/500 [09:54<3:11:56, 24.19s/it]  5%|▌         | 25/500 [10:26<3:30:46, 26.62s/it]  5%|▌         | 26/500 [10:50<3:22:45, 25.67s/it]  5%|▌         | 27/500 [11:18<3:28:20, 26.43s/it]  6%|▌         | 28/500 [11:41<3:19:21, 25.34s/it]  6%|▌         | 29/500 [12:08<3:24:13, 26.02s/it]  6%|▌         | 30/500 [12:31<3:16:30, 25.09s/it]  6%|▌         | 31/500 [12:54<3:10:25, 24.36s/it]  6%|▋         | 32/500 [13:21<3:17:19, 25.30s/it]  7%|▋         | 33/500 [13:44<3:12:02, 24.67s/it]  7%|▋         | 34/500 [14:07<3:07:55, 24.20s/it]  7%|▋         | 35/500 [14:30<3:03:58, 23.74s/it]  7%|▋         | 36/500 [14:58<3:12:43, 24.92s/it]  7%|▋         | 37/500 [15:26<3:19:51, 25.90s/it]  8%|▊         | 38/500 [15:54<3:23:43, 26.46s/it]  8%|▊         | 39/500 [16:17<3:14:51, 25.36s/it]  8%|▊         | 40/500 [16:40<3:08:58, 24.65s/it]  8%|▊         | 41/500 [17:04<3:08:58, 24.70s/it]  8%|▊         | 42/500 [17:27<3:03:52, 24.09s/it]  9%|▊         | 43/500 [17:49<2:59:28, 23.56s/it]  9%|▊         | 43/500 [17:49<3:09:30, 24.88s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.315 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.025 MB of 0.315 MB uploadedwandb: / 0.311 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▂▃▅▅▅▆▂▁▃▃▄▄▅▃▆▆▆▆▅▆▇▇▆▇▇▇▇▇▇███████████
wandb:     train_loss ▅▅▅▅▂▅▆▆▅▅▅▅▄▂▄▄▆▃▂█▄▂▂▂▃▂▃█▂▁▂▁▂▂▃▁▁▃▂▄
wandb:   val_accuracy ▃▂▇▆▆█▂▁▂▂▃▃▃▂▄▅▆▇▅▆▅▇▇▇▆▇▆▆▆▆▆▇▆▆▆▆▆▆▅▅
wandb:       val_loss ▃▃▃▂▃▃▄▃▃▃▃▄▃▄▂▄▂▂▂▂▂▂▃▂▄▁▅▄▂▃▃▃▂▂▂▂█▂▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 42
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.87073
wandb:     train_loss 0.88175
wandb:   val_accuracy 0.46889
wandb:       val_loss 1.23392
wandb: 
wandb: 🚀 View run happy-leaf-589 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/u9wz2wmu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_122906-u9wz2wmu/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_124749-aujy8rmm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-snow-591
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/aujy8rmm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:03, 25.62s/it]  0%|          | 2/500 [00:49<3:24:46, 24.67s/it]  1%|          | 3/500 [01:13<3:22:05, 24.40s/it]  1%|          | 4/500 [01:37<3:19:56, 24.19s/it]  1%|          | 5/500 [02:01<3:17:38, 23.96s/it]  1%|          | 6/500 [02:24<3:15:46, 23.78s/it]  1%|▏         | 7/500 [02:50<3:21:08, 24.48s/it]  2%|▏         | 8/500 [03:14<3:19:49, 24.37s/it]  2%|▏         | 9/500 [03:38<3:17:31, 24.14s/it]  2%|▏         | 10/500 [04:11<3:40:07, 26.95s/it]  2%|▏         | 11/500 [04:39<3:42:28, 27.30s/it]  2%|▏         | 12/500 [05:08<3:46:15, 27.82s/it]  3%|▎         | 13/500 [05:32<3:36:04, 26.62s/it]  3%|▎         | 14/500 [06:04<3:48:31, 28.21s/it]  3%|▎         | 15/500 [06:33<3:50:40, 28.54s/it]  3%|▎         | 16/500 [06:57<3:38:47, 27.12s/it]  3%|▎         | 17/500 [07:25<3:40:48, 27.43s/it]  4%|▎         | 18/500 [07:53<3:41:53, 27.62s/it]  4%|▍         | 19/500 [08:17<3:32:21, 26.49s/it]  4%|▍         | 20/500 [08:40<3:24:38, 25.58s/it]  4%|▍         | 21/500 [09:04<3:19:28, 24.99s/it]  4%|▍         | 22/500 [09:28<3:15:59, 24.60s/it]  5%|▍         | 23/500 [09:51<3:13:09, 24.30s/it]  5%|▍         | 24/500 [10:15<3:11:14, 24.11s/it]  5%|▌         | 25/500 [10:38<3:08:51, 23.86s/it]  5%|▌         | 26/500 [11:02<3:07:33, 23.74s/it]  5%|▌         | 27/500 [11:25<3:06:51, 23.70s/it]  6%|▌         | 28/500 [11:49<3:06:14, 23.68s/it]  6%|▌         | 29/500 [12:12<3:04:45, 23.54s/it]  6%|▌         | 30/500 [12:36<3:04:13, 23.52s/it]  6%|▌         | 31/500 [12:59<3:03:53, 23.53s/it]  6%|▋         | 32/500 [13:23<3:04:19, 23.63s/it]  7%|▋         | 33/500 [13:47<3:03:23, 23.56s/it]  7%|▋         | 34/500 [14:10<3:02:16, 23.47s/it]  7%|▋         | 35/500 [14:33<3:01:24, 23.41s/it]  7%|▋         | 36/500 [14:57<3:01:43, 23.50s/it]  7%|▋         | 37/500 [15:25<3:12:22, 24.93s/it]  8%|▊         | 38/500 [15:49<3:09:10, 24.57s/it]  8%|▊         | 39/500 [16:12<3:05:50, 24.19s/it]  8%|▊         | 40/500 [16:36<3:04:15, 24.03s/it]  8%|▊         | 41/500 [17:05<3:15:04, 25.50s/it]  8%|▊         | 42/500 [17:28<3:10:02, 24.90s/it]  9%|▊         | 43/500 [18:04<3:33:36, 28.05s/it]  9%|▉         | 44/500 [18:27<3:22:45, 26.68s/it]  9%|▉         | 45/500 [18:50<3:14:46, 25.68s/it]  9%|▉         | 46/500 [19:14<3:09:27, 25.04s/it]  9%|▉         | 47/500 [19:37<3:05:34, 24.58s/it] 10%|▉         | 48/500 [20:05<3:12:27, 25.55s/it] 10%|▉         | 49/500 [20:29<3:06:58, 24.88s/it] 10%|█         | 50/500 [20:52<3:02:35, 24.35s/it] 10%|█         | 51/500 [21:20<3:11:16, 25.56s/it] 10%|█         | 52/500 [21:48<3:16:32, 26.32s/it] 11%|█         | 53/500 [22:12<3:09:35, 25.45s/it] 11%|█         | 54/500 [22:40<3:16:35, 26.45s/it] 11%|█         | 55/500 [23:09<3:20:26, 27.02s/it] 11%|█         | 56/500 [23:32<3:11:48, 25.92s/it] 11%|█▏        | 57/500 [23:56<3:06:22, 25.24s/it] 12%|█▏        | 58/500 [24:19<3:01:48, 24.68s/it] 12%|█▏        | 59/500 [24:42<2:58:00, 24.22s/it] 12%|█▏        | 60/500 [25:06<2:55:42, 23.96s/it] 12%|█▏        | 61/500 [25:29<2:53:40, 23.74s/it] 12%|█▏        | 61/500 [25:29<3:03:25, 25.07s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.030 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.125 MB of 0.309 MB uploadedwandb: \ 0.300 MB of 0.309 MB uploadedwandb: | 0.300 MB of 0.309 MB uploadedwandb: / 0.300 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▃▄▃▁▅▆▇▇▇▇▇▇██████████████████████████
wandb:     train_loss █▆▆▂▇▁▅▇▁▄▂▁▁▄▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▅▃▅▁▄▇█▇▇▆▆▇▇▆▇▅▆▆▆▆▅▅▆▅▆▅▆▅▅▄▆▅▆▆▅▅▅▅
wandb:       val_loss ▁▁▁▁▁█▁▁▂▁▁▆▂▁▁▂▁▁▁▃▁▃▂▂▁▃▃▃▁▁▁▂▁▁▂▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 0.00013
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.45111
wandb:       val_loss 0.4216
wandb: 
wandb: 🚀 View run smart-snow-591 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/aujy8rmm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_124749-aujy8rmm/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_131402-2uo3wxza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-frog-593
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2uo3wxza
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:36:51, 26.08s/it]  0%|          | 2/500 [00:49<3:23:53, 24.56s/it]  1%|          | 3/500 [01:13<3:19:55, 24.14s/it]  1%|          | 4/500 [01:36<3:18:03, 23.96s/it]  1%|          | 5/500 [02:00<3:15:53, 23.74s/it]  1%|          | 6/500 [02:34<3:44:48, 27.30s/it]  1%|▏         | 7/500 [02:57<3:34:04, 26.05s/it]  2%|▏         | 8/500 [03:21<3:27:00, 25.24s/it]  2%|▏         | 9/500 [03:50<3:36:45, 26.49s/it]  2%|▏         | 10/500 [04:22<3:48:34, 27.99s/it]  2%|▏         | 11/500 [04:53<3:56:21, 29.00s/it]  2%|▏         | 12/500 [05:30<4:17:06, 31.61s/it]  3%|▎         | 13/500 [05:54<3:57:03, 29.21s/it]  3%|▎         | 14/500 [06:24<3:58:36, 29.46s/it]  3%|▎         | 15/500 [06:48<3:44:44, 27.80s/it]  3%|▎         | 16/500 [07:12<3:34:02, 26.53s/it]  3%|▎         | 17/500 [07:35<3:26:01, 25.59s/it]  4%|▎         | 18/500 [07:58<3:20:17, 24.93s/it]  4%|▍         | 19/500 [08:22<3:15:26, 24.38s/it]  4%|▍         | 20/500 [08:49<3:23:25, 25.43s/it]  4%|▍         | 21/500 [09:12<3:16:27, 24.61s/it]  4%|▍         | 22/500 [09:44<3:32:38, 26.69s/it]  5%|▍         | 23/500 [10:07<3:24:04, 25.67s/it]  5%|▍         | 24/500 [10:31<3:18:36, 25.03s/it]  5%|▌         | 25/500 [10:53<3:13:11, 24.40s/it]  5%|▌         | 26/500 [11:17<3:09:49, 24.03s/it]  5%|▌         | 27/500 [11:41<3:10:47, 24.20s/it]  6%|▌         | 28/500 [12:04<3:08:03, 23.90s/it]  6%|▌         | 29/500 [12:27<3:05:19, 23.61s/it]  6%|▌         | 30/500 [12:50<3:03:36, 23.44s/it]  6%|▌         | 31/500 [13:21<3:18:57, 25.45s/it]  6%|▋         | 32/500 [13:49<3:25:38, 26.36s/it]  7%|▋         | 33/500 [14:12<3:17:34, 25.38s/it]  7%|▋         | 34/500 [14:36<3:12:35, 24.80s/it]  7%|▋         | 35/500 [15:03<3:19:07, 25.69s/it]  7%|▋         | 36/500 [15:26<3:12:12, 24.86s/it]  7%|▋         | 37/500 [15:49<3:07:19, 24.28s/it]  8%|▊         | 38/500 [16:12<3:04:05, 23.91s/it]  8%|▊         | 39/500 [16:35<3:01:27, 23.62s/it]  8%|▊         | 40/500 [16:58<2:59:06, 23.36s/it]  8%|▊         | 41/500 [17:21<2:58:21, 23.32s/it]  8%|▊         | 42/500 [17:44<2:57:12, 23.22s/it]  9%|▊         | 43/500 [18:13<3:09:19, 24.86s/it]  9%|▉         | 44/500 [18:36<3:06:03, 24.48s/it]  9%|▉         | 45/500 [19:00<3:04:38, 24.35s/it]  9%|▉         | 46/500 [19:24<3:03:23, 24.24s/it]  9%|▉         | 47/500 [19:48<3:01:36, 24.05s/it] 10%|▉         | 48/500 [20:12<3:00:25, 23.95s/it] 10%|▉         | 49/500 [20:35<2:59:23, 23.87s/it] 10%|█         | 50/500 [20:59<2:58:35, 23.81s/it] 10%|█         | 51/500 [21:23<2:58:36, 23.87s/it] 10%|█         | 51/500 [21:23<3:08:20, 25.17s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.029 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▁▄▅▆▃▅▆▇▇▇▇▇▇▇█████▇▇▇████████████████
wandb:     train_loss ▇▄▇▅▂▇█▂▅▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb:   val_accuracy ▁▆▂▅▇█▁▄▅▆▆▆▄▆▆▄▅▅▅▅▄▃▃▄▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▄
wandb:       val_loss ▃▂▃▂▂▃▃▃▅▅▆▁▆▇▃▃▁▅▄▁█▂▇▆▁▅▆▁▂▂▅▁▅▇▆▂▂▂▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 50
wandb:  learning_rate 0.00033
wandb: train_accuracy 1.0
wandb:     train_loss 0.00016
wandb:   val_accuracy 0.47111
wandb:       val_loss 1.81309
wandb: 
wandb: 🚀 View run ethereal-frog-593 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2uo3wxza
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_131402-2uo3wxza/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_133615-jx66mpzq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-violet-610
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jx66mpzq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:59, 25.73s/it]  0%|          | 2/500 [00:49<3:23:31, 24.52s/it]  1%|          | 3/500 [01:13<3:22:30, 24.45s/it]  1%|          | 4/500 [01:37<3:19:18, 24.11s/it]  1%|          | 5/500 [02:00<3:16:49, 23.86s/it]  1%|          | 6/500 [02:24<3:15:06, 23.70s/it]  1%|▏         | 7/500 [02:47<3:15:00, 23.73s/it]  2%|▏         | 8/500 [03:11<3:15:10, 23.80s/it]  2%|▏         | 9/500 [03:35<3:13:48, 23.68s/it]  2%|▏         | 10/500 [03:58<3:12:54, 23.62s/it]  2%|▏         | 11/500 [04:22<3:12:46, 23.65s/it]  2%|▏         | 12/500 [04:45<3:11:41, 23.57s/it]  3%|▎         | 13/500 [05:09<3:11:48, 23.63s/it]  3%|▎         | 14/500 [05:33<3:11:56, 23.70s/it]  3%|▎         | 15/500 [05:57<3:12:29, 23.81s/it]  3%|▎         | 16/500 [06:20<3:10:39, 23.63s/it]  3%|▎         | 17/500 [06:45<3:12:39, 23.93s/it]  4%|▎         | 18/500 [07:08<3:11:12, 23.80s/it]  4%|▍         | 19/500 [07:32<3:10:13, 23.73s/it]  4%|▍         | 20/500 [07:56<3:10:40, 23.83s/it]  4%|▍         | 21/500 [08:20<3:10:58, 23.92s/it]  4%|▍         | 22/500 [08:44<3:10:58, 23.97s/it]  5%|▍         | 23/500 [09:09<3:11:05, 24.04s/it]  5%|▍         | 24/500 [09:32<3:09:31, 23.89s/it]  5%|▌         | 25/500 [09:56<3:09:53, 23.99s/it]  5%|▌         | 26/500 [10:20<3:07:56, 23.79s/it]  5%|▌         | 27/500 [10:43<3:06:32, 23.66s/it]  6%|▌         | 28/500 [11:06<3:05:10, 23.54s/it]  6%|▌         | 29/500 [11:30<3:04:16, 23.47s/it]  6%|▌         | 30/500 [11:53<3:04:32, 23.56s/it]  6%|▌         | 31/500 [12:17<3:05:36, 23.75s/it]  6%|▋         | 32/500 [12:41<3:04:42, 23.68s/it]  7%|▋         | 33/500 [13:06<3:06:22, 23.95s/it]  7%|▋         | 34/500 [13:30<3:06:17, 23.99s/it]  7%|▋         | 35/500 [13:53<3:04:55, 23.86s/it]  7%|▋         | 36/500 [14:17<3:03:21, 23.71s/it]  7%|▋         | 37/500 [14:40<3:02:13, 23.61s/it]  8%|▊         | 38/500 [15:04<3:01:58, 23.63s/it]  8%|▊         | 39/500 [15:27<3:02:03, 23.70s/it]  8%|▊         | 40/500 [15:51<3:01:56, 23.73s/it]  8%|▊         | 41/500 [16:16<3:02:50, 23.90s/it]  8%|▊         | 42/500 [16:39<3:02:08, 23.86s/it]  9%|▊         | 43/500 [17:03<3:01:50, 23.87s/it]  9%|▉         | 44/500 [17:27<3:00:31, 23.75s/it]  9%|▉         | 45/500 [17:51<3:00:20, 23.78s/it]  9%|▉         | 46/500 [18:14<2:58:17, 23.56s/it]  9%|▉         | 47/500 [18:37<2:57:33, 23.52s/it] 10%|▉         | 48/500 [19:01<2:57:37, 23.58s/it] 10%|▉         | 49/500 [19:24<2:57:02, 23.55s/it] 10%|█         | 50/500 [19:48<2:55:58, 23.46s/it] 10%|█         | 51/500 [20:11<2:55:47, 23.49s/it] 10%|█         | 52/500 [20:35<2:55:24, 23.49s/it] 11%|█         | 53/500 [20:57<2:53:39, 23.31s/it] 11%|█         | 54/500 [21:22<2:56:17, 23.72s/it] 11%|█         | 55/500 [21:46<2:55:53, 23.71s/it] 11%|█         | 56/500 [22:11<2:58:37, 24.14s/it] 11%|█▏        | 57/500 [22:35<2:57:51, 24.09s/it] 11%|█▏        | 57/500 [22:35<2:55:34, 23.78s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.020 MB of 0.316 MB uploadedwandb: | 0.312 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▆▇▇▇█▇▇██████
wandb:     train_loss ▅▆▇▃▇▁▅█▁▁▅▅▅▂▆▂▃▁▄▄▅▂▁▁▂▃▂▂▃▄▁▃▄▁▁▂▁▂▃▁
wandb:   val_accuracy ▁▁▄▅▇▇▇█▇█▇▇▇█▆█▇▇▇▆▇▅▇▇▇▆▆▄▇▆▇▆▇▄▆▆▇▆▆▅
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▄▁▂▂█▂▁▃▁▃▁▂▂▁▁▂▂▂▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.87519
wandb:     train_loss 0.02977
wandb:   val_accuracy 0.5
wandb:       val_loss 2.25696
wandb: 
wandb: 🚀 View run expert-violet-610 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jx66mpzq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_133615-jx66mpzq/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_135936-vlii064n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-jazz-613
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vlii064n
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:37:59, 26.21s/it]  0%|          | 2/500 [00:50<3:28:36, 25.13s/it]  1%|          | 3/500 [01:14<3:24:48, 24.73s/it]  1%|          | 4/500 [01:39<3:22:38, 24.51s/it]  1%|          | 5/500 [02:03<3:21:36, 24.44s/it]  1%|          | 6/500 [02:26<3:18:50, 24.15s/it]  1%|▏         | 7/500 [02:50<3:17:48, 24.07s/it]  2%|▏         | 8/500 [03:14<3:16:19, 23.94s/it]  2%|▏         | 9/500 [03:38<3:16:21, 23.99s/it]  2%|▏         | 10/500 [04:02<3:15:25, 23.93s/it]  2%|▏         | 11/500 [04:26<3:14:25, 23.86s/it]  2%|▏         | 12/500 [04:49<3:14:06, 23.87s/it]  3%|▎         | 13/500 [05:13<3:13:08, 23.80s/it]  3%|▎         | 14/500 [05:36<3:11:33, 23.65s/it]  3%|▎         | 15/500 [05:59<3:08:56, 23.37s/it]  3%|▎         | 16/500 [06:23<3:09:03, 23.44s/it]  3%|▎         | 17/500 [06:46<3:08:35, 23.43s/it]  4%|▎         | 18/500 [07:10<3:08:18, 23.44s/it]  4%|▍         | 19/500 [07:34<3:10:47, 23.80s/it]  4%|▍         | 20/500 [07:57<3:09:04, 23.63s/it]  4%|▍         | 21/500 [08:21<3:08:34, 23.62s/it]  4%|▍         | 22/500 [08:45<3:07:50, 23.58s/it]  5%|▍         | 23/500 [09:08<3:07:16, 23.56s/it]  5%|▍         | 24/500 [09:31<3:06:10, 23.47s/it]  5%|▌         | 25/500 [09:54<3:05:01, 23.37s/it]  5%|▌         | 26/500 [10:18<3:03:53, 23.28s/it]  5%|▌         | 27/500 [10:41<3:04:01, 23.34s/it]  6%|▌         | 28/500 [11:04<3:03:40, 23.35s/it]  6%|▌         | 29/500 [11:28<3:04:38, 23.52s/it]  6%|▌         | 30/500 [11:52<3:04:29, 23.55s/it]  6%|▌         | 31/500 [12:15<3:03:24, 23.46s/it]  6%|▋         | 32/500 [12:39<3:03:34, 23.53s/it]  7%|▋         | 33/500 [13:02<3:02:46, 23.48s/it]  7%|▋         | 34/500 [13:26<3:02:30, 23.50s/it]  7%|▋         | 35/500 [13:51<3:05:57, 23.99s/it]  7%|▋         | 36/500 [14:14<3:04:11, 23.82s/it]  7%|▋         | 37/500 [14:38<3:02:58, 23.71s/it]  8%|▊         | 38/500 [15:02<3:02:43, 23.73s/it]  8%|▊         | 39/500 [15:25<3:01:50, 23.67s/it]  8%|▊         | 40/500 [15:49<3:01:38, 23.69s/it]  8%|▊         | 41/500 [16:13<3:03:20, 23.97s/it]  8%|▊         | 42/500 [16:37<3:00:56, 23.71s/it]  9%|▊         | 43/500 [17:00<3:00:05, 23.64s/it]  9%|▉         | 44/500 [17:29<3:12:17, 25.30s/it]  9%|▉         | 45/500 [17:53<3:09:08, 24.94s/it]  9%|▉         | 46/500 [18:17<3:05:08, 24.47s/it]  9%|▉         | 47/500 [18:40<3:02:37, 24.19s/it]  9%|▉         | 47/500 [18:40<3:00:04, 23.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.138 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▄▄▅▅▅▆▆▇▇▇▇█▇█████████▅▆▇▇▇▇▇▇▇▇▇▇▇▇
wandb:     train_loss ▆▅▅▅▁▄▁█▂▄▁▁▄▁▁▂▁▁▁▂▁▁▁▁▂▂▁▁▁▇▁▂▁▇▁▁▃▁▁▁
wandb:   val_accuracy ▁▂▆█▇▆▅▄▃▃▄▆▆▅▄▆▅▅▅▄▃▆▅▆▅▂▅▄▃▄▇▄▃▂▃▂▂▃▃▃
wandb:       val_loss ▁▁▁▁▁▁▂▁▁▁▁▂▁▂█▁▂▂▁▂▂▁▃▁▄▄▁▂▁▂▂▂▃▃▂▂▁▂▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 46
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.94205
wandb:     train_loss 0.00134
wandb:   val_accuracy 0.43111
wandb:       val_loss 0.76842
wandb: 
wandb: 🚀 View run confused-jazz-613 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vlii064n
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_135936-vlii064n/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_141901-dxtybpso
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-frost-616
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dxtybpso
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:45:00, 27.06s/it]  0%|          | 2/500 [00:50<3:28:51, 25.16s/it]  1%|          | 3/500 [01:15<3:25:18, 24.79s/it]  1%|          | 4/500 [01:38<3:21:06, 24.33s/it]  1%|          | 5/500 [02:02<3:18:19, 24.04s/it]  1%|          | 6/500 [02:26<3:17:15, 23.96s/it]  1%|▏         | 7/500 [02:49<3:16:04, 23.86s/it]  2%|▏         | 8/500 [03:13<3:14:32, 23.73s/it]  2%|▏         | 9/500 [03:37<3:14:58, 23.83s/it]  2%|▏         | 10/500 [04:01<3:14:59, 23.88s/it]  2%|▏         | 11/500 [04:25<3:14:52, 23.91s/it]  2%|▏         | 12/500 [04:49<3:14:28, 23.91s/it]  3%|▎         | 13/500 [05:13<3:13:47, 23.88s/it]  3%|▎         | 14/500 [05:36<3:13:18, 23.87s/it]  3%|▎         | 15/500 [06:00<3:12:59, 23.88s/it]  3%|▎         | 16/500 [06:24<3:11:40, 23.76s/it]  3%|▎         | 17/500 [06:47<3:10:26, 23.66s/it]  4%|▎         | 18/500 [07:10<3:09:07, 23.54s/it]  4%|▍         | 19/500 [07:34<3:08:30, 23.51s/it]  4%|▍         | 20/500 [07:57<3:08:10, 23.52s/it]  4%|▍         | 21/500 [08:21<3:08:59, 23.67s/it]  4%|▍         | 22/500 [08:46<3:11:30, 24.04s/it]  5%|▍         | 23/500 [09:10<3:11:06, 24.04s/it]  5%|▍         | 24/500 [09:34<3:09:23, 23.87s/it]  5%|▌         | 25/500 [09:57<3:07:13, 23.65s/it]  5%|▌         | 26/500 [10:20<3:04:54, 23.41s/it]  5%|▌         | 27/500 [10:49<3:18:48, 25.22s/it]  6%|▌         | 28/500 [11:13<3:14:27, 24.72s/it]  6%|▌         | 29/500 [11:37<3:12:03, 24.47s/it]  6%|▌         | 30/500 [12:01<3:10:18, 24.29s/it]  6%|▌         | 30/500 [12:01<3:08:17, 24.04s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.313 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.200 MB of 0.313 MB uploadedwandb: / 0.200 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▅▄▄▅▄▆▆▄▅▄▆▇▄▆▆▅▇█▆█▇█▇██▇▇▇█
wandb:     train_loss ▇▃▃▅▁▄▅▁▅▂▃▁▁▁▁█▃▂▂▁▆▅▃▁▁▃▁▁▂▁
wandb:   val_accuracy ▁█▆▁▃▃▅▆▂▂▁▃▄▁▁▁▂▂▃▅▅▃▄▄▅▄▂▅▄▄
wandb:       val_loss ▂▂▂▂▃▂▂▃▁▂▂▃▄▁▃▂█▃▄▄▃▂▅▃▃▄▅▁▇▆
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.93462
wandb:     train_loss 0.10379
wandb:   val_accuracy 0.44444
wandb:       val_loss 4.57984
wandb: 
wandb: 🚀 View run generous-frost-616 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dxtybpso
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_141901-dxtybpso/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_143145-apvnggxt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-fog-618
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/apvnggxt
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:41:21, 26.62s/it]  0%|          | 2/500 [00:50<3:28:36, 25.13s/it]  1%|          | 3/500 [01:14<3:22:33, 24.45s/it]  1%|          | 4/500 [01:38<3:20:07, 24.21s/it]  1%|          | 5/500 [02:01<3:18:07, 24.02s/it]  1%|          | 6/500 [02:25<3:15:36, 23.76s/it]  1%|▏         | 7/500 [02:48<3:15:21, 23.78s/it]  2%|▏         | 8/500 [03:12<3:14:05, 23.67s/it]  2%|▏         | 9/500 [03:35<3:12:54, 23.57s/it]  2%|▏         | 10/500 [03:59<3:12:18, 23.55s/it]  2%|▏         | 11/500 [04:22<3:11:51, 23.54s/it]  2%|▏         | 12/500 [04:46<3:11:07, 23.50s/it]  3%|▎         | 13/500 [05:09<3:09:58, 23.41s/it]  3%|▎         | 14/500 [05:32<3:09:58, 23.45s/it]  3%|▎         | 15/500 [05:56<3:09:06, 23.40s/it]  3%|▎         | 16/500 [06:19<3:08:00, 23.31s/it]  3%|▎         | 17/500 [06:42<3:08:17, 23.39s/it]  4%|▎         | 18/500 [07:06<3:08:53, 23.51s/it]  4%|▍         | 19/500 [07:30<3:08:51, 23.56s/it]  4%|▍         | 20/500 [07:54<3:09:21, 23.67s/it]  4%|▍         | 21/500 [08:17<3:08:38, 23.63s/it]  4%|▍         | 22/500 [08:41<3:08:53, 23.71s/it]  5%|▍         | 23/500 [09:05<3:08:19, 23.69s/it]  5%|▍         | 24/500 [09:29<3:08:10, 23.72s/it]  5%|▌         | 25/500 [09:52<3:06:58, 23.62s/it]  5%|▌         | 26/500 [10:20<3:17:20, 24.98s/it]  5%|▌         | 27/500 [10:44<3:13:53, 24.60s/it]  6%|▌         | 28/500 [11:07<3:10:59, 24.28s/it]  6%|▌         | 29/500 [11:31<3:08:29, 24.01s/it]  6%|▌         | 30/500 [11:54<3:07:13, 23.90s/it]  6%|▌         | 31/500 [12:19<3:07:19, 23.97s/it]  6%|▋         | 32/500 [12:42<3:05:24, 23.77s/it]  7%|▋         | 33/500 [13:06<3:06:16, 23.93s/it]  7%|▋         | 34/500 [13:30<3:05:20, 23.86s/it]  7%|▋         | 35/500 [13:53<3:03:36, 23.69s/it]  7%|▋         | 36/500 [14:17<3:02:46, 23.64s/it]  7%|▋         | 37/500 [14:40<3:01:27, 23.51s/it]  8%|▊         | 38/500 [15:03<3:00:13, 23.41s/it]  8%|▊         | 39/500 [15:26<2:59:35, 23.37s/it]  8%|▊         | 40/500 [15:51<3:02:06, 23.75s/it]  8%|▊         | 41/500 [16:14<3:00:32, 23.60s/it]  8%|▊         | 42/500 [16:39<3:02:09, 23.86s/it]  9%|▊         | 43/500 [17:02<3:00:51, 23.75s/it]  9%|▉         | 44/500 [17:26<3:00:37, 23.77s/it]  9%|▉         | 45/500 [17:50<3:01:52, 23.98s/it]  9%|▉         | 46/500 [18:14<3:00:23, 23.84s/it]  9%|▉         | 47/500 [18:37<2:58:38, 23.66s/it] 10%|▉         | 48/500 [19:01<2:57:22, 23.55s/it] 10%|▉         | 49/500 [19:24<2:56:58, 23.54s/it] 10%|█         | 50/500 [19:47<2:55:31, 23.40s/it] 10%|█         | 51/500 [20:11<2:55:30, 23.45s/it] 10%|█         | 52/500 [20:34<2:54:46, 23.41s/it] 11%|█         | 53/500 [20:58<2:54:51, 23.47s/it] 11%|█         | 54/500 [21:21<2:54:24, 23.46s/it] 11%|█         | 55/500 [21:44<2:53:36, 23.41s/it] 11%|█         | 56/500 [22:08<2:54:13, 23.54s/it] 11%|█▏        | 57/500 [22:32<2:53:32, 23.50s/it] 12%|█▏        | 58/500 [22:54<2:51:28, 23.28s/it] 12%|█▏        | 59/500 [23:18<2:52:34, 23.48s/it] 12%|█▏        | 60/500 [23:42<2:52:08, 23.47s/it] 12%|█▏        | 61/500 [24:05<2:51:59, 23.51s/it] 12%|█▏        | 62/500 [24:29<2:51:37, 23.51s/it] 13%|█▎        | 63/500 [24:53<2:52:25, 23.67s/it] 13%|█▎        | 64/500 [25:17<2:52:54, 23.79s/it] 13%|█▎        | 65/500 [25:40<2:51:35, 23.67s/it] 13%|█▎        | 66/500 [26:05<2:53:19, 23.96s/it] 13%|█▎        | 67/500 [26:28<2:51:44, 23.80s/it] 14%|█▎        | 68/500 [26:53<2:52:03, 23.90s/it] 14%|█▍        | 69/500 [27:16<2:51:07, 23.82s/it] 14%|█▍        | 70/500 [27:39<2:49:22, 23.63s/it] 14%|█▍        | 71/500 [28:03<2:48:23, 23.55s/it] 14%|█▍        | 72/500 [28:26<2:47:59, 23.55s/it] 15%|█▍        | 73/500 [28:50<2:47:49, 23.58s/it] 15%|█▍        | 74/500 [29:13<2:46:43, 23.48s/it] 15%|█▌        | 75/500 [29:36<2:45:13, 23.33s/it] 15%|█▌        | 76/500 [30:00<2:45:04, 23.36s/it] 15%|█▌        | 77/500 [30:23<2:44:19, 23.31s/it] 16%|█▌        | 78/500 [30:46<2:44:19, 23.36s/it] 16%|█▌        | 79/500 [31:09<2:42:44, 23.19s/it] 16%|█▌        | 80/500 [31:41<3:00:05, 25.73s/it] 16%|█▌        | 81/500 [32:04<2:54:50, 25.04s/it] 16%|█▋        | 82/500 [32:27<2:50:40, 24.50s/it] 17%|█▋        | 83/500 [32:51<2:47:55, 24.16s/it] 17%|█▋        | 84/500 [33:14<2:45:43, 23.90s/it] 17%|█▋        | 85/500 [33:37<2:43:58, 23.71s/it] 17%|█▋        | 85/500 [33:37<2:44:11, 23.74s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.233 MB of 0.318 MB uploadedwandb: - 0.233 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▅▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇█▇▇█▇▇▆▇▆▇▇▆▇▇▇▆▆▇▅▅▇▅▆
wandb:     train_loss ▄▄▂▅▄▆▁▄▄▃▂▁▃▅▃▂▁▂▂▃▂▅▃▂▁▂▁▃▁▁▁▂▁▂▂▁▁▁█▁
wandb:   val_accuracy ▁▅▅▅▆▇▆▇█▆█▆▇▇▆▇▇▇▅▇▇▇▅▇▇▇▇▇▆▇▅▇▆▇▆▇▇▆▇▇
wandb:       val_loss ▃▃▃▃▃▃▂▃▃▃▂▂▂▆▂▂▃▂▄▂▂▄▃▂▂▃▂▃▄▂▅▂▁█▃▂▁▃▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 84
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.68351
wandb:     train_loss 0.0722
wandb:   val_accuracy 0.60667
wandb:       val_loss 1.39165
wandb: 
wandb: 🚀 View run faithful-fog-618 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/apvnggxt
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_143145-apvnggxt/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_150604-7j561u4a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-water-622
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7j561u4a
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:56, 25.72s/it]  0%|          | 2/500 [00:49<3:24:09, 24.60s/it]  1%|          | 3/500 [01:13<3:21:37, 24.34s/it]  1%|          | 4/500 [01:37<3:19:56, 24.19s/it]  1%|          | 5/500 [02:01<3:18:28, 24.06s/it]  1%|          | 6/500 [02:24<3:16:43, 23.89s/it]  1%|▏         | 7/500 [02:48<3:15:09, 23.75s/it]  2%|▏         | 8/500 [03:12<3:15:37, 23.86s/it]  2%|▏         | 9/500 [03:36<3:14:57, 23.82s/it]  2%|▏         | 10/500 [03:59<3:14:18, 23.79s/it]  2%|▏         | 11/500 [04:24<3:14:58, 23.92s/it]  2%|▏         | 12/500 [04:48<3:16:50, 24.20s/it]  3%|▎         | 13/500 [05:12<3:14:26, 23.96s/it]  3%|▎         | 14/500 [05:36<3:13:39, 23.91s/it]  3%|▎         | 15/500 [05:59<3:12:33, 23.82s/it]  3%|▎         | 16/500 [06:23<3:11:53, 23.79s/it]  3%|▎         | 17/500 [06:47<3:12:08, 23.87s/it]  4%|▎         | 18/500 [07:11<3:10:54, 23.76s/it]  4%|▍         | 19/500 [07:35<3:10:59, 23.82s/it]  4%|▍         | 20/500 [07:58<3:10:16, 23.78s/it]  4%|▍         | 21/500 [08:21<3:08:34, 23.62s/it]  4%|▍         | 22/500 [08:45<3:08:43, 23.69s/it]  5%|▍         | 23/500 [09:09<3:08:12, 23.67s/it]  5%|▍         | 24/500 [09:33<3:07:56, 23.69s/it]  5%|▌         | 25/500 [09:56<3:07:38, 23.70s/it]  5%|▌         | 26/500 [10:20<3:06:33, 23.61s/it]  5%|▌         | 27/500 [10:46<3:11:31, 24.29s/it]  6%|▌         | 28/500 [11:09<3:08:30, 23.96s/it]  6%|▌         | 29/500 [11:32<3:05:26, 23.62s/it]  6%|▌         | 30/500 [11:55<3:03:12, 23.39s/it]  6%|▌         | 30/500 [11:55<3:06:43, 23.84s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.020 MB of 0.308 MB uploadedwandb: \ 0.307 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▁▂▄▃▄▅▄▆▅▆▅▆▇▇▇▇▆▇▇███▇▇█████
wandb:     train_loss ▄▄▅▄▁▄▃▁▃▁█▁▁▁▁▁▁▂▁▄▂▁▁▁▁▁▁▃▁▁
wandb:   val_accuracy ▂▁▂▆▃▇▇▆▆▆▇▆▆▇▆▇█▅█▇███▇█▇▇▇▇▇
wandb:       val_loss ▂▂▃▁▄▃▂▂▅▁▂▄▂▁▃▁█▂▄▃▂▂▁▁▁▁▁▃▆▇
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.95691
wandb:     train_loss 0.01423
wandb:   val_accuracy 0.53111
wandb:       val_loss 5.11331
wandb: 
wandb: 🚀 View run wandering-water-622 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7j561u4a
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_150604-7j561u4a/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_151854-7oro281z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-dragon-624
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7oro281z
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:40:40, 26.53s/it]  0%|          | 2/500 [00:50<3:29:43, 25.27s/it]  1%|          | 3/500 [01:15<3:28:23, 25.16s/it]  1%|          | 4/500 [01:40<3:25:31, 24.86s/it]  1%|          | 5/500 [02:04<3:22:12, 24.51s/it]  1%|          | 6/500 [02:28<3:21:47, 24.51s/it]  1%|▏         | 7/500 [02:53<3:23:13, 24.73s/it]  2%|▏         | 8/500 [03:18<3:21:27, 24.57s/it]  2%|▏         | 9/500 [03:42<3:20:33, 24.51s/it]  2%|▏         | 10/500 [04:06<3:19:52, 24.47s/it]  2%|▏         | 11/500 [04:31<3:20:19, 24.58s/it]  2%|▏         | 12/500 [04:56<3:19:37, 24.54s/it]  3%|▎         | 13/500 [05:20<3:19:29, 24.58s/it]  3%|▎         | 14/500 [05:45<3:20:12, 24.72s/it]  3%|▎         | 15/500 [06:10<3:19:06, 24.63s/it]  3%|▎         | 16/500 [06:35<3:19:01, 24.67s/it]  3%|▎         | 17/500 [06:59<3:17:21, 24.52s/it]  4%|▎         | 18/500 [07:23<3:16:37, 24.48s/it]  4%|▍         | 19/500 [07:48<3:16:24, 24.50s/it]  4%|▍         | 20/500 [08:12<3:15:36, 24.45s/it]  4%|▍         | 21/500 [08:37<3:16:36, 24.63s/it]  4%|▍         | 22/500 [09:01<3:15:24, 24.53s/it]  5%|▍         | 23/500 [09:26<3:14:54, 24.52s/it]  5%|▍         | 23/500 [09:26<3:15:46, 24.63s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.021 MB of 0.310 MB uploadedwandb: / 0.230 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▆▅▁▂▁▂▃▃▅▄▄▄▅▄▄▅▅▆█▅▅▆
wandb:     train_loss ▂▁▁█▂▂▂▁▁▁▂▁▁▁▁▁▁▃▁▁▁▁▃
wandb:   val_accuracy ▂▆█▂▃▂▁▃▃▄▄▄▄▄▅▃▄▄▄▅▄▅▄
wandb:       val_loss ▂▂▁█▂▂▁▂▅▁▃▆▂▄▄▆▇▄▆▂▄█▆
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.62259
wandb:     train_loss 3.81021
wandb:   val_accuracy 0.43111
wandb:       val_loss 3.91629
wandb: 
wandb: 🚀 View run graceful-dragon-624 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7oro281z
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_151854-7oro281z/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_152910-mv7pd81h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-elevator-625
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mv7pd81h
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:21, 25.77s/it]  0%|          | 2/500 [00:49<3:24:17, 24.61s/it]  1%|          | 3/500 [01:14<3:23:42, 24.59s/it]  1%|          | 4/500 [01:38<3:22:45, 24.53s/it]  1%|          | 5/500 [02:02<3:20:18, 24.28s/it]  1%|          | 6/500 [02:26<3:18:37, 24.12s/it]  1%|▏         | 7/500 [02:50<3:17:24, 24.03s/it]  2%|▏         | 8/500 [03:14<3:16:53, 24.01s/it]  2%|▏         | 9/500 [03:37<3:15:26, 23.88s/it]  2%|▏         | 10/500 [04:01<3:14:34, 23.83s/it]  2%|▏         | 11/500 [04:25<3:15:02, 23.93s/it]  2%|▏         | 12/500 [04:49<3:14:11, 23.88s/it]  3%|▎         | 13/500 [05:13<3:14:49, 24.00s/it]  3%|▎         | 14/500 [05:37<3:14:34, 24.02s/it]  3%|▎         | 15/500 [06:01<3:13:46, 23.97s/it]  3%|▎         | 16/500 [06:25<3:13:41, 24.01s/it]  3%|▎         | 17/500 [06:49<3:12:25, 23.90s/it]  4%|▎         | 18/500 [07:12<3:09:48, 23.63s/it]  4%|▍         | 19/500 [07:35<3:07:24, 23.38s/it]  4%|▍         | 20/500 [08:05<3:25:01, 25.63s/it]  4%|▍         | 21/500 [08:29<3:20:02, 25.06s/it]  4%|▍         | 22/500 [08:53<3:15:40, 24.56s/it]  5%|▍         | 23/500 [09:16<3:13:22, 24.32s/it]  5%|▍         | 24/500 [09:40<3:11:04, 24.09s/it]  5%|▌         | 25/500 [10:03<3:09:22, 23.92s/it]  5%|▌         | 26/500 [10:27<3:08:50, 23.90s/it]  5%|▌         | 27/500 [10:51<3:07:55, 23.84s/it]  6%|▌         | 28/500 [11:14<3:06:57, 23.77s/it]  6%|▌         | 29/500 [11:38<3:05:59, 23.69s/it]  6%|▌         | 30/500 [12:01<3:04:50, 23.60s/it]  6%|▌         | 31/500 [12:25<3:05:08, 23.69s/it]  6%|▋         | 32/500 [12:48<3:03:10, 23.48s/it]  7%|▋         | 33/500 [13:12<3:02:36, 23.46s/it]  7%|▋         | 34/500 [13:35<3:01:52, 23.42s/it]  7%|▋         | 35/500 [13:58<3:01:01, 23.36s/it]  7%|▋         | 36/500 [14:22<3:00:38, 23.36s/it]  7%|▋         | 37/500 [14:45<2:59:50, 23.30s/it]  8%|▊         | 38/500 [15:08<2:59:05, 23.26s/it]  8%|▊         | 39/500 [15:31<2:58:36, 23.25s/it]  8%|▊         | 40/500 [15:55<2:59:47, 23.45s/it]  8%|▊         | 41/500 [16:19<2:59:34, 23.47s/it]  8%|▊         | 42/500 [16:42<2:58:28, 23.38s/it]  9%|▊         | 43/500 [17:06<2:59:26, 23.56s/it]  9%|▉         | 44/500 [17:29<2:58:58, 23.55s/it]  9%|▉         | 45/500 [17:53<2:58:02, 23.48s/it]  9%|▉         | 46/500 [18:16<2:57:53, 23.51s/it]  9%|▉         | 47/500 [18:40<2:57:38, 23.53s/it] 10%|▉         | 48/500 [19:03<2:57:00, 23.50s/it] 10%|▉         | 49/500 [19:26<2:55:32, 23.35s/it] 10%|█         | 50/500 [19:49<2:54:01, 23.20s/it] 10%|█         | 51/500 [20:12<2:53:50, 23.23s/it] 10%|█         | 52/500 [20:36<2:54:39, 23.39s/it] 11%|█         | 53/500 [21:00<2:56:10, 23.65s/it] 11%|█         | 54/500 [21:24<2:56:06, 23.69s/it] 11%|█         | 55/500 [21:48<2:56:07, 23.75s/it] 11%|█         | 56/500 [22:12<2:55:21, 23.70s/it] 11%|█▏        | 57/500 [22:35<2:55:08, 23.72s/it] 12%|█▏        | 58/500 [23:00<2:56:11, 23.92s/it] 12%|█▏        | 59/500 [23:23<2:54:36, 23.76s/it] 12%|█▏        | 60/500 [23:47<2:53:28, 23.66s/it] 12%|█▏        | 61/500 [24:10<2:52:37, 23.59s/it] 12%|█▏        | 62/500 [24:33<2:51:48, 23.53s/it] 13%|█▎        | 63/500 [24:57<2:50:49, 23.46s/it] 13%|█▎        | 64/500 [25:20<2:50:46, 23.50s/it] 13%|█▎        | 65/500 [25:44<2:50:17, 23.49s/it] 13%|█▎        | 66/500 [26:07<2:50:02, 23.51s/it] 13%|█▎        | 67/500 [26:31<2:49:12, 23.45s/it] 14%|█▎        | 68/500 [26:54<2:48:37, 23.42s/it] 14%|█▍        | 69/500 [27:18<2:49:06, 23.54s/it] 14%|█▍        | 70/500 [27:42<2:50:18, 23.76s/it] 14%|█▍        | 71/500 [28:06<2:51:11, 23.94s/it] 14%|█▍        | 72/500 [28:30<2:50:43, 23.93s/it] 14%|█▍        | 72/500 [28:30<2:49:30, 23.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.316 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.315 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▆▁▂▂▃▅▆▅▇▅▆▅▇▇▇▇▇▇█▇████▇████▇████████
wandb:     train_loss ▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▂▁▁▃▂▁▂▁▄▂▁▁▁▁▁▁▁▁▃▁▃
wandb:   val_accuracy ▂▂▆▁▁▁▂▆█▇▇▂█▅▇█▇▇▇▇█▆▇█▇█▇██▆█▇████████
wandb:       val_loss ▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▃▁▂▃▃▂▁▂▂▂▁▁▂▂▂▂▅▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 71
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.70877
wandb:     train_loss 1.70799
wandb:   val_accuracy 0.56444
wandb:       val_loss 0.82931
wandb: 
wandb: 🚀 View run glad-elevator-625 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/mv7pd81h
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_152910-mv7pd81h/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_155825-de6n9krh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-tree-630
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/de6n9krh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:33:45, 25.70s/it]  0%|          | 2/500 [00:49<3:25:16, 24.73s/it]  1%|          | 3/500 [01:13<3:22:34, 24.46s/it]  1%|          | 4/500 [01:38<3:21:51, 24.42s/it]  1%|          | 5/500 [02:02<3:20:31, 24.31s/it]  1%|          | 6/500 [02:26<3:18:54, 24.16s/it]  1%|▏         | 7/500 [02:50<3:18:41, 24.18s/it]  2%|▏         | 8/500 [03:14<3:17:35, 24.10s/it]  2%|▏         | 9/500 [03:38<3:17:04, 24.08s/it]  2%|▏         | 10/500 [04:02<3:15:47, 23.98s/it]  2%|▏         | 11/500 [04:25<3:15:02, 23.93s/it]  2%|▏         | 12/500 [04:50<3:15:08, 23.99s/it]  3%|▎         | 13/500 [05:14<3:14:34, 23.97s/it]  3%|▎         | 14/500 [05:38<3:15:20, 24.12s/it]  3%|▎         | 15/500 [06:02<3:14:45, 24.09s/it]  3%|▎         | 16/500 [06:26<3:13:45, 24.02s/it]  3%|▎         | 17/500 [06:50<3:12:23, 23.90s/it]  4%|▎         | 18/500 [07:13<3:11:45, 23.87s/it]  4%|▍         | 19/500 [07:38<3:12:30, 24.01s/it]  4%|▍         | 20/500 [08:02<3:13:48, 24.23s/it]  4%|▍         | 21/500 [08:27<3:14:53, 24.41s/it]  4%|▍         | 22/500 [08:51<3:12:47, 24.20s/it]  5%|▍         | 23/500 [09:15<3:12:15, 24.18s/it]  5%|▍         | 24/500 [09:40<3:13:18, 24.37s/it]  5%|▌         | 25/500 [10:04<3:11:39, 24.21s/it]  5%|▌         | 26/500 [10:29<3:13:10, 24.45s/it]  5%|▌         | 27/500 [10:53<3:12:54, 24.47s/it]  6%|▌         | 28/500 [11:17<3:10:22, 24.20s/it]  6%|▌         | 29/500 [11:40<3:06:36, 23.77s/it]  6%|▌         | 30/500 [12:03<3:04:34, 23.56s/it]  6%|▌         | 30/500 [12:03<3:08:49, 24.11s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.231 MB of 0.314 MB uploadedwandb: \ 0.231 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▅▁▄▆▆▅▆▇▅▆▆▇▄▆▆▅▇▇▆▇▇█▇█▇▅██▇
wandb:     train_loss ▄▄▄▃▁▃▃▁▂▁▇▁▁▄▃▄▄▂▁▂█▁▄▁▁▄▆▂▂▂
wandb:   val_accuracy ▁▄▁▆▇█▆▆▆▇▇█▇▄▇▆▆▆▆▇▇▄▆▅▆▅▅▆▆▅
wandb:       val_loss ▄▄▃▂▃▃▃▅▄▃▃▂▂▁▂▃█▂▂▃▅▆▁▇▂▇▃▂▇▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.75037
wandb:     train_loss 0.17773
wandb:   val_accuracy 0.53556
wandb:       val_loss 1.43815
wandb: 
wandb: 🚀 View run dandy-tree-630 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/de6n9krh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_155825-de6n9krh/logs
Successfully processed 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161113-xtypno6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-glade-632
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xtypno6h
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run polar-glade-632 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/xtypno6h
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161113-xtypno6h/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161153-7a4w8fxu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-universe-633
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7a4w8fxu
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run polar-universe-633 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7a4w8fxu
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161153-7a4w8fxu/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161226-y0oedgbn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-yogurt-634
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y0oedgbn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run radiant-yogurt-634 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y0oedgbn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161226-y0oedgbn/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161302-e98dcigf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-lion-635
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e98dcigf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run dark-lion-635 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e98dcigf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161302-e98dcigf/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161333-46u4zwl4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-blaze-636
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/46u4zwl4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run stellar-blaze-636 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/46u4zwl4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161333-46u4zwl4/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161410-56c7zffl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-wind-637
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/56c7zffl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run azure-wind-637 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/56c7zffl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161410-56c7zffl/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161444-zp2stpf2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-smoke-638
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zp2stpf2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.020 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run gentle-smoke-638 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zp2stpf2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161444-zp2stpf2/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161519-th4gbwhm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-galaxy-639
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/th4gbwhm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.017 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fresh-galaxy-639 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/th4gbwhm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161519-th4gbwhm/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161553-pvmkff1g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sea-640
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pvmkff1g
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run devoted-sea-640 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pvmkff1g
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161553-pvmkff1g/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161627-ol8wp1m4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-water-641
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ol8wp1m4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.024 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run fresh-water-641 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ol8wp1m4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161627-ol8wp1m4/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161702-gxopekkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-wood-642
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gxopekkb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run sparkling-wood-642 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gxopekkb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161702-gxopekkb/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161735-o1yqioc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-eon-643
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/o1yqioc2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run brisk-eon-643 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/o1yqioc2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161735-o1yqioc2/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161809-zn20khdt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-haze-644
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zn20khdt
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run ancient-haze-644 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zn20khdt
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161809-zn20khdt/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161842-oid06efj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-thunder-645
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oid06efj
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.020 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run spring-thunder-645 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/oid06efj
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161842-oid06efj/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161915-3uvevy7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-wood-646
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3uvevy7t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run efficient-wood-646 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/3uvevy7t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161915-3uvevy7t/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140413
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_161947-gdhu1mjx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-haze-647
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gdhu1mjx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:07:26, 15.32s/it]  0%|          | 2/500 [00:27<1:51:05, 13.38s/it]  1%|          | 3/500 [00:39<1:45:05, 12.69s/it]  1%|          | 4/500 [00:50<1:39:25, 12.03s/it]  1%|          | 5/500 [01:01<1:35:48, 11.61s/it]  1%|          | 6/500 [01:11<1:33:35, 11.37s/it]  1%|▏         | 7/500 [01:23<1:33:19, 11.36s/it]  2%|▏         | 8/500 [01:35<1:35:22, 11.63s/it]  2%|▏         | 9/500 [01:47<1:35:53, 11.72s/it]  2%|▏         | 10/500 [01:58<1:33:42, 11.48s/it]  2%|▏         | 11/500 [02:10<1:35:17, 11.69s/it]  2%|▏         | 12/500 [02:21<1:33:38, 11.51s/it]  3%|▎         | 13/500 [02:32<1:32:08, 11.35s/it]  3%|▎         | 14/500 [02:43<1:31:38, 11.31s/it]  3%|▎         | 15/500 [02:54<1:30:09, 11.15s/it]  3%|▎         | 16/500 [03:05<1:29:12, 11.06s/it]  3%|▎         | 17/500 [03:15<1:26:49, 10.79s/it]  4%|▎         | 18/500 [03:25<1:24:26, 10.51s/it]  4%|▍         | 19/500 [03:36<1:24:52, 10.59s/it]  4%|▍         | 20/500 [03:47<1:25:33, 10.70s/it]  4%|▍         | 21/500 [03:58<1:26:13, 10.80s/it]  4%|▍         | 22/500 [04:09<1:26:21, 10.84s/it]  5%|▍         | 23/500 [04:21<1:29:05, 11.21s/it]  5%|▍         | 24/500 [04:33<1:31:53, 11.58s/it]  5%|▌         | 25/500 [04:44<1:29:44, 11.34s/it]  5%|▌         | 26/500 [04:56<1:30:21, 11.44s/it]  5%|▌         | 27/500 [05:07<1:28:53, 11.28s/it]  6%|▌         | 28/500 [05:18<1:28:11, 11.21s/it]  6%|▌         | 29/500 [05:30<1:29:57, 11.46s/it]  6%|▌         | 30/500 [05:41<1:28:56, 11.35s/it]  6%|▌         | 30/500 [05:41<1:29:06, 11.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.027 MB of 0.310 MB uploadedwandb: - 0.027 MB of 0.310 MB uploadedwandb: \ 0.231 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▁▁▁▂▃▆▁▄▁▁█▂▁▁▁▆▃▁▁▄▁▇▆▁▁█▂▃▂
wandb:     train_loss ▂▁▅█▁▄▁▇▁▆▃▁▃▁▃▄▃▄▄▆▁▃▁▁█▄▁▁▂▄
wandb:   val_accuracy ▅▅▅▅▃▁█▅▂▅▅█▃▅▅▅▃▇▅▅▆▅▅█▅▅▇▆▆▆
wandb:       val_loss ▂▃▇▅▇▄▄▆▄▅▃▄▃▁▄▅▅▅▃▇█▃▆▃▁▄█▄▁▅
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.45319
wandb:     train_loss 3.72259
wandb:   val_accuracy 0.37111
wandb:       val_loss 5.76678
wandb: 
wandb: 🚀 View run dainty-haze-647 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gdhu1mjx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_161947-gdhu1mjx/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_162619-qcdf9ihx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-glade-649
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qcdf9ihx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:43:45, 19.69s/it]  0%|          | 2/500 [00:33<2:17:00, 16.51s/it]  1%|          | 3/500 [00:48<2:07:25, 15.38s/it]  1%|          | 4/500 [01:01<2:01:45, 14.73s/it]  1%|          | 5/500 [01:13<1:53:49, 13.80s/it]  1%|          | 6/500 [01:26<1:49:20, 13.28s/it]  1%|▏         | 7/500 [01:39<1:48:25, 13.19s/it]  2%|▏         | 8/500 [01:51<1:46:07, 12.94s/it]  2%|▏         | 9/500 [02:05<1:47:33, 13.14s/it]  2%|▏         | 10/500 [02:17<1:44:28, 12.79s/it]  2%|▏         | 11/500 [02:29<1:43:33, 12.71s/it]  2%|▏         | 12/500 [02:41<1:42:03, 12.55s/it]  3%|▎         | 13/500 [02:54<1:41:30, 12.51s/it]  3%|▎         | 14/500 [03:07<1:43:11, 12.74s/it]  3%|▎         | 15/500 [03:18<1:39:40, 12.33s/it]  3%|▎         | 16/500 [03:32<1:42:48, 12.74s/it]  3%|▎         | 17/500 [03:45<1:42:50, 12.77s/it]  4%|▎         | 18/500 [03:58<1:43:25, 12.87s/it]  4%|▍         | 19/500 [04:11<1:43:06, 12.86s/it]  4%|▍         | 20/500 [04:24<1:42:20, 12.79s/it]  4%|▍         | 21/500 [04:37<1:43:18, 12.94s/it]  4%|▍         | 22/500 [04:48<1:38:41, 12.39s/it]  5%|▍         | 23/500 [05:02<1:42:10, 12.85s/it]  5%|▍         | 24/500 [05:14<1:39:59, 12.60s/it]  5%|▌         | 25/500 [05:26<1:38:47, 12.48s/it]  5%|▌         | 26/500 [05:38<1:37:17, 12.32s/it]  5%|▌         | 27/500 [05:50<1:37:23, 12.35s/it]  6%|▌         | 28/500 [06:03<1:36:48, 12.31s/it]  6%|▌         | 29/500 [06:14<1:34:17, 12.01s/it]  6%|▌         | 30/500 [06:25<1:32:32, 11.81s/it]  6%|▌         | 31/500 [06:37<1:31:20, 11.69s/it]  6%|▋         | 32/500 [06:49<1:32:57, 11.92s/it]  7%|▋         | 33/500 [07:01<1:31:41, 11.78s/it]  7%|▋         | 34/500 [07:12<1:29:42, 11.55s/it]  7%|▋         | 35/500 [07:23<1:28:59, 11.48s/it]  7%|▋         | 36/500 [07:34<1:27:14, 11.28s/it]  7%|▋         | 37/500 [07:45<1:26:34, 11.22s/it]  8%|▊         | 38/500 [07:57<1:27:40, 11.39s/it]  8%|▊         | 39/500 [08:08<1:26:45, 11.29s/it]  8%|▊         | 40/500 [08:18<1:24:59, 11.09s/it]  8%|▊         | 41/500 [08:29<1:24:14, 11.01s/it]  8%|▊         | 41/500 [08:29<1:35:06, 12.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.021 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▁▁▄▁▁▃▃▂▃▃▆▁▁▂▁▃▃▁▂▁▄▂▂▃▂▇▄▂█▇▂▁▇▁█▂▃▂
wandb:     train_loss ▂▂▂▂▂▃▂▂▂▁▂▂▂▁▁▃▃▃▂▃▃▃▂▄▃▁▅▂▃▄▁▃█▄▂▄▁▄▃▁
wandb:   val_accuracy ▆▆▆▅▇▅▇▇▇▅▇▃▆▆▆▆▆▇▇▆▃▆▇▆▆▁▄█▁▆▆█▅▆█▆█▆▂▆
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▃▂▃▂▁▆▂▃▂▂▂▃▁▂▁▁▄▄▃▂▂█▂▃▄▂▂▁▂▄▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.36107
wandb:     train_loss 0.20362
wandb:   val_accuracy 0.33556
wandb:       val_loss 0.69134
wandb: 
wandb: 🚀 View run kind-glade-649 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qcdf9ihx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_162619-qcdf9ihx/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_163533-wyjefxo2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-fire-650
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wyjefxo2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:10:43, 15.72s/it]  0%|          | 2/500 [00:28<1:56:54, 14.09s/it]  1%|          | 3/500 [00:40<1:48:18, 13.08s/it]  1%|          | 4/500 [00:51<1:42:49, 12.44s/it]  1%|          | 5/500 [01:04<1:41:31, 12.31s/it]  1%|          | 6/500 [01:15<1:39:01, 12.03s/it]  1%|▏         | 7/500 [01:27<1:38:02, 11.93s/it]  2%|▏         | 8/500 [01:38<1:36:09, 11.73s/it]  2%|▏         | 9/500 [01:50<1:36:34, 11.80s/it]  2%|▏         | 10/500 [02:01<1:35:08, 11.65s/it]  2%|▏         | 11/500 [02:13<1:35:40, 11.74s/it]  2%|▏         | 12/500 [02:25<1:34:48, 11.66s/it]  3%|▎         | 13/500 [02:37<1:35:34, 11.77s/it]  3%|▎         | 14/500 [02:49<1:36:00, 11.85s/it]  3%|▎         | 15/500 [03:00<1:34:59, 11.75s/it]  3%|▎         | 16/500 [03:12<1:34:57, 11.77s/it]  3%|▎         | 17/500 [03:24<1:34:54, 11.79s/it]  4%|▎         | 18/500 [03:36<1:34:08, 11.72s/it]  4%|▍         | 19/500 [03:48<1:35:11, 11.87s/it]  4%|▍         | 20/500 [04:00<1:35:07, 11.89s/it]  4%|▍         | 21/500 [04:12<1:35:58, 12.02s/it]  4%|▍         | 22/500 [04:24<1:35:03, 11.93s/it]  5%|▍         | 23/500 [04:35<1:33:49, 11.80s/it]  5%|▍         | 24/500 [04:47<1:33:22, 11.77s/it]  5%|▌         | 25/500 [04:59<1:32:54, 11.74s/it]  5%|▌         | 26/500 [05:12<1:36:08, 12.17s/it]  5%|▌         | 27/500 [05:24<1:35:53, 12.16s/it]  6%|▌         | 28/500 [05:36<1:35:28, 12.14s/it]  6%|▌         | 29/500 [05:48<1:34:40, 12.06s/it]  6%|▌         | 30/500 [06:00<1:34:24, 12.05s/it]  6%|▌         | 30/500 [06:00<1:34:07, 12.02s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.021 MB of 0.310 MB uploadedwandb: / 0.027 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▁▁▆▂▁▁▂▁▁▃▁▁▇▁▄█▇▇▁▁█▁▇▁▇███
wandb:     train_loss ▁▂▂▁▁▃▄▄▁▆█▁▄▁▁█▃▁▁▁▆▇▁▃▁▄▁▁▁▁
wandb:   val_accuracy ▅▅▄▅▂▃▅▅▃▅▅▆▅▅▃▅▁▆▇█▄▅█▅█▅▄▃▃▄
wandb:       val_loss ▁▁▂▅▂▃▅▃▄▄▇▄▄▁▃█▃▃▁▃▆▄▃▂▂▃▄▄▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.98217
wandb:     train_loss 0.00044
wandb:   val_accuracy 0.29333
wandb:       val_loss 5.25431
wandb: 
wandb: 🚀 View run copper-fire-650 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wyjefxo2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_163533-wyjefxo2/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_164234-ku6xdh1k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-music-652
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ku6xdh1k
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:04:18, 14.95s/it]  0%|          | 2/500 [00:27<1:51:29, 13.43s/it]  1%|          | 3/500 [00:41<1:52:32, 13.59s/it]  1%|          | 4/500 [00:53<1:49:31, 13.25s/it]  1%|          | 5/500 [01:05<1:45:51, 12.83s/it]  1%|          | 6/500 [01:18<1:44:41, 12.72s/it]  1%|▏         | 7/500 [01:32<1:47:36, 13.10s/it]  2%|▏         | 8/500 [01:44<1:44:58, 12.80s/it]  2%|▏         | 9/500 [01:56<1:43:43, 12.67s/it]  2%|▏         | 10/500 [02:09<1:44:01, 12.74s/it]  2%|▏         | 11/500 [02:22<1:44:19, 12.80s/it]  2%|▏         | 12/500 [02:34<1:42:20, 12.58s/it]  3%|▎         | 13/500 [02:47<1:41:23, 12.49s/it]  3%|▎         | 14/500 [02:59<1:40:59, 12.47s/it]  3%|▎         | 15/500 [03:12<1:41:05, 12.51s/it]  3%|▎         | 16/500 [03:24<1:41:07, 12.54s/it]  3%|▎         | 17/500 [03:37<1:40:52, 12.53s/it]  4%|▎         | 18/500 [03:49<1:41:07, 12.59s/it]  4%|▍         | 19/500 [04:02<1:41:25, 12.65s/it]  4%|▍         | 20/500 [04:14<1:39:47, 12.47s/it]  4%|▍         | 21/500 [04:26<1:38:24, 12.33s/it]  4%|▍         | 22/500 [04:38<1:36:01, 12.05s/it]  5%|▍         | 23/500 [04:50<1:35:21, 12.00s/it]  5%|▍         | 24/500 [05:02<1:36:41, 12.19s/it]  5%|▌         | 25/500 [05:14<1:36:20, 12.17s/it]  5%|▌         | 25/500 [05:14<1:39:42, 12.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.021 MB of 0.310 MB uploadedwandb: / 0.230 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▃▄▇▇▄▆▁▇▃▇▆▁▁▂▂▁██▁▄▃▅
wandb:     train_loss ▁▁▃▂▁▁▁▁▁▂▁▁▁▁▂▃█▂▂▁▁▃▁▁▁
wandb:   val_accuracy ▅▅▅▅▁█▅▁█▅█▇██▅▅▄▅▅█▄▅▇▇▇
wandb:       val_loss ▁▁▂▁▃▁▂▃▁▁▁▃▂▂▄▃█▄▃▄▃▂▄▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.69837
wandb:     train_loss 1.25309
wandb:   val_accuracy 0.46444
wandb:       val_loss 2.31639
wandb: 
wandb: 🚀 View run solar-music-652 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ku6xdh1k
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_164234-ku6xdh1k/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_164830-jkyeeebq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-eon-653
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jkyeeebq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:07:05, 15.28s/it]  0%|          | 2/500 [00:27<1:53:15, 13.64s/it]  1%|          | 3/500 [00:39<1:47:27, 12.97s/it]  1%|          | 4/500 [00:51<1:43:41, 12.54s/it]  1%|          | 5/500 [01:03<1:41:51, 12.35s/it]  1%|          | 6/500 [01:14<1:38:18, 11.94s/it]  1%|▏         | 7/500 [01:26<1:36:57, 11.80s/it]  2%|▏         | 8/500 [01:37<1:35:13, 11.61s/it]  2%|▏         | 9/500 [01:49<1:34:13, 11.52s/it]  2%|▏         | 10/500 [02:00<1:33:16, 11.42s/it]  2%|▏         | 11/500 [02:11<1:33:27, 11.47s/it]  2%|▏         | 12/500 [02:23<1:34:16, 11.59s/it]  3%|▎         | 13/500 [02:35<1:33:43, 11.55s/it]  3%|▎         | 14/500 [02:46<1:33:37, 11.56s/it]  3%|▎         | 15/500 [02:59<1:35:19, 11.79s/it]  3%|▎         | 16/500 [03:12<1:38:42, 12.24s/it]  3%|▎         | 17/500 [03:23<1:37:05, 12.06s/it]  4%|▎         | 18/500 [03:35<1:35:06, 11.84s/it]  4%|▍         | 19/500 [03:46<1:33:37, 11.68s/it]  4%|▍         | 20/500 [03:57<1:32:30, 11.56s/it]  4%|▍         | 21/500 [04:08<1:30:18, 11.31s/it]  4%|▍         | 22/500 [04:20<1:31:36, 11.50s/it]  5%|▍         | 23/500 [04:33<1:34:17, 11.86s/it]  5%|▍         | 24/500 [04:45<1:34:17, 11.89s/it]  5%|▌         | 25/500 [04:57<1:35:41, 12.09s/it]  5%|▌         | 26/500 [05:09<1:35:33, 12.10s/it]  5%|▌         | 27/500 [05:21<1:33:44, 11.89s/it]  6%|▌         | 28/500 [05:34<1:35:37, 12.16s/it]  6%|▌         | 29/500 [05:46<1:36:36, 12.31s/it]  6%|▌         | 30/500 [05:59<1:37:38, 12.46s/it]  6%|▌         | 31/500 [06:13<1:40:21, 12.84s/it]  6%|▋         | 32/500 [06:27<1:42:49, 13.18s/it]  7%|▋         | 33/500 [06:39<1:41:25, 13.03s/it]  7%|▋         | 34/500 [06:53<1:43:10, 13.28s/it]  7%|▋         | 35/500 [07:07<1:44:13, 13.45s/it]  7%|▋         | 36/500 [07:20<1:42:35, 13.27s/it]  7%|▋         | 37/500 [07:33<1:42:40, 13.31s/it]  8%|▊         | 38/500 [07:47<1:44:16, 13.54s/it]  8%|▊         | 39/500 [08:03<1:48:03, 14.06s/it]  8%|▊         | 40/500 [08:15<1:43:48, 13.54s/it]  8%|▊         | 41/500 [08:27<1:40:55, 13.19s/it]  8%|▊         | 42/500 [08:41<1:42:33, 13.44s/it]  9%|▊         | 43/500 [08:54<1:40:37, 13.21s/it]  9%|▉         | 44/500 [09:08<1:40:46, 13.26s/it]  9%|▉         | 45/500 [09:21<1:40:09, 13.21s/it]  9%|▉         | 46/500 [09:33<1:39:00, 13.08s/it]  9%|▉         | 47/500 [09:46<1:38:31, 13.05s/it] 10%|▉         | 48/500 [10:01<1:42:14, 13.57s/it] 10%|▉         | 49/500 [10:15<1:41:36, 13.52s/it] 10%|█         | 50/500 [10:29<1:43:23, 13.79s/it] 10%|█         | 51/500 [10:43<1:43:23, 13.82s/it] 10%|█         | 52/500 [10:56<1:41:52, 13.64s/it] 11%|█         | 53/500 [11:10<1:41:33, 13.63s/it] 11%|█         | 54/500 [11:23<1:39:51, 13.43s/it] 11%|█         | 55/500 [11:36<1:39:21, 13.40s/it] 11%|█         | 56/500 [11:50<1:41:22, 13.70s/it] 11%|█▏        | 57/500 [12:04<1:40:58, 13.68s/it] 12%|█▏        | 58/500 [12:18<1:41:41, 13.80s/it] 12%|█▏        | 59/500 [12:31<1:39:45, 13.57s/it] 12%|█▏        | 60/500 [12:44<1:37:49, 13.34s/it] 12%|█▏        | 61/500 [12:58<1:38:17, 13.43s/it] 12%|█▏        | 62/500 [13:10<1:36:12, 13.18s/it] 13%|█▎        | 63/500 [13:24<1:36:40, 13.27s/it] 13%|█▎        | 64/500 [13:36<1:35:15, 13.11s/it] 13%|█▎        | 65/500 [13:51<1:37:20, 13.43s/it] 13%|█▎        | 66/500 [14:04<1:38:11, 13.57s/it] 13%|█▎        | 67/500 [14:19<1:39:59, 13.85s/it] 14%|█▎        | 68/500 [14:32<1:37:11, 13.50s/it] 14%|█▍        | 69/500 [14:45<1:36:26, 13.43s/it] 14%|█▍        | 70/500 [14:58<1:35:31, 13.33s/it] 14%|█▍        | 71/500 [15:11<1:34:59, 13.29s/it] 14%|█▍        | 71/500 [15:12<1:31:50, 12.85s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.010 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▅▅▁▂▁▇▃▂▇▁▂▄▄▄▄▂▅▄▅▁▂▃▄▄▇▁▄▄█▂▃▃▁█▅▂
wandb:     train_loss ▂▂▂▂▂▂▃▂▁▂▂▂▁▁▁▁▁▂▁▁▁▂▂▁▃▃▁▁▁█▂▁▁▅▁▁▁▁▁▁
wandb:   val_accuracy ▃▃▃▃▂▂▁▂▁▅▂▂█▁▃▃▃▃▃▁█▄▃▂▁▃▂▂▆▁▃▂▇▃▂▁▂█▃▂
wandb:       val_loss ▂▂▂▂▂▁▃▁▄▂▂▂▁█▂▂▂▂▂▂▁▂▁▅▁▂▁▂▂▇▁▂▁▃▃▂▃▂▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 70
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.36404
wandb:     train_loss 0.00033
wandb:   val_accuracy 0.32667
wandb:       val_loss 0.71581
wandb: 
wandb: 🚀 View run silvery-eon-653 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jkyeeebq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_164830-jkyeeebq/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_170420-wrcvurk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-mountain-655
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wrcvurk1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:14<2:02:34, 14.74s/it]  0%|          | 2/500 [00:26<1:46:25, 12.82s/it]  1%|          | 3/500 [00:37<1:41:53, 12.30s/it]  1%|          | 4/500 [00:49<1:39:43, 12.06s/it]  1%|          | 5/500 [01:01<1:37:49, 11.86s/it]  1%|          | 6/500 [01:12<1:35:53, 11.65s/it]  1%|▏         | 7/500 [01:24<1:35:53, 11.67s/it]  2%|▏         | 8/500 [01:35<1:35:12, 11.61s/it]  2%|▏         | 9/500 [01:46<1:34:28, 11.54s/it]  2%|▏         | 10/500 [01:58<1:34:43, 11.60s/it]  2%|▏         | 11/500 [02:10<1:34:14, 11.56s/it]  2%|▏         | 12/500 [02:21<1:34:33, 11.63s/it]  3%|▎         | 13/500 [02:33<1:34:02, 11.59s/it]  3%|▎         | 14/500 [02:44<1:33:46, 11.58s/it]  3%|▎         | 15/500 [02:57<1:35:10, 11.77s/it]  3%|▎         | 16/500 [03:08<1:33:26, 11.58s/it]  3%|▎         | 17/500 [03:19<1:32:53, 11.54s/it]  4%|▎         | 18/500 [03:31<1:33:19, 11.62s/it]  4%|▍         | 19/500 [03:43<1:32:45, 11.57s/it]  4%|▍         | 20/500 [03:54<1:33:11, 11.65s/it]  4%|▍         | 21/500 [04:06<1:32:35, 11.60s/it]  4%|▍         | 22/500 [04:17<1:32:08, 11.57s/it]  5%|▍         | 23/500 [04:29<1:31:32, 11.51s/it]  5%|▍         | 24/500 [04:40<1:31:21, 11.52s/it]  5%|▌         | 25/500 [04:52<1:30:58, 11.49s/it]  5%|▌         | 26/500 [05:03<1:31:20, 11.56s/it]  5%|▌         | 27/500 [05:15<1:31:20, 11.59s/it]  5%|▌         | 27/500 [05:15<1:32:08, 11.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.020 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▂▆▃▆▆▁▁▄▇▂▄▁▁▃▆▁▄█▁██▅▇█
wandb:     train_loss ▂▂▂▂▂▁▁▁▄▃▂▁▁▁▄▄█▁▃▁▁▄▁▁▄▁▁
wandb:   val_accuracy ▅▅▅▅▅▅▆▃▄▅▅▇▅▁▅▅▂█▄▇▆▅█▇▁▄▆
wandb:       val_loss ▁▁▁▁▁▁▂▁▆▁▁█▂▄▂▃▄▃▁▂▁▂▄▃▄▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.97177
wandb:     train_loss 0.0411
wandb:   val_accuracy 0.37111
wandb:       val_loss 8.53939
wandb: 
wandb: 🚀 View run misty-mountain-655 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wrcvurk1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_170420-wrcvurk1/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_171020-el6nok55
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-energy-656
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/el6nok55
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:07:06, 15.28s/it]  0%|          | 2/500 [00:27<1:50:27, 13.31s/it]  1%|          | 3/500 [00:39<1:45:15, 12.71s/it]  1%|          | 4/500 [00:50<1:41:42, 12.30s/it]  1%|          | 5/500 [01:02<1:38:42, 11.97s/it]  1%|          | 6/500 [01:13<1:37:08, 11.80s/it]  1%|▏         | 7/500 [01:24<1:35:27, 11.62s/it]  2%|▏         | 8/500 [01:36<1:35:39, 11.67s/it]  2%|▏         | 9/500 [01:48<1:35:07, 11.62s/it]  2%|▏         | 10/500 [01:59<1:34:32, 11.58s/it]  2%|▏         | 11/500 [02:11<1:33:44, 11.50s/it]  2%|▏         | 12/500 [02:22<1:33:38, 11.51s/it]  3%|▎         | 13/500 [02:33<1:32:53, 11.44s/it]  3%|▎         | 14/500 [02:45<1:32:33, 11.43s/it]  3%|▎         | 15/500 [02:56<1:32:34, 11.45s/it]  3%|▎         | 16/500 [03:08<1:31:55, 11.40s/it]  3%|▎         | 17/500 [03:19<1:31:02, 11.31s/it]  4%|▎         | 18/500 [03:30<1:30:40, 11.29s/it]  4%|▍         | 19/500 [03:42<1:33:06, 11.61s/it]  4%|▍         | 20/500 [03:54<1:32:59, 11.62s/it]  4%|▍         | 21/500 [04:05<1:31:39, 11.48s/it]  4%|▍         | 22/500 [04:17<1:31:24, 11.47s/it]  5%|▍         | 23/500 [04:28<1:30:29, 11.38s/it]  5%|▍         | 24/500 [04:39<1:30:31, 11.41s/it]  5%|▌         | 25/500 [04:50<1:29:14, 11.27s/it]  5%|▌         | 26/500 [05:01<1:28:55, 11.26s/it]  5%|▌         | 27/500 [05:13<1:28:34, 11.24s/it]  5%|▌         | 27/500 [05:13<1:31:24, 11.60s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.306 MB uploadedwandb: / 0.010 MB of 0.306 MB uploadedwandb: - 0.195 MB of 0.306 MB uploadedwandb: \ 0.306 MB of 0.306 MB uploadedwandb: | 0.306 MB of 0.306 MB uploadedwandb: / 0.306 MB of 0.306 MB uploadedwandb: - 0.306 MB of 0.306 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▃▇▁▃▇▁▁▇▁▆▅▁▃▂▅▁██▁▇▃█▅▄
wandb:     train_loss ▁▂▂▂▁▁▁▁▁▂▁▁▁▁▂▃█▁▂▁▁▃▁▁▁▁▂
wandb:   val_accuracy ▅▅▅▅█▅▁█▅▅█▅▃█▅▇▄█▅█▅▅█▆█▁▂
wandb:       val_loss ▂▂▂▁▁▁▃▁▂▃▁▅▃▂▄▃█▅▂▄▃▂▅▃▃▅█
wandb: 
wandb: Run summary:
wandb:          epoch 26
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.61218
wandb:     train_loss 2.66856
wandb:   val_accuracy 0.12889
wandb:       val_loss 12.06508
wandb: 
wandb: 🚀 View run jumping-energy-656 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/el6nok55
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_171020-el6nok55/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_171619-jay1vbyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-smoke-657
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jay1vbyr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:53:39, 13.67s/it]  0%|          | 2/500 [00:25<1:46:39, 12.85s/it]  1%|          | 3/500 [00:38<1:47:01, 12.92s/it]  1%|          | 4/500 [00:51<1:47:07, 12.96s/it]  1%|          | 5/500 [01:04<1:45:36, 12.80s/it]  1%|          | 6/500 [01:16<1:43:34, 12.58s/it]  1%|▏         | 7/500 [01:28<1:42:11, 12.44s/it]  2%|▏         | 8/500 [01:41<1:42:12, 12.46s/it]  2%|▏         | 9/500 [01:53<1:41:10, 12.36s/it]  2%|▏         | 10/500 [02:05<1:41:22, 12.41s/it]  2%|▏         | 11/500 [02:18<1:41:17, 12.43s/it]  2%|▏         | 12/500 [02:30<1:40:49, 12.40s/it]  3%|▎         | 13/500 [02:43<1:40:42, 12.41s/it]  3%|▎         | 14/500 [02:56<1:41:51, 12.58s/it]  3%|▎         | 15/500 [03:08<1:41:17, 12.53s/it]  3%|▎         | 16/500 [03:20<1:40:02, 12.40s/it]  3%|▎         | 17/500 [03:33<1:40:31, 12.49s/it]  4%|▎         | 18/500 [03:45<1:40:00, 12.45s/it]  4%|▍         | 19/500 [03:58<1:39:41, 12.44s/it]  4%|▍         | 20/500 [04:10<1:40:16, 12.53s/it]  4%|▍         | 21/500 [04:24<1:41:32, 12.72s/it]  4%|▍         | 22/500 [04:36<1:40:39, 12.63s/it]  5%|▍         | 23/500 [04:48<1:39:17, 12.49s/it]  5%|▍         | 24/500 [05:00<1:38:12, 12.38s/it]  5%|▌         | 25/500 [05:13<1:37:54, 12.37s/it]  5%|▌         | 26/500 [05:24<1:36:28, 12.21s/it]  5%|▌         | 27/500 [05:37<1:37:03, 12.31s/it]  6%|▌         | 28/500 [05:48<1:34:46, 12.05s/it]  6%|▌         | 29/500 [06:01<1:36:55, 12.35s/it]  6%|▌         | 30/500 [06:15<1:39:07, 12.66s/it]  6%|▌         | 31/500 [06:28<1:39:09, 12.68s/it]  6%|▋         | 32/500 [06:40<1:38:11, 12.59s/it]  7%|▋         | 33/500 [06:53<1:37:59, 12.59s/it]  7%|▋         | 34/500 [07:05<1:36:58, 12.49s/it]  7%|▋         | 35/500 [07:17<1:36:20, 12.43s/it]  7%|▋         | 36/500 [07:30<1:36:17, 12.45s/it]  7%|▋         | 37/500 [07:42<1:35:36, 12.39s/it]  8%|▊         | 38/500 [07:54<1:34:53, 12.32s/it]  8%|▊         | 39/500 [08:07<1:35:20, 12.41s/it]  8%|▊         | 40/500 [08:19<1:35:47, 12.49s/it]  8%|▊         | 41/500 [08:32<1:36:09, 12.57s/it]  8%|▊         | 41/500 [08:32<1:35:40, 12.51s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.025 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▃▁▁▅▃▃▃▁▄▄▁▁▁▁▁▁▂▂▁▁▃▁▁▃▃▁▁▂▃▂▂▂▃▁▂█▁▂
wandb:     train_loss ▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▂▂▂▂▁▃▂▁▁▂▂▂▆▂▂▂▂▄▂▂▂▁▁█▁
wandb:   val_accuracy ▅▅▅▃▁▃▅▅▅▃█▇▅▅▃▅▅▅▆▇▃▅▇▃▅▆▆▃▅▆▅▅▃▅▃▅▅▇▃▅
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▅▂▂▂▁▇▂▂▂▂▂▅▂▂▇▁▂▂▆▁▂▄▃▂▂▄▃▂▂█▁
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.3997
wandb:     train_loss 0.22768
wandb:   val_accuracy 0.34222
wandb:       val_loss 0.54967
wandb: 
wandb: 🚀 View run azure-smoke-657 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/jay1vbyr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_171619-jay1vbyr/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_172538-wuhh0kzd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-disco-659
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wuhh0kzd
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:53:28, 13.64s/it]  0%|          | 2/500 [00:25<1:46:26, 12.82s/it]  1%|          | 3/500 [00:38<1:44:41, 12.64s/it]  1%|          | 4/500 [00:50<1:44:31, 12.64s/it]  1%|          | 5/500 [01:02<1:42:19, 12.40s/it]  1%|          | 6/500 [01:15<1:42:05, 12.40s/it]  1%|▏         | 7/500 [01:27<1:42:33, 12.48s/it]  2%|▏         | 8/500 [01:41<1:44:04, 12.69s/it]  2%|▏         | 9/500 [01:53<1:43:47, 12.68s/it]  2%|▏         | 10/500 [02:06<1:43:23, 12.66s/it]  2%|▏         | 11/500 [02:19<1:43:35, 12.71s/it]  2%|▏         | 12/500 [02:31<1:43:25, 12.72s/it]  3%|▎         | 13/500 [02:44<1:43:28, 12.75s/it]  3%|▎         | 14/500 [02:57<1:42:19, 12.63s/it]  3%|▎         | 15/500 [03:09<1:41:02, 12.50s/it]  3%|▎         | 16/500 [03:22<1:41:54, 12.63s/it]  3%|▎         | 17/500 [03:35<1:42:45, 12.77s/it]  4%|▎         | 18/500 [03:48<1:44:08, 12.96s/it]  4%|▍         | 19/500 [04:01<1:43:12, 12.88s/it]  4%|▍         | 20/500 [04:14<1:42:40, 12.83s/it]  4%|▍         | 21/500 [04:26<1:42:14, 12.81s/it]  4%|▍         | 22/500 [04:40<1:42:47, 12.90s/it]  5%|▍         | 23/500 [04:52<1:40:40, 12.66s/it]  5%|▍         | 24/500 [05:05<1:42:26, 12.91s/it]  5%|▌         | 25/500 [05:18<1:41:04, 12.77s/it]  5%|▌         | 26/500 [05:32<1:44:05, 13.18s/it]  5%|▌         | 27/500 [05:45<1:43:43, 13.16s/it]  6%|▌         | 28/500 [05:58<1:43:33, 13.16s/it]  6%|▌         | 29/500 [06:11<1:42:39, 13.08s/it]  6%|▌         | 30/500 [06:25<1:45:20, 13.45s/it]  6%|▌         | 31/500 [06:39<1:45:58, 13.56s/it]  6%|▋         | 32/500 [06:53<1:46:08, 13.61s/it]  7%|▋         | 33/500 [07:04<1:41:14, 13.01s/it]  7%|▋         | 34/500 [07:15<1:36:30, 12.43s/it]  7%|▋         | 35/500 [07:27<1:33:18, 12.04s/it]  7%|▋         | 36/500 [07:39<1:33:23, 12.08s/it]  7%|▋         | 37/500 [07:51<1:32:33, 11.99s/it]  7%|▋         | 37/500 [07:51<1:38:14, 12.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.010 MB of 0.313 MB uploadedwandb: - 0.021 MB of 0.313 MB uploadedwandb: \ 0.306 MB of 0.313 MB uploadedwandb: | 0.308 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▁▁▁▇▇▃▇▁▃▂▃▇▁▁▄▁▂▁▁▂▁▇▇█▆▃▇▃█▂▃▂▂▇
wandb:     train_loss ▁▁▂▄▁▁▁▁▁▁█▁▁▁▁▁▆▁▃▆▂▂▆▂▂▁▁▁▃▁▁▂▂▂▂▂▁
wandb:   val_accuracy ▅▅▅▅▅▅▅▇▂▆▅▁▆▁▇▅▅▇▅▃▅▅▄▅▄█▆▃▂▄▂█▅▁▅▅▇
wandb:       val_loss ▁▁▂▂█▃▂▁▃▂▆▃▂▄▂▇▆▃▂▃▁▂▆▂▃▁▃▃▃▃▃▁▃▃▁▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.90342
wandb:     train_loss 0.0
wandb:   val_accuracy 0.47333
wandb:       val_loss 0.46416
wandb: 
wandb: 🚀 View run peach-disco-659 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wuhh0kzd
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_172538-wuhh0kzd/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_173417-4171s0oa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-frog-661
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4171s0oa
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:15<2:05:35, 15.10s/it]  0%|          | 2/500 [00:27<1:51:43, 13.46s/it]  1%|          | 3/500 [00:39<1:47:32, 12.98s/it]  1%|          | 4/500 [00:51<1:43:15, 12.49s/it]  1%|          | 5/500 [01:03<1:41:27, 12.30s/it]  1%|          | 6/500 [01:15<1:40:34, 12.22s/it]  1%|▏         | 7/500 [01:27<1:38:39, 12.01s/it]  2%|▏         | 8/500 [01:38<1:36:46, 11.80s/it]  2%|▏         | 9/500 [01:49<1:35:32, 11.67s/it]  2%|▏         | 10/500 [02:01<1:35:43, 11.72s/it]  2%|▏         | 11/500 [02:13<1:34:31, 11.60s/it]  2%|▏         | 12/500 [02:24<1:33:33, 11.50s/it]  3%|▎         | 13/500 [02:35<1:32:32, 11.40s/it]  3%|▎         | 14/500 [02:46<1:31:30, 11.30s/it]  3%|▎         | 15/500 [02:57<1:31:23, 11.31s/it]  3%|▎         | 16/500 [03:09<1:31:05, 11.29s/it]  3%|▎         | 17/500 [03:20<1:31:56, 11.42s/it]  4%|▎         | 18/500 [03:32<1:31:43, 11.42s/it]  4%|▍         | 19/500 [03:43<1:31:35, 11.43s/it]  4%|▍         | 20/500 [03:54<1:30:55, 11.37s/it]  4%|▍         | 21/500 [04:06<1:30:40, 11.36s/it]  4%|▍         | 22/500 [04:17<1:31:11, 11.45s/it]  4%|▍         | 22/500 [04:18<1:33:25, 11.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.131 MB of 0.309 MB uploadedwandb: - 0.131 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▂▁▃▂▁▃▂▄▃▇▁▂█▁▁▂▂▁▆█▁
wandb:     train_loss ▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂█▂▂▁▁▃
wandb:   val_accuracy ▅▅▅▄▄▅▄▅▁▂▄▄▅▇▅▄▄▅▄█▄▅
wandb:       val_loss ▁▁▂▂▆▁▃▁▃█▁▃▂▂▃▃▇▄▂▃▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.31055
wandb:     train_loss 6.14241
wandb:   val_accuracy 0.34667
wandb:       val_loss 1.31463
wandb: 
wandb: 🚀 View run frosty-frog-661 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4171s0oa
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_173417-4171s0oa/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_173917-sxm7qzhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sun-662
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sxm7qzhn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:52:06, 13.48s/it]  0%|          | 2/500 [00:25<1:45:57, 12.77s/it]  1%|          | 3/500 [00:37<1:40:37, 12.15s/it]  1%|          | 4/500 [00:49<1:41:04, 12.23s/it]  1%|          | 5/500 [01:01<1:39:10, 12.02s/it]  1%|          | 6/500 [01:12<1:37:06, 11.79s/it]  1%|▏         | 7/500 [01:23<1:35:13, 11.59s/it]  2%|▏         | 8/500 [01:34<1:34:12, 11.49s/it]  2%|▏         | 9/500 [01:46<1:33:44, 11.45s/it]  2%|▏         | 10/500 [01:57<1:33:29, 11.45s/it]  2%|▏         | 11/500 [02:09<1:33:38, 11.49s/it]  2%|▏         | 12/500 [02:21<1:34:53, 11.67s/it]  3%|▎         | 13/500 [02:33<1:34:37, 11.66s/it]  3%|▎         | 14/500 [02:45<1:35:24, 11.78s/it]  3%|▎         | 15/500 [02:57<1:37:01, 12.00s/it]  3%|▎         | 16/500 [03:09<1:36:47, 12.00s/it]  3%|▎         | 17/500 [03:21<1:35:59, 11.92s/it]  4%|▎         | 18/500 [03:32<1:34:14, 11.73s/it]  4%|▍         | 19/500 [03:44<1:33:26, 11.66s/it]  4%|▍         | 20/500 [03:55<1:32:54, 11.61s/it]  4%|▍         | 21/500 [04:06<1:31:33, 11.47s/it]  4%|▍         | 22/500 [04:18<1:31:07, 11.44s/it]  5%|▍         | 23/500 [04:29<1:30:39, 11.40s/it]  5%|▍         | 23/500 [04:29<1:33:08, 11.72s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.132 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▂▃▅▅▅▆█▂▂▂▂▂▂▂▂▂▂▆▃▅▅▂
wandb:     train_loss ▁▁▁▁▁▁▁▁▂▁▄▃█▄▁▁▃▁▁▁▁▁▄
wandb:   val_accuracy ▄▃▅▄▄▆▆█▃▃▃▃▃▃▃▃▃▃▃▁▂▃▃
wandb:       val_loss ▁▁▁▁▁▁▁▁▁▅▆▃█▇▁█▃▅▁▁▁▁█
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 8.60551
wandb:   val_accuracy 0.32222
wandb:       val_loss 9.00535
wandb: 
wandb: 🚀 View run misty-sun-662 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sxm7qzhn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_173917-sxm7qzhn/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_174428-akuetsfy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-wave-663
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/akuetsfy
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:49:50, 13.21s/it]  0%|          | 2/500 [00:25<1:43:09, 12.43s/it]  1%|          | 3/500 [00:38<1:46:26, 12.85s/it]  1%|          | 4/500 [00:50<1:43:06, 12.47s/it]  1%|          | 5/500 [01:02<1:41:01, 12.24s/it]  1%|          | 6/500 [01:16<1:46:29, 12.93s/it]  1%|▏         | 7/500 [01:30<1:48:34, 13.21s/it]  2%|▏         | 8/500 [01:43<1:48:44, 13.26s/it]  2%|▏         | 9/500 [01:55<1:46:12, 12.98s/it]  2%|▏         | 10/500 [02:09<1:46:16, 13.01s/it]  2%|▏         | 11/500 [02:22<1:46:13, 13.03s/it]  2%|▏         | 12/500 [02:34<1:44:17, 12.82s/it]  3%|▎         | 13/500 [02:47<1:44:22, 12.86s/it]  3%|▎         | 14/500 [03:01<1:46:02, 13.09s/it]  3%|▎         | 15/500 [03:13<1:44:57, 12.98s/it]  3%|▎         | 16/500 [03:26<1:44:40, 12.98s/it]  3%|▎         | 17/500 [03:39<1:44:09, 12.94s/it]  3%|▎         | 17/500 [03:39<1:44:00, 12.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.010 MB of 0.311 MB uploadedwandb: - 0.025 MB of 0.311 MB uploadedwandb: \ 0.166 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▁▃▂▃▇▄▆▃▇▃▆██▁▁▄
wandb:     train_loss ▂▂▂▁▃▂▁▁▁▁█▁▁▁▆▅▃
wandb:   val_accuracy ▅▆▅▃▅▆▁▂▃▆▃███▅▅▇
wandb:       val_loss ▁▁▁▁▁▁▁▁▅▃▅█▂▂▂▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 16
wandb:  learning_rate 0.0004
wandb: train_accuracy 0.54383
wandb:     train_loss 1.651
wandb:   val_accuracy 0.40444
wandb:       val_loss 2.88223
wandb: 
wandb: 🚀 View run floral-wave-663 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/akuetsfy
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_174428-akuetsfy/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_174850-nir2wign
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-mountain-665
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nir2wign
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:43:32, 19.66s/it]  0%|          | 2/500 [00:32<2:08:13, 15.45s/it]  1%|          | 3/500 [00:44<1:57:44, 14.21s/it]  1%|          | 4/500 [00:56<1:50:32, 13.37s/it]  1%|          | 5/500 [01:08<1:45:02, 12.73s/it]  1%|          | 6/500 [01:20<1:43:30, 12.57s/it]  1%|▏         | 7/500 [01:32<1:41:00, 12.29s/it]  2%|▏         | 8/500 [01:44<1:39:08, 12.09s/it]  2%|▏         | 9/500 [01:55<1:37:29, 11.91s/it]  2%|▏         | 10/500 [02:07<1:36:18, 11.79s/it]  2%|▏         | 11/500 [02:19<1:36:30, 11.84s/it]  2%|▏         | 12/500 [02:30<1:36:08, 11.82s/it]  3%|▎         | 13/500 [02:42<1:36:21, 11.87s/it]  3%|▎         | 14/500 [02:55<1:37:35, 12.05s/it]  3%|▎         | 15/500 [03:07<1:38:07, 12.14s/it]  3%|▎         | 16/500 [03:19<1:37:03, 12.03s/it]  3%|▎         | 17/500 [03:31<1:36:42, 12.01s/it]  4%|▎         | 18/500 [03:43<1:36:35, 12.02s/it]  4%|▍         | 19/500 [03:55<1:37:04, 12.11s/it]  4%|▍         | 20/500 [04:07<1:36:18, 12.04s/it]  4%|▍         | 21/500 [04:20<1:38:32, 12.34s/it]  4%|▍         | 22/500 [04:32<1:37:36, 12.25s/it]  5%|▍         | 23/500 [04:45<1:38:26, 12.38s/it]  5%|▍         | 24/500 [04:58<1:40:05, 12.62s/it]  5%|▌         | 25/500 [05:10<1:38:12, 12.41s/it]  5%|▌         | 26/500 [05:22<1:37:29, 12.34s/it]  5%|▌         | 27/500 [05:34<1:35:52, 12.16s/it]  6%|▌         | 28/500 [05:46<1:35:17, 12.11s/it]  6%|▌         | 29/500 [05:58<1:34:23, 12.02s/it]  6%|▌         | 30/500 [06:10<1:33:25, 11.93s/it]  6%|▌         | 31/500 [06:21<1:32:40, 11.86s/it]  6%|▋         | 32/500 [06:33<1:32:16, 11.83s/it]  7%|▋         | 33/500 [06:46<1:33:37, 12.03s/it]  7%|▋         | 34/500 [06:58<1:35:06, 12.25s/it]  7%|▋         | 35/500 [07:11<1:36:27, 12.45s/it]  7%|▋         | 36/500 [07:24<1:36:16, 12.45s/it]  7%|▋         | 37/500 [07:36<1:36:48, 12.54s/it]  8%|▊         | 38/500 [07:49<1:35:45, 12.44s/it]  8%|▊         | 39/500 [08:01<1:36:18, 12.54s/it]  8%|▊         | 40/500 [08:13<1:33:39, 12.22s/it]  8%|▊         | 41/500 [08:25<1:32:14, 12.06s/it]  8%|▊         | 42/500 [08:36<1:31:01, 11.93s/it]  9%|▊         | 43/500 [08:48<1:31:41, 12.04s/it]  9%|▉         | 44/500 [09:01<1:31:35, 12.05s/it]  9%|▉         | 45/500 [09:13<1:31:36, 12.08s/it]  9%|▉         | 46/500 [09:25<1:32:11, 12.18s/it]  9%|▉         | 47/500 [09:37<1:30:31, 11.99s/it] 10%|▉         | 48/500 [09:50<1:32:21, 12.26s/it] 10%|▉         | 49/500 [10:01<1:31:25, 12.16s/it] 10%|█         | 50/500 [10:13<1:30:25, 12.06s/it] 10%|█         | 51/500 [10:25<1:29:46, 12.00s/it] 10%|█         | 52/500 [10:38<1:30:31, 12.12s/it] 11%|█         | 53/500 [10:49<1:29:32, 12.02s/it] 11%|█         | 53/500 [10:49<1:31:20, 12.26s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.138 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▃▄▇▁▇▆▁▇▆▅▃█▄▅▆▂▆▇███▇▆▇██▆█▇█▇██▇▇▇▇▇█
wandb:     train_loss ▃▃▄▂▁▁▁▄▁▁▁▁▅▂█▁▇▁▁▁▁▁▁▃▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃█▄▆▃▄▆▃█▇▇▄▄▆▇█▄▆▇█▆▆▆▇▄▇▆▁▆▃▆▂▆▇▃▇▄▆▅▄
wandb:       val_loss ▂▂▂▂▁▂▁▃▁▃▂▅▃▄▁▁▃▅▂▁▅▁▃█▂▃▂▁▁▅▆▄▂▄▂▅▁▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 52
wandb:  learning_rate 0.00033
wandb: train_accuracy 0.93314
wandb:     train_loss 0.0056
wandb:   val_accuracy 0.39556
wandb:       val_loss 3.61133
wandb: 
wandb: 🚀 View run chocolate-mountain-665 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nir2wign
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_174850-nir2wign/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_180024-wu8huy5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-cosmos-666
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wu8huy5c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:49:41, 13.19s/it]  0%|          | 2/500 [00:24<1:41:50, 12.27s/it]  1%|          | 3/500 [00:36<1:40:45, 12.16s/it]  1%|          | 4/500 [00:48<1:40:22, 12.14s/it]  1%|          | 5/500 [01:00<1:37:25, 11.81s/it]  1%|          | 6/500 [01:11<1:35:24, 11.59s/it]  1%|▏         | 7/500 [01:23<1:35:26, 11.61s/it]  2%|▏         | 8/500 [01:34<1:35:25, 11.64s/it]  2%|▏         | 9/500 [01:46<1:35:34, 11.68s/it]  2%|▏         | 10/500 [01:57<1:34:25, 11.56s/it]  2%|▏         | 11/500 [02:08<1:33:24, 11.46s/it]  2%|▏         | 12/500 [02:20<1:33:29, 11.49s/it]  3%|▎         | 13/500 [02:31<1:32:32, 11.40s/it]  3%|▎         | 14/500 [02:42<1:30:39, 11.19s/it]  3%|▎         | 15/500 [02:52<1:28:49, 10.99s/it]  3%|▎         | 16/500 [03:03<1:28:13, 10.94s/it]  3%|▎         | 17/500 [03:16<1:32:42, 11.52s/it]  4%|▎         | 18/500 [03:28<1:32:42, 11.54s/it]  4%|▍         | 19/500 [03:39<1:32:01, 11.48s/it]  4%|▍         | 20/500 [03:50<1:31:00, 11.38s/it]  4%|▍         | 21/500 [04:02<1:30:55, 11.39s/it]  4%|▍         | 22/500 [04:13<1:30:16, 11.33s/it]  5%|▍         | 23/500 [04:24<1:29:31, 11.26s/it]  5%|▍         | 24/500 [04:36<1:30:47, 11.45s/it]  5%|▌         | 25/500 [04:48<1:31:21, 11.54s/it]  5%|▌         | 26/500 [04:59<1:31:01, 11.52s/it]  5%|▌         | 27/500 [05:11<1:31:44, 11.64s/it]  6%|▌         | 28/500 [05:23<1:32:14, 11.73s/it]  6%|▌         | 29/500 [05:35<1:32:43, 11.81s/it]  6%|▌         | 30/500 [05:47<1:34:05, 12.01s/it]  6%|▌         | 31/500 [05:59<1:33:32, 11.97s/it]  6%|▋         | 32/500 [06:12<1:35:22, 12.23s/it]  7%|▋         | 33/500 [06:24<1:34:34, 12.15s/it]  7%|▋         | 34/500 [06:36<1:34:17, 12.14s/it]  7%|▋         | 35/500 [06:49<1:34:31, 12.20s/it]  7%|▋         | 36/500 [07:00<1:33:26, 12.08s/it]  7%|▋         | 37/500 [07:12<1:32:12, 11.95s/it]  8%|▊         | 38/500 [07:23<1:30:35, 11.76s/it]  8%|▊         | 39/500 [07:35<1:29:16, 11.62s/it]  8%|▊         | 40/500 [07:46<1:28:25, 11.53s/it]  8%|▊         | 41/500 [07:57<1:27:54, 11.49s/it]  8%|▊         | 42/500 [08:09<1:27:24, 11.45s/it]  9%|▊         | 43/500 [08:20<1:26:17, 11.33s/it]  9%|▉         | 44/500 [08:31<1:25:41, 11.28s/it]  9%|▉         | 45/500 [08:42<1:25:03, 11.22s/it]  9%|▉         | 46/500 [08:53<1:25:14, 11.27s/it]  9%|▉         | 47/500 [09:05<1:25:56, 11.38s/it] 10%|▉         | 48/500 [09:16<1:25:13, 11.31s/it] 10%|▉         | 49/500 [09:27<1:24:26, 11.23s/it] 10%|█         | 50/500 [09:38<1:24:22, 11.25s/it] 10%|█         | 51/500 [09:49<1:23:42, 11.19s/it] 10%|█         | 52/500 [10:01<1:23:22, 11.17s/it] 11%|█         | 53/500 [10:12<1:23:02, 11.15s/it] 11%|█         | 54/500 [10:23<1:24:06, 11.32s/it] 11%|█         | 55/500 [10:35<1:23:53, 11.31s/it] 11%|█         | 56/500 [10:46<1:23:59, 11.35s/it] 11%|█         | 56/500 [10:46<1:25:27, 11.55s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.139 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▂▄▇▇▅▇██▇█▆▇█▄██▅▆█▆▇▆▅█▇▆▇█▅▆▆▇▅▆▆▇▆▅
wandb:     train_loss ▄▅▄▄▄▄▅▄▄▄▅▅▄▄▃▅▃▄▄▃▃▆▆▄▄▄▄▄▂▃▄▃▃▃▁▄▇▂▃█
wandb:   val_accuracy ▂▂▁▄▆▆▆▅▆▇▆▇▆▇▆▇██▇▇██▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▆▇
wandb:       val_loss ▆▆▅▆▆▆▄▆▅▃▆▄▆▅▆▅▅▅▆▃▇▆▅▇▇▃▅▆▁▆█▆▅▆█▆▅▅▅▂
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.48588
wandb:     train_loss 2.13427
wandb:   val_accuracy 0.48889
wandb:       val_loss 0.33325
wandb: 
wandb: 🚀 View run honest-cosmos-666 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/wu8huy5c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_180024-wu8huy5c/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_181159-lokpos7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sun-668
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lokpos7w
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:13<1:54:44, 13.80s/it]  0%|          | 2/500 [00:25<1:44:52, 12.64s/it]  1%|          | 3/500 [00:37<1:40:25, 12.12s/it]  1%|          | 4/500 [00:48<1:38:54, 11.96s/it]  1%|          | 5/500 [01:00<1:38:21, 11.92s/it]  1%|          | 6/500 [01:12<1:37:14, 11.81s/it]  1%|▏         | 7/500 [01:24<1:37:20, 11.85s/it]  2%|▏         | 8/500 [01:36<1:37:40, 11.91s/it]  2%|▏         | 9/500 [01:47<1:36:58, 11.85s/it]  2%|▏         | 10/500 [02:02<1:42:54, 12.60s/it]  2%|▏         | 11/500 [02:17<1:48:16, 13.29s/it]  2%|▏         | 12/500 [02:29<1:45:02, 12.91s/it]  3%|▎         | 13/500 [02:40<1:41:24, 12.49s/it]  3%|▎         | 14/500 [02:52<1:39:07, 12.24s/it]  3%|▎         | 15/500 [03:04<1:38:42, 12.21s/it]  3%|▎         | 16/500 [03:16<1:38:36, 12.22s/it]  3%|▎         | 17/500 [03:28<1:36:47, 12.02s/it]  4%|▎         | 18/500 [03:42<1:41:47, 12.67s/it]  4%|▍         | 19/500 [03:56<1:45:14, 13.13s/it]  4%|▍         | 20/500 [04:09<1:45:04, 13.13s/it]  4%|▍         | 21/500 [04:23<1:46:07, 13.29s/it]  4%|▍         | 22/500 [04:35<1:43:46, 13.03s/it]  5%|▍         | 23/500 [04:48<1:42:27, 12.89s/it]  5%|▍         | 24/500 [05:00<1:41:20, 12.77s/it]  5%|▌         | 25/500 [05:13<1:41:39, 12.84s/it]  5%|▌         | 26/500 [05:26<1:41:26, 12.84s/it]  5%|▌         | 27/500 [05:39<1:42:03, 12.95s/it]  6%|▌         | 28/500 [05:52<1:40:17, 12.75s/it]  6%|▌         | 29/500 [06:05<1:41:36, 12.94s/it]  6%|▌         | 30/500 [06:18<1:41:59, 13.02s/it]  6%|▌         | 31/500 [06:32<1:43:15, 13.21s/it]  6%|▋         | 32/500 [06:46<1:45:53, 13.58s/it]  7%|▋         | 33/500 [07:01<1:47:59, 13.87s/it]  7%|▋         | 34/500 [07:14<1:45:01, 13.52s/it]  7%|▋         | 35/500 [07:28<1:45:43, 13.64s/it]  7%|▋         | 36/500 [07:40<1:43:27, 13.38s/it]  7%|▋         | 37/500 [07:53<1:41:58, 13.22s/it]  7%|▋         | 37/500 [07:53<1:38:48, 12.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.021 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▁▂▁▁▁▁▃▁▂▂▁▆▅█▁▁▃▅▄▅▁▃▁▅▂▅█▄▃▅▂▆▆▄▅█
wandb:     train_loss ▁▁▁▂▆█▃▁▁▁▂▁▁▁▁▄▃▁▁▄▁▃▁▃▁▁▃▁▃▁▁▄▂▃▄▃▁
wandb:   val_accuracy ▅▅▅▅▅▅▅▃▅▅▆▅▂▆▇▅▅▃▂▂▆▅▅▅▇▅▂█▂▄▁▅▇▂▁▁█
wandb:       val_loss ▁▁▁▁▃█▄▂▃▇▁▅▂▁▂▄▃▃▄▂▁▃▃▂▂▂▄▂▃▆▄▄▂▃▄▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 36
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.88113
wandb:     train_loss 0.0
wandb:   val_accuracy 0.44444
wandb:       val_loss 0.00658
wandb: 
wandb: 🚀 View run classic-sun-668 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lokpos7w
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_181159-lokpos7w/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_182038-e9q2eoy0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-brook-670
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e9q2eoy0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:03, 18.16s/it]  0%|          | 2/500 [00:34<2:21:07, 17.00s/it]  1%|          | 3/500 [00:50<2:17:06, 16.55s/it]  1%|          | 4/500 [01:06<2:14:11, 16.23s/it]  1%|          | 5/500 [01:21<2:12:19, 16.04s/it]  1%|          | 6/500 [01:37<2:11:55, 16.02s/it]  1%|▏         | 7/500 [01:53<2:10:41, 15.91s/it]  2%|▏         | 8/500 [02:09<2:10:08, 15.87s/it]  2%|▏         | 9/500 [02:25<2:09:57, 15.88s/it]  2%|▏         | 10/500 [02:40<2:08:31, 15.74s/it]  2%|▏         | 11/500 [02:56<2:09:03, 15.84s/it]  2%|▏         | 12/500 [03:12<2:08:28, 15.80s/it]  3%|▎         | 13/500 [03:28<2:09:21, 15.94s/it]  3%|▎         | 14/500 [03:44<2:09:59, 16.05s/it]  3%|▎         | 15/500 [04:00<2:08:49, 15.94s/it]  3%|▎         | 16/500 [04:16<2:07:25, 15.80s/it]  3%|▎         | 17/500 [04:31<2:06:32, 15.72s/it]  4%|▎         | 18/500 [04:47<2:07:08, 15.83s/it]  4%|▍         | 19/500 [05:03<2:07:19, 15.88s/it]  4%|▍         | 20/500 [05:19<2:06:57, 15.87s/it]  4%|▍         | 21/500 [05:35<2:06:50, 15.89s/it]  4%|▍         | 22/500 [05:51<2:06:35, 15.89s/it]  5%|▍         | 23/500 [06:07<2:06:24, 15.90s/it]  5%|▍         | 24/500 [06:22<2:05:26, 15.81s/it]  5%|▌         | 25/500 [06:39<2:06:14, 15.95s/it]  5%|▌         | 26/500 [06:55<2:06:06, 15.96s/it]  5%|▌         | 27/500 [07:10<2:05:01, 15.86s/it]  6%|▌         | 28/500 [07:26<2:04:44, 15.86s/it]  6%|▌         | 29/500 [07:42<2:03:49, 15.77s/it]  6%|▌         | 30/500 [07:57<2:03:21, 15.75s/it]  6%|▌         | 31/500 [08:13<2:03:12, 15.76s/it]  6%|▋         | 32/500 [08:29<2:03:22, 15.82s/it]  7%|▋         | 33/500 [08:45<2:04:03, 15.94s/it]  7%|▋         | 34/500 [09:01<2:03:40, 15.92s/it]  7%|▋         | 35/500 [09:18<2:04:39, 16.09s/it]  7%|▋         | 36/500 [09:34<2:05:16, 16.20s/it]  7%|▋         | 37/500 [09:50<2:04:11, 16.09s/it]  8%|▊         | 38/500 [10:06<2:03:20, 16.02s/it]  8%|▊         | 39/500 [10:22<2:02:47, 15.98s/it]  8%|▊         | 40/500 [10:37<2:01:38, 15.87s/it]  8%|▊         | 41/500 [10:53<2:01:05, 15.83s/it]  8%|▊         | 42/500 [11:10<2:03:04, 16.12s/it]  9%|▊         | 43/500 [11:26<2:03:50, 16.26s/it]  9%|▉         | 44/500 [11:43<2:04:41, 16.41s/it]  9%|▉         | 45/500 [12:01<2:07:50, 16.86s/it]  9%|▉         | 46/500 [12:18<2:08:24, 16.97s/it]  9%|▉         | 47/500 [12:36<2:08:39, 17.04s/it] 10%|▉         | 48/500 [12:54<2:11:32, 17.46s/it] 10%|▉         | 49/500 [13:11<2:11:03, 17.44s/it] 10%|█         | 50/500 [13:29<2:10:45, 17.44s/it] 10%|█         | 51/500 [13:47<2:12:19, 17.68s/it] 10%|█         | 52/500 [14:04<2:11:09, 17.57s/it] 11%|█         | 53/500 [14:22<2:11:33, 17.66s/it] 11%|█         | 54/500 [14:40<2:10:36, 17.57s/it] 11%|█         | 55/500 [14:57<2:10:09, 17.55s/it] 11%|█         | 56/500 [15:15<2:11:18, 17.74s/it] 11%|█         | 56/500 [15:15<2:01:01, 16.35s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.020 MB of 0.311 MB uploadedwandb: - 0.310 MB of 0.311 MB uploadedwandb: \ 0.310 MB of 0.311 MB uploadedwandb: | 0.310 MB of 0.311 MB uploadedwandb: / 0.310 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▆▂▇▄▄▆▆█▆█▇███████████████████████████
wandb:     train_loss ▃▄▃▇▁█▁▁▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▄▅▂▁▇▂▁▃▃▇▄▆▃▅▆▅▇▆▅▅▆█▅▇▇▇▅▆▇▇▇▇▆▆▇▆▆▇▇▇
wandb:       val_loss ▂▂▁▄▂▂▃▂▂▂▃▃▆▂▃▁▁▆█▁▃▃▁▁▅▆▁▄▁▁▁▁▃▄▁▂▃▃▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00033
wandb: train_accuracy 1.0
wandb:     train_loss 0.01532
wandb:   val_accuracy 0.50444
wandb:       val_loss 0.08907
wandb: 
wandb: 🚀 View run morning-brook-670 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/e9q2eoy0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_182038-e9q2eoy0/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_183638-udt088gi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-feather-672
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/udt088gi
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:34:37, 18.59s/it]  0%|          | 2/500 [00:35<2:28:13, 17.86s/it]  1%|          | 3/500 [00:52<2:23:13, 17.29s/it]  1%|          | 4/500 [01:08<2:18:48, 16.79s/it]  1%|          | 5/500 [01:24<2:14:48, 16.34s/it]  1%|          | 6/500 [01:39<2:12:56, 16.15s/it]  1%|▏         | 7/500 [01:55<2:10:57, 15.94s/it]  2%|▏         | 8/500 [02:10<2:09:36, 15.81s/it]  2%|▏         | 9/500 [02:27<2:11:48, 16.11s/it]  2%|▏         | 10/500 [02:43<2:09:59, 15.92s/it]  2%|▏         | 11/500 [02:59<2:09:45, 15.92s/it]  2%|▏         | 12/500 [03:14<2:09:01, 15.86s/it]  3%|▎         | 13/500 [03:30<2:08:05, 15.78s/it]  3%|▎         | 14/500 [03:46<2:07:24, 15.73s/it]  3%|▎         | 15/500 [04:01<2:06:52, 15.70s/it]  3%|▎         | 16/500 [04:17<2:05:50, 15.60s/it]  3%|▎         | 17/500 [04:32<2:05:06, 15.54s/it]  4%|▎         | 18/500 [04:47<2:04:16, 15.47s/it]  4%|▍         | 19/500 [05:03<2:03:54, 15.46s/it]  4%|▍         | 20/500 [05:18<2:03:10, 15.40s/it]  4%|▍         | 21/500 [05:34<2:04:43, 15.62s/it]  4%|▍         | 22/500 [05:50<2:04:11, 15.59s/it]  5%|▍         | 23/500 [06:05<2:03:45, 15.57s/it]  5%|▍         | 24/500 [06:21<2:03:13, 15.53s/it]  5%|▌         | 25/500 [06:36<2:03:34, 15.61s/it]  5%|▌         | 26/500 [06:52<2:02:51, 15.55s/it]  5%|▌         | 27/500 [07:07<2:02:19, 15.52s/it]  6%|▌         | 28/500 [07:23<2:01:36, 15.46s/it]  6%|▌         | 29/500 [07:38<2:01:27, 15.47s/it]  6%|▌         | 30/500 [07:54<2:01:26, 15.50s/it]  6%|▌         | 31/500 [08:09<2:00:11, 15.38s/it]  6%|▋         | 32/500 [08:24<1:59:16, 15.29s/it]  7%|▋         | 33/500 [08:39<1:59:34, 15.36s/it]  7%|▋         | 34/500 [08:55<1:59:58, 15.45s/it]  7%|▋         | 35/500 [09:11<2:00:24, 15.54s/it]  7%|▋         | 36/500 [09:27<2:00:51, 15.63s/it]  7%|▋         | 37/500 [09:42<2:01:05, 15.69s/it]  8%|▊         | 38/500 [09:58<2:00:05, 15.60s/it]  8%|▊         | 39/500 [10:14<2:00:46, 15.72s/it]  8%|▊         | 40/500 [10:29<1:59:58, 15.65s/it]  8%|▊         | 41/500 [10:45<1:59:24, 15.61s/it]  8%|▊         | 42/500 [11:01<2:00:37, 15.80s/it]  9%|▊         | 43/500 [11:17<2:00:36, 15.84s/it]  9%|▉         | 44/500 [11:33<2:00:26, 15.85s/it]  9%|▉         | 45/500 [11:49<2:00:35, 15.90s/it]  9%|▉         | 46/500 [12:04<1:59:02, 15.73s/it]  9%|▉         | 47/500 [12:20<1:58:39, 15.72s/it] 10%|▉         | 48/500 [12:36<1:59:01, 15.80s/it] 10%|▉         | 49/500 [12:52<1:58:32, 15.77s/it] 10%|█         | 50/500 [13:07<1:57:57, 15.73s/it] 10%|█         | 51/500 [13:23<1:58:04, 15.78s/it] 10%|█         | 52/500 [13:39<1:57:32, 15.74s/it] 11%|█         | 53/500 [13:54<1:57:07, 15.72s/it] 11%|█         | 54/500 [14:10<1:56:52, 15.72s/it] 11%|█         | 55/500 [14:26<1:56:50, 15.75s/it] 11%|█         | 56/500 [14:42<1:56:36, 15.76s/it] 11%|█▏        | 57/500 [14:57<1:56:07, 15.73s/it] 12%|█▏        | 58/500 [15:13<1:55:14, 15.64s/it] 12%|█▏        | 59/500 [15:28<1:53:58, 15.51s/it] 12%|█▏        | 60/500 [15:43<1:53:32, 15.48s/it] 12%|█▏        | 60/500 [15:43<1:55:22, 15.73s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.020 MB of 0.316 MB uploadedwandb: \ 0.295 MB of 0.316 MB uploadedwandb: | 0.295 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▃▄▄▅▄▅▅▅▆▅▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇███▇███████
wandb:     train_loss ▆▆▆▅▄▆▅▅▄▂▅▄▅▇▅▅▆▃▂▅▄▃▃▁▃▁▅▃▃▅▁▂▂▄▃▄▄█▁▂
wandb:   val_accuracy ▅▅▅▅▄▂▂▂▁▁▄▁▃▁▃▂▃▂▃▅▅▃▅▆▆▅▆▇▆▇▇▅▇▇▇▇██▇▇
wandb:       val_loss ▂▃▂▃▃▂▂▃▄▃▃▄▂▄▂▂▂▆▄▂▃▄▇▆▂▅▃▅▂▁▄▅▇▆▅█▃▁▄▅
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.83804
wandb:     train_loss 0.32296
wandb:   val_accuracy 0.39333
wandb:       val_loss 2.30341
wandb: 
wandb: 🚀 View run elated-feather-672 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/udt088gi
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_183638-udt088gi/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_185311-zd0v8j10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-mountain-674
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zd0v8j10
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:42, 18.48s/it]  0%|          | 2/500 [00:34<2:20:10, 16.89s/it]  1%|          | 3/500 [00:50<2:15:38, 16.38s/it]  1%|          | 4/500 [01:05<2:12:40, 16.05s/it]  1%|          | 5/500 [01:21<2:12:20, 16.04s/it]  1%|          | 6/500 [01:37<2:10:57, 15.91s/it]  1%|▏         | 7/500 [01:52<2:09:36, 15.77s/it]  2%|▏         | 8/500 [02:08<2:08:27, 15.67s/it]  2%|▏         | 9/500 [02:24<2:10:29, 15.95s/it]  2%|▏         | 10/500 [02:40<2:10:08, 15.94s/it]  2%|▏         | 11/500 [02:56<2:09:27, 15.88s/it]  2%|▏         | 12/500 [03:12<2:09:18, 15.90s/it]  3%|▎         | 13/500 [03:27<2:07:40, 15.73s/it]  3%|▎         | 14/500 [03:43<2:06:49, 15.66s/it]  3%|▎         | 15/500 [03:58<2:05:24, 15.51s/it]  3%|▎         | 16/500 [04:14<2:05:43, 15.59s/it]  3%|▎         | 17/500 [04:29<2:05:50, 15.63s/it]  4%|▎         | 18/500 [04:45<2:05:36, 15.64s/it]  4%|▍         | 19/500 [05:01<2:05:33, 15.66s/it]  4%|▍         | 20/500 [05:17<2:05:59, 15.75s/it]  4%|▍         | 21/500 [05:33<2:06:15, 15.82s/it]  4%|▍         | 22/500 [05:49<2:06:16, 15.85s/it]  5%|▍         | 23/500 [06:05<2:07:17, 16.01s/it]  5%|▍         | 23/500 [06:05<2:06:20, 15.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.021 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▁▂▃▄▂▂▁▂▁▁▁▁▂▂▁▁▁▂▁█▁▄
wandb:     train_loss ▁▂▂▁▁▁▃▆▂▁▃▆▃▁▄█▅▁▂▁▁▄▁
wandb:   val_accuracy ▅▆▃▅▇▁▄▄▃▆▆▆▆▁▃▅▆▆▆▆▃▆█
wandb:       val_loss ▂▂▂▂▃▃▁▇▃▇▃█▅▄▅▆▅▄▂▆▅▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.55126
wandb:     train_loss 5e-05
wandb:   val_accuracy 0.40667
wandb:       val_loss 5.36745
wandb: 
wandb: 🚀 View run azure-mountain-674 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/zd0v8j10
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_185311-zd0v8j10/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_190005-1vt0cxnn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-field-676
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1vt0cxnn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:25:56, 17.55s/it]  0%|          | 2/500 [00:33<2:16:28, 16.44s/it]  1%|          | 3/500 [00:49<2:14:22, 16.22s/it]  1%|          | 4/500 [01:05<2:12:53, 16.08s/it]  1%|          | 5/500 [01:20<2:11:02, 15.88s/it]  1%|          | 6/500 [01:36<2:10:11, 15.81s/it]  1%|▏         | 7/500 [01:51<2:09:12, 15.73s/it]  2%|▏         | 8/500 [02:07<2:08:22, 15.66s/it]  2%|▏         | 9/500 [02:23<2:08:47, 15.74s/it]  2%|▏         | 10/500 [02:38<2:08:27, 15.73s/it]  2%|▏         | 11/500 [02:54<2:07:41, 15.67s/it]  2%|▏         | 12/500 [03:09<2:06:53, 15.60s/it]  3%|▎         | 13/500 [03:25<2:06:58, 15.64s/it]  3%|▎         | 14/500 [03:41<2:06:15, 15.59s/it]  3%|▎         | 15/500 [03:56<2:05:15, 15.50s/it]  3%|▎         | 16/500 [04:11<2:05:09, 15.52s/it]  3%|▎         | 17/500 [04:27<2:05:10, 15.55s/it]  4%|▎         | 18/500 [04:42<2:04:05, 15.45s/it]  4%|▍         | 19/500 [04:58<2:03:26, 15.40s/it]  4%|▍         | 20/500 [05:13<2:03:05, 15.39s/it]  4%|▍         | 21/500 [05:28<2:02:47, 15.38s/it]  4%|▍         | 22/500 [05:44<2:02:35, 15.39s/it]  5%|▍         | 23/500 [05:59<2:01:20, 15.26s/it]  5%|▍         | 24/500 [06:14<2:00:02, 15.13s/it]  5%|▌         | 25/500 [06:29<2:00:53, 15.27s/it]  5%|▌         | 26/500 [06:44<2:00:56, 15.31s/it]  5%|▌         | 27/500 [07:00<2:00:48, 15.32s/it]  6%|▌         | 28/500 [07:15<2:00:52, 15.37s/it]  6%|▌         | 29/500 [07:31<2:01:26, 15.47s/it]  6%|▌         | 30/500 [07:46<2:01:07, 15.46s/it]  6%|▌         | 31/500 [08:02<2:00:50, 15.46s/it]  6%|▋         | 32/500 [08:17<2:00:18, 15.42s/it]  7%|▋         | 33/500 [08:33<1:59:44, 15.39s/it]  7%|▋         | 34/500 [08:48<1:59:45, 15.42s/it]  7%|▋         | 35/500 [09:04<1:59:56, 15.48s/it]  7%|▋         | 36/500 [09:19<2:00:00, 15.52s/it]  7%|▋         | 37/500 [09:35<2:00:12, 15.58s/it]  8%|▊         | 38/500 [09:51<2:00:05, 15.60s/it]  8%|▊         | 38/500 [09:51<1:59:49, 15.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.308 MB uploadedwandb: / 0.020 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▅▇▃▇▇▄▆█▇▆██████▇███████████████████
wandb:     train_loss ▃▃▂▁▅▁▁█▁▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▅▆▃█▁▇▆▂▃▇█▅██▅█▇▆▅▇█▇▇▇▅█▇▇██▇▆▆█▇▇▇▇
wandb:       val_loss ▂▂▂▃▅▃▄▃▃▃▃▂▃▁▃▅▆▆▂▃▂▁▁▆▁▆▃▂▄▃▂▂▁█▄▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.99851
wandb:     train_loss 3e-05
wandb:   val_accuracy 0.43111
wandb:       val_loss 4.62346
wandb: 
wandb: 🚀 View run deep-field-676 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1vt0cxnn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_190005-1vt0cxnn/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_191040-j4ta6tza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-music-678
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j4ta6tza
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:16<2:20:51, 16.94s/it]  0%|          | 2/500 [00:32<2:13:48, 16.12s/it]  1%|          | 3/500 [00:48<2:11:39, 15.90s/it]  1%|          | 4/500 [01:03<2:10:36, 15.80s/it]  1%|          | 5/500 [01:19<2:09:10, 15.66s/it]  1%|          | 6/500 [01:34<2:08:41, 15.63s/it]  1%|▏         | 7/500 [01:50<2:08:32, 15.64s/it]  2%|▏         | 8/500 [02:05<2:07:53, 15.60s/it]  2%|▏         | 9/500 [02:21<2:07:16, 15.55s/it]  2%|▏         | 10/500 [02:36<2:07:09, 15.57s/it]  2%|▏         | 11/500 [02:52<2:06:48, 15.56s/it]  2%|▏         | 12/500 [03:07<2:05:52, 15.48s/it]  3%|▎         | 13/500 [03:23<2:06:20, 15.57s/it]  3%|▎         | 14/500 [03:38<2:05:44, 15.52s/it]  3%|▎         | 15/500 [03:54<2:05:47, 15.56s/it]  3%|▎         | 16/500 [04:10<2:05:08, 15.51s/it]  3%|▎         | 17/500 [04:25<2:04:23, 15.45s/it]  4%|▎         | 18/500 [04:44<2:12:31, 16.50s/it]  4%|▍         | 19/500 [05:00<2:10:38, 16.30s/it]  4%|▍         | 20/500 [05:15<2:07:56, 15.99s/it]  4%|▍         | 21/500 [05:30<2:06:03, 15.79s/it]  4%|▍         | 22/500 [05:46<2:04:36, 15.64s/it]  5%|▍         | 23/500 [06:01<2:04:21, 15.64s/it]  5%|▍         | 24/500 [06:17<2:03:33, 15.58s/it]  5%|▌         | 25/500 [06:32<2:03:07, 15.55s/it]  5%|▌         | 25/500 [06:32<2:04:19, 15.70s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.011 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▆▇▇▇███
wandb:     train_loss ▃▃▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂▃▃▃▃▂▂▂█
wandb:   val_accuracy ▆▇▅▇▆▅▆▆█▇▅▆▅█▆▆▇██▅▄▃▁▁▁
wandb:       val_loss ▃▃▃▃▄▁▂▂▁▂▂▂▂▆▂▃▇▇▃█▄▂▄█▃
wandb: 
wandb: Run summary:
wandb:          epoch 24
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.73997
wandb:     train_loss 3.63326
wandb:   val_accuracy 0.3
wandb:       val_loss 1.10099
wandb: 
wandb: 🚀 View run rare-music-678 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/j4ta6tza
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_191040-j4ta6tza/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_191756-f861qxhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-frost-679
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f861qxhl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:28:18, 17.83s/it]  0%|          | 2/500 [00:33<2:18:28, 16.68s/it]  1%|          | 3/500 [00:49<2:14:30, 16.24s/it]  1%|          | 4/500 [01:04<2:10:50, 15.83s/it]  1%|          | 5/500 [01:20<2:10:02, 15.76s/it]  1%|          | 6/500 [01:36<2:11:13, 15.94s/it]  1%|▏         | 7/500 [01:52<2:11:29, 16.00s/it]  2%|▏         | 8/500 [02:08<2:10:47, 15.95s/it]  2%|▏         | 9/500 [02:24<2:10:45, 15.98s/it]  2%|▏         | 10/500 [02:40<2:10:55, 16.03s/it]  2%|▏         | 11/500 [02:56<2:10:00, 15.95s/it]  2%|▏         | 12/500 [03:12<2:10:32, 16.05s/it]  3%|▎         | 13/500 [03:29<2:11:04, 16.15s/it]  3%|▎         | 14/500 [03:45<2:11:07, 16.19s/it]  3%|▎         | 15/500 [04:01<2:10:51, 16.19s/it]  3%|▎         | 16/500 [04:17<2:10:27, 16.17s/it]  3%|▎         | 17/500 [04:33<2:09:50, 16.13s/it]  4%|▎         | 18/500 [04:49<2:09:01, 16.06s/it]  4%|▍         | 19/500 [05:05<2:08:32, 16.03s/it]  4%|▍         | 20/500 [05:21<2:08:31, 16.07s/it]  4%|▍         | 21/500 [05:38<2:08:52, 16.14s/it]  4%|▍         | 22/500 [05:54<2:09:21, 16.24s/it]  5%|▍         | 23/500 [06:10<2:09:17, 16.26s/it]  5%|▍         | 23/500 [06:10<2:08:12, 16.13s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.027 MB uploadedwandb: / 0.010 MB of 0.310 MB uploadedwandb: - 0.168 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▂▄▁▅▁▂▁▃▂▁▁▁▁█▁▃▃▂▁▁▆▄█
wandb:     train_loss ▁▁▁▁█▁▄▃▁▁▄▃▄▁▁▃▁▁▄▁▁▁▁
wandb:   val_accuracy ▅▅▅▂▅▂▅▁▃▅▅▅▅▄▅▂▁▆▅▅▂▇█
wandb:       val_loss ▁▁▁▂█▂▁▃▃▅▄▄▇▁▃▃▅▃▃▃▄▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.88113
wandb:     train_loss 0.00084
wandb:   val_accuracy 0.51333
wandb:       val_loss 0.72289
wandb: 
wandb: 🚀 View run warm-frost-679 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/f861qxhl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_191756-f861qxhl/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_192447-151wdgma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-durian-681
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/151wdgma
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:50, 17.42s/it]  0%|          | 2/500 [00:34<2:21:21, 17.03s/it]  1%|          | 3/500 [00:50<2:20:14, 16.93s/it]  1%|          | 4/500 [01:07<2:18:01, 16.70s/it]  1%|          | 5/500 [01:23<2:16:06, 16.50s/it]  1%|          | 6/500 [01:39<2:14:04, 16.28s/it]  1%|▏         | 7/500 [01:55<2:13:26, 16.24s/it]  2%|▏         | 8/500 [02:11<2:12:50, 16.20s/it]  2%|▏         | 9/500 [02:27<2:12:05, 16.14s/it]  2%|▏         | 10/500 [02:43<2:10:58, 16.04s/it]  2%|▏         | 11/500 [02:59<2:10:04, 15.96s/it]  2%|▏         | 12/500 [03:15<2:09:24, 15.91s/it]  3%|▎         | 13/500 [03:30<2:07:59, 15.77s/it]  3%|▎         | 14/500 [03:46<2:07:23, 15.73s/it]  3%|▎         | 15/500 [04:01<2:06:23, 15.64s/it]  3%|▎         | 16/500 [04:17<2:06:13, 15.65s/it]  3%|▎         | 17/500 [04:33<2:06:51, 15.76s/it]  4%|▎         | 18/500 [04:49<2:07:15, 15.84s/it]  4%|▍         | 19/500 [05:05<2:07:05, 15.85s/it]  4%|▍         | 20/500 [05:20<2:06:46, 15.85s/it]  4%|▍         | 21/500 [05:36<2:06:39, 15.87s/it]  4%|▍         | 22/500 [05:53<2:07:06, 15.96s/it]  5%|▍         | 23/500 [06:09<2:08:00, 16.10s/it]  5%|▍         | 24/500 [06:24<2:05:48, 15.86s/it]  5%|▌         | 25/500 [06:40<2:04:37, 15.74s/it]  5%|▌         | 26/500 [06:55<2:03:42, 15.66s/it]  5%|▌         | 27/500 [07:11<2:04:21, 15.78s/it]  6%|▌         | 28/500 [07:27<2:05:06, 15.90s/it]  6%|▌         | 29/500 [07:44<2:05:22, 15.97s/it]  6%|▌         | 30/500 [08:00<2:05:31, 16.02s/it]  6%|▌         | 31/500 [08:16<2:05:09, 16.01s/it]  6%|▋         | 32/500 [08:32<2:04:51, 16.01s/it]  7%|▋         | 33/500 [08:48<2:06:00, 16.19s/it]  7%|▋         | 34/500 [09:05<2:07:20, 16.39s/it]  7%|▋         | 35/500 [09:21<2:06:51, 16.37s/it]  7%|▋         | 36/500 [09:38<2:07:00, 16.42s/it]  7%|▋         | 37/500 [09:54<2:06:29, 16.39s/it]  8%|▊         | 38/500 [10:10<2:05:07, 16.25s/it]  8%|▊         | 39/500 [10:26<2:04:39, 16.22s/it]  8%|▊         | 40/500 [10:48<2:16:00, 17.74s/it]  8%|▊         | 41/500 [11:04<2:11:49, 17.23s/it]  8%|▊         | 42/500 [11:20<2:09:03, 16.91s/it]  9%|▊         | 43/500 [11:36<2:07:01, 16.68s/it]  9%|▉         | 44/500 [11:52<2:06:11, 16.60s/it]  9%|▉         | 45/500 [12:09<2:05:07, 16.50s/it]  9%|▉         | 46/500 [12:24<2:02:59, 16.26s/it]  9%|▉         | 47/500 [12:41<2:02:38, 16.24s/it] 10%|▉         | 48/500 [12:57<2:02:36, 16.28s/it] 10%|▉         | 49/500 [13:13<2:00:46, 16.07s/it] 10%|▉         | 49/500 [13:13<2:01:40, 16.19s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▁▆▆▇█████████▇████████████████████████
wandb:     train_loss ▄▄█▁▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▄▃▄▁▁▇▆▅█▇▆▄▇▆▆▆▆▆▆▆▅▅▆▆▇▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇
wandb:       val_loss ▂▂▃▄▆▆▂▅▄▂▃▂▄▇▆▃▅▄▁█▁▅▁▅▆▃▁▄▅▇▃▅▁▃▄▃▃▄▃▆
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.99851
wandb:     train_loss 0.00165
wandb:   val_accuracy 0.46667
wandb:       val_loss 5.4724
wandb: 
wandb: 🚀 View run gentle-durian-681 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/151wdgma
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_192447-151wdgma/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_193857-dlo1lzeh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-rain-683
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dlo1lzeh
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:34:05, 18.53s/it]  0%|          | 2/500 [00:35<2:24:31, 17.41s/it]  1%|          | 3/500 [00:52<2:23:07, 17.28s/it]  1%|          | 4/500 [01:08<2:19:58, 16.93s/it]  1%|          | 5/500 [01:24<2:17:44, 16.70s/it]  1%|          | 6/500 [01:41<2:18:26, 16.81s/it]  1%|▏         | 7/500 [01:58<2:18:02, 16.80s/it]  2%|▏         | 8/500 [02:15<2:17:59, 16.83s/it]  2%|▏         | 9/500 [02:33<2:19:27, 17.04s/it]  2%|▏         | 10/500 [02:49<2:16:19, 16.69s/it]  2%|▏         | 11/500 [03:05<2:14:42, 16.53s/it]  2%|▏         | 12/500 [03:21<2:13:32, 16.42s/it]  3%|▎         | 13/500 [03:38<2:14:44, 16.60s/it]  3%|▎         | 14/500 [03:54<2:13:43, 16.51s/it]  3%|▎         | 15/500 [04:11<2:13:26, 16.51s/it]  3%|▎         | 16/500 [04:26<2:10:10, 16.14s/it]  3%|▎         | 17/500 [04:41<2:07:25, 15.83s/it]  4%|▎         | 18/500 [04:58<2:10:25, 16.24s/it]  4%|▍         | 19/500 [05:15<2:10:56, 16.33s/it]  4%|▍         | 20/500 [05:31<2:09:52, 16.23s/it]  4%|▍         | 21/500 [05:47<2:10:30, 16.35s/it]  4%|▍         | 22/500 [06:04<2:10:57, 16.44s/it]  5%|▍         | 23/500 [06:21<2:10:52, 16.46s/it]  5%|▍         | 24/500 [06:38<2:13:34, 16.84s/it]  5%|▌         | 25/500 [06:55<2:12:09, 16.69s/it]  5%|▌         | 26/500 [07:11<2:10:49, 16.56s/it]  5%|▌         | 27/500 [07:28<2:10:39, 16.57s/it]  6%|▌         | 28/500 [07:43<2:08:41, 16.36s/it]  6%|▌         | 29/500 [07:59<2:07:42, 16.27s/it]  6%|▌         | 30/500 [08:15<2:06:39, 16.17s/it]  6%|▌         | 31/500 [08:33<2:08:56, 16.50s/it]  6%|▋         | 32/500 [08:49<2:08:07, 16.43s/it]  7%|▋         | 33/500 [09:06<2:09:31, 16.64s/it]  7%|▋         | 34/500 [09:22<2:07:27, 16.41s/it]  7%|▋         | 35/500 [09:39<2:07:53, 16.50s/it]  7%|▋         | 36/500 [09:55<2:07:18, 16.46s/it]  7%|▋         | 37/500 [10:12<2:09:15, 16.75s/it]  8%|▊         | 38/500 [10:29<2:07:29, 16.56s/it]  8%|▊         | 39/500 [10:45<2:07:19, 16.57s/it]  8%|▊         | 40/500 [11:02<2:06:57, 16.56s/it]  8%|▊         | 41/500 [11:18<2:05:33, 16.41s/it]  8%|▊         | 42/500 [11:35<2:06:01, 16.51s/it]  9%|▊         | 43/500 [11:51<2:05:24, 16.46s/it]  9%|▉         | 44/500 [12:07<2:04:02, 16.32s/it]  9%|▉         | 45/500 [12:23<2:02:48, 16.19s/it]  9%|▉         | 46/500 [12:39<2:03:27, 16.32s/it]  9%|▉         | 47/500 [12:56<2:03:09, 16.31s/it] 10%|▉         | 48/500 [13:12<2:02:57, 16.32s/it] 10%|▉         | 49/500 [13:28<2:02:21, 16.28s/it] 10%|█         | 50/500 [13:45<2:03:45, 16.50s/it] 10%|█         | 51/500 [14:01<2:02:25, 16.36s/it] 10%|█         | 52/500 [14:17<2:01:40, 16.29s/it] 11%|█         | 53/500 [14:33<2:00:44, 16.21s/it] 11%|█         | 54/500 [14:50<2:01:17, 16.32s/it] 11%|█         | 55/500 [15:06<2:00:37, 16.26s/it] 11%|█         | 56/500 [15:22<2:00:25, 16.27s/it] 11%|█▏        | 57/500 [15:39<1:59:58, 16.25s/it] 12%|█▏        | 58/500 [15:55<1:59:32, 16.23s/it] 12%|█▏        | 59/500 [16:11<1:59:00, 16.19s/it] 12%|█▏        | 60/500 [16:26<1:57:27, 16.02s/it] 12%|█▏        | 61/500 [16:43<1:58:56, 16.26s/it] 12%|█▏        | 62/500 [16:59<1:58:12, 16.19s/it] 13%|█▎        | 63/500 [17:15<1:56:45, 16.03s/it] 13%|█▎        | 64/500 [17:31<1:57:04, 16.11s/it] 13%|█▎        | 64/500 [17:31<1:59:25, 16.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.318 MB uploadedwandb: | 0.020 MB of 0.318 MB uploadedwandb: / 0.309 MB of 0.318 MB uploadedwandb: - 0.313 MB of 0.318 MB uploadedwandb: \ 0.313 MB of 0.318 MB uploadedwandb: | 0.313 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▂▃▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆█▇▇█▇██▇▇█▇▇██
wandb:     train_loss █▇▇▇▆▆▇▇▆▄▇▇▆▅▇▇▂▁▄▃▃▄▂▁▅▂▃▅▁▂▂▆▃▂▂▂▂▄▃▂
wandb:   val_accuracy ▄▃▄▆▁▂▂▁▁▁▃▂▃▄▃▂▄▃▃▃▃▃▂▅▅▃▆▄▅▆▂▆▇▄▅▆▆▅▇█
wandb:       val_loss ▂▂▂▃▂▂▂▂▃▂▄▄▃▂▂▂█▃▃▃▁▆▇▄▃▄▁▄▃▄▆▄▄▃▂▄▆▆▅▇
wandb: 
wandb: Run summary:
wandb:          epoch 63
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.81575
wandb:     train_loss 0.37915
wandb:   val_accuracy 0.39111
wandb:       val_loss 3.20211
wandb: 
wandb: 🚀 View run revived-rain-683 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dlo1lzeh
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_193857-dlo1lzeh/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_195715-csctpd13
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-thunder-686
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/csctpd13
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:49, 17.41s/it]  0%|          | 2/500 [00:33<2:16:34, 16.45s/it]  1%|          | 3/500 [00:49<2:14:31, 16.24s/it]  1%|          | 4/500 [01:04<2:12:45, 16.06s/it]  1%|          | 5/500 [01:20<2:11:13, 15.91s/it]  1%|          | 6/500 [01:36<2:10:32, 15.85s/it]  1%|▏         | 7/500 [01:51<2:09:30, 15.76s/it]  2%|▏         | 8/500 [02:08<2:11:56, 16.09s/it]  2%|▏         | 9/500 [02:24<2:10:45, 15.98s/it]  2%|▏         | 10/500 [02:40<2:10:43, 16.01s/it]  2%|▏         | 11/500 [02:56<2:09:58, 15.95s/it]  2%|▏         | 12/500 [03:12<2:09:16, 15.89s/it]  3%|▎         | 13/500 [03:28<2:09:13, 15.92s/it]  3%|▎         | 14/500 [03:44<2:09:01, 15.93s/it]  3%|▎         | 15/500 [03:59<2:06:58, 15.71s/it]  3%|▎         | 16/500 [04:15<2:06:55, 15.73s/it]  3%|▎         | 17/500 [04:30<2:06:30, 15.71s/it]  4%|▎         | 18/500 [04:46<2:05:30, 15.62s/it]  4%|▍         | 19/500 [05:01<2:05:30, 15.66s/it]  4%|▍         | 20/500 [05:17<2:05:54, 15.74s/it]  4%|▍         | 21/500 [05:34<2:07:48, 16.01s/it]  4%|▍         | 22/500 [05:50<2:07:36, 16.02s/it]  5%|▍         | 23/500 [06:06<2:07:40, 16.06s/it]  5%|▍         | 24/500 [06:22<2:06:26, 15.94s/it]  5%|▌         | 25/500 [06:38<2:07:29, 16.10s/it]  5%|▌         | 26/500 [06:54<2:06:39, 16.03s/it]  5%|▌         | 27/500 [07:10<2:06:26, 16.04s/it]  6%|▌         | 28/500 [07:26<2:05:04, 15.90s/it]  6%|▌         | 29/500 [07:42<2:05:59, 16.05s/it]  6%|▌         | 30/500 [07:58<2:05:11, 15.98s/it]  6%|▌         | 31/500 [08:13<2:03:43, 15.83s/it]  6%|▋         | 32/500 [08:29<2:03:57, 15.89s/it]  7%|▋         | 33/500 [08:45<2:02:35, 15.75s/it]  7%|▋         | 34/500 [09:00<2:01:35, 15.66s/it]  7%|▋         | 35/500 [09:16<2:00:51, 15.59s/it]  7%|▋         | 36/500 [09:31<2:00:21, 15.56s/it]  7%|▋         | 37/500 [09:47<2:00:11, 15.58s/it]  8%|▊         | 38/500 [10:03<2:00:01, 15.59s/it]  8%|▊         | 39/500 [10:19<2:02:06, 15.89s/it]  8%|▊         | 39/500 [10:19<2:02:04, 15.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.200 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▄▅▄▅▄▄▆▇▆▇▇▇▇▅▇█▇▇██▇██████▇████████
wandb:     train_loss ▄▄▄▂▄▁▂█▁▁▃▃▁▁▁▁▁▁▁▁▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▅▄▃▂▃▁▃▁▂▄▇▄▆▆▆▆▄▆█▆█▇▇▇█▇▇████▇▇██████
wandb:       val_loss ▂▂▂▂▃▃▃▂▃▅▂▂█▁▃▃▅▆▁▄▂▁▁▅▁▇▁▃▃▄▃▁▁▃▇▆▅▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 38
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.99703
wandb:     train_loss 0.01517
wandb:   val_accuracy 0.50444
wandb:       val_loss 5.51329
wandb: 
wandb: 🚀 View run effortless-thunder-686 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/csctpd13
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_195715-csctpd13/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_200817-k3juwjnn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-water-687
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k3juwjnn
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:41:08, 19.38s/it]  0%|          | 2/500 [00:35<2:24:50, 17.45s/it]  1%|          | 3/500 [00:51<2:19:18, 16.82s/it]  1%|          | 4/500 [01:12<2:31:36, 18.34s/it]  1%|          | 5/500 [01:27<2:23:01, 17.34s/it]  1%|          | 6/500 [01:43<2:18:16, 16.79s/it]  1%|▏         | 7/500 [01:59<2:15:10, 16.45s/it]  2%|▏         | 8/500 [02:15<2:14:18, 16.38s/it]  2%|▏         | 9/500 [02:31<2:12:07, 16.15s/it]  2%|▏         | 10/500 [02:47<2:12:05, 16.17s/it]  2%|▏         | 11/500 [03:03<2:11:24, 16.12s/it]  2%|▏         | 12/500 [03:19<2:11:37, 16.18s/it]  3%|▎         | 13/500 [03:35<2:10:51, 16.12s/it]  3%|▎         | 14/500 [03:53<2:13:45, 16.51s/it]  3%|▎         | 15/500 [04:08<2:11:55, 16.32s/it]  3%|▎         | 16/500 [04:25<2:13:17, 16.52s/it]  3%|▎         | 17/500 [04:42<2:12:36, 16.47s/it]  4%|▎         | 18/500 [04:58<2:10:56, 16.30s/it]  4%|▍         | 19/500 [05:14<2:11:10, 16.36s/it]  4%|▍         | 20/500 [05:30<2:09:36, 16.20s/it]  4%|▍         | 21/500 [05:46<2:08:34, 16.11s/it]  4%|▍         | 22/500 [06:02<2:07:26, 16.00s/it]  5%|▍         | 23/500 [06:17<2:05:59, 15.85s/it]  5%|▍         | 24/500 [06:33<2:05:36, 15.83s/it]  5%|▌         | 25/500 [06:49<2:06:30, 15.98s/it]  5%|▌         | 26/500 [07:05<2:05:18, 15.86s/it]  5%|▌         | 27/500 [07:21<2:06:20, 16.03s/it]  6%|▌         | 28/500 [07:38<2:06:42, 16.11s/it]  6%|▌         | 29/500 [07:53<2:05:40, 16.01s/it]  6%|▌         | 30/500 [08:15<2:17:45, 17.59s/it]  6%|▌         | 31/500 [08:38<2:30:43, 19.28s/it]  6%|▋         | 32/500 [09:00<2:36:26, 20.06s/it]  7%|▋         | 33/500 [09:16<2:28:18, 19.06s/it]  7%|▋         | 34/500 [09:33<2:22:17, 18.32s/it]  7%|▋         | 35/500 [09:50<2:17:49, 17.78s/it]  7%|▋         | 36/500 [10:07<2:16:33, 17.66s/it]  7%|▋         | 37/500 [10:23<2:12:50, 17.22s/it]  8%|▊         | 38/500 [10:40<2:12:25, 17.20s/it]  8%|▊         | 39/500 [10:57<2:11:52, 17.16s/it]  8%|▊         | 40/500 [11:14<2:11:26, 17.15s/it]  8%|▊         | 41/500 [11:31<2:08:36, 16.81s/it]  8%|▊         | 42/500 [11:47<2:07:18, 16.68s/it]  9%|▊         | 43/500 [12:04<2:07:06, 16.69s/it]  9%|▉         | 44/500 [12:19<2:04:17, 16.36s/it]  9%|▉         | 45/500 [12:35<2:02:46, 16.19s/it]  9%|▉         | 46/500 [12:52<2:04:52, 16.50s/it]  9%|▉         | 47/500 [13:09<2:05:00, 16.56s/it] 10%|▉         | 48/500 [13:26<2:05:34, 16.67s/it] 10%|▉         | 49/500 [13:43<2:05:41, 16.72s/it] 10%|▉         | 49/500 [13:43<2:06:16, 16.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.308 MB uploadedwandb: - 0.010 MB of 0.308 MB uploadedwandb: \ 0.020 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb: / 0.308 MB of 0.308 MB uploadedwandb: - 0.308 MB of 0.308 MB uploadedwandb: \ 0.308 MB of 0.308 MB uploadedwandb: | 0.308 MB of 0.308 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▄▂▆▄▇▇▇▇█▇▇▇▇█▇██▇████████████████████
wandb:     train_loss █▇▇▄▂▁▂▂▁▂▁▁▁▅▂▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▁▆▆▇▅▆▇▆▇▇█▇▇█▇▇▇▇▆▆▇▇▆▇▇▇▇▆▆▇▇▇▇▇▆▇▇▇▆
wandb:       val_loss ▂▂▂▂▂▄▁▃▃▁▃▃▂▃▂▃▃▄▂▇▁▅▄▅▃▄▁█▅▃▄▃▁▄▅▁▅▃▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.99257
wandb:     train_loss 0.03957
wandb:   val_accuracy 0.40667
wandb:       val_loss 3.23688
wandb: 
wandb: 🚀 View run glorious-water-687 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/k3juwjnn
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_200817-k3juwjnn/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_202249-qazqfh7u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-salad-689
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qazqfh7u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:37:14, 18.91s/it]  0%|          | 2/500 [00:34<2:21:49, 17.09s/it]  1%|          | 3/500 [00:51<2:20:08, 16.92s/it]  1%|          | 4/500 [01:08<2:19:59, 16.93s/it]  1%|          | 5/500 [01:25<2:21:25, 17.14s/it]  1%|          | 6/500 [01:42<2:18:49, 16.86s/it]  1%|▏         | 7/500 [01:58<2:15:44, 16.52s/it]  2%|▏         | 8/500 [02:13<2:13:32, 16.29s/it]  2%|▏         | 9/500 [02:29<2:12:21, 16.17s/it]  2%|▏         | 10/500 [02:45<2:11:49, 16.14s/it]  2%|▏         | 11/500 [03:01<2:10:45, 16.04s/it]  2%|▏         | 12/500 [03:17<2:09:39, 15.94s/it]  3%|▎         | 13/500 [03:33<2:09:18, 15.93s/it]  3%|▎         | 14/500 [03:49<2:08:42, 15.89s/it]  3%|▎         | 15/500 [04:04<2:07:33, 15.78s/it]  3%|▎         | 16/500 [04:20<2:08:03, 15.88s/it]  3%|▎         | 17/500 [04:36<2:08:07, 15.92s/it]  4%|▎         | 18/500 [04:52<2:07:53, 15.92s/it]  4%|▍         | 19/500 [05:08<2:07:52, 15.95s/it]  4%|▍         | 20/500 [05:24<2:07:47, 15.97s/it]  4%|▍         | 21/500 [05:40<2:06:43, 15.87s/it]  4%|▍         | 22/500 [05:56<2:06:57, 15.94s/it]  5%|▍         | 23/500 [06:12<2:07:01, 15.98s/it]  5%|▍         | 24/500 [06:28<2:06:31, 15.95s/it]  5%|▌         | 25/500 [06:44<2:06:13, 15.95s/it]  5%|▌         | 26/500 [07:00<2:05:35, 15.90s/it]  5%|▌         | 27/500 [07:16<2:07:36, 16.19s/it]  6%|▌         | 28/500 [07:32<2:06:39, 16.10s/it]  6%|▌         | 29/500 [07:48<2:06:03, 16.06s/it]  6%|▌         | 30/500 [08:04<2:05:35, 16.03s/it]  6%|▌         | 31/500 [08:21<2:06:24, 16.17s/it]  6%|▋         | 32/500 [08:37<2:05:10, 16.05s/it]  7%|▋         | 33/500 [08:53<2:04:59, 16.06s/it]  7%|▋         | 34/500 [09:08<2:04:12, 15.99s/it]  7%|▋         | 35/500 [09:24<2:03:25, 15.93s/it]  7%|▋         | 36/500 [09:40<2:02:59, 15.90s/it]  7%|▋         | 37/500 [09:56<2:03:00, 15.94s/it]  8%|▊         | 38/500 [10:12<2:02:51, 15.96s/it]  8%|▊         | 39/500 [10:28<2:01:58, 15.88s/it]  8%|▊         | 40/500 [10:45<2:03:50, 16.15s/it]  8%|▊         | 41/500 [11:00<2:02:55, 16.07s/it]  8%|▊         | 42/500 [11:17<2:02:41, 16.07s/it]  9%|▊         | 43/500 [11:33<2:02:19, 16.06s/it]  9%|▉         | 44/500 [11:49<2:03:16, 16.22s/it]  9%|▉         | 45/500 [12:05<2:01:51, 16.07s/it]  9%|▉         | 46/500 [12:20<1:59:09, 15.75s/it]  9%|▉         | 47/500 [12:35<1:58:27, 15.69s/it] 10%|▉         | 48/500 [12:51<1:57:16, 15.57s/it] 10%|▉         | 49/500 [13:07<1:59:42, 15.93s/it] 10%|▉         | 49/500 [13:08<2:00:52, 16.08s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.228 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▂▃▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇█▇███▇█
wandb:     train_loss ██▆▇▇▆▇▇▆▇▆▅▆█▆▆▇▆▆▇▄▄▂▇▃▃▃▂▁▃▅▃▅▁▂█▁▂▂▂
wandb:   val_accuracy ▃▃▂▂▄▂▂▃▂▃▅▆▆█▇█▇▇▆▆▅▆▆▆▆▆▅▅▆▆▄▅▅▄▄▄▃▅▁▅
wandb:       val_loss ▃▄▄▃▄▃▃▃▄▃▃▄▃▄▄▃▅▃▃▅▃▆▄▄▅▄▁▇▆█▁▄▄▃▄▃▅▅▇█
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.87519
wandb:     train_loss 0.25058
wandb:   val_accuracy 0.39111
wandb:       val_loss 2.599
wandb: 
wandb: 🚀 View run copper-salad-689 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qazqfh7u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_202249-qazqfh7u/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_203643-vruklw05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-breeze-691
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vruklw05
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:34, 17.38s/it]  0%|          | 2/500 [00:33<2:19:29, 16.81s/it]  1%|          | 3/500 [00:49<2:15:17, 16.33s/it]  1%|          | 4/500 [01:05<2:13:01, 16.09s/it]  1%|          | 5/500 [01:20<2:11:19, 15.92s/it]  1%|          | 6/500 [01:36<2:09:40, 15.75s/it]  1%|▏         | 7/500 [01:52<2:09:17, 15.73s/it]  2%|▏         | 8/500 [02:07<2:09:01, 15.73s/it]  2%|▏         | 9/500 [02:23<2:08:35, 15.71s/it]  2%|▏         | 10/500 [02:39<2:09:26, 15.85s/it]  2%|▏         | 11/500 [02:55<2:09:08, 15.85s/it]  2%|▏         | 12/500 [03:11<2:08:58, 15.86s/it]  3%|▎         | 13/500 [03:27<2:09:32, 15.96s/it]  3%|▎         | 14/500 [03:43<2:08:52, 15.91s/it]  3%|▎         | 15/500 [04:02<2:17:16, 16.98s/it]  3%|▎         | 16/500 [04:18<2:13:53, 16.60s/it]  3%|▎         | 17/500 [04:34<2:12:05, 16.41s/it]  4%|▎         | 18/500 [04:50<2:09:50, 16.16s/it]  4%|▍         | 19/500 [05:10<2:19:31, 17.40s/it]  4%|▍         | 20/500 [05:31<2:28:23, 18.55s/it]  4%|▍         | 21/500 [05:47<2:22:09, 17.81s/it]  4%|▍         | 22/500 [06:09<2:30:48, 18.93s/it]  5%|▍         | 23/500 [06:25<2:24:29, 18.17s/it]  5%|▍         | 24/500 [06:41<2:19:19, 17.56s/it]  5%|▌         | 25/500 [06:58<2:16:40, 17.26s/it]  5%|▌         | 26/500 [07:14<2:14:47, 17.06s/it]  5%|▌         | 27/500 [07:31<2:12:54, 16.86s/it]  6%|▌         | 28/500 [07:47<2:11:14, 16.68s/it]  6%|▌         | 29/500 [08:04<2:10:43, 16.65s/it]  6%|▌         | 30/500 [08:22<2:15:26, 17.29s/it]  6%|▌         | 31/500 [08:38<2:12:14, 16.92s/it]  6%|▋         | 32/500 [08:55<2:10:08, 16.68s/it]  7%|▋         | 33/500 [09:10<2:07:43, 16.41s/it]  7%|▋         | 34/500 [09:26<2:06:28, 16.28s/it]  7%|▋         | 35/500 [09:42<2:05:10, 16.15s/it]  7%|▋         | 36/500 [09:58<2:04:37, 16.12s/it]  7%|▋         | 37/500 [10:14<2:04:17, 16.11s/it]  8%|▊         | 38/500 [10:31<2:05:17, 16.27s/it]  8%|▊         | 39/500 [10:47<2:04:50, 16.25s/it]  8%|▊         | 40/500 [11:04<2:05:32, 16.37s/it]  8%|▊         | 41/500 [11:21<2:06:01, 16.47s/it]  8%|▊         | 42/500 [11:37<2:05:00, 16.38s/it]  9%|▊         | 43/500 [11:53<2:03:53, 16.27s/it]  9%|▉         | 44/500 [12:09<2:04:16, 16.35s/it]  9%|▉         | 45/500 [12:25<2:03:19, 16.26s/it]  9%|▉         | 46/500 [12:42<2:04:09, 16.41s/it]  9%|▉         | 47/500 [12:58<2:03:29, 16.36s/it] 10%|▉         | 48/500 [13:14<2:02:41, 16.29s/it] 10%|▉         | 49/500 [13:31<2:02:55, 16.35s/it] 10%|▉         | 49/500 [13:31<2:04:28, 16.56s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.020 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▁▃▃▅▅▂▇▅▆▇▅▄▅█▅███████████████████████
wandb:     train_loss ▃▃█▂▃▁▅▂▁▄▁▁▁▁▁▁▆▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▂▃▂▄▄▆▇▂▆▂▄▅▂▁▂▆▂▇▇▇▇▆▇▇▇▇▇▇██▇▇█▇█▇▇▇▇▇
wandb:       val_loss ▂▂▇▂▂▃▂▃▃▃▁▁▃▆▅▃▄▄▂▆▁▆▂▂▄▃▁█▆▅▁▃▁▃▁▁▅▄▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.99257
wandb:     train_loss 0.01068
wandb:   val_accuracy 0.47111
wandb:       val_loss 3.50172
wandb: 
wandb: 🚀 View run peach-breeze-691 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/vruklw05
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_203643-vruklw05/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_205058-1r26q2t5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sunset-692
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1r26q2t5
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:27:49, 17.77s/it]  0%|          | 2/500 [00:33<2:16:37, 16.46s/it]  1%|          | 3/500 [00:48<2:12:33, 16.00s/it]  1%|          | 4/500 [01:04<2:12:01, 15.97s/it]  1%|          | 5/500 [01:21<2:14:22, 16.29s/it]  1%|          | 6/500 [01:37<2:12:50, 16.13s/it]  1%|▏         | 7/500 [01:53<2:12:32, 16.13s/it]  2%|▏         | 8/500 [02:08<2:10:08, 15.87s/it]  2%|▏         | 9/500 [02:24<2:09:13, 15.79s/it]  2%|▏         | 10/500 [02:39<2:08:16, 15.71s/it]  2%|▏         | 11/500 [03:00<2:19:04, 17.06s/it]  2%|▏         | 12/500 [03:16<2:17:59, 16.97s/it]  3%|▎         | 13/500 [03:32<2:14:57, 16.63s/it]  3%|▎         | 14/500 [03:49<2:14:44, 16.64s/it]  3%|▎         | 15/500 [04:05<2:12:20, 16.37s/it]  3%|▎         | 16/500 [04:26<2:23:37, 17.80s/it]  3%|▎         | 17/500 [04:41<2:18:07, 17.16s/it]  4%|▎         | 18/500 [04:57<2:13:48, 16.66s/it]  4%|▍         | 19/500 [05:12<2:10:18, 16.25s/it]  4%|▍         | 20/500 [05:29<2:10:28, 16.31s/it]  4%|▍         | 21/500 [05:44<2:08:03, 16.04s/it]  4%|▍         | 22/500 [06:00<2:06:37, 15.89s/it]  5%|▍         | 23/500 [06:15<2:06:14, 15.88s/it]  5%|▍         | 24/500 [06:31<2:04:54, 15.74s/it]  5%|▌         | 25/500 [06:48<2:08:01, 16.17s/it]  5%|▌         | 26/500 [07:08<2:17:35, 17.42s/it]  5%|▌         | 27/500 [07:24<2:12:48, 16.85s/it]  6%|▌         | 28/500 [07:39<2:09:14, 16.43s/it]  6%|▌         | 29/500 [07:55<2:07:47, 16.28s/it]  6%|▌         | 29/500 [07:55<2:08:46, 16.41s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.309 MB uploadedwandb: | 0.011 MB of 0.309 MB uploadedwandb: / 0.299 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▄▄▅▆▃▃▄▇▇█▇▅▇▆█▇▇██▇█▇█▅▇▇▆▆
wandb:     train_loss ▆▆▅▃▇█▁▁▂▁▃▅█▂▁▃▃▁▁▁▁▁▁▂▁▁▁▃▂
wandb:   val_accuracy ▁▄▆▆▇▆▅▅▆▇▆▇▇█▆▇▅█▇▇▇▇███▇███
wandb:       val_loss ▃▃▄▃▁▁▇▁▅▃▃▁▁▁▄▂▅▆▄▅▆▂▁▇▂█▅▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.82169
wandb:     train_loss 0.13596
wandb:   val_accuracy 0.54444
wandb:       val_loss 0.21555
wandb: 
wandb: 🚀 View run wandering-sunset-692 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1r26q2t5
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_205058-1r26q2t5/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_205945-86p1diln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sun-694
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/86p1diln
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:59, 18.40s/it]  0%|          | 2/500 [00:33<2:18:57, 16.74s/it]  1%|          | 3/500 [00:50<2:19:31, 16.84s/it]  1%|          | 4/500 [01:06<2:16:30, 16.51s/it]  1%|          | 5/500 [01:22<2:13:21, 16.16s/it]  1%|          | 6/500 [01:38<2:11:59, 16.03s/it]  1%|▏         | 7/500 [01:54<2:12:03, 16.07s/it]  2%|▏         | 8/500 [02:10<2:11:08, 15.99s/it]  2%|▏         | 9/500 [02:26<2:10:56, 16.00s/it]  2%|▏         | 10/500 [02:41<2:09:51, 15.90s/it]  2%|▏         | 11/500 [02:57<2:09:33, 15.90s/it]  2%|▏         | 12/500 [03:13<2:08:17, 15.77s/it]  3%|▎         | 13/500 [03:29<2:08:31, 15.83s/it]  3%|▎         | 14/500 [03:45<2:08:19, 15.84s/it]  3%|▎         | 15/500 [04:01<2:08:31, 15.90s/it]  3%|▎         | 16/500 [04:16<2:07:26, 15.80s/it]  3%|▎         | 17/500 [04:32<2:06:31, 15.72s/it]  4%|▎         | 18/500 [04:48<2:07:28, 15.87s/it]  4%|▍         | 19/500 [05:04<2:07:44, 15.93s/it]  4%|▍         | 20/500 [05:20<2:08:24, 16.05s/it]  4%|▍         | 21/500 [05:37<2:08:13, 16.06s/it]  4%|▍         | 22/500 [05:52<2:07:40, 16.03s/it]  5%|▍         | 23/500 [06:09<2:07:38, 16.05s/it]  5%|▍         | 24/500 [06:25<2:08:10, 16.16s/it]  5%|▌         | 25/500 [06:41<2:07:57, 16.16s/it]  5%|▌         | 26/500 [06:57<2:07:17, 16.11s/it]  5%|▌         | 27/500 [07:13<2:06:01, 15.99s/it]  6%|▌         | 28/500 [07:29<2:06:12, 16.04s/it]  6%|▌         | 29/500 [07:45<2:06:40, 16.14s/it]  6%|▌         | 29/500 [07:45<2:06:06, 16.07s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.137 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▆▆▆▆▇▇▇▇▇▇▇██▇▆▆▆▆▆▆▆▅▆▇▅▅
wandb:     train_loss ▅▄▄▄▄▃▃▅▃▄▅▄▄▁▂▃▅▄▄▄▃▃▄▃▃▂▆█▄
wandb:   val_accuracy ▁▁▁▄▆▆▆▆▇▆▆▆▇█▇▇█▇▇▆▇▇█▇▇▆▇▆▇
wandb:       val_loss ▅▆▅▅▅▄▆▅▅▄▅▅▁▆▅▃█▆▄▇▅▄▅▆▃▇▆▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.51263
wandb:     train_loss 1.13193
wandb:   val_accuracy 0.47333
wandb:       val_loss 1.10883
wandb: 
wandb: 🚀 View run glowing-sun-694 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/86p1diln
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_205945-86p1diln/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_210815-0btbt18u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-wind-695
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0btbt18u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:40, 17.64s/it]  0%|          | 2/500 [00:33<2:16:29, 16.44s/it]  1%|          | 3/500 [00:50<2:17:34, 16.61s/it]  1%|          | 4/500 [01:05<2:14:53, 16.32s/it]  1%|          | 5/500 [01:21<2:12:52, 16.11s/it]  1%|          | 6/500 [01:37<2:11:47, 16.01s/it]  1%|▏         | 7/500 [01:57<2:23:38, 17.48s/it]  2%|▏         | 8/500 [02:14<2:21:39, 17.28s/it]  2%|▏         | 9/500 [02:31<2:20:10, 17.13s/it]  2%|▏         | 10/500 [02:47<2:17:05, 16.79s/it]  2%|▏         | 11/500 [03:04<2:15:48, 16.66s/it]  2%|▏         | 12/500 [03:20<2:13:51, 16.46s/it]  3%|▎         | 13/500 [03:37<2:15:06, 16.65s/it]  3%|▎         | 14/500 [03:54<2:15:57, 16.79s/it]  3%|▎         | 15/500 [04:10<2:14:12, 16.60s/it]  3%|▎         | 16/500 [04:30<2:21:48, 17.58s/it]  3%|▎         | 17/500 [04:45<2:16:56, 17.01s/it]  4%|▎         | 18/500 [05:01<2:12:39, 16.51s/it]  4%|▍         | 19/500 [05:21<2:20:51, 17.57s/it]  4%|▍         | 20/500 [05:37<2:16:10, 17.02s/it]  4%|▍         | 21/500 [05:52<2:12:43, 16.62s/it]  4%|▍         | 22/500 [06:08<2:09:58, 16.32s/it]  5%|▍         | 23/500 [06:23<2:07:30, 16.04s/it]  5%|▍         | 23/500 [06:23<2:12:39, 16.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.314 MB uploadedwandb: \ 0.019 MB of 0.314 MB uploadedwandb: | 0.312 MB of 0.314 MB uploadedwandb: / 0.312 MB of 0.314 MB uploadedwandb: - 0.312 MB of 0.314 MB uploadedwandb: \ 0.312 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁
wandb: train_accuracy ▁▂▄▄▃▃▁▃▃▃▅▆▇▇▆▄▅█▇█▇██
wandb:     train_loss ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▅▅▇▆▃▂▅▅▂▁▂▄▅▆▃▂▂▆█▇▆▇█
wandb:       val_loss ▃▃▃▃▄▄▁▃▃▅▃▄▃▂▄▃█▇▂▄▅▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 22
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.97771
wandb:     train_loss 0.00528
wandb:   val_accuracy 0.45333
wandb:       val_loss 0.7233
wandb: 
wandb: 🚀 View run golden-wind-695 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0btbt18u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_210815-0btbt18u/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_211525-z4nmx6vq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-breeze-697
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z4nmx6vq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:41, 25.81s/it]  0%|          | 2/500 [00:56<3:56:21, 28.48s/it]  1%|          | 3/500 [01:24<3:54:41, 28.33s/it]  1%|          | 4/500 [01:51<3:51:12, 27.97s/it]  1%|          | 5/500 [02:20<3:53:43, 28.33s/it]  1%|          | 6/500 [02:44<3:40:52, 26.83s/it]  1%|▏         | 7/500 [03:08<3:32:18, 25.84s/it]  2%|▏         | 8/500 [03:38<3:43:14, 27.22s/it]  2%|▏         | 9/500 [04:12<4:00:47, 29.43s/it]  2%|▏         | 10/500 [04:36<3:46:56, 27.79s/it]  2%|▏         | 11/500 [05:00<3:36:48, 26.60s/it]  2%|▏         | 12/500 [05:29<3:41:08, 27.19s/it]  3%|▎         | 13/500 [05:58<3:45:45, 27.82s/it]  3%|▎         | 14/500 [06:22<3:35:36, 26.62s/it]  3%|▎         | 15/500 [06:57<3:55:23, 29.12s/it]  3%|▎         | 16/500 [07:22<3:44:45, 27.86s/it]  3%|▎         | 17/500 [07:47<3:37:33, 27.03s/it]  4%|▎         | 18/500 [08:21<3:53:57, 29.12s/it]  4%|▍         | 19/500 [08:45<3:40:52, 27.55s/it]  4%|▍         | 20/500 [09:10<3:33:49, 26.73s/it]  4%|▍         | 21/500 [09:33<3:26:19, 25.84s/it]  4%|▍         | 22/500 [10:03<3:33:33, 26.81s/it]  5%|▍         | 23/500 [10:27<3:27:49, 26.14s/it]  5%|▍         | 24/500 [10:51<3:22:01, 25.46s/it]  5%|▌         | 25/500 [11:15<3:17:45, 24.98s/it]  5%|▌         | 26/500 [11:39<3:14:52, 24.67s/it]  5%|▌         | 27/500 [12:03<3:12:31, 24.42s/it]  6%|▌         | 28/500 [12:27<3:12:46, 24.51s/it]  6%|▌         | 29/500 [12:51<3:11:12, 24.36s/it]  6%|▌         | 30/500 [13:17<3:14:49, 24.87s/it]  6%|▌         | 31/500 [13:42<3:13:08, 24.71s/it]  6%|▋         | 32/500 [14:10<3:21:32, 25.84s/it]  7%|▋         | 33/500 [14:39<3:28:58, 26.85s/it]  7%|▋         | 34/500 [15:03<3:21:41, 25.97s/it]  7%|▋         | 35/500 [15:27<3:15:46, 25.26s/it]  7%|▋         | 36/500 [15:51<3:11:58, 24.82s/it]  7%|▋         | 37/500 [16:14<3:08:53, 24.48s/it]  8%|▊         | 38/500 [16:38<3:05:51, 24.14s/it]  8%|▊         | 39/500 [17:07<3:16:10, 25.53s/it]  8%|▊         | 40/500 [17:31<3:12:12, 25.07s/it]  8%|▊         | 41/500 [17:55<3:10:26, 24.90s/it]  8%|▊         | 42/500 [18:19<3:08:04, 24.64s/it]  9%|▊         | 43/500 [18:43<3:05:31, 24.36s/it]  9%|▉         | 44/500 [19:07<3:03:52, 24.19s/it]  9%|▉         | 45/500 [19:31<3:04:29, 24.33s/it]  9%|▉         | 46/500 [19:59<3:12:48, 25.48s/it]  9%|▉         | 47/500 [20:24<3:09:36, 25.11s/it] 10%|▉         | 48/500 [20:53<3:18:51, 26.40s/it] 10%|▉         | 49/500 [21:17<3:12:37, 25.63s/it] 10%|█         | 50/500 [21:46<3:19:37, 26.62s/it] 10%|█         | 51/500 [22:15<3:24:27, 27.32s/it] 10%|█         | 52/500 [22:38<3:15:25, 26.17s/it] 11%|█         | 53/500 [23:02<3:09:13, 25.40s/it] 11%|█         | 54/500 [23:26<3:04:54, 24.88s/it] 11%|█         | 55/500 [23:49<3:01:49, 24.52s/it] 11%|█         | 56/500 [24:13<2:59:26, 24.25s/it] 11%|█▏        | 57/500 [24:42<3:10:43, 25.83s/it] 12%|█▏        | 58/500 [25:12<3:18:25, 26.94s/it] 12%|█▏        | 59/500 [25:36<3:11:30, 26.06s/it] 12%|█▏        | 60/500 [26:05<3:17:28, 26.93s/it] 12%|█▏        | 60/500 [26:10<3:11:57, 26.18s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.020 MB of 0.314 MB uploadedwandb: - 0.233 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▆▁▁▂▄▇█▇██████████████████████████████
wandb:     train_loss ▃▂▁▃▅▃▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▁▂▃▅▄█▇▇▆█▆███▅▇█▅██▇▇▅▇█▇▇▇▆▅▆▇▇▇▇▆▇▇
wandb:       val_loss ▁▂▂▂▃▂▂▂▂▄▃▅▂▁▃▁▄▁▁▄▅▁▄▅▁▂█▁▁▁▁▃▆▃▃▂▆▅▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 0.00026
wandb: train_accuracy 1.0
wandb:     train_loss 1e-05
wandb:   val_accuracy 0.52
wandb:       val_loss 4.25136
wandb: 
wandb: 🚀 View run lively-breeze-697 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/z4nmx6vq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_211525-z4nmx6vq/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_214235-5zc7qcdf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-paper-699
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5zc7qcdf
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:51, 26.32s/it]  0%|          | 2/500 [00:50<3:28:35, 25.13s/it]  1%|          | 3/500 [01:15<3:25:21, 24.79s/it]  1%|          | 4/500 [01:39<3:24:03, 24.68s/it]  1%|          | 5/500 [02:03<3:22:43, 24.57s/it]  1%|          | 6/500 [02:30<3:28:10, 25.29s/it]  1%|▏         | 7/500 [02:54<3:25:23, 25.00s/it]  2%|▏         | 8/500 [03:19<3:22:40, 24.72s/it]  2%|▏         | 9/500 [03:43<3:20:38, 24.52s/it]  2%|▏         | 10/500 [04:07<3:20:06, 24.50s/it]  2%|▏         | 11/500 [04:31<3:18:58, 24.41s/it]  2%|▏         | 12/500 [04:55<3:17:49, 24.32s/it]  3%|▎         | 13/500 [05:20<3:16:49, 24.25s/it]  3%|▎         | 14/500 [05:44<3:16:16, 24.23s/it]  3%|▎         | 15/500 [06:08<3:15:49, 24.23s/it]  3%|▎         | 16/500 [06:32<3:15:12, 24.20s/it]  3%|▎         | 17/500 [06:56<3:14:31, 24.17s/it]  4%|▎         | 18/500 [07:20<3:13:51, 24.13s/it]  4%|▍         | 19/500 [07:44<3:13:41, 24.16s/it]  4%|▍         | 20/500 [08:09<3:13:41, 24.21s/it]  4%|▍         | 21/500 [08:33<3:13:00, 24.18s/it]  4%|▍         | 22/500 [08:57<3:12:52, 24.21s/it]  5%|▍         | 23/500 [09:21<3:12:04, 24.16s/it]  5%|▍         | 24/500 [09:45<3:11:12, 24.10s/it]  5%|▌         | 25/500 [10:10<3:12:14, 24.28s/it]  5%|▌         | 26/500 [10:34<3:12:22, 24.35s/it]  5%|▌         | 27/500 [10:59<3:13:23, 24.53s/it]  6%|▌         | 28/500 [11:24<3:13:14, 24.56s/it]  6%|▌         | 29/500 [11:49<3:14:14, 24.74s/it]  6%|▌         | 30/500 [12:14<3:13:01, 24.64s/it]  6%|▌         | 31/500 [12:38<3:11:41, 24.52s/it]  6%|▋         | 32/500 [13:02<3:10:52, 24.47s/it]  7%|▋         | 33/500 [13:30<3:19:21, 25.61s/it]  7%|▋         | 34/500 [13:55<3:16:15, 25.27s/it]  7%|▋         | 35/500 [14:19<3:13:27, 24.96s/it]  7%|▋         | 36/500 [14:43<3:11:35, 24.77s/it]  7%|▋         | 37/500 [15:08<3:10:16, 24.66s/it]  8%|▊         | 38/500 [15:32<3:09:32, 24.62s/it]  8%|▊         | 39/500 [15:57<3:07:58, 24.47s/it]  8%|▊         | 40/500 [16:21<3:07:26, 24.45s/it]  8%|▊         | 41/500 [16:45<3:06:57, 24.44s/it]  8%|▊         | 42/500 [17:10<3:06:28, 24.43s/it]  9%|▊         | 43/500 [17:34<3:06:13, 24.45s/it]  9%|▉         | 44/500 [17:59<3:06:11, 24.50s/it]  9%|▉         | 45/500 [18:24<3:07:17, 24.70s/it]  9%|▉         | 46/500 [18:50<3:09:23, 25.03s/it]  9%|▉         | 47/500 [19:15<3:09:04, 25.04s/it] 10%|▉         | 48/500 [19:43<3:15:45, 25.98s/it] 10%|▉         | 49/500 [20:13<3:23:42, 27.10s/it] 10%|█         | 50/500 [20:44<3:32:55, 28.39s/it] 10%|█         | 51/500 [21:09<3:24:04, 27.27s/it] 10%|█         | 52/500 [21:33<3:17:09, 26.40s/it] 11%|█         | 53/500 [21:57<3:11:25, 25.69s/it] 11%|█         | 54/500 [22:22<3:08:58, 25.42s/it] 11%|█         | 55/500 [22:46<3:06:15, 25.11s/it] 11%|█         | 56/500 [23:11<3:04:54, 24.99s/it] 11%|█▏        | 57/500 [23:35<3:02:45, 24.75s/it] 12%|█▏        | 58/500 [24:00<3:01:18, 24.61s/it] 12%|█▏        | 59/500 [24:24<3:00:18, 24.53s/it] 12%|█▏        | 60/500 [24:53<3:09:22, 25.82s/it] 12%|█▏        | 61/500 [25:22<3:16:26, 26.85s/it] 12%|█▏        | 62/500 [25:46<3:10:43, 26.13s/it] 12%|█▏        | 62/500 [25:46<3:02:08, 24.95s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.233 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▂▃▄▄▄▅▅▅▅▅▆▅▆▆▇▇▅▇▇▇▇▇▇▇▇▇▇▇▂▇▇▇▇▇█▇███
wandb:     train_loss ▆▇█▆▇▅▆▅▄▄▅▃▆▇▅▄█▃▃▆▆▂▅▆▅▃▅▄▂▁▁▂▅▂▂▅▁▁▃▂
wandb:   val_accuracy ▄▅▂▃▂▃▁▁▁▃▃▂▃▃▁▃▅▃▃▃▄▄▄▄▃▇▆▇▇▃█▇███▇▇▇▇▇
wandb:       val_loss ▃▃▃▃▄▃▂▃▃▂▄▂▃▂▄▂▃▃▆▄▃▂▆▄▂█▁▃▁▁▅▅▆▃▂█▄▃▆▄
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.80535
wandb:     train_loss 0.19805
wandb:   val_accuracy 0.40222
wandb:       val_loss 1.42509
wandb: 
wandb: 🚀 View run fragrant-paper-699 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5zc7qcdf
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_214235-5zc7qcdf/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_220911-2aj0b629
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-jazz-701
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2aj0b629
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:41, 25.45s/it]  0%|          | 2/500 [00:49<3:22:34, 24.41s/it]  1%|          | 3/500 [01:13<3:22:10, 24.41s/it]  1%|          | 4/500 [01:38<3:23:00, 24.56s/it]  1%|          | 5/500 [02:07<3:35:15, 26.09s/it]  1%|          | 6/500 [02:31<3:30:32, 25.57s/it]  1%|▏         | 7/500 [02:55<3:25:43, 25.04s/it]  2%|▏         | 8/500 [03:24<3:35:39, 26.30s/it]  2%|▏         | 9/500 [03:48<3:29:07, 25.55s/it]  2%|▏         | 10/500 [04:13<3:26:41, 25.31s/it]  2%|▏         | 11/500 [04:37<3:22:34, 24.86s/it]  2%|▏         | 12/500 [05:00<3:19:09, 24.49s/it]  3%|▎         | 13/500 [05:24<3:16:21, 24.19s/it]  3%|▎         | 14/500 [05:48<3:15:10, 24.10s/it]  3%|▎         | 15/500 [06:11<3:13:47, 23.97s/it]  3%|▎         | 16/500 [06:40<3:24:18, 25.33s/it]  3%|▎         | 17/500 [07:03<3:19:50, 24.83s/it]  4%|▎         | 18/500 [07:27<3:17:04, 24.53s/it]  4%|▍         | 19/500 [07:51<3:14:14, 24.23s/it]  4%|▍         | 20/500 [08:15<3:13:21, 24.17s/it]  4%|▍         | 21/500 [08:39<3:11:48, 24.03s/it]  4%|▍         | 22/500 [09:02<3:10:08, 23.87s/it]  5%|▍         | 23/500 [09:26<3:09:51, 23.88s/it]  5%|▍         | 24/500 [09:54<3:18:25, 25.01s/it]  5%|▌         | 25/500 [10:18<3:15:21, 24.68s/it]  5%|▌         | 26/500 [10:47<3:26:13, 26.10s/it]  5%|▌         | 27/500 [11:16<3:33:00, 27.02s/it]  6%|▌         | 28/500 [11:41<3:27:00, 26.31s/it]  6%|▌         | 29/500 [12:05<3:22:45, 25.83s/it]  6%|▌         | 30/500 [12:30<3:19:30, 25.47s/it]  6%|▌         | 31/500 [13:00<3:29:10, 26.76s/it]  6%|▋         | 32/500 [13:24<3:21:28, 25.83s/it]  7%|▋         | 33/500 [13:48<3:17:04, 25.32s/it]  7%|▋         | 34/500 [14:11<3:13:00, 24.85s/it]  7%|▋         | 35/500 [14:35<3:10:35, 24.59s/it]  7%|▋         | 36/500 [14:59<3:07:46, 24.28s/it]  7%|▋         | 37/500 [15:23<3:06:18, 24.14s/it]  8%|▊         | 38/500 [15:46<3:04:44, 23.99s/it]  8%|▊         | 39/500 [16:10<3:03:28, 23.88s/it]  8%|▊         | 40/500 [16:34<3:02:57, 23.86s/it]  8%|▊         | 41/500 [16:58<3:02:21, 23.84s/it]  8%|▊         | 42/500 [17:21<3:01:51, 23.82s/it]  9%|▊         | 43/500 [17:46<3:02:13, 23.93s/it]  9%|▉         | 44/500 [18:09<3:01:07, 23.83s/it]  9%|▉         | 45/500 [18:36<3:07:58, 24.79s/it]  9%|▉         | 46/500 [19:01<3:06:37, 24.66s/it]  9%|▉         | 47/500 [19:25<3:04:41, 24.46s/it] 10%|▉         | 48/500 [19:49<3:03:04, 24.30s/it] 10%|▉         | 49/500 [20:12<3:01:37, 24.16s/it] 10%|█         | 50/500 [20:36<3:00:05, 24.01s/it] 10%|█         | 51/500 [21:00<2:59:48, 24.03s/it] 10%|█         | 52/500 [21:24<2:58:43, 23.94s/it] 11%|█         | 53/500 [21:48<2:57:42, 23.85s/it] 11%|█         | 54/500 [22:11<2:56:46, 23.78s/it] 11%|█         | 55/500 [22:35<2:56:15, 23.77s/it] 11%|█         | 56/500 [22:58<2:55:27, 23.71s/it] 11%|█▏        | 57/500 [23:22<2:53:58, 23.56s/it] 12%|█▏        | 58/500 [23:52<3:09:07, 25.67s/it] 12%|█▏        | 59/500 [24:21<3:16:09, 26.69s/it] 12%|█▏        | 60/500 [24:51<3:21:28, 27.47s/it] 12%|█▏        | 61/500 [25:25<3:36:46, 29.63s/it] 12%|█▏        | 62/500 [25:49<3:24:13, 27.98s/it] 13%|█▎        | 63/500 [26:13<3:14:44, 26.74s/it] 13%|█▎        | 64/500 [26:37<3:08:10, 25.90s/it] 13%|█▎        | 65/500 [27:01<3:02:38, 25.19s/it] 13%|█▎        | 66/500 [27:24<2:58:47, 24.72s/it] 13%|█▎        | 67/500 [27:48<2:56:07, 24.41s/it] 14%|█▎        | 68/500 [28:12<2:54:24, 24.22s/it] 14%|█▍        | 69/500 [28:39<3:00:23, 25.11s/it] 14%|█▍        | 70/500 [29:03<2:58:19, 24.88s/it] 14%|█▍        | 71/500 [29:27<2:55:26, 24.54s/it] 14%|█▍        | 72/500 [29:50<2:52:35, 24.19s/it] 15%|█▍        | 73/500 [30:14<2:50:49, 24.00s/it] 15%|█▍        | 74/500 [30:38<2:49:43, 23.90s/it] 15%|█▌        | 75/500 [31:01<2:48:45, 23.82s/it] 15%|█▌        | 76/500 [31:25<2:48:47, 23.89s/it] 15%|█▌        | 77/500 [31:52<2:54:42, 24.78s/it] 15%|█▌        | 77/500 [31:52<2:55:07, 24.84s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.317 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.020 MB of 0.317 MB uploadedwandb: - 0.168 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▆▄▃▄▆▇▆▅▆▇▆▇▇▇█▇▇█▂▇▆▇▇▇██▇██▇█▁▇▄██
wandb:     train_loss ▂▂▂▂▂▇▂▃▁▁▁▇▁▂▁▁▁▁▁▄▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁█▁▆▁▁
wandb:   val_accuracy ▃▃▃▃▃▆▇▄▆▇▇▆▆███████▇▇▃▅▁▆▆▅▅▅▅▄▅▅▄▂▅▄▃▅
wandb:       val_loss ▂▂▂▁▃▃▂▆▄▁▁▂▂▁▁▃▁▄▃▁▃▂▁▁▅█▄▄▇▃█▁▆▄▅▄▅▅▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.0001
wandb: train_accuracy 0.99703
wandb:     train_loss 0.00049
wandb:   val_accuracy 0.41111
wandb:       val_loss 3.12218
wandb: 
wandb: 🚀 View run peach-jazz-701 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2aj0b629
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_220911-2aj0b629/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_224151-664pgc19
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-cloud-704
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/664pgc19
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:39:34, 26.40s/it]  0%|          | 2/500 [00:50<3:26:46, 24.91s/it]  1%|          | 3/500 [01:14<3:25:15, 24.78s/it]  1%|          | 4/500 [01:39<3:23:20, 24.60s/it]  1%|          | 5/500 [02:03<3:20:35, 24.31s/it]  1%|          | 6/500 [02:26<3:19:00, 24.17s/it]  1%|▏         | 7/500 [02:50<3:16:36, 23.93s/it]  2%|▏         | 8/500 [03:13<3:14:04, 23.67s/it]  2%|▏         | 9/500 [03:41<3:24:12, 24.95s/it]  2%|▏         | 10/500 [04:04<3:18:33, 24.31s/it]  2%|▏         | 11/500 [04:31<3:25:55, 25.27s/it]  2%|▏         | 12/500 [04:54<3:19:42, 24.55s/it]  3%|▎         | 13/500 [05:17<3:15:02, 24.03s/it]  3%|▎         | 14/500 [05:40<3:11:33, 23.65s/it]  3%|▎         | 15/500 [06:03<3:11:05, 23.64s/it]  3%|▎         | 16/500 [06:26<3:09:20, 23.47s/it]  3%|▎         | 17/500 [06:49<3:07:54, 23.34s/it]  4%|▎         | 18/500 [07:12<3:06:47, 23.25s/it]  4%|▍         | 19/500 [07:35<3:05:26, 23.13s/it]  4%|▍         | 20/500 [07:58<3:04:36, 23.08s/it]  4%|▍         | 21/500 [08:21<3:03:53, 23.03s/it]  4%|▍         | 22/500 [08:44<3:03:26, 23.03s/it]  5%|▍         | 23/500 [09:14<3:20:09, 25.18s/it]  5%|▍         | 24/500 [09:37<3:14:11, 24.48s/it]  5%|▌         | 25/500 [10:00<3:10:19, 24.04s/it]  5%|▌         | 26/500 [10:23<3:07:52, 23.78s/it]  5%|▌         | 27/500 [10:47<3:06:25, 23.65s/it]  6%|▌         | 28/500 [11:10<3:04:56, 23.51s/it]  6%|▌         | 29/500 [11:34<3:05:59, 23.69s/it]  6%|▌         | 30/500 [11:57<3:04:20, 23.53s/it]  6%|▌         | 31/500 [12:27<3:18:08, 25.35s/it]  6%|▋         | 32/500 [12:50<3:13:03, 24.75s/it]  7%|▋         | 33/500 [13:18<3:19:56, 25.69s/it]  7%|▋         | 34/500 [13:41<3:13:02, 24.85s/it]  7%|▋         | 35/500 [14:04<3:08:56, 24.38s/it]  7%|▋         | 36/500 [14:27<3:05:29, 23.99s/it]  7%|▋         | 37/500 [14:50<3:02:36, 23.66s/it]  8%|▊         | 38/500 [15:17<3:09:41, 24.64s/it]  8%|▊         | 39/500 [15:40<3:05:59, 24.21s/it]  8%|▊         | 40/500 [16:03<3:02:30, 23.81s/it]  8%|▊         | 41/500 [16:38<3:27:30, 27.13s/it]  8%|▊         | 42/500 [17:01<3:18:05, 25.95s/it]  9%|▊         | 43/500 [17:25<3:13:02, 25.35s/it]  9%|▉         | 44/500 [17:49<3:09:51, 24.98s/it]  9%|▉         | 45/500 [18:12<3:05:26, 24.45s/it]  9%|▉         | 46/500 [18:35<3:01:22, 23.97s/it]  9%|▉         | 47/500 [18:58<2:58:19, 23.62s/it] 10%|▉         | 48/500 [19:24<3:02:47, 24.27s/it] 10%|▉         | 49/500 [19:47<3:00:00, 23.95s/it] 10%|█         | 50/500 [20:15<3:09:24, 25.25s/it] 10%|█         | 51/500 [20:38<3:04:03, 24.60s/it] 10%|█         | 52/500 [21:02<3:00:24, 24.16s/it] 11%|█         | 53/500 [21:25<2:58:30, 23.96s/it] 11%|█         | 54/500 [21:48<2:56:06, 23.69s/it] 11%|█         | 55/500 [22:11<2:54:00, 23.46s/it] 11%|█         | 56/500 [22:34<2:53:04, 23.39s/it] 11%|█▏        | 57/500 [23:03<3:04:11, 24.95s/it] 11%|█▏        | 57/500 [23:09<2:59:56, 24.37s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.231 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▁▁▁▇▂▇▇▇███▇███████████████████████████
wandb:     train_loss ▄▃▁█▁▅▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▂▁▂▇▃▇▇▇▂▇▅▅▄▃██▇▄▅▄▆▆▅▆▅▆▅▅▆▆▆▅█▆▅▅▆▆▇
wandb:       val_loss ▂▃▂▄▁▅▆▃▅▄▂▄▄▂▃▇▃▅▃▂▅▆▁▄▄▄▄█▁▃▁▁▁▂▃█▆▄▁▄
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 0.00033
wandb: train_accuracy 1.0
wandb:     train_loss 0.00148
wandb:   val_accuracy 0.51556
wandb:       val_loss 2.91932
wandb: 
wandb: 🚀 View run copper-cloud-704 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/664pgc19
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_224151-664pgc19/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_230543-rc9x1jd4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-bee-706
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rc9x1jd4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:44, 25.46s/it]  0%|          | 2/500 [00:49<3:22:39, 24.42s/it]  1%|          | 3/500 [01:13<3:20:31, 24.21s/it]  1%|          | 4/500 [01:37<3:19:10, 24.09s/it]  1%|          | 5/500 [02:00<3:17:26, 23.93s/it]  1%|          | 6/500 [02:24<3:16:29, 23.87s/it]  1%|▏         | 7/500 [02:48<3:15:24, 23.78s/it]  2%|▏         | 8/500 [03:11<3:15:18, 23.82s/it]  2%|▏         | 9/500 [03:40<3:28:17, 25.45s/it]  2%|▏         | 10/500 [04:04<3:23:07, 24.87s/it]  2%|▏         | 11/500 [04:28<3:20:01, 24.54s/it]  2%|▏         | 12/500 [04:52<3:17:40, 24.30s/it]  3%|▎         | 13/500 [05:20<3:28:30, 25.69s/it]  3%|▎         | 14/500 [05:44<3:22:58, 25.06s/it]  3%|▎         | 15/500 [06:08<3:19:49, 24.72s/it]  3%|▎         | 16/500 [06:32<3:17:22, 24.47s/it]  3%|▎         | 17/500 [06:56<3:15:05, 24.24s/it]  4%|▎         | 18/500 [07:19<3:13:44, 24.12s/it]  4%|▍         | 19/500 [07:44<3:13:18, 24.11s/it]  4%|▍         | 20/500 [08:08<3:13:16, 24.16s/it]  4%|▍         | 21/500 [08:32<3:12:02, 24.06s/it]  4%|▍         | 22/500 [09:01<3:23:30, 25.55s/it]  5%|▍         | 23/500 [09:29<3:29:58, 26.41s/it]  5%|▍         | 24/500 [09:58<3:36:18, 27.27s/it]  5%|▌         | 25/500 [10:29<3:44:44, 28.39s/it]  5%|▌         | 26/500 [10:58<3:45:35, 28.56s/it]  5%|▌         | 27/500 [11:22<3:33:58, 27.14s/it]  6%|▌         | 28/500 [11:56<3:48:53, 29.10s/it]  6%|▌         | 29/500 [12:20<3:36:05, 27.53s/it]  6%|▌         | 30/500 [12:44<3:27:19, 26.47s/it]  6%|▌         | 31/500 [13:08<3:20:56, 25.71s/it]  6%|▋         | 32/500 [13:31<3:16:15, 25.16s/it]  7%|▋         | 33/500 [13:55<3:12:19, 24.71s/it]  7%|▋         | 34/500 [14:19<3:09:12, 24.36s/it]  7%|▋         | 35/500 [14:42<3:06:35, 24.08s/it]  7%|▋         | 36/500 [15:10<3:15:58, 25.34s/it]  7%|▋         | 37/500 [15:34<3:12:18, 24.92s/it]  8%|▊         | 38/500 [15:58<3:09:06, 24.56s/it]  8%|▊         | 39/500 [16:22<3:06:53, 24.32s/it]  8%|▊         | 40/500 [16:45<3:04:44, 24.10s/it]  8%|▊         | 41/500 [17:09<3:03:32, 23.99s/it]  8%|▊         | 42/500 [17:33<3:02:37, 23.92s/it]  9%|▊         | 43/500 [17:56<3:01:23, 23.82s/it]  9%|▉         | 44/500 [18:20<3:00:53, 23.80s/it]  9%|▉         | 45/500 [18:47<3:08:11, 24.82s/it]  9%|▉         | 46/500 [19:11<3:05:07, 24.47s/it]  9%|▉         | 47/500 [19:35<3:03:19, 24.28s/it] 10%|▉         | 48/500 [19:59<3:01:32, 24.10s/it] 10%|▉         | 49/500 [20:27<3:09:59, 25.28s/it] 10%|█         | 50/500 [20:50<3:06:11, 24.83s/it] 10%|█         | 51/500 [21:14<3:02:59, 24.45s/it] 10%|█         | 52/500 [21:38<3:00:58, 24.24s/it] 11%|█         | 53/500 [22:02<2:59:55, 24.15s/it] 11%|█         | 54/500 [22:26<2:59:31, 24.15s/it] 11%|█         | 55/500 [22:54<3:08:18, 25.39s/it] 11%|█         | 56/500 [23:23<3:15:40, 26.44s/it] 11%|█▏        | 57/500 [23:52<3:21:17, 27.26s/it] 12%|█▏        | 58/500 [24:23<3:29:30, 28.44s/it] 12%|█▏        | 59/500 [24:55<3:35:59, 29.39s/it] 12%|█▏        | 60/500 [25:19<3:23:04, 27.69s/it] 12%|█▏        | 60/500 [25:19<3:05:40, 25.32s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.019 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▄▁▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇██▇████████
wandb:     train_loss ▆▇▇▁█▆▇▆▅▆▇▆▇▄▅▅▃▃▅▂▂▆▂▇█▃▃▆▅▂▂▁▃▃▂▃▄▂▁▁
wandb:   val_accuracy ▄▄▄▁▄▅▅▃▄▇▄▅▆██▇▃▆▅▅▆▅▆▅▂▅▄▆▄▅▆▁▃▄▄▅▇▃▇▇
wandb:       val_loss ▁▁▁█▁▁▁▁▁▂▂▁▁▁▂▁▂▁▁▂▂▁▂▁▂▁▂▁▁▁▂▂▂▂▂▂▂▄▂▂
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.8425
wandb:     train_loss 0.01144
wandb:   val_accuracy 0.37778
wandb:       val_loss 1.61482
wandb: 
wandb: 🚀 View run unique-bee-706 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rc9x1jd4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_230543-rc9x1jd4/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_233144-sox6kq7m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-microwave-709
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sox6kq7m
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:31:22, 25.42s/it]  0%|          | 2/500 [00:49<3:25:14, 24.73s/it]  1%|          | 3/500 [01:14<3:23:31, 24.57s/it]  1%|          | 4/500 [01:38<3:21:15, 24.35s/it]  1%|          | 5/500 [02:02<3:20:02, 24.25s/it]  1%|          | 6/500 [02:31<3:33:31, 25.93s/it]  1%|▏         | 7/500 [02:55<3:28:24, 25.36s/it]  2%|▏         | 8/500 [03:19<3:23:59, 24.88s/it]  2%|▏         | 9/500 [03:48<3:34:05, 26.16s/it]  2%|▏         | 10/500 [04:17<3:42:11, 27.21s/it]  2%|▏         | 11/500 [04:41<3:33:51, 26.24s/it]  2%|▏         | 12/500 [05:11<3:41:51, 27.28s/it]  3%|▎         | 13/500 [05:40<3:46:05, 27.86s/it]  3%|▎         | 14/500 [06:04<3:36:09, 26.69s/it]  3%|▎         | 15/500 [06:28<3:29:15, 25.89s/it]  3%|▎         | 16/500 [06:52<3:24:16, 25.32s/it]  3%|▎         | 17/500 [07:16<3:19:58, 24.84s/it]  4%|▎         | 18/500 [07:40<3:17:38, 24.60s/it]  4%|▍         | 19/500 [08:05<3:19:04, 24.83s/it]  4%|▍         | 20/500 [08:30<3:16:50, 24.61s/it]  4%|▍         | 21/500 [08:54<3:15:11, 24.45s/it]  4%|▍         | 22/500 [09:18<3:13:52, 24.34s/it]  5%|▍         | 23/500 [09:42<3:12:43, 24.24s/it]  5%|▍         | 24/500 [10:06<3:11:38, 24.16s/it]  5%|▌         | 25/500 [10:30<3:10:43, 24.09s/it]  5%|▌         | 26/500 [10:54<3:09:54, 24.04s/it]  5%|▌         | 27/500 [11:17<3:09:21, 24.02s/it]  6%|▌         | 28/500 [11:42<3:09:31, 24.09s/it]  6%|▌         | 29/500 [12:11<3:20:51, 25.59s/it]  6%|▌         | 30/500 [12:35<3:16:52, 25.13s/it]  6%|▌         | 31/500 [12:59<3:14:01, 24.82s/it]  6%|▋         | 32/500 [13:22<3:10:07, 24.37s/it]  7%|▋         | 33/500 [13:46<3:07:36, 24.10s/it]  7%|▋         | 34/500 [14:10<3:07:19, 24.12s/it]  7%|▋         | 35/500 [14:34<3:06:13, 24.03s/it]  7%|▋         | 36/500 [14:57<3:04:51, 23.90s/it]  7%|▋         | 37/500 [15:21<3:03:31, 23.78s/it]  8%|▊         | 38/500 [15:45<3:03:36, 23.85s/it]  8%|▊         | 39/500 [16:09<3:02:50, 23.80s/it]  8%|▊         | 40/500 [16:32<3:02:00, 23.74s/it]  8%|▊         | 41/500 [16:56<3:01:41, 23.75s/it]  8%|▊         | 42/500 [17:20<3:01:10, 23.73s/it]  9%|▊         | 43/500 [17:43<3:00:46, 23.73s/it]  9%|▉         | 44/500 [18:07<3:00:32, 23.75s/it]  9%|▉         | 45/500 [18:42<3:24:27, 26.96s/it]  9%|▉         | 46/500 [19:06<3:17:34, 26.11s/it]  9%|▉         | 47/500 [19:30<3:12:23, 25.48s/it] 10%|▉         | 48/500 [20:00<3:23:49, 27.06s/it] 10%|▉         | 49/500 [20:25<3:16:31, 26.15s/it] 10%|█         | 50/500 [20:48<3:11:09, 25.49s/it] 10%|█         | 51/500 [21:18<3:19:19, 26.64s/it] 10%|█         | 52/500 [21:42<3:12:39, 25.80s/it] 11%|█         | 53/500 [22:11<3:19:10, 26.73s/it] 11%|█         | 54/500 [22:34<3:12:10, 25.85s/it] 11%|█         | 55/500 [22:58<3:07:14, 25.25s/it] 11%|█         | 56/500 [23:22<3:03:38, 24.82s/it] 11%|█▏        | 57/500 [23:51<3:13:08, 26.16s/it] 12%|█▏        | 58/500 [24:15<3:06:58, 25.38s/it] 12%|█▏        | 59/500 [24:39<3:02:46, 24.87s/it] 12%|█▏        | 60/500 [25:02<3:00:14, 24.58s/it] 12%|█▏        | 61/500 [25:26<2:57:52, 24.31s/it] 12%|█▏        | 62/500 [25:51<2:59:52, 24.64s/it] 12%|█▏        | 62/500 [25:52<3:02:44, 25.03s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.318 MB uploadedwandb: - 0.020 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▂▂▁▁▅▃▆▆▇▆▆▆▄▇▄▅▇▆█▆▅▇▇▇▆▆▇█▅▆▇▃██▇▄▇▅█▄
wandb:     train_loss ▂▂▂▃▂▂▂▁▂▁▁▁▃▁█▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▂▁▁▁
wandb:   val_accuracy ▂▁▁▁▂▂▆▇█▇▅▇▅▇▇▆▇▆█▇▇███▇▇█▇▆▄▆▂▂▃▂▄▅▄▄▃
wandb:       val_loss ▂▂▂▁▂▂▂▂▃▂▃▁▁▁▂▁▁▁▄▃▁▁▆▃▁█▂▁▃▁▄▇▇▃▂▅▃▃▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.63893
wandb:     train_loss 0.02783
wandb:   val_accuracy 0.38
wandb:       val_loss 3.37491
wandb: 
wandb: 🚀 View run mild-microwave-709 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/sox6kq7m
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_233144-sox6kq7m/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240922_235825-ihz1jg8y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sound-711
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ihz1jg8y
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:37:59, 26.21s/it]  0%|          | 2/500 [00:50<3:26:41, 24.90s/it]  1%|          | 3/500 [01:14<3:24:10, 24.65s/it]  1%|          | 4/500 [01:38<3:21:25, 24.37s/it]  1%|          | 5/500 [02:02<3:20:47, 24.34s/it]  1%|          | 6/500 [02:27<3:21:11, 24.44s/it]  1%|▏         | 7/500 [02:51<3:18:57, 24.21s/it]  2%|▏         | 8/500 [03:19<3:29:32, 25.55s/it]  2%|▏         | 9/500 [03:43<3:25:24, 25.10s/it]  2%|▏         | 10/500 [04:07<3:21:05, 24.62s/it]  2%|▏         | 11/500 [04:36<3:32:36, 26.09s/it]  2%|▏         | 12/500 [05:00<3:26:34, 25.40s/it]  3%|▎         | 13/500 [05:24<3:23:18, 25.05s/it]  3%|▎         | 14/500 [05:48<3:19:46, 24.66s/it]  3%|▎         | 15/500 [06:12<3:17:25, 24.42s/it]  3%|▎         | 16/500 [06:35<3:14:33, 24.12s/it]  3%|▎         | 17/500 [06:58<3:12:01, 23.85s/it]  4%|▎         | 18/500 [07:30<3:31:12, 26.29s/it]  4%|▍         | 19/500 [07:54<3:24:26, 25.50s/it]  4%|▍         | 20/500 [08:18<3:20:37, 25.08s/it]  4%|▍         | 21/500 [08:43<3:18:58, 24.92s/it]  4%|▍         | 22/500 [09:07<3:16:41, 24.69s/it]  5%|▍         | 23/500 [09:31<3:14:01, 24.40s/it]  5%|▍         | 24/500 [09:55<3:13:06, 24.34s/it]  5%|▌         | 25/500 [10:19<3:11:29, 24.19s/it]  5%|▌         | 26/500 [10:43<3:11:50, 24.28s/it]  5%|▌         | 27/500 [11:11<3:19:54, 25.36s/it]  6%|▌         | 28/500 [11:35<3:16:59, 25.04s/it]  6%|▌         | 29/500 [12:05<3:26:30, 26.31s/it]  6%|▌         | 30/500 [12:28<3:20:17, 25.57s/it]  6%|▌         | 31/500 [12:52<3:15:54, 25.06s/it]  6%|▋         | 32/500 [13:16<3:12:44, 24.71s/it]  7%|▋         | 33/500 [13:45<3:22:00, 25.95s/it]  7%|▋         | 34/500 [14:09<3:16:39, 25.32s/it]  7%|▋         | 35/500 [14:37<3:23:19, 26.24s/it]  7%|▋         | 36/500 [15:01<3:16:47, 25.45s/it]  7%|▋         | 37/500 [15:25<3:12:40, 24.97s/it]  8%|▊         | 38/500 [15:49<3:11:36, 24.88s/it]  8%|▊         | 39/500 [16:14<3:10:50, 24.84s/it]  8%|▊         | 40/500 [16:38<3:08:34, 24.60s/it]  8%|▊         | 41/500 [17:07<3:17:17, 25.79s/it]  8%|▊         | 42/500 [17:31<3:12:29, 25.22s/it]  9%|▊         | 43/500 [17:59<3:19:40, 26.22s/it]  9%|▉         | 44/500 [18:23<3:13:52, 25.51s/it]  9%|▉         | 45/500 [18:47<3:09:22, 24.97s/it]  9%|▉         | 46/500 [19:11<3:06:13, 24.61s/it]  9%|▉         | 47/500 [19:35<3:04:20, 24.42s/it] 10%|▉         | 48/500 [19:58<3:02:12, 24.19s/it] 10%|▉         | 49/500 [20:22<3:01:19, 24.12s/it] 10%|█         | 50/500 [20:46<2:59:16, 23.90s/it] 10%|█         | 51/500 [21:10<2:59:20, 23.97s/it] 10%|█         | 52/500 [21:38<3:08:13, 25.21s/it] 11%|█         | 53/500 [22:01<3:04:23, 24.75s/it] 11%|█         | 54/500 [22:30<3:13:04, 25.97s/it] 11%|█         | 55/500 [22:54<3:07:12, 25.24s/it] 11%|█         | 56/500 [23:18<3:04:01, 24.87s/it] 11%|█▏        | 57/500 [23:41<3:00:39, 24.47s/it] 12%|█▏        | 58/500 [24:10<3:09:50, 25.77s/it] 12%|█▏        | 59/500 [24:39<3:17:06, 26.82s/it] 12%|█▏        | 60/500 [25:03<3:09:15, 25.81s/it] 12%|█▏        | 60/500 [25:11<3:04:41, 25.19s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.030 MB uploadedwandb: | 0.020 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▁▁▄▂▆█▄█▇▇▇▇██▇██▇▇██▅███████████████
wandb:     train_loss ▂▂▁▇▆▂▄▁▁▂▁▂▂▁▁▁▁▂▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▃▂▁▂▃▂▇▇▇▆▇▇▇▇▅█▇█▆█▇█▇▃▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆
wandb:       val_loss ▁▁▂▁▂▂▂▁▂▅▃▄▂▁▂▁▆▁▁▅▆▁▃▅▁▁█▁▁▁▁▃▇▃▂▁▆█▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 0.00026
wandb: train_accuracy 1.0
wandb:     train_loss 0.00786
wandb:   val_accuracy 0.49778
wandb:       val_loss 4.50763
wandb: 
wandb: 🚀 View run robust-sound-711 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ihz1jg8y
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_235825-ihz1jg8y/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_002418-1ejikf6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-shape-714
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1ejikf6u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:29<4:04:39, 29.42s/it]  0%|          | 2/500 [00:57<3:57:20, 28.60s/it]  1%|          | 3/500 [01:26<3:57:29, 28.67s/it]  1%|          | 4/500 [01:51<3:44:25, 27.15s/it]  1%|          | 5/500 [02:15<3:34:46, 26.03s/it]  1%|          | 6/500 [02:39<3:28:40, 25.35s/it]  1%|▏         | 7/500 [03:08<3:38:18, 26.57s/it]  2%|▏         | 8/500 [03:37<3:43:46, 27.29s/it]  2%|▏         | 9/500 [04:00<3:34:07, 26.17s/it]  2%|▏         | 10/500 [04:29<3:41:20, 27.10s/it]  2%|▏         | 11/500 [04:54<3:33:36, 26.21s/it]  2%|▏         | 12/500 [05:22<3:38:45, 26.90s/it]  3%|▎         | 13/500 [05:46<3:31:17, 26.03s/it]  3%|▎         | 14/500 [06:10<3:25:34, 25.38s/it]  3%|▎         | 15/500 [06:39<3:33:06, 26.36s/it]  3%|▎         | 16/500 [07:08<3:39:14, 27.18s/it]  3%|▎         | 17/500 [07:37<3:43:53, 27.81s/it]  4%|▎         | 18/500 [08:01<3:34:06, 26.65s/it]  4%|▍         | 19/500 [08:25<3:27:34, 25.89s/it]  4%|▍         | 20/500 [08:54<3:34:29, 26.81s/it]  4%|▍         | 21/500 [09:18<3:27:04, 25.94s/it]  4%|▍         | 22/500 [09:42<3:21:37, 25.31s/it]  5%|▍         | 23/500 [10:06<3:18:34, 24.98s/it]  5%|▍         | 24/500 [10:30<3:16:39, 24.79s/it]  5%|▌         | 25/500 [10:54<3:13:38, 24.46s/it]  5%|▌         | 26/500 [11:21<3:20:15, 25.35s/it]  5%|▌         | 27/500 [11:46<3:18:03, 25.12s/it]  6%|▌         | 28/500 [12:10<3:14:36, 24.74s/it]  6%|▌         | 29/500 [12:34<3:11:53, 24.45s/it]  6%|▌         | 30/500 [12:57<3:09:58, 24.25s/it]  6%|▌         | 31/500 [13:21<3:08:41, 24.14s/it]  6%|▋         | 32/500 [13:50<3:18:05, 25.40s/it]  7%|▋         | 33/500 [14:19<3:26:52, 26.58s/it]  7%|▋         | 34/500 [14:43<3:20:52, 25.86s/it]  7%|▋         | 35/500 [15:13<3:29:37, 27.05s/it]  7%|▋         | 36/500 [15:37<3:22:43, 26.22s/it]  7%|▋         | 37/500 [16:02<3:18:40, 25.75s/it]  8%|▊         | 38/500 [16:26<3:14:15, 25.23s/it]  8%|▊         | 39/500 [16:50<3:11:17, 24.90s/it]  8%|▊         | 40/500 [17:14<3:08:45, 24.62s/it]  8%|▊         | 41/500 [17:38<3:07:41, 24.54s/it]  8%|▊         | 42/500 [18:03<3:08:37, 24.71s/it]  9%|▊         | 43/500 [18:33<3:19:01, 26.13s/it]  9%|▉         | 44/500 [18:58<3:15:39, 25.75s/it]  9%|▉         | 45/500 [19:22<3:12:15, 25.35s/it]  9%|▉         | 46/500 [19:47<3:10:45, 25.21s/it]  9%|▉         | 47/500 [20:12<3:08:54, 25.02s/it] 10%|▉         | 48/500 [20:37<3:08:25, 25.01s/it] 10%|▉         | 49/500 [21:01<3:06:33, 24.82s/it] 10%|█         | 50/500 [21:25<3:04:32, 24.61s/it] 10%|█         | 51/500 [21:49<3:02:50, 24.43s/it] 10%|█         | 52/500 [22:14<3:03:21, 24.56s/it] 11%|█         | 53/500 [22:38<3:02:04, 24.44s/it] 11%|█         | 54/500 [23:02<3:01:26, 24.41s/it] 11%|█         | 55/500 [23:26<2:59:50, 24.25s/it] 11%|█         | 56/500 [23:50<2:58:41, 24.15s/it] 11%|█▏        | 57/500 [24:14<2:58:04, 24.12s/it] 11%|█▏        | 57/500 [24:14<3:08:26, 25.52s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▁▃▁▄▅▅▄▆▅▆▆▅▅▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇▆▇▇█▇██████
wandb:     train_loss ▄▅▇▄▄▃▅▄▂▃▃▅▂▄▅▂▂▂▆▃▂▂▅▂▃▆▃▅▄█▁▃▂▃▂▃▁▁▄▁
wandb:   val_accuracy ▃▄▂▅▁▃▅▃▃▂▅▃▄▄▂▂▃▄▅▅▅▄▅▅▆▇▆█▇▆▇▇▇▇▆▆█▇█▇
wandb:       val_loss ▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▂▁▂▁▁▃▂▁▂▃▂▁▂▁▂█▁▂▁▂▃▂▁▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 56
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.7786
wandb:     train_loss 0.11082
wandb:   val_accuracy 0.40222
wandb:       val_loss 1.38717
wandb: 
wandb: 🚀 View run curious-shape-714 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1ejikf6u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_002418-1ejikf6u/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_004919-ifg80cey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-glade-716
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ifg80cey
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:38:54, 26.32s/it]  0%|          | 2/500 [00:50<3:28:49, 25.16s/it]  1%|          | 3/500 [01:15<3:26:16, 24.90s/it]  1%|          | 4/500 [01:39<3:22:54, 24.55s/it]  1%|          | 5/500 [02:04<3:24:27, 24.78s/it]  1%|          | 6/500 [02:29<3:24:00, 24.78s/it]  1%|▏         | 7/500 [02:53<3:22:12, 24.61s/it]  2%|▏         | 8/500 [03:17<3:20:49, 24.49s/it]  2%|▏         | 9/500 [03:42<3:19:50, 24.42s/it]  2%|▏         | 10/500 [04:05<3:17:59, 24.24s/it]  2%|▏         | 11/500 [04:31<3:21:31, 24.73s/it]  2%|▏         | 12/500 [04:55<3:19:37, 24.54s/it]  3%|▎         | 13/500 [05:19<3:17:40, 24.35s/it]  3%|▎         | 14/500 [05:43<3:16:55, 24.31s/it]  3%|▎         | 15/500 [06:08<3:15:57, 24.24s/it]  3%|▎         | 16/500 [06:32<3:15:33, 24.24s/it]  3%|▎         | 17/500 [06:56<3:15:46, 24.32s/it]  4%|▎         | 18/500 [07:21<3:16:41, 24.48s/it]  4%|▍         | 19/500 [07:45<3:15:12, 24.35s/it]  4%|▍         | 20/500 [08:09<3:14:02, 24.26s/it]  4%|▍         | 21/500 [08:38<3:24:30, 25.62s/it]  4%|▍         | 22/500 [09:02<3:20:41, 25.19s/it]  5%|▍         | 23/500 [09:26<3:17:12, 24.81s/it]  5%|▍         | 24/500 [09:51<3:16:53, 24.82s/it]  5%|▌         | 25/500 [10:15<3:15:48, 24.73s/it]  5%|▌         | 26/500 [10:43<3:21:43, 25.53s/it]  5%|▌         | 27/500 [11:08<3:19:39, 25.33s/it]  6%|▌         | 28/500 [11:31<3:15:07, 24.80s/it]  6%|▌         | 29/500 [12:03<3:30:33, 26.82s/it]  6%|▌         | 30/500 [12:28<3:25:59, 26.30s/it]  6%|▌         | 31/500 [12:52<3:20:36, 25.67s/it]  6%|▋         | 32/500 [13:16<3:16:45, 25.23s/it]  7%|▋         | 33/500 [13:40<3:13:44, 24.89s/it]  7%|▋         | 34/500 [14:05<3:12:03, 24.73s/it]  7%|▋         | 35/500 [14:29<3:11:26, 24.70s/it]  7%|▋         | 36/500 [14:53<3:09:17, 24.48s/it]  7%|▋         | 37/500 [15:17<3:07:58, 24.36s/it]  8%|▊         | 38/500 [15:41<3:05:37, 24.11s/it]  8%|▊         | 39/500 [16:09<3:13:20, 25.16s/it]  8%|▊         | 40/500 [16:32<3:09:55, 24.77s/it]  8%|▊         | 40/500 [16:32<3:10:18, 24.82s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.316 MB uploadedwandb: | 0.019 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▂▂▂▂▂▁▅▆▅▅▅▆▆▆▇▃▆▆▇▄▇▆▆▅▇▆▆▆▇▅▄▅▆▆▇██▆▅▅
wandb:     train_loss ▃▂▃▃▃▂▃▂▂▂▂▁▂▂▁▁█▁▁▅▁▁▁▁▁▂▁▁▁▂▂▁▁▃▁▁▁▁▁█
wandb:   val_accuracy ▃▄▃▄▁▃▃▆▇▇▇▇▇▆▇▅▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▂▁▂▂▆▁▃▃▁▁▂▁▂▁▁▇▁▁▁▃▂█▁▁▃▆▂▁▁▂█
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.68796
wandb:     train_loss 4.56387
wandb:   val_accuracy 0.44222
wandb:       val_loss 13.29135
wandb: 
wandb: 🚀 View run legendary-glade-716 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ifg80cey
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_004919-ifg80cey/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_010631-qlt699em
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sun-718
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qlt699em
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:37:43, 26.18s/it]  0%|          | 2/500 [00:50<3:27:41, 25.02s/it]  1%|          | 3/500 [01:15<3:25:43, 24.84s/it]  1%|          | 4/500 [01:39<3:22:41, 24.52s/it]  1%|          | 5/500 [02:03<3:21:55, 24.47s/it]  1%|          | 6/500 [02:28<3:23:00, 24.66s/it]  1%|▏         | 7/500 [02:53<3:22:31, 24.65s/it]  2%|▏         | 8/500 [03:17<3:20:31, 24.45s/it]  2%|▏         | 9/500 [03:41<3:19:57, 24.43s/it]  2%|▏         | 10/500 [04:05<3:19:11, 24.39s/it]  2%|▏         | 11/500 [04:29<3:18:18, 24.33s/it]  2%|▏         | 12/500 [04:53<3:17:04, 24.23s/it]  3%|▎         | 13/500 [05:18<3:17:54, 24.38s/it]  3%|▎         | 14/500 [05:42<3:16:03, 24.20s/it]  3%|▎         | 15/500 [06:07<3:16:23, 24.30s/it]  3%|▎         | 16/500 [06:31<3:15:30, 24.24s/it]  3%|▎         | 17/500 [06:55<3:14:48, 24.20s/it]  4%|▎         | 18/500 [07:19<3:14:05, 24.16s/it]  4%|▍         | 19/500 [07:43<3:13:30, 24.14s/it]  4%|▍         | 20/500 [08:12<3:24:23, 25.55s/it]  4%|▍         | 21/500 [08:36<3:21:32, 25.24s/it]  4%|▍         | 22/500 [09:01<3:18:56, 24.97s/it]  5%|▍         | 23/500 [09:29<3:26:21, 25.96s/it]  5%|▍         | 24/500 [09:54<3:23:21, 25.63s/it]  5%|▌         | 25/500 [10:20<3:25:28, 25.95s/it]  5%|▌         | 26/500 [10:45<3:22:38, 25.65s/it]  5%|▌         | 27/500 [11:10<3:20:25, 25.42s/it]  6%|▌         | 28/500 [11:35<3:18:09, 25.19s/it]  6%|▌         | 29/500 [12:00<3:16:34, 25.04s/it]  6%|▌         | 30/500 [12:25<3:16:02, 25.03s/it]  6%|▌         | 31/500 [12:50<3:15:58, 25.07s/it]  6%|▋         | 32/500 [13:14<3:14:09, 24.89s/it]  7%|▋         | 33/500 [13:39<3:13:27, 24.86s/it]  7%|▋         | 34/500 [14:03<3:12:08, 24.74s/it]  7%|▋         | 35/500 [14:28<3:11:51, 24.76s/it]  7%|▋         | 36/500 [14:54<3:12:44, 24.92s/it]  7%|▋         | 37/500 [15:18<3:10:59, 24.75s/it]  8%|▊         | 38/500 [15:43<3:10:45, 24.77s/it]  8%|▊         | 39/500 [16:07<3:09:47, 24.70s/it]  8%|▊         | 40/500 [16:32<3:09:50, 24.76s/it]  8%|▊         | 41/500 [16:57<3:09:51, 24.82s/it]  8%|▊         | 42/500 [17:21<3:07:16, 24.53s/it]  8%|▊         | 42/500 [17:21<3:09:17, 24.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.312 MB uploadedwandb: \ 0.020 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▂▂▃▁▇▂▃▆▅▂▆▅▇▇▅▅▇▇▅█▇▇▇▇█▇██▇██████████
wandb:     train_loss ▂▂▂▁▄▂▂▅▁▁▁▂▃▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▂
wandb:   val_accuracy ▂▂▂▃▁▄▃▃▇▆▃▇▆▇█▆█▆▇▆▇▇▆▇▇█▆▇▆▆▇▇▇▆▇▆▇▇▇▆
wandb:       val_loss ▁▂▁▂▂▂▂▂▃▂▂▃▃▄▁▂▄▁▂▂▃▁▁▄▁▁▁▃▂▃▁▁▄▅▂▁▁▃█▂
wandb: 
wandb: Run summary:
wandb:          epoch 41
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.96285
wandb:     train_loss 0.75941
wandb:   val_accuracy 0.50667
wandb:       val_loss 2.66043
wandb: 
wandb: 🚀 View run frosty-sun-718 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qlt699em
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_010631-qlt699em/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_012438-7tua167c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sun-721
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7tua167c
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:49:17, 27.57s/it]  0%|          | 2/500 [00:51<3:33:10, 25.68s/it]  1%|          | 3/500 [01:16<3:29:00, 25.23s/it]  1%|          | 4/500 [01:40<3:24:21, 24.72s/it]  1%|          | 5/500 [02:04<3:21:42, 24.45s/it]  1%|          | 6/500 [02:28<3:20:27, 24.35s/it]  1%|▏         | 7/500 [02:52<3:19:20, 24.26s/it]  2%|▏         | 8/500 [03:17<3:20:30, 24.45s/it]  2%|▏         | 9/500 [03:41<3:18:19, 24.24s/it]  2%|▏         | 10/500 [04:05<3:17:32, 24.19s/it]  2%|▏         | 11/500 [04:29<3:16:28, 24.11s/it]  2%|▏         | 12/500 [04:53<3:15:34, 24.05s/it]  3%|▎         | 13/500 [05:17<3:15:30, 24.09s/it]  3%|▎         | 14/500 [05:41<3:14:51, 24.06s/it]  3%|▎         | 15/500 [06:05<3:15:13, 24.15s/it]  3%|▎         | 16/500 [06:30<3:15:23, 24.22s/it]  3%|▎         | 17/500 [06:54<3:14:46, 24.20s/it]  4%|▎         | 18/500 [07:18<3:14:21, 24.19s/it]  4%|▍         | 19/500 [07:42<3:13:28, 24.13s/it]  4%|▍         | 20/500 [08:06<3:13:34, 24.20s/it]  4%|▍         | 21/500 [08:31<3:13:12, 24.20s/it]  4%|▍         | 22/500 [08:55<3:12:19, 24.14s/it]  5%|▍         | 23/500 [09:19<3:13:10, 24.30s/it]  5%|▍         | 24/500 [09:43<3:11:38, 24.16s/it]  5%|▌         | 25/500 [10:07<3:10:28, 24.06s/it]  5%|▌         | 26/500 [10:31<3:09:13, 23.95s/it]  5%|▌         | 27/500 [10:55<3:09:11, 24.00s/it]  6%|▌         | 28/500 [11:19<3:08:37, 23.98s/it]  6%|▌         | 29/500 [11:42<3:07:10, 23.84s/it]  6%|▌         | 30/500 [12:05<3:05:02, 23.62s/it]  6%|▌         | 31/500 [12:29<3:04:45, 23.64s/it]  6%|▋         | 32/500 [12:53<3:05:27, 23.78s/it]  7%|▋         | 33/500 [13:18<3:06:38, 23.98s/it]  7%|▋         | 34/500 [13:42<3:06:29, 24.01s/it]  7%|▋         | 35/500 [14:06<3:07:35, 24.21s/it]  7%|▋         | 36/500 [14:35<3:16:54, 25.46s/it]  7%|▋         | 37/500 [14:59<3:13:10, 25.03s/it]  8%|▊         | 38/500 [15:24<3:12:34, 25.01s/it]  8%|▊         | 39/500 [15:48<3:09:35, 24.68s/it]  8%|▊         | 40/500 [16:12<3:09:17, 24.69s/it]  8%|▊         | 41/500 [16:37<3:08:09, 24.59s/it]  8%|▊         | 42/500 [17:00<3:06:02, 24.37s/it]  9%|▊         | 43/500 [17:24<3:03:34, 24.10s/it]  9%|▉         | 44/500 [17:49<3:05:13, 24.37s/it]  9%|▉         | 45/500 [18:16<3:09:44, 25.02s/it]  9%|▉         | 46/500 [18:39<3:06:40, 24.67s/it]  9%|▉         | 47/500 [19:07<3:13:15, 25.60s/it] 10%|▉         | 48/500 [19:31<3:08:49, 25.07s/it] 10%|▉         | 49/500 [20:00<3:16:24, 26.13s/it] 10%|█         | 50/500 [20:24<3:11:25, 25.52s/it] 10%|█         | 51/500 [20:48<3:08:03, 25.13s/it] 10%|█         | 52/500 [21:12<3:05:17, 24.82s/it] 11%|█         | 53/500 [21:39<3:10:08, 25.52s/it] 11%|█         | 54/500 [22:03<3:06:53, 25.14s/it] 11%|█         | 55/500 [22:31<3:11:22, 25.80s/it] 11%|█         | 56/500 [22:55<3:07:32, 25.34s/it] 11%|█▏        | 57/500 [23:23<3:12:05, 26.02s/it] 12%|█▏        | 58/500 [23:47<3:07:24, 25.44s/it] 12%|█▏        | 59/500 [24:11<3:04:28, 25.10s/it] 12%|█▏        | 60/500 [24:35<3:00:45, 24.65s/it] 12%|█▏        | 60/500 [24:35<3:00:17, 24.59s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.010 MB of 0.316 MB uploadedwandb: - 0.019 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▃▄▅▄▄▅▅▅▅▄▄▅▆▆▆▆▆▆▆▆▇▇▇▅▇▆▇▇▇█▇██▇▇███
wandb:     train_loss ▄▄▅▄▅▃▆▄▂▃▄▄█▃▂▂▂▅▄▂▂▃▂▃▂▇▃▄▃▂▂▁▃▃▁▃▃▂▁▂
wandb:   val_accuracy ▃▄▁▅▂▃▃▃▂▂▄▁▄▅▄▄▅▄▃▄▃▃▅▇▆▄▇▆▆▆▅█▆▇▆▆▆▇▆▆
wandb:       val_loss ▃▃▃▃▃▃▂▃▃▅▃▄▃▂▄▂▄▃▂▅▄▃▄▆▄▂▇▁▃▁▃▄▅▄▄▅█▇▃▅
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.80832
wandb:     train_loss 0.32431
wandb:   val_accuracy 0.40222
wandb:       val_loss 2.19728
wandb: 
wandb: 🚀 View run fast-sun-721 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7tua167c
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_012438-7tua167c/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_014958-pmqb9e9e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-valley-723
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pmqb9e9e
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:36<5:07:02, 36.92s/it]  0%|          | 2/500 [01:01<4:04:23, 29.45s/it]  1%|          | 3/500 [01:25<3:45:20, 27.20s/it]  1%|          | 4/500 [01:49<3:33:26, 25.82s/it]  1%|          | 5/500 [02:14<3:29:52, 25.44s/it]  1%|          | 6/500 [02:38<3:27:17, 25.18s/it]  1%|▏         | 7/500 [03:02<3:23:26, 24.76s/it]  2%|▏         | 8/500 [03:30<3:30:01, 25.61s/it]  2%|▏         | 9/500 [03:54<3:25:39, 25.13s/it]  2%|▏         | 10/500 [04:21<3:30:00, 25.72s/it]  2%|▏         | 11/500 [04:45<3:26:01, 25.28s/it]  2%|▏         | 12/500 [05:09<3:22:19, 24.88s/it]  3%|▎         | 13/500 [05:33<3:19:17, 24.55s/it]  3%|▎         | 14/500 [05:57<3:17:29, 24.38s/it]  3%|▎         | 15/500 [06:21<3:17:08, 24.39s/it]  3%|▎         | 16/500 [06:46<3:17:22, 24.47s/it]  3%|▎         | 17/500 [07:10<3:16:15, 24.38s/it]  4%|▎         | 18/500 [07:34<3:14:15, 24.18s/it]  4%|▍         | 19/500 [07:58<3:13:28, 24.13s/it]  4%|▍         | 20/500 [08:22<3:13:28, 24.19s/it]  4%|▍         | 21/500 [08:46<3:12:36, 24.13s/it]  4%|▍         | 22/500 [09:10<3:10:49, 23.95s/it]  5%|▍         | 23/500 [09:34<3:11:21, 24.07s/it]  5%|▍         | 24/500 [09:58<3:10:18, 23.99s/it]  5%|▌         | 25/500 [10:21<3:09:20, 23.92s/it]  5%|▌         | 26/500 [10:46<3:11:14, 24.21s/it]  5%|▌         | 27/500 [11:15<3:21:54, 25.61s/it]  6%|▌         | 28/500 [11:39<3:17:31, 25.11s/it]  6%|▌         | 29/500 [12:03<3:14:08, 24.73s/it]  6%|▌         | 30/500 [12:27<3:11:22, 24.43s/it]  6%|▌         | 31/500 [12:51<3:10:13, 24.34s/it]  6%|▋         | 32/500 [13:15<3:08:10, 24.12s/it]  7%|▋         | 33/500 [13:38<3:06:55, 24.02s/it]  7%|▋         | 34/500 [14:02<3:05:13, 23.85s/it]  7%|▋         | 35/500 [14:26<3:05:32, 23.94s/it]  7%|▋         | 36/500 [14:49<3:03:43, 23.76s/it]  7%|▋         | 37/500 [15:13<3:03:14, 23.75s/it]  8%|▊         | 38/500 [15:37<3:03:03, 23.77s/it]  8%|▊         | 39/500 [16:00<3:01:44, 23.65s/it]  8%|▊         | 40/500 [16:24<3:01:59, 23.74s/it]  8%|▊         | 41/500 [16:53<3:14:22, 25.41s/it]  8%|▊         | 42/500 [17:22<3:20:52, 26.31s/it]  9%|▊         | 43/500 [17:46<3:14:47, 25.57s/it]  9%|▉         | 44/500 [18:15<3:23:59, 26.84s/it]  9%|▉         | 45/500 [18:40<3:17:18, 26.02s/it]  9%|▉         | 46/500 [19:04<3:12:35, 25.45s/it]  9%|▉         | 47/500 [19:27<3:08:18, 24.94s/it] 10%|▉         | 48/500 [19:52<3:07:17, 24.86s/it] 10%|▉         | 49/500 [20:16<3:03:52, 24.46s/it] 10%|█         | 50/500 [20:40<3:02:14, 24.30s/it] 10%|█         | 51/500 [21:04<3:01:39, 24.27s/it] 10%|█         | 52/500 [21:27<2:59:58, 24.10s/it] 11%|█         | 53/500 [21:51<2:58:23, 23.94s/it] 11%|█         | 54/500 [22:15<2:58:05, 23.96s/it] 11%|█         | 55/500 [22:44<3:07:42, 25.31s/it] 11%|█         | 56/500 [23:07<3:03:52, 24.85s/it] 11%|█▏        | 57/500 [23:31<3:00:32, 24.45s/it] 12%|█▏        | 58/500 [23:55<2:59:10, 24.32s/it] 12%|█▏        | 59/500 [24:19<2:57:31, 24.15s/it] 12%|█▏        | 60/500 [24:42<2:56:14, 24.03s/it] 12%|█▏        | 61/500 [25:06<2:55:36, 24.00s/it] 12%|█▏        | 62/500 [25:30<2:54:45, 23.94s/it] 12%|█▏        | 62/500 [25:30<3:00:13, 24.69s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▂▁▂▁▃▄▃▆▅▅▅▄▅▄▄▅▃▇▆▅▇▆▇▄▆█▆█▇▄▅██▆▅▃█▆▄
wandb:     train_loss ▂▂▂▃▃▁▂▂▁▂▁▁▁▁█▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▇▁▁▁
wandb:   val_accuracy ▂▃▂▂▁▂▅▅▆▇▆▇▆▇▇▆▇▆▇▇▇██▇▇▇█▇▆▇▅▅▆▆▆▅▄▇▅▄
wandb:       val_loss ▂▂▂▁▂▂▂▁▂▁▃▁▁▁▂▁▁▁▄▃▁▁▆▂▁█▅▁▁▁▃▇▅▃▂▆▃▄▃▃
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.67608
wandb:     train_loss 5e-05
wandb:   val_accuracy 0.39778
wandb:       val_loss 4.66725
wandb: 
wandb: 🚀 View run deft-valley-723 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/pmqb9e9e
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_014958-pmqb9e9e/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_021606-4aikdtar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-voice-726
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4aikdtar
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:32:04, 25.50s/it]  0%|          | 2/500 [00:49<3:25:44, 24.79s/it]  1%|          | 3/500 [01:14<3:25:44, 24.84s/it]  1%|          | 4/500 [01:40<3:29:38, 25.36s/it]  1%|          | 5/500 [02:04<3:25:02, 24.85s/it]  1%|          | 6/500 [02:28<3:21:20, 24.45s/it]  1%|▏         | 7/500 [02:52<3:18:59, 24.22s/it]  2%|▏         | 8/500 [03:15<3:16:57, 24.02s/it]  2%|▏         | 9/500 [03:39<3:15:58, 23.95s/it]  2%|▏         | 10/500 [04:06<3:22:58, 24.85s/it]  2%|▏         | 11/500 [04:31<3:22:02, 24.79s/it]  2%|▏         | 12/500 [04:54<3:19:06, 24.48s/it]  3%|▎         | 13/500 [05:18<3:16:20, 24.19s/it]  3%|▎         | 14/500 [05:41<3:13:40, 23.91s/it]  3%|▎         | 15/500 [06:05<3:12:54, 23.86s/it]  3%|▎         | 16/500 [06:29<3:11:52, 23.79s/it]  3%|▎         | 17/500 [06:52<3:11:15, 23.76s/it]  4%|▎         | 18/500 [07:17<3:13:59, 24.15s/it]  4%|▍         | 19/500 [07:42<3:14:19, 24.24s/it]  4%|▍         | 20/500 [08:09<3:21:49, 25.23s/it]  4%|▍         | 21/500 [08:33<3:18:34, 24.87s/it]  4%|▍         | 22/500 [08:57<3:16:04, 24.61s/it]  5%|▍         | 23/500 [09:26<3:24:38, 25.74s/it]  5%|▍         | 24/500 [09:49<3:17:51, 24.94s/it]  5%|▌         | 25/500 [10:12<3:14:10, 24.53s/it]  5%|▌         | 26/500 [10:37<3:13:31, 24.50s/it]  5%|▌         | 27/500 [11:00<3:11:09, 24.25s/it]  6%|▌         | 28/500 [11:24<3:09:28, 24.09s/it]  6%|▌         | 29/500 [11:48<3:07:58, 23.95s/it]  6%|▌         | 30/500 [12:11<3:06:40, 23.83s/it]  6%|▌         | 31/500 [12:35<3:05:52, 23.78s/it]  6%|▋         | 32/500 [13:00<3:07:42, 24.06s/it]  7%|▋         | 33/500 [13:23<3:05:57, 23.89s/it]  7%|▋         | 34/500 [13:47<3:05:16, 23.85s/it]  7%|▋         | 35/500 [14:12<3:06:52, 24.11s/it]  7%|▋         | 36/500 [14:35<3:05:05, 23.93s/it]  7%|▋         | 37/500 [14:59<3:04:00, 23.85s/it]  8%|▊         | 38/500 [15:22<3:02:37, 23.72s/it]  8%|▊         | 39/500 [15:46<3:01:33, 23.63s/it]  8%|▊         | 40/500 [16:09<3:00:34, 23.55s/it]  8%|▊         | 41/500 [16:33<3:00:40, 23.62s/it]  8%|▊         | 42/500 [16:56<2:59:47, 23.55s/it]  9%|▊         | 43/500 [17:21<3:01:11, 23.79s/it]  9%|▉         | 44/500 [17:44<3:00:26, 23.74s/it]  9%|▉         | 45/500 [18:08<3:00:34, 23.81s/it]  9%|▉         | 46/500 [18:37<3:11:01, 25.25s/it]  9%|▉         | 47/500 [19:01<3:08:26, 24.96s/it] 10%|▉         | 48/500 [19:25<3:05:04, 24.57s/it] 10%|▉         | 49/500 [19:48<3:02:11, 24.24s/it] 10%|█         | 50/500 [20:12<3:00:30, 24.07s/it] 10%|█         | 51/500 [20:36<2:59:19, 23.96s/it] 10%|█         | 52/500 [20:59<2:58:26, 23.90s/it] 11%|█         | 53/500 [21:24<3:00:04, 24.17s/it] 11%|█         | 54/500 [21:50<3:02:43, 24.58s/it] 11%|█         | 55/500 [22:13<3:00:16, 24.31s/it] 11%|█         | 56/500 [22:41<3:07:22, 25.32s/it] 11%|█▏        | 57/500 [23:05<3:03:14, 24.82s/it] 12%|█▏        | 58/500 [23:28<2:59:10, 24.32s/it] 12%|█▏        | 59/500 [23:51<2:57:01, 24.08s/it] 12%|█▏        | 60/500 [24:24<3:15:34, 26.67s/it] 12%|█▏        | 61/500 [24:48<3:08:40, 25.79s/it] 12%|█▏        | 62/500 [25:11<3:02:57, 25.06s/it] 12%|█▏        | 62/500 [25:11<2:57:59, 24.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.019 MB of 0.317 MB uploadedwandb: - 0.317 MB of 0.317 MB uploadedwandb: \ 0.317 MB of 0.317 MB uploadedwandb: | 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁
wandb: train_accuracy ▁▁▂▃▃▄▄▆▆▃▇▆▅▆█▇█▆▇███████▇█████████████
wandb:     train_loss ▄▃▇▁█▁▁▁▁▆▇▁▁▄▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▅▅▄▂▁▂▁▄▂▂▄▄▂▄▆▄▇▃▄▅▅▆▅▇▆█▄▆█▇▅▄▆▆▆▆█▆▆▆
wandb:       val_loss ▂▁▂▄▅▃▂▃▃▃▃▁▃▄▆▃▁▅▃▄▁▁▁▅▂█▁▁▁▁▆▅▇▂▁▃▄▄▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 61
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.99851
wandb:     train_loss 0.0
wandb:   val_accuracy 0.40667
wandb:       val_loss 5.37365
wandb: 
wandb: 🚀 View run fanciful-voice-726 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/4aikdtar
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_021606-4aikdtar/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_024158-iq5tw9wx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-valley-729
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/iq5tw9wx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:36:41, 26.06s/it]  0%|          | 2/500 [00:50<3:29:01, 25.18s/it]  1%|          | 3/500 [01:15<3:26:12, 24.89s/it]  1%|          | 4/500 [01:39<3:24:02, 24.68s/it]  1%|          | 5/500 [02:04<3:24:30, 24.79s/it]  1%|          | 6/500 [02:29<3:25:33, 24.97s/it]  1%|▏         | 7/500 [02:54<3:23:16, 24.74s/it]  2%|▏         | 8/500 [03:18<3:22:58, 24.75s/it]  2%|▏         | 9/500 [03:43<3:22:39, 24.77s/it]  2%|▏         | 10/500 [04:08<3:21:21, 24.66s/it]  2%|▏         | 11/500 [04:32<3:19:18, 24.45s/it]  2%|▏         | 12/500 [04:56<3:18:45, 24.44s/it]  3%|▎         | 13/500 [05:21<3:19:02, 24.52s/it]  3%|▎         | 14/500 [05:46<3:21:22, 24.86s/it]  3%|▎         | 15/500 [06:11<3:20:03, 24.75s/it]  3%|▎         | 16/500 [06:35<3:18:56, 24.66s/it]  3%|▎         | 17/500 [07:00<3:19:06, 24.73s/it]  4%|▎         | 18/500 [07:25<3:17:44, 24.61s/it]  4%|▍         | 19/500 [07:50<3:18:58, 24.82s/it]  4%|▍         | 20/500 [08:15<3:18:33, 24.82s/it]  4%|▍         | 21/500 [08:39<3:17:17, 24.71s/it]  4%|▍         | 22/500 [09:03<3:15:24, 24.53s/it]  5%|▍         | 23/500 [09:27<3:13:45, 24.37s/it]  5%|▍         | 24/500 [09:51<3:12:54, 24.32s/it]  5%|▌         | 25/500 [10:16<3:12:29, 24.31s/it]  5%|▌         | 26/500 [10:40<3:12:25, 24.36s/it]  5%|▌         | 27/500 [11:04<3:11:39, 24.31s/it]  6%|▌         | 28/500 [11:28<3:10:44, 24.25s/it]  6%|▌         | 29/500 [11:53<3:09:50, 24.18s/it]  6%|▌         | 30/500 [12:16<3:08:58, 24.12s/it]  6%|▌         | 31/500 [12:46<3:20:49, 25.69s/it]  6%|▋         | 32/500 [13:10<3:16:37, 25.21s/it]  7%|▋         | 33/500 [13:34<3:13:41, 24.89s/it]  7%|▋         | 34/500 [13:58<3:11:37, 24.67s/it]  7%|▋         | 35/500 [14:23<3:10:20, 24.56s/it]  7%|▋         | 36/500 [14:47<3:09:10, 24.46s/it]  7%|▋         | 37/500 [15:16<3:18:51, 25.77s/it]  8%|▊         | 38/500 [15:45<3:26:16, 26.79s/it]  8%|▊         | 39/500 [16:13<3:28:03, 27.08s/it]  8%|▊         | 40/500 [16:37<3:22:19, 26.39s/it]  8%|▊         | 41/500 [17:02<3:17:52, 25.87s/it]  8%|▊         | 42/500 [17:26<3:14:16, 25.45s/it]  9%|▊         | 43/500 [17:51<3:11:36, 25.16s/it]  9%|▉         | 44/500 [18:15<3:09:44, 24.97s/it]  9%|▉         | 45/500 [18:39<3:07:01, 24.66s/it]  9%|▉         | 46/500 [19:04<3:05:25, 24.51s/it]  9%|▉         | 47/500 [19:32<3:12:58, 25.56s/it] 10%|▉         | 48/500 [19:56<3:10:14, 25.25s/it] 10%|▉         | 49/500 [20:21<3:08:35, 25.09s/it] 10%|█         | 50/500 [20:45<3:06:02, 24.81s/it] 10%|█         | 51/500 [21:09<3:03:34, 24.53s/it] 10%|█         | 52/500 [21:33<3:02:50, 24.49s/it] 11%|█         | 53/500 [21:57<3:01:50, 24.41s/it] 11%|█         | 54/500 [22:22<3:02:52, 24.60s/it] 11%|█         | 55/500 [22:47<3:01:25, 24.46s/it] 11%|█         | 56/500 [23:11<3:00:14, 24.36s/it] 11%|█▏        | 57/500 [23:35<2:59:38, 24.33s/it] 12%|█▏        | 58/500 [24:03<3:06:30, 25.32s/it] 12%|█▏        | 59/500 [24:27<3:03:21, 24.95s/it] 12%|█▏        | 60/500 [24:54<3:09:01, 25.78s/it] 12%|█▏        | 60/500 [24:54<3:02:42, 24.92s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.030 MB uploadedwandb: / 0.135 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▄▃▃▄▅▅▆▅▆▆▆▇▇▇▇▇▇▇▇█▇█▇████████▆▆▇▇▇▇█
wandb:     train_loss ▆▆▆▅▆▆▆▇▄▆█▅▅▃▂▅▁▃▇▂▁▂▁▄▆▁▁▂▂▁▁▁▂▆▁▃▃▃▁▁
wandb:   val_accuracy ▂▁▆▄▂▅▇█▇▆▅▆▅▆▄▄▅▅▄▄▄▄▅▅▄▄▄▄▄▄▅▅▅▅▆▆▅▅▅▅
wandb:       val_loss ▄▄▃▄▃▄▃▄▄▃▃▆▃▂▆▃▄▃▂▇▄▂▅▂▄▃▄▁▂▁▇▆▂█▃▆▃▄▃▄
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.83804
wandb:     train_loss 0.02554
wandb:   val_accuracy 0.41556
wandb:       val_loss 1.35823
wandb: 
wandb: 🚀 View run bright-valley-729 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/iq5tw9wx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_024158-iq5tw9wx/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_030733-7y9t4954
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-durian-731
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7y9t4954
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:16, 28.41s/it]  0%|          | 2/500 [00:52<3:33:43, 25.75s/it]  1%|          | 3/500 [01:19<3:39:53, 26.55s/it]  1%|          | 4/500 [01:44<3:31:56, 25.64s/it]  1%|          | 5/500 [02:07<3:25:47, 24.94s/it]wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
  1%|          | 6/500 [02:31<3:23:14, 24.68s/it]  1%|▏         | 7/500 [02:56<3:21:57, 24.58s/it]  2%|▏         | 8/500 [03:20<3:20:06, 24.40s/it]  2%|▏         | 9/500 [03:43<3:17:47, 24.17s/it]  2%|▏         | 10/500 [04:08<3:17:41, 24.21s/it]  2%|▏         | 11/500 [04:31<3:15:22, 23.97s/it]  2%|▏         | 12/500 [04:55<3:13:52, 23.84s/it]  3%|▎         | 13/500 [05:18<3:12:26, 23.71s/it]  3%|▎         | 14/500 [05:42<3:12:19, 23.74s/it]  3%|▎         | 15/500 [06:06<3:12:54, 23.86s/it]  3%|▎         | 16/500 [06:30<3:13:02, 23.93s/it]  3%|▎         | 17/500 [06:54<3:12:06, 23.87s/it]  4%|▎         | 18/500 [07:18<3:11:23, 23.82s/it]  4%|▍         | 19/500 [07:42<3:11:49, 23.93s/it]  4%|▍         | 20/500 [08:06<3:11:59, 24.00s/it]  4%|▍         | 21/500 [08:30<3:11:54, 24.04s/it]  4%|▍         | 22/500 [08:54<3:11:04, 23.98s/it]  5%|▍         | 23/500 [09:19<3:12:12, 24.18s/it]  5%|▍         | 24/500 [09:42<3:10:37, 24.03s/it]  5%|▌         | 25/500 [10:06<3:09:37, 23.95s/it]  5%|▌         | 26/500 [10:30<3:08:12, 23.82s/it]  5%|▌         | 26/500 [10:30<3:11:27, 24.23s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.314 MB uploadedwandb: / 0.010 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▂▅▁▂▂▂▃▄▃▇▄▃▇▄▅▅▄▄███▇█▇
wandb:     train_loss ▂▂▂▂▃▂▃▅▂▁▅▁▇▂▁▁▄▁█▁▁▁▁▁▁▂
wandb:   val_accuracy ▃▅▃█▄▂▃▅▃▅▃▇▅▁▆▃▃▃▃▃▆▆▇▆▇▆
wandb:       val_loss ▃▃▃▂▃▂▂▄▃▁▃▃█▆▂▄▅▃▇▆█▇▁▂▇▃
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.789
wandb:     train_loss 0.93455
wandb:   val_accuracy 0.44444
wandb:       val_loss 0.89881
wandb: 
wandb: 🚀 View run giddy-durian-731 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/7y9t4954
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_030733-7y9t4954/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_031842-dckfn6cg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-dream-733
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dckfn6cg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:21<2:58:22, 21.45s/it]  0%|          | 2/500 [00:37<2:31:04, 18.20s/it]  1%|          | 3/500 [00:53<2:24:45, 17.48s/it]  1%|          | 4/500 [01:10<2:21:04, 17.07s/it]  1%|          | 5/500 [01:25<2:16:17, 16.52s/it]  1%|          | 6/500 [01:41<2:13:17, 16.19s/it]  1%|▏         | 7/500 [01:57<2:11:27, 16.00s/it]  2%|▏         | 8/500 [02:13<2:11:43, 16.06s/it]  2%|▏         | 9/500 [02:28<2:10:19, 15.93s/it]  2%|▏         | 10/500 [02:45<2:10:34, 15.99s/it]  2%|▏         | 11/500 [03:01<2:10:15, 15.98s/it]  2%|▏         | 12/500 [03:17<2:10:18, 16.02s/it]  3%|▎         | 13/500 [03:34<2:13:57, 16.50s/it]  3%|▎         | 14/500 [03:51<2:15:19, 16.71s/it]  3%|▎         | 15/500 [04:08<2:13:44, 16.55s/it]  3%|▎         | 16/500 [04:24<2:12:47, 16.46s/it]  3%|▎         | 17/500 [04:40<2:11:17, 16.31s/it]  4%|▎         | 18/500 [04:56<2:10:28, 16.24s/it]  4%|▍         | 19/500 [05:12<2:10:13, 16.24s/it]  4%|▍         | 20/500 [05:28<2:08:28, 16.06s/it]  4%|▍         | 21/500 [05:43<2:07:11, 15.93s/it]  4%|▍         | 22/500 [06:00<2:07:22, 15.99s/it]  5%|▍         | 23/500 [06:15<2:06:28, 15.91s/it]  5%|▍         | 24/500 [06:32<2:07:20, 16.05s/it]  5%|▌         | 25/500 [06:47<2:04:52, 15.77s/it]  5%|▌         | 26/500 [07:02<2:03:43, 15.66s/it]  5%|▌         | 27/500 [07:17<2:01:27, 15.41s/it]  6%|▌         | 28/500 [07:32<2:01:07, 15.40s/it]  6%|▌         | 29/500 [07:48<2:00:33, 15.36s/it]  6%|▌         | 30/500 [08:03<1:59:31, 15.26s/it]  6%|▌         | 31/500 [08:18<1:59:57, 15.35s/it]  6%|▋         | 32/500 [08:33<1:59:25, 15.31s/it]  7%|▋         | 33/500 [08:49<1:58:58, 15.29s/it]  7%|▋         | 34/500 [09:03<1:57:30, 15.13s/it]  7%|▋         | 35/500 [09:18<1:56:51, 15.08s/it]  7%|▋         | 36/500 [09:34<1:56:45, 15.10s/it]  7%|▋         | 37/500 [09:48<1:55:50, 15.01s/it]  8%|▊         | 38/500 [10:03<1:55:41, 15.03s/it]  8%|▊         | 39/500 [10:18<1:55:22, 15.02s/it]  8%|▊         | 40/500 [10:33<1:55:14, 15.03s/it]  8%|▊         | 41/500 [10:48<1:54:49, 15.01s/it]  8%|▊         | 41/500 [10:49<2:01:05, 15.83s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▃▆▆▂▇▇▂▅▇█▇██▇██▇▇█████████████████████
wandb:     train_loss ▂▃▂▁▇▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▄▆▂▄▁▆▅▂▂▅▆▅▆▅▄▇▇▄▅▅▇▆▇▇▆▅█▇█▇▇▆▇█▇███▆▇
wandb:       val_loss ▂▂▂▃▆▃▅▅▄▃▄▂▄▂▃▅▇▅▃▆▄▁▁▅▁█▁▁▃▄▄▂▁▆▅▅▂▃▆▃
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.47111
wandb:       val_loss 2.97103
wandb: 
wandb: 🚀 View run bright-dream-733 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dckfn6cg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_031842-dckfn6cg/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_033016-b2a45z5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-yogurt-735
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/b2a45z5e
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:59, 17.67s/it]  0%|          | 2/500 [00:33<2:18:11, 16.65s/it]  1%|          | 3/500 [00:49<2:15:22, 16.34s/it]  1%|          | 4/500 [01:05<2:12:57, 16.08s/it]  1%|          | 5/500 [01:20<2:10:15, 15.79s/it]  1%|          | 6/500 [01:36<2:09:54, 15.78s/it]  1%|▏         | 7/500 [01:51<2:08:55, 15.69s/it]  2%|▏         | 8/500 [02:07<2:07:59, 15.61s/it]  2%|▏         | 9/500 [02:22<2:06:40, 15.48s/it]  2%|▏         | 10/500 [02:42<2:18:01, 16.90s/it]  2%|▏         | 11/500 [02:58<2:14:21, 16.49s/it]  2%|▏         | 12/500 [03:18<2:22:38, 17.54s/it]  3%|▎         | 13/500 [03:33<2:17:37, 16.96s/it]  3%|▎         | 14/500 [03:49<2:13:41, 16.51s/it]  3%|▎         | 15/500 [04:04<2:11:02, 16.21s/it]  3%|▎         | 16/500 [04:20<2:09:13, 16.02s/it]  3%|▎         | 17/500 [04:35<2:07:35, 15.85s/it]  4%|▎         | 18/500 [04:50<2:06:02, 15.69s/it]  4%|▍         | 19/500 [05:06<2:05:42, 15.68s/it]  4%|▍         | 20/500 [05:23<2:07:12, 15.90s/it]  4%|▍         | 21/500 [05:38<2:05:58, 15.78s/it]  4%|▍         | 22/500 [05:53<2:04:29, 15.63s/it]  4%|▍         | 22/500 [05:53<2:08:07, 16.08s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.021 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▁▃▄▄▅▅▆▆▅▆▆▆▇▇▆▇█▇█▇█
wandb:     train_loss ▇▆▅▆▅▄▄▆▄▅█▅▄▁▃▃▄▄▅▁▆▃
wandb:   val_accuracy ▇▆▇█▇▅▄▃▁▅▄▃▃▁▂▅▂▂▅▄▅▄
wandb:       val_loss ▁▂▁▂▂▁▂▁▁▃▂▁▃▂▂▄██▄▅▃▁
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.78455
wandb:     train_loss 0.76987
wandb:   val_accuracy 0.27556
wandb:       val_loss 1.0495
wandb: 
wandb: 🚀 View run likely-yogurt-735 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/b2a45z5e
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_033016-b2a45z5e/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_033652-i2so0eut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-meadow-737
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i2so0eut
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:33:25, 18.45s/it]  0%|          | 2/500 [00:33<2:17:41, 16.59s/it]  1%|          | 3/500 [00:49<2:13:35, 16.13s/it]  1%|          | 4/500 [01:04<2:09:22, 15.65s/it]  1%|          | 5/500 [01:19<2:06:53, 15.38s/it]  1%|          | 6/500 [01:34<2:05:46, 15.28s/it]  1%|▏         | 7/500 [01:49<2:05:28, 15.27s/it]  2%|▏         | 8/500 [02:04<2:04:28, 15.18s/it]  2%|▏         | 9/500 [02:19<2:03:12, 15.06s/it]  2%|▏         | 10/500 [02:34<2:03:10, 15.08s/it]  2%|▏         | 11/500 [02:49<2:02:29, 15.03s/it]  2%|▏         | 12/500 [03:04<2:02:45, 15.09s/it]  3%|▎         | 13/500 [03:19<2:02:18, 15.07s/it]  3%|▎         | 14/500 [03:34<2:02:29, 15.12s/it]  3%|▎         | 15/500 [03:49<2:02:25, 15.14s/it]  3%|▎         | 16/500 [04:04<2:01:26, 15.06s/it]  3%|▎         | 17/500 [04:19<2:01:26, 15.09s/it]  4%|▎         | 18/500 [04:35<2:01:08, 15.08s/it]  4%|▍         | 19/500 [04:50<2:02:41, 15.30s/it]  4%|▍         | 20/500 [05:05<2:01:50, 15.23s/it]  4%|▍         | 21/500 [05:20<2:00:41, 15.12s/it]  4%|▍         | 22/500 [05:35<1:59:32, 15.01s/it]  5%|▍         | 23/500 [05:50<1:59:22, 15.02s/it]  5%|▍         | 24/500 [06:05<1:59:06, 15.01s/it]  5%|▌         | 25/500 [06:20<1:59:39, 15.11s/it]  5%|▌         | 26/500 [06:35<1:58:42, 15.03s/it]  5%|▌         | 27/500 [06:50<1:58:35, 15.04s/it]  6%|▌         | 28/500 [07:05<1:58:31, 15.07s/it]  6%|▌         | 29/500 [07:21<1:58:35, 15.11s/it]  6%|▌         | 30/500 [07:36<1:58:09, 15.08s/it]  6%|▌         | 30/500 [07:36<1:59:07, 15.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.137 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁
wandb: train_accuracy ▁▂▁▃▅▅▄▇▇▃▅▅▄▇▇▆█▇▆█▃█▇▇██████
wandb:     train_loss ▄▄▄▃▂▂█▄▁▁▇▄▅▁▁▁▁▁▂▁█▁▁▁▁▁▁▁▂▁
wandb:   val_accuracy ▂▂▁▁▄▄▄▁▃▅▅▅▅▇▇▅▇▆▆▇▄█▇█▇▇▇███
wandb:       val_loss ▂▂▂▂▃▃▄▂▃▅▃▁▆▁▄▃█▇▁▅▁▁▂▆▁▇▁▇▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 29
wandb:  learning_rate 0.00026
wandb: train_accuracy 0.98663
wandb:     train_loss 0.00554
wandb:   val_accuracy 0.54444
wandb:       val_loss 4.42078
wandb: 
wandb: 🚀 View run rich-meadow-737 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/i2so0eut
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_033652-i2so0eut/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_034515-y5b4wzn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-valley-738
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y5b4wzn4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:19<2:39:58, 19.24s/it]  0%|          | 2/500 [00:35<2:25:26, 17.52s/it]  1%|          | 3/500 [00:57<2:40:25, 19.37s/it]  1%|          | 4/500 [01:15<2:35:55, 18.86s/it]  1%|          | 5/500 [01:32<2:30:42, 18.27s/it]  1%|          | 6/500 [01:49<2:27:46, 17.95s/it]  1%|▏         | 7/500 [02:08<2:30:46, 18.35s/it]  2%|▏         | 8/500 [02:25<2:27:05, 17.94s/it]  2%|▏         | 9/500 [02:42<2:22:39, 17.43s/it]  2%|▏         | 10/500 [02:58<2:18:59, 17.02s/it]  2%|▏         | 11/500 [03:18<2:25:56, 17.91s/it]  2%|▏         | 12/500 [03:35<2:23:57, 17.70s/it]  3%|▎         | 13/500 [03:51<2:20:28, 17.31s/it]  3%|▎         | 14/500 [04:08<2:18:20, 17.08s/it]  3%|▎         | 15/500 [04:26<2:20:11, 17.34s/it]  3%|▎         | 16/500 [04:43<2:19:13, 17.26s/it]  3%|▎         | 17/500 [05:00<2:17:30, 17.08s/it]  4%|▎         | 18/500 [05:17<2:17:09, 17.07s/it]  4%|▍         | 19/500 [05:34<2:17:18, 17.13s/it]  4%|▍         | 20/500 [05:50<2:15:19, 16.92s/it]  4%|▍         | 21/500 [06:07<2:13:37, 16.74s/it]  4%|▍         | 22/500 [06:24<2:14:18, 16.86s/it]  5%|▍         | 23/500 [06:41<2:14:26, 16.91s/it]  5%|▍         | 24/500 [06:58<2:15:45, 17.11s/it]  5%|▌         | 25/500 [07:15<2:14:49, 17.03s/it]  5%|▌         | 26/500 [07:33<2:16:32, 17.28s/it]  5%|▌         | 27/500 [07:51<2:17:12, 17.41s/it]  6%|▌         | 28/500 [08:09<2:17:31, 17.48s/it]  6%|▌         | 29/500 [08:26<2:16:58, 17.45s/it]  6%|▌         | 30/500 [08:43<2:15:36, 17.31s/it]  6%|▌         | 31/500 [08:59<2:13:20, 17.06s/it]  6%|▌         | 31/500 [08:59<2:16:08, 17.42s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.309 MB uploadedwandb: - 0.137 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁
wandb: train_accuracy ▁▅▇▁▁▃▄▄▅▄▄▆▄▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇██
wandb:     train_loss ▃▄▃▄▃▂▂▃▂▄▄▂▄▃▁▂▂▁▃█▁▅▁▁▁▄▁▁▃▁▁
wandb:   val_accuracy ▄▄▄█▄▁▂▄▃▅▅▃▄▅▄▄▃▄▂▃▃▂▁▂▂▁▁▂▁▃▁
wandb:       val_loss ▂▂▂▂▂▂▂▂▂▂▃▃▄▃▁▄▅▄▃▅▃▃▃▂▁▅▃▇██▂
wandb: 
wandb: Run summary:
wandb:          epoch 30
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.91679
wandb:     train_loss 0.02477
wandb:   val_accuracy 0.25111
wandb:       val_loss 1.0578
wandb: 
wandb: 🚀 View run peach-valley-738 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y5b4wzn4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_034515-y5b4wzn4/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_035501-qu00pr72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-plant-740
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qu00pr72
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:29:34, 17.98s/it]  0%|          | 2/500 [00:33<2:18:13, 16.65s/it]  1%|          | 3/500 [00:49<2:15:33, 16.37s/it]  1%|          | 4/500 [01:05<2:13:15, 16.12s/it]  1%|          | 5/500 [01:21<2:11:53, 15.99s/it]  1%|          | 6/500 [01:37<2:11:48, 16.01s/it]  1%|▏         | 7/500 [01:52<2:10:09, 15.84s/it]  2%|▏         | 8/500 [02:08<2:09:00, 15.73s/it]  2%|▏         | 9/500 [02:24<2:08:52, 15.75s/it]  2%|▏         | 10/500 [02:39<2:08:14, 15.70s/it]  2%|▏         | 11/500 [02:55<2:08:07, 15.72s/it]  2%|▏         | 12/500 [03:10<2:07:24, 15.66s/it]  3%|▎         | 13/500 [03:26<2:06:48, 15.62s/it]  3%|▎         | 14/500 [03:41<2:05:03, 15.44s/it]  3%|▎         | 15/500 [03:56<2:03:47, 15.31s/it]  3%|▎         | 16/500 [04:16<2:14:54, 16.72s/it]  3%|▎         | 17/500 [04:36<2:23:12, 17.79s/it]  4%|▎         | 18/500 [04:52<2:17:53, 17.16s/it]  4%|▍         | 19/500 [05:08<2:14:11, 16.74s/it]  4%|▍         | 20/500 [05:24<2:11:51, 16.48s/it]  4%|▍         | 21/500 [05:40<2:10:11, 16.31s/it]  4%|▍         | 22/500 [05:56<2:10:08, 16.34s/it]  5%|▍         | 23/500 [06:12<2:09:23, 16.27s/it]  5%|▍         | 24/500 [06:27<2:06:48, 15.98s/it]  5%|▌         | 25/500 [06:43<2:06:23, 15.97s/it]  5%|▌         | 26/500 [06:59<2:05:01, 15.83s/it]  5%|▌         | 27/500 [07:15<2:06:08, 16.00s/it]  6%|▌         | 28/500 [07:31<2:05:23, 15.94s/it]  6%|▌         | 29/500 [07:46<2:03:53, 15.78s/it]  6%|▌         | 30/500 [08:02<2:04:10, 15.85s/it]  6%|▌         | 31/500 [08:18<2:02:38, 15.69s/it]  6%|▋         | 32/500 [08:33<2:02:31, 15.71s/it]  7%|▋         | 33/500 [08:49<2:02:41, 15.76s/it]  7%|▋         | 34/500 [09:05<2:02:04, 15.72s/it]  7%|▋         | 35/500 [09:24<2:10:20, 16.82s/it]  7%|▋         | 36/500 [09:40<2:07:18, 16.46s/it]  7%|▋         | 37/500 [09:56<2:04:52, 16.18s/it]  8%|▊         | 38/500 [10:11<2:02:42, 15.94s/it]  8%|▊         | 39/500 [10:26<2:01:14, 15.78s/it]  8%|▊         | 40/500 [10:42<2:00:15, 15.69s/it]  8%|▊         | 41/500 [10:57<1:59:47, 15.66s/it]  8%|▊         | 42/500 [11:13<1:59:54, 15.71s/it]  9%|▊         | 43/500 [11:29<1:58:45, 15.59s/it]  9%|▉         | 44/500 [11:44<1:58:14, 15.56s/it]  9%|▉         | 45/500 [12:00<1:58:47, 15.66s/it]  9%|▉         | 46/500 [12:15<1:57:49, 15.57s/it]  9%|▉         | 47/500 [12:31<1:58:29, 15.69s/it] 10%|▉         | 48/500 [12:47<1:58:15, 15.70s/it] 10%|▉         | 49/500 [13:03<1:57:55, 15.69s/it] 10%|█         | 50/500 [13:18<1:57:38, 15.69s/it] 10%|█         | 51/500 [13:34<1:57:03, 15.64s/it] 10%|█         | 52/500 [13:49<1:56:42, 15.63s/it] 11%|█         | 53/500 [14:05<1:56:04, 15.58s/it] 11%|█         | 54/500 [14:20<1:55:23, 15.52s/it] 11%|█         | 55/500 [14:36<1:55:22, 15.56s/it] 11%|█         | 56/500 [14:51<1:55:01, 15.54s/it] 11%|█▏        | 57/500 [15:07<1:54:28, 15.51s/it] 12%|█▏        | 58/500 [15:23<1:55:10, 15.63s/it] 12%|█▏        | 59/500 [15:39<1:55:45, 15.75s/it] 12%|█▏        | 60/500 [15:55<1:56:01, 15.82s/it] 12%|█▏        | 61/500 [16:10<1:55:28, 15.78s/it] 12%|█▏        | 62/500 [16:26<1:55:00, 15.75s/it] 13%|█▎        | 63/500 [16:41<1:53:30, 15.58s/it] 13%|█▎        | 64/500 [16:57<1:52:33, 15.49s/it] 13%|█▎        | 65/500 [17:12<1:51:40, 15.40s/it] 13%|█▎        | 66/500 [17:27<1:51:49, 15.46s/it] 13%|█▎        | 67/500 [17:43<1:51:34, 15.46s/it] 14%|█▎        | 68/500 [17:59<1:51:59, 15.55s/it] 14%|█▍        | 69/500 [18:14<1:51:08, 15.47s/it] 14%|█▍        | 70/500 [18:30<1:51:12, 15.52s/it] 14%|█▍        | 71/500 [18:45<1:51:28, 15.59s/it] 14%|█▍        | 72/500 [19:01<1:50:45, 15.53s/it] 15%|█▍        | 73/500 [19:16<1:50:27, 15.52s/it] 15%|█▍        | 74/500 [19:32<1:50:09, 15.51s/it] 15%|█▌        | 75/500 [19:47<1:49:45, 15.50s/it] 15%|█▌        | 76/500 [20:03<1:49:47, 15.54s/it] 15%|█▌        | 76/500 [20:03<1:51:53, 15.83s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.319 MB uploadedwandb: \ 0.010 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb: / 0.319 MB of 0.319 MB uploadedwandb: - 0.319 MB of 0.319 MB uploadedwandb: \ 0.319 MB of 0.319 MB uploadedwandb: | 0.319 MB of 0.319 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▃▃▅▆▆▇▇▆▇▇▇▇▇▇██▇█▇██████████▆▄▄▅▅▃▇▄
wandb:     train_loss ▆▅▅▅▅▆▅▂▅▅▄▄▅▃▁▄▂▂▂▅▃▃▄▁▁▂▃▁▂▅▃▃▃▃▂▃▁▂▁█
wandb:   val_accuracy ▂▂▁▂▂▂▂▄▅▆▆▇█▇▇▇█▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▂▃▆▇▅▇▂
wandb:       val_loss ▃▃▃▃▃▃▃▃▃▄▃▃▅▄▃▃▃▂▄▂▃▃▂▁▃▅▅▃▃▂▄▁▅▃▃█▄▁▅▄
wandb: 
wandb: Run summary:
wandb:          epoch 75
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.53938
wandb:     train_loss 1.93066
wandb:   val_accuracy 0.36
wandb:       val_loss 1.65378
wandb: 
wandb: 🚀 View run wobbly-plant-740 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qu00pr72
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_035501-qu00pr72/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_041546-iyskapxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-breeze-742
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/iyskapxo
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:30, 18.34s/it]  0%|          | 2/500 [00:34<2:19:51, 16.85s/it]  1%|          | 3/500 [00:49<2:14:34, 16.25s/it]  1%|          | 4/500 [01:04<2:10:39, 15.81s/it]  1%|          | 5/500 [01:20<2:09:05, 15.65s/it]  1%|          | 6/500 [01:36<2:11:32, 15.98s/it]  1%|▏         | 7/500 [01:52<2:11:27, 16.00s/it]  2%|▏         | 8/500 [02:08<2:10:55, 15.97s/it]  2%|▏         | 9/500 [02:25<2:12:14, 16.16s/it]  2%|▏         | 10/500 [02:45<2:23:03, 17.52s/it]  2%|▏         | 11/500 [03:01<2:18:44, 17.02s/it]  2%|▏         | 12/500 [03:16<2:12:52, 16.34s/it]  3%|▎         | 13/500 [03:31<2:09:34, 15.96s/it]  3%|▎         | 14/500 [03:51<2:18:15, 17.07s/it]  3%|▎         | 15/500 [04:06<2:13:48, 16.55s/it]  3%|▎         | 16/500 [04:22<2:12:31, 16.43s/it]  3%|▎         | 17/500 [04:37<2:08:40, 15.98s/it]  4%|▎         | 18/500 [04:52<2:05:34, 15.63s/it]  4%|▍         | 19/500 [05:07<2:03:09, 15.36s/it]  4%|▍         | 20/500 [05:22<2:02:36, 15.33s/it]  4%|▍         | 21/500 [05:37<2:01:34, 15.23s/it]  4%|▍         | 22/500 [05:52<2:00:56, 15.18s/it]  5%|▍         | 23/500 [06:07<2:00:16, 15.13s/it]  5%|▍         | 24/500 [06:23<2:00:45, 15.22s/it]  5%|▌         | 25/500 [06:37<1:59:41, 15.12s/it]  5%|▌         | 26/500 [06:52<1:58:11, 14.96s/it]  5%|▌         | 27/500 [07:07<1:58:20, 15.01s/it]  6%|▌         | 28/500 [07:22<1:57:43, 14.96s/it]  6%|▌         | 29/500 [07:37<1:57:09, 14.92s/it]  6%|▌         | 30/500 [07:52<1:56:48, 14.91s/it]  6%|▌         | 31/500 [08:07<1:56:53, 14.95s/it]  6%|▋         | 32/500 [08:22<1:57:34, 15.07s/it]  7%|▋         | 33/500 [08:38<1:58:25, 15.22s/it]  7%|▋         | 34/500 [08:52<1:57:12, 15.09s/it]  7%|▋         | 35/500 [09:07<1:56:05, 14.98s/it]  7%|▋         | 36/500 [09:22<1:56:21, 15.05s/it]  7%|▋         | 37/500 [09:37<1:55:31, 14.97s/it]  8%|▊         | 38/500 [09:52<1:55:16, 14.97s/it]  8%|▊         | 39/500 [10:07<1:54:56, 14.96s/it]  8%|▊         | 40/500 [10:22<1:54:17, 14.91s/it]  8%|▊         | 41/500 [10:37<1:54:08, 14.92s/it]  8%|▊         | 42/500 [10:54<1:58:25, 15.51s/it]  9%|▊         | 43/500 [11:09<1:56:56, 15.35s/it]  9%|▉         | 44/500 [11:24<1:55:45, 15.23s/it]  9%|▉         | 45/500 [11:39<1:55:42, 15.26s/it]  9%|▉         | 46/500 [11:54<1:55:07, 15.21s/it]  9%|▉         | 47/500 [12:09<1:54:17, 15.14s/it] 10%|▉         | 48/500 [12:25<1:55:46, 15.37s/it] 10%|▉         | 49/500 [12:40<1:55:15, 15.33s/it] 10%|█         | 50/500 [12:55<1:54:02, 15.21s/it] 10%|█         | 51/500 [13:10<1:53:06, 15.11s/it] 10%|█         | 52/500 [13:24<1:51:00, 14.87s/it] 11%|█         | 53/500 [13:39<1:50:46, 14.87s/it] 11%|█         | 54/500 [13:54<1:50:51, 14.91s/it] 11%|█         | 55/500 [14:10<1:51:56, 15.09s/it] 11%|█         | 56/500 [14:26<1:54:00, 15.41s/it] 11%|█▏        | 57/500 [14:45<2:03:10, 16.68s/it] 12%|█▏        | 58/500 [15:00<1:58:42, 16.11s/it] 12%|█▏        | 59/500 [15:16<1:58:14, 16.09s/it] 12%|█▏        | 59/500 [15:21<1:54:46, 15.62s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.310 MB uploadedwandb: / 0.010 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▁▆▅▇▇▂▆▆▇█▇█▅█▆██▇▇█▇▇███████████████▆
wandb:     train_loss ▂▃▂▂▂▄▁█▁▂▁▁▁▁▅▁▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▁▄▃▅▆▂▆▇▇█▇█▅█▇██▇▇▇▇▇█▇▇▇▇▇▇▆▇▇▆▇▇▇▆▅
wandb:       val_loss ▂▂▂▃▂▂▃▃▁▁▃▇▆▄▂▁▅▆▁▃▄▁▁█▇▄▅▆▂▅▇▅▁▇▃█▃▁▃▇
wandb: 
wandb: Run summary:
wandb:          epoch 58
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.80238
wandb:     train_loss 0.00029
wandb:   val_accuracy 0.43333
wandb:       val_loss 8.68386
wandb: 
wandb: 🚀 View run lively-breeze-742 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/iyskapxo
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_041546-iyskapxo/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_043154-kq50g8u9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-grass-745
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kq50g8u9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:29:47, 18.01s/it]  0%|          | 2/500 [00:33<2:17:35, 16.58s/it]  1%|          | 3/500 [00:49<2:13:01, 16.06s/it]  1%|          | 4/500 [01:04<2:10:26, 15.78s/it]  1%|          | 5/500 [01:19<2:08:52, 15.62s/it]  1%|          | 6/500 [01:35<2:07:56, 15.54s/it]  1%|▏         | 7/500 [01:50<2:08:21, 15.62s/it]  2%|▏         | 8/500 [02:06<2:07:21, 15.53s/it]  2%|▏         | 9/500 [02:22<2:08:22, 15.69s/it]  2%|▏         | 10/500 [02:42<2:19:01, 17.02s/it]  2%|▏         | 11/500 [02:57<2:15:21, 16.61s/it]  2%|▏         | 12/500 [03:13<2:12:01, 16.23s/it]  3%|▎         | 13/500 [03:33<2:20:37, 17.33s/it]  3%|▎         | 14/500 [03:48<2:15:43, 16.76s/it]  3%|▎         | 15/500 [04:04<2:13:08, 16.47s/it]  3%|▎         | 16/500 [04:19<2:10:37, 16.19s/it]  3%|▎         | 17/500 [04:36<2:10:30, 16.21s/it]  4%|▎         | 18/500 [04:52<2:09:16, 16.09s/it]  4%|▍         | 19/500 [05:10<2:14:50, 16.82s/it]  4%|▍         | 20/500 [05:26<2:11:58, 16.50s/it]  4%|▍         | 21/500 [05:41<2:08:41, 16.12s/it]  4%|▍         | 22/500 [06:01<2:18:03, 17.33s/it]  5%|▍         | 23/500 [06:17<2:13:49, 16.83s/it]  5%|▍         | 24/500 [06:32<2:10:20, 16.43s/it]  5%|▌         | 25/500 [06:48<2:07:31, 16.11s/it]  5%|▌         | 26/500 [07:08<2:17:14, 17.37s/it]  5%|▌         | 27/500 [07:24<2:12:45, 16.84s/it]  6%|▌         | 28/500 [07:39<2:09:35, 16.47s/it]  6%|▌         | 29/500 [08:00<2:18:51, 17.69s/it]  6%|▌         | 30/500 [08:15<2:13:44, 17.07s/it]  6%|▌         | 31/500 [08:31<2:09:30, 16.57s/it]  6%|▋         | 32/500 [08:46<2:07:06, 16.30s/it]  7%|▋         | 33/500 [09:02<2:05:00, 16.06s/it]  7%|▋         | 34/500 [09:17<2:03:20, 15.88s/it]  7%|▋         | 35/500 [09:33<2:02:30, 15.81s/it]  7%|▋         | 36/500 [09:50<2:04:08, 16.05s/it]  7%|▋         | 37/500 [10:06<2:04:41, 16.16s/it]  8%|▊         | 38/500 [10:22<2:03:28, 16.03s/it]  8%|▊         | 38/500 [10:22<2:06:06, 16.38s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.019 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▆▅▁▄▆▂▃▄▇▅▅▆▇▇▇▇▆▇█▇▇▇▄█▇████▇▇███▇█
wandb:     train_loss ▂▃▂▁█▁▁█▁▁▂▃▁▁▁▁▁▁▂▁▁▁▁▃▅▁▁▁▁▁▁▁▁▁▁▁▃▁
wandb:   val_accuracy ▃▅▆▂▂▂▄▁▂▂█▃▃▄▄▆▅▅▃▆▆▅▆▆▂▆▅▆▇▇▆▅▅▆▅▆▃▇
wandb:       val_loss ▂▂▂▄▆▃▅▄▄▂▃▃▁▂▃▃█▆▃▃▄▁▁█▃█▁▄▃▄▅▁▁▆▅▇▂▄
wandb: 
wandb: Run summary:
wandb:          epoch 37
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.99257
wandb:     train_loss 0.01814
wandb:   val_accuracy 0.48
wandb:       val_loss 4.10255
wandb: 
wandb: 🚀 View run quiet-grass-745 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kq50g8u9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_043154-kq50g8u9/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_044256-za3b2hwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-darkness-746
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/za3b2hwp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:28, 17.61s/it]  0%|          | 2/500 [00:33<2:16:44, 16.48s/it]  1%|          | 3/500 [00:49<2:15:54, 16.41s/it]  1%|          | 4/500 [01:05<2:14:00, 16.21s/it]  1%|          | 5/500 [01:21<2:12:19, 16.04s/it]  1%|          | 6/500 [01:36<2:10:27, 15.84s/it]  1%|▏         | 7/500 [01:52<2:11:00, 15.94s/it]  2%|▏         | 8/500 [02:08<2:10:09, 15.87s/it]  2%|▏         | 9/500 [02:25<2:12:32, 16.20s/it]  2%|▏         | 10/500 [02:41<2:11:49, 16.14s/it]  2%|▏         | 11/500 [02:57<2:10:13, 15.98s/it]  2%|▏         | 12/500 [03:12<2:09:16, 15.89s/it]  3%|▎         | 13/500 [03:28<2:08:15, 15.80s/it]  3%|▎         | 14/500 [03:43<2:07:05, 15.69s/it]  3%|▎         | 15/500 [03:59<2:06:06, 15.60s/it]  3%|▎         | 16/500 [04:15<2:08:19, 15.91s/it]  3%|▎         | 17/500 [04:31<2:07:32, 15.84s/it]  4%|▎         | 18/500 [04:47<2:06:43, 15.77s/it]  4%|▍         | 19/500 [05:02<2:06:17, 15.75s/it]  4%|▍         | 20/500 [05:18<2:06:29, 15.81s/it]  4%|▍         | 21/500 [05:35<2:07:35, 15.98s/it]  4%|▍         | 22/500 [05:50<2:06:20, 15.86s/it]  5%|▍         | 23/500 [06:06<2:04:34, 15.67s/it]  5%|▍         | 24/500 [06:21<2:04:38, 15.71s/it]  5%|▌         | 25/500 [06:37<2:04:11, 15.69s/it]  5%|▌         | 26/500 [06:53<2:04:06, 15.71s/it]  5%|▌         | 27/500 [07:08<2:03:42, 15.69s/it]  6%|▌         | 28/500 [07:24<2:04:28, 15.82s/it]  6%|▌         | 29/500 [07:40<2:03:44, 15.76s/it]  6%|▌         | 30/500 [07:56<2:03:50, 15.81s/it]  6%|▌         | 31/500 [08:12<2:04:00, 15.86s/it]  6%|▋         | 32/500 [08:28<2:03:13, 15.80s/it]  7%|▋         | 33/500 [08:43<2:02:25, 15.73s/it]  7%|▋         | 34/500 [08:59<2:03:05, 15.85s/it]  7%|▋         | 35/500 [09:15<2:02:25, 15.80s/it]  7%|▋         | 36/500 [09:32<2:03:45, 16.00s/it]  7%|▋         | 37/500 [09:47<2:02:59, 15.94s/it]  8%|▊         | 38/500 [10:03<2:01:34, 15.79s/it]  8%|▊         | 39/500 [10:23<2:11:10, 17.07s/it]  8%|▊         | 40/500 [10:39<2:08:09, 16.72s/it]  8%|▊         | 41/500 [10:54<2:05:31, 16.41s/it]  8%|▊         | 42/500 [11:11<2:05:20, 16.42s/it]  9%|▊         | 43/500 [11:26<2:03:10, 16.17s/it]  9%|▉         | 44/500 [11:43<2:02:40, 16.14s/it]  9%|▉         | 45/500 [11:58<2:00:42, 15.92s/it]  9%|▉         | 46/500 [12:14<1:59:47, 15.83s/it]  9%|▉         | 47/500 [12:29<1:58:46, 15.73s/it] 10%|▉         | 48/500 [12:45<1:58:49, 15.77s/it] 10%|▉         | 49/500 [13:00<1:58:02, 15.70s/it] 10%|▉         | 49/500 [13:00<1:59:48, 15.94s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.318 MB uploadedwandb: / 0.020 MB of 0.318 MB uploadedwandb: - 0.318 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb: | 0.318 MB of 0.318 MB uploadedwandb: / 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▂▃▃▄▄▅▄▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇███████▇██████
wandb:     train_loss ▇▅▆▇▅▅▅▅▅▆▅▃▄▅▄▅█▅▅▇▇▃▁▄▃▂▂▂▁▃▄▃▅▄▁▄▁▂▂▄
wandb:   val_accuracy ▃▂▂▂▃▃▂▂▁▃▃▅▅▅▅▅▆▆▅▆▆▆▆███▇▇▇▇▆▇▇▇▇▆▇▆▆▆
wandb:       val_loss ▄▅▄▄▅▄▄▄▄▄▃▅▃▆▅▄▆▄▃▆▃█▂▃▃▄▁▇▆▅▃▄▃▃▄▃▃▅▅█
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.83507
wandb:     train_loss 0.80067
wandb:   val_accuracy 0.43111
wandb:       val_loss 2.15054
wandb: 
wandb: 🚀 View run peachy-darkness-746 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/za3b2hwp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_044256-za3b2hwp/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_045638-o0tj9ub2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-plant-748
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/o0tj9ub2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:24:21, 17.36s/it]  0%|          | 2/500 [00:36<2:30:37, 18.15s/it]  1%|          | 3/500 [00:52<2:23:30, 17.33s/it]  1%|          | 4/500 [01:07<2:17:04, 16.58s/it]  1%|          | 5/500 [01:23<2:14:21, 16.29s/it]  1%|          | 6/500 [01:39<2:13:26, 16.21s/it]  1%|▏         | 7/500 [01:55<2:11:45, 16.04s/it]  2%|▏         | 8/500 [02:11<2:10:34, 15.92s/it]  2%|▏         | 9/500 [02:26<2:09:42, 15.85s/it]  2%|▏         | 10/500 [02:42<2:08:40, 15.76s/it]  2%|▏         | 11/500 [03:02<2:19:58, 17.18s/it]  2%|▏         | 12/500 [03:18<2:16:05, 16.73s/it]  3%|▎         | 13/500 [03:33<2:12:40, 16.35s/it]  3%|▎         | 14/500 [03:50<2:12:25, 16.35s/it]  3%|▎         | 15/500 [04:09<2:19:21, 17.24s/it]  3%|▎         | 16/500 [04:25<2:15:13, 16.76s/it]  3%|▎         | 17/500 [04:41<2:14:23, 16.69s/it]  4%|▎         | 18/500 [04:57<2:11:53, 16.42s/it]  4%|▍         | 19/500 [05:13<2:09:55, 16.21s/it]  4%|▍         | 20/500 [05:28<2:08:30, 16.06s/it]  4%|▍         | 21/500 [05:45<2:10:29, 16.35s/it]  4%|▍         | 22/500 [06:02<2:10:08, 16.33s/it]  5%|▍         | 23/500 [06:18<2:08:56, 16.22s/it]  5%|▍         | 24/500 [06:34<2:08:09, 16.16s/it]  5%|▌         | 25/500 [06:54<2:18:54, 17.55s/it]  5%|▌         | 26/500 [07:11<2:16:40, 17.30s/it]  5%|▌         | 27/500 [07:27<2:13:34, 16.94s/it]  6%|▌         | 28/500 [07:43<2:10:01, 16.53s/it]  6%|▌         | 29/500 [07:58<2:07:34, 16.25s/it]  6%|▌         | 30/500 [08:14<2:05:24, 16.01s/it]  6%|▌         | 31/500 [08:30<2:04:24, 15.92s/it]  6%|▋         | 32/500 [08:46<2:05:41, 16.11s/it]  7%|▋         | 33/500 [09:03<2:05:52, 16.17s/it]  7%|▋         | 34/500 [09:18<2:04:06, 15.98s/it]  7%|▋         | 35/500 [09:34<2:03:46, 15.97s/it]  7%|▋         | 36/500 [09:50<2:02:33, 15.85s/it]  7%|▋         | 37/500 [10:06<2:03:52, 16.05s/it]  8%|▊         | 38/500 [10:21<2:01:28, 15.78s/it]  8%|▊         | 39/500 [10:37<2:00:50, 15.73s/it]  8%|▊         | 40/500 [10:54<2:03:05, 16.06s/it]  8%|▊         | 41/500 [11:10<2:02:27, 16.01s/it]  8%|▊         | 42/500 [11:26<2:03:45, 16.21s/it]  9%|▊         | 43/500 [11:42<2:02:17, 16.06s/it]  9%|▉         | 44/500 [11:58<2:02:02, 16.06s/it]  9%|▉         | 45/500 [12:15<2:03:38, 16.30s/it]  9%|▉         | 46/500 [12:36<2:14:56, 17.83s/it]  9%|▉         | 47/500 [12:57<2:22:13, 18.84s/it] 10%|▉         | 48/500 [13:13<2:15:00, 17.92s/it] 10%|▉         | 49/500 [13:29<2:09:33, 17.24s/it] 10%|█         | 50/500 [13:44<2:05:30, 16.73s/it] 10%|█         | 51/500 [14:00<2:02:33, 16.38s/it] 10%|█         | 52/500 [14:15<2:00:21, 16.12s/it] 11%|█         | 53/500 [14:31<1:58:21, 15.89s/it] 11%|█         | 54/500 [14:46<1:57:16, 15.78s/it] 11%|█         | 55/500 [15:07<2:07:15, 17.16s/it] 11%|█         | 56/500 [15:26<2:12:43, 17.94s/it] 11%|█         | 56/500 [15:26<2:02:29, 16.55s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.020 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb: train_accuracy ▂▂▁▅▃▆▆▂▆▃▇▆█▄█▇▇██▇█▆█▆█████▇██▇███████
wandb:     train_loss ▃▃▃▂▃▂▁▂▂▃▁▁▁▃▁▁▁▁▁▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▁▅▂▅▆▂▆▄▇▅▆▄▇█▆▇▇███▇▇█▅▆▆██▇█▇▆▆▇▆▇▇▇
wandb:       val_loss ▁▁▁▂▂▁▂▃▁▄▂▂▄▃▃▁▁▄█▁▂▃▁▁▃▅▃▄▁▄▁▄▃▃▁▅▂▅▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 55
wandb:  learning_rate 0.00016
wandb: train_accuracy 0.99851
wandb:     train_loss 0.16511
wandb:   val_accuracy 0.53556
wandb:       val_loss 0.02122
wandb: 
wandb: 🚀 View run absurd-plant-748 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/o0tj9ub2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_045638-o0tj9ub2/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_051248-c5ocj1uq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-aardvark-750
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c5ocj1uq
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:29:13, 17.94s/it]  0%|          | 2/500 [00:33<2:19:03, 16.75s/it]  1%|          | 3/500 [00:50<2:16:44, 16.51s/it]  1%|          | 4/500 [01:09<2:26:07, 17.68s/it]  1%|          | 5/500 [01:25<2:21:01, 17.09s/it]  1%|          | 6/500 [01:41<2:16:25, 16.57s/it]  1%|▏         | 7/500 [01:56<2:13:40, 16.27s/it]  2%|▏         | 8/500 [02:12<2:12:06, 16.11s/it]  2%|▏         | 9/500 [02:28<2:10:10, 15.91s/it]  2%|▏         | 10/500 [02:48<2:21:51, 17.37s/it]  2%|▏         | 11/500 [03:04<2:17:21, 16.85s/it]  2%|▏         | 12/500 [03:19<2:13:37, 16.43s/it]  3%|▎         | 13/500 [03:39<2:22:22, 17.54s/it]  3%|▎         | 14/500 [03:55<2:17:26, 16.97s/it]  3%|▎         | 15/500 [04:11<2:14:07, 16.59s/it]  3%|▎         | 16/500 [04:26<2:11:15, 16.27s/it]  3%|▎         | 17/500 [04:43<2:11:33, 16.34s/it]  4%|▎         | 18/500 [05:03<2:20:47, 17.53s/it]  4%|▍         | 19/500 [05:19<2:15:29, 16.90s/it]  4%|▍         | 20/500 [05:34<2:12:00, 16.50s/it]  4%|▍         | 21/500 [05:55<2:21:08, 17.68s/it]  4%|▍         | 22/500 [06:10<2:15:23, 17.00s/it]  5%|▍         | 23/500 [06:26<2:13:15, 16.76s/it]  5%|▍         | 24/500 [06:42<2:10:29, 16.45s/it]  5%|▌         | 25/500 [06:57<2:08:10, 16.19s/it]  5%|▌         | 26/500 [07:19<2:19:32, 17.66s/it]  5%|▌         | 26/500 [07:19<2:13:24, 16.89s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.311 MB uploadedwandb: | 0.010 MB of 0.311 MB uploadedwandb: / 0.137 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▆▇▃▇▇▅▅▂███▆▇▇██▇▄████▄█
wandb:     train_loss ▃▃▃▁▅▁▁▇▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▃▁
wandb:   val_accuracy ▄▄▄▆▁▆▆▃▂▂▆▆▅▄▅▄▇█▄▄▅▆█▇▄█
wandb:       val_loss ▂▂▂▃▅▄▅▃▅▁▄▂▅▂▄▄▇▇▃▃▄▂▁█▄▇
wandb: 
wandb: Run summary:
wandb:          epoch 25
wandb:  learning_rate 0.00064
wandb: train_accuracy 0.99851
wandb:     train_loss 0.03131
wandb:   val_accuracy 0.52
wandb:       val_loss 6.42993
wandb: 
wandb: 🚀 View run cool-aardvark-750 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c5ocj1uq
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_051248-c5ocj1uq/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_052047-isbzzhbg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-bush-751
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/isbzzhbg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:31:15, 18.19s/it]  0%|          | 2/500 [00:34<2:21:31, 17.05s/it]  1%|          | 3/500 [00:51<2:21:05, 17.03s/it]  1%|          | 4/500 [01:07<2:15:59, 16.45s/it]  1%|          | 5/500 [01:22<2:13:09, 16.14s/it]  1%|          | 6/500 [01:42<2:22:51, 17.35s/it]  1%|▏         | 7/500 [01:57<2:17:27, 16.73s/it]  2%|▏         | 8/500 [02:14<2:16:56, 16.70s/it]  2%|▏         | 9/500 [02:30<2:15:37, 16.57s/it]  2%|▏         | 10/500 [02:47<2:15:38, 16.61s/it]  2%|▏         | 11/500 [03:02<2:12:11, 16.22s/it]  2%|▏         | 12/500 [03:18<2:11:04, 16.11s/it]  3%|▎         | 13/500 [03:39<2:21:38, 17.45s/it]  3%|▎         | 14/500 [03:54<2:16:47, 16.89s/it]  3%|▎         | 15/500 [04:10<2:13:06, 16.47s/it]  3%|▎         | 16/500 [04:25<2:10:22, 16.16s/it]  3%|▎         | 17/500 [04:41<2:09:18, 16.06s/it]  4%|▎         | 18/500 [05:01<2:19:46, 17.40s/it]  4%|▍         | 19/500 [05:18<2:17:56, 17.21s/it]  4%|▍         | 20/500 [05:35<2:15:23, 16.92s/it]  4%|▍         | 21/500 [05:56<2:25:17, 18.20s/it]  4%|▍         | 22/500 [06:11<2:18:42, 17.41s/it]  5%|▍         | 23/500 [06:27<2:14:21, 16.90s/it]  5%|▍         | 24/500 [06:48<2:23:20, 18.07s/it]  5%|▌         | 25/500 [07:06<2:24:15, 18.22s/it]  5%|▌         | 26/500 [07:22<2:18:22, 17.52s/it]  5%|▌         | 27/500 [07:38<2:14:00, 17.00s/it]  6%|▌         | 28/500 [07:54<2:10:28, 16.59s/it]  6%|▌         | 29/500 [08:14<2:19:14, 17.74s/it]  6%|▌         | 29/500 [08:19<2:15:08, 17.21s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.021 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▄▃▃▄▅▅▅▇▇▇▇▇▇▇▇█▇██▇██████
wandb:     train_loss ▇▆▆▆▅▅▅▅▆▆▆▇▄▂▃█▄▅▅▇▅▅▅█▇▃▁▁▄
wandb:   val_accuracy ▃▄▂▃▂▂▃▃▁▁▃▃▃▄▃▇▄█▇█▇▅▇▇▇▇▆▇▇
wandb:       val_loss ▄▄▄▄▄▃▄▃▃▃▄▃▁▄▄▂▆▆▃▅▄▄▃█▄█▅▂▅
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.76077
wandb:     train_loss 0.80937
wandb:   val_accuracy 0.39333
wandb:       val_loss 1.45
wandb: 
wandb: 🚀 View run lucky-bush-751 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/isbzzhbg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_052047-isbzzhbg/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_052956-g9suwmbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-shape-753
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g9suwmbb
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:32:11, 18.30s/it]  0%|          | 2/500 [00:33<2:17:46, 16.60s/it]  1%|          | 3/500 [00:49<2:12:46, 16.03s/it]  1%|          | 4/500 [01:03<2:08:28, 15.54s/it]  1%|          | 5/500 [01:18<2:05:45, 15.24s/it]  1%|          | 6/500 [01:33<2:04:31, 15.12s/it]  1%|▏         | 7/500 [01:48<2:03:09, 14.99s/it]  2%|▏         | 8/500 [02:03<2:04:05, 15.13s/it]  2%|▏         | 9/500 [02:18<2:03:39, 15.11s/it]  2%|▏         | 10/500 [02:33<2:03:05, 15.07s/it]  2%|▏         | 11/500 [02:48<2:02:25, 15.02s/it]  2%|▏         | 12/500 [03:03<2:01:14, 14.91s/it]  3%|▎         | 13/500 [03:18<2:01:13, 14.94s/it]  3%|▎         | 14/500 [03:33<2:00:59, 14.94s/it]  3%|▎         | 15/500 [03:47<2:00:25, 14.90s/it]  3%|▎         | 16/500 [04:02<2:00:17, 14.91s/it]  3%|▎         | 17/500 [04:17<2:00:18, 14.94s/it]  4%|▎         | 18/500 [04:33<2:00:27, 14.99s/it]  4%|▍         | 19/500 [04:48<2:00:32, 15.04s/it]  4%|▍         | 20/500 [05:03<2:00:40, 15.09s/it]  4%|▍         | 21/500 [05:18<2:00:36, 15.11s/it]  4%|▍         | 22/500 [05:33<1:59:48, 15.04s/it]  5%|▍         | 23/500 [05:48<1:59:42, 15.06s/it]  5%|▍         | 24/500 [06:03<1:59:00, 15.00s/it]  5%|▌         | 25/500 [06:18<1:59:10, 15.05s/it]  5%|▌         | 26/500 [06:33<1:59:15, 15.10s/it]  5%|▌         | 27/500 [06:48<1:59:07, 15.11s/it]  6%|▌         | 28/500 [07:03<1:58:01, 15.00s/it]  6%|▌         | 29/500 [07:18<1:58:18, 15.07s/it]  6%|▌         | 30/500 [07:33<1:57:25, 14.99s/it]  6%|▌         | 31/500 [07:48<1:56:45, 14.94s/it]  6%|▋         | 32/500 [08:04<1:58:36, 15.21s/it]  7%|▋         | 33/500 [08:19<1:57:36, 15.11s/it]  7%|▋         | 34/500 [08:34<1:58:09, 15.21s/it]  7%|▋         | 35/500 [08:49<1:57:41, 15.19s/it]  7%|▋         | 36/500 [09:04<1:56:42, 15.09s/it]  7%|▋         | 37/500 [09:19<1:56:09, 15.05s/it]  8%|▊         | 38/500 [09:34<1:55:46, 15.04s/it]  8%|▊         | 39/500 [09:49<1:55:59, 15.10s/it]  8%|▊         | 40/500 [10:04<1:55:26, 15.06s/it]  8%|▊         | 41/500 [10:20<1:56:21, 15.21s/it]  8%|▊         | 41/500 [10:20<1:55:45, 15.13s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.028 MB uploadedwandb: / 0.025 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb: \ 0.311 MB of 0.311 MB uploadedwandb: | 0.311 MB of 0.311 MB uploadedwandb: / 0.311 MB of 0.311 MB uploadedwandb: - 0.311 MB of 0.311 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▃▆▄▃▇▅▅▄▅▄▄▄▃▅▅▂▄▄▄▃▄▄▄▅▄▃▄▃▄▆▃▅▅▅▄▇█
wandb:     train_loss ▂▃▂▂▂▂▂▂▂▁▆▂▄▃▁▁▄▁▂▁▁▁▁▂▁▂▄█▂▁▃▁▂▁▂▁▁▁▁▁
wandb:   val_accuracy ▂▃▃▂▅▂▃▆▄▃▃▄▃▃▃▄▄▂▁▃▂▂▃▂▂▃▃▂▁▂▃▃▅▂▄▃▄▂▆█
wandb:       val_loss ▂▂▂▂▂▃▂▁▂▄▂▁▃▂▂▅▅▄▂▃▂▁▃▅▁▄▂█▂▆▄▂▁█▆█▁▃▃▂
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.88707
wandb:     train_loss 0.02923
wandb:   val_accuracy 0.47556
wandb:       val_loss 0.96994
wandb: 
wandb: 🚀 View run misty-shape-753 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/g9suwmbb
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_052956-g9suwmbb/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_054104-lycq0dks
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-vortex-754
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lycq0dks
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:18<2:35:34, 18.71s/it]  0%|          | 2/500 [00:34<2:21:11, 17.01s/it]  1%|          | 3/500 [00:55<2:36:10, 18.85s/it]  1%|          | 4/500 [01:11<2:26:50, 17.76s/it]  1%|          | 5/500 [01:32<2:37:04, 19.04s/it]  1%|          | 6/500 [01:49<2:28:59, 18.10s/it]  1%|▏         | 7/500 [02:05<2:23:55, 17.52s/it]  2%|▏         | 8/500 [02:21<2:20:44, 17.16s/it]  2%|▏         | 9/500 [02:38<2:18:41, 16.95s/it]  2%|▏         | 10/500 [02:54<2:17:14, 16.80s/it]  2%|▏         | 11/500 [03:10<2:14:46, 16.54s/it]  2%|▏         | 12/500 [03:27<2:15:25, 16.65s/it]  3%|▎         | 13/500 [03:43<2:14:02, 16.51s/it]  3%|▎         | 14/500 [04:00<2:12:46, 16.39s/it]  3%|▎         | 15/500 [04:16<2:11:38, 16.29s/it]  3%|▎         | 16/500 [04:32<2:11:10, 16.26s/it]  3%|▎         | 17/500 [04:48<2:10:33, 16.22s/it]  4%|▎         | 18/500 [05:04<2:09:56, 16.18s/it]  4%|▍         | 19/500 [05:24<2:20:01, 17.47s/it]  4%|▍         | 20/500 [05:41<2:16:22, 17.05s/it]  4%|▍         | 21/500 [05:57<2:15:09, 16.93s/it]  4%|▍         | 22/500 [06:18<2:25:09, 18.22s/it]  5%|▍         | 23/500 [06:35<2:19:52, 17.59s/it]  5%|▍         | 24/500 [06:51<2:16:40, 17.23s/it]  5%|▌         | 25/500 [07:07<2:13:59, 16.92s/it]  5%|▌         | 26/500 [07:23<2:11:07, 16.60s/it]  5%|▌         | 27/500 [07:39<2:08:43, 16.33s/it]  6%|▌         | 28/500 [07:54<2:06:02, 16.02s/it]  6%|▌         | 29/500 [08:10<2:06:08, 16.07s/it]  6%|▌         | 30/500 [08:27<2:06:53, 16.20s/it]  6%|▌         | 31/500 [08:43<2:08:00, 16.38s/it]  6%|▋         | 32/500 [09:00<2:08:26, 16.47s/it]  7%|▋         | 33/500 [09:17<2:09:52, 16.69s/it]  7%|▋         | 34/500 [09:35<2:10:41, 16.83s/it]  7%|▋         | 35/500 [09:57<2:22:51, 18.43s/it]  7%|▋         | 36/500 [10:13<2:18:19, 17.89s/it]  7%|▋         | 37/500 [10:30<2:14:11, 17.39s/it]  8%|▊         | 38/500 [10:51<2:23:52, 18.69s/it]  8%|▊         | 39/500 [11:08<2:17:59, 17.96s/it]  8%|▊         | 40/500 [11:24<2:13:38, 17.43s/it]  8%|▊         | 41/500 [11:40<2:10:45, 17.09s/it]  8%|▊         | 41/500 [11:40<2:10:42, 17.09s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.309 MB uploadedwandb: / 0.138 MB of 0.309 MB uploadedwandb: - 0.309 MB of 0.309 MB uploadedwandb: \ 0.309 MB of 0.309 MB uploadedwandb: | 0.309 MB of 0.309 MB uploadedwandb: / 0.309 MB of 0.309 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▄▅▄▂▇▂▅▅▅█▅▇▇█▄▅▆▆▇▇▇▇▇▆▆▅▇▆▆▅▅▆▆▅▃▅▅▅
wandb:     train_loss ▂▂▂▂▂▂▁▁▂▃▂▂▁▁▂▂▃▁▁▆▁▁▇▁▁▁▁▃▁▁▁▇▁▁▁▂▂▅█▃
wandb:   val_accuracy ▁▄▇▆▃▃▆▁▆▆▅▅▄▅▅▅▃▄▄▅▄▄▄▄▅▄▅▂▃▄▄▅▇▃▆█▆▆▆▃
wandb:       val_loss ▂▂▂▂▁▁▂▁▂▂▂▁▁▂▂▂▅▄▂▄▃▁▃▄▁█▆▂▃▄▃▂▁▄▁▄▂▃▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 40
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.61218
wandb:     train_loss 2.03763
wandb:   val_accuracy 0.39556
wandb:       val_loss 3.21607
wandb: 
wandb: 🚀 View run still-vortex-754 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/lycq0dks
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_054104-lycq0dks/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_055337-30911tve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sun-756
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/30911tve
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:17<2:26:46, 17.65s/it]  0%|          | 2/500 [00:33<2:18:24, 16.68s/it]  1%|          | 3/500 [00:49<2:16:12, 16.44s/it]  1%|          | 4/500 [01:06<2:15:38, 16.41s/it]  1%|          | 5/500 [01:22<2:14:37, 16.32s/it]  1%|          | 6/500 [01:38<2:14:19, 16.32s/it]  1%|▏         | 7/500 [01:54<2:14:06, 16.32s/it]  2%|▏         | 8/500 [02:10<2:13:00, 16.22s/it]  2%|▏         | 9/500 [02:27<2:13:13, 16.28s/it]  2%|▏         | 10/500 [02:43<2:11:58, 16.16s/it]  2%|▏         | 11/500 [02:59<2:11:15, 16.11s/it]  2%|▏         | 12/500 [03:15<2:11:02, 16.11s/it]  3%|▎         | 13/500 [03:31<2:10:39, 16.10s/it]  3%|▎         | 14/500 [03:47<2:10:31, 16.11s/it]  3%|▎         | 15/500 [04:03<2:10:23, 16.13s/it]  3%|▎         | 16/500 [04:19<2:10:03, 16.12s/it]  3%|▎         | 17/500 [04:36<2:10:23, 16.20s/it]  4%|▎         | 18/500 [04:52<2:09:27, 16.11s/it]  4%|▍         | 19/500 [05:08<2:09:33, 16.16s/it]  4%|▍         | 20/500 [05:24<2:09:00, 16.13s/it]  4%|▍         | 21/500 [05:45<2:21:04, 17.67s/it]  4%|▍         | 22/500 [06:01<2:16:40, 17.16s/it]  5%|▍         | 23/500 [06:18<2:15:47, 17.08s/it]  5%|▍         | 24/500 [06:39<2:25:34, 18.35s/it]  5%|▌         | 25/500 [06:57<2:22:45, 18.03s/it]  5%|▌         | 26/500 [07:13<2:18:33, 17.54s/it]  5%|▌         | 27/500 [07:29<2:15:03, 17.13s/it]  6%|▌         | 28/500 [07:45<2:12:04, 16.79s/it]  6%|▌         | 29/500 [08:01<2:09:39, 16.52s/it]  6%|▌         | 29/500 [08:01<2:10:22, 16.61s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.106 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▃▅▅▅▅▆▅▆▆▆▆▇▇▇█▇▇▇▇██▇▆▇▇█▆
wandb:     train_loss ▄▄▃▃▃▂▂▃▃▃▄▃▄▁▂▄▄▂▂▄▃▂▃▃█▁▂▁▂
wandb:   val_accuracy ▃▃▁▇▆▆▇▇▇▇▇▇▇█▇██▇██▇▇▆█▇▇█▇▇
wandb:       val_loss ▄▄▄▄▄▃▅▄▄▃▄▄▁▅▄▃▇▆▃▆▄▂▃▅▂█▅▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 28
wandb:  learning_rate 6e-05
wandb: train_accuracy 0.60327
wandb:     train_loss 0.82366
wandb:   val_accuracy 0.46444
wandb:       val_loss 0.88848
wandb: 
wandb: 🚀 View run icy-sun-756 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/30911tve
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_055337-30911tve/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_060224-d855cysr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sound-757
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d855cysr
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:21:52, 24.27s/it]  0%|          | 2/500 [00:39<2:37:39, 18.99s/it]  1%|          | 3/500 [00:55<2:24:44, 17.47s/it]  1%|          | 4/500 [01:09<2:15:32, 16.40s/it]  1%|          | 5/500 [01:24<2:10:43, 15.85s/it]  1%|          | 6/500 [01:39<2:08:28, 15.61s/it]  1%|▏         | 7/500 [01:54<2:06:25, 15.39s/it]  2%|▏         | 8/500 [02:14<2:17:38, 16.79s/it]  2%|▏         | 9/500 [02:29<2:13:29, 16.31s/it]  2%|▏         | 10/500 [02:44<2:09:21, 15.84s/it]  2%|▏         | 11/500 [02:59<2:06:03, 15.47s/it]  2%|▏         | 12/500 [03:18<2:15:58, 16.72s/it]  3%|▎         | 13/500 [03:34<2:11:34, 16.21s/it]  3%|▎         | 14/500 [03:48<2:07:54, 15.79s/it]  3%|▎         | 15/500 [04:03<2:05:34, 15.53s/it]  3%|▎         | 16/500 [04:18<2:03:49, 15.35s/it]  3%|▎         | 17/500 [04:33<2:02:20, 15.20s/it]  4%|▎         | 18/500 [04:52<2:11:41, 16.39s/it]  4%|▍         | 19/500 [05:07<2:08:01, 15.97s/it]  4%|▍         | 20/500 [05:22<2:05:08, 15.64s/it]  4%|▍         | 21/500 [05:37<2:02:44, 15.38s/it]  4%|▍         | 22/500 [05:56<2:10:28, 16.38s/it]  5%|▍         | 23/500 [06:10<2:04:49, 15.70s/it]  5%|▍         | 24/500 [06:24<2:01:44, 15.35s/it]  5%|▌         | 25/500 [06:39<2:01:02, 15.29s/it]  5%|▌         | 26/500 [06:55<2:00:49, 15.29s/it]  5%|▌         | 27/500 [07:10<2:00:28, 15.28s/it]  6%|▌         | 28/500 [07:25<1:59:07, 15.14s/it]  6%|▌         | 29/500 [07:40<1:58:41, 15.12s/it]  6%|▌         | 30/500 [07:55<1:58:15, 15.10s/it]  6%|▌         | 31/500 [08:10<1:57:09, 14.99s/it]  6%|▋         | 32/500 [08:24<1:56:38, 14.95s/it]  7%|▋         | 33/500 [08:39<1:56:14, 14.93s/it]  7%|▋         | 34/500 [08:54<1:56:03, 14.94s/it]  7%|▋         | 35/500 [09:09<1:55:14, 14.87s/it]  7%|▋         | 36/500 [09:27<2:01:15, 15.68s/it]  7%|▋         | 37/500 [09:42<2:00:07, 15.57s/it]  8%|▊         | 38/500 [09:57<1:58:16, 15.36s/it]  8%|▊         | 39/500 [10:12<1:56:39, 15.18s/it]  8%|▊         | 40/500 [10:31<2:06:41, 16.53s/it]  8%|▊         | 41/500 [10:46<2:02:49, 16.06s/it]  8%|▊         | 42/500 [11:01<2:00:31, 15.79s/it]  9%|▊         | 43/500 [11:16<1:58:08, 15.51s/it]  9%|▉         | 44/500 [11:31<1:55:57, 15.26s/it]  9%|▉         | 45/500 [11:46<1:54:33, 15.11s/it]  9%|▉         | 46/500 [12:03<2:00:01, 15.86s/it]  9%|▉         | 47/500 [12:18<1:57:46, 15.60s/it] 10%|▉         | 48/500 [12:33<1:56:35, 15.48s/it] 10%|▉         | 49/500 [12:48<1:55:22, 15.35s/it] 10%|█         | 50/500 [13:04<1:54:32, 15.27s/it] 10%|█         | 51/500 [13:18<1:53:36, 15.18s/it] 10%|█         | 52/500 [13:38<2:03:41, 16.57s/it] 11%|█         | 53/500 [13:53<1:59:13, 16.00s/it] 11%|█         | 54/500 [14:08<1:56:28, 15.67s/it] 11%|█         | 55/500 [14:28<2:05:03, 16.86s/it] 11%|█         | 56/500 [14:43<2:00:47, 16.32s/it] 11%|█▏        | 57/500 [14:57<1:57:21, 15.90s/it] 12%|█▏        | 58/500 [15:12<1:54:46, 15.58s/it] 12%|█▏        | 59/500 [15:28<1:53:51, 15.49s/it] 12%|█▏        | 60/500 [15:44<1:54:52, 15.66s/it] 12%|█▏        | 61/500 [16:00<1:55:04, 15.73s/it] 12%|█▏        | 62/500 [16:15<1:53:44, 15.58s/it] 13%|█▎        | 63/500 [16:30<1:52:21, 15.43s/it] 13%|█▎        | 64/500 [16:45<1:51:31, 15.35s/it] 13%|█▎        | 64/500 [16:45<1:54:10, 15.71s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.318 MB uploadedwandb: - 0.010 MB of 0.318 MB uploadedwandb: \ 0.233 MB of 0.318 MB uploadedwandb: | 0.233 MB of 0.318 MB uploadedwandb: / 0.233 MB of 0.318 MB uploadedwandb: - 0.233 MB of 0.318 MB uploadedwandb: \ 0.318 MB of 0.318 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▂▂▂▂▄▂▄▅▆█▄▆▅▄▇▅▆█▄▇▅▇▅▅▄▆▆▅▆▆▇▆▆█▆▇▇▆▇
wandb:     train_loss ▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▂▁▁▁▁▃▃▄█▁▁▃▁▁▁▁
wandb:   val_accuracy ▁▃▁▂▂▄▂▃▅▅▆▂▅▅▃▇▄▅█▄▆▅▆▅▅▄▆▆▆▄▆▆▅▅▇▅▆▆▆▄
wandb:       val_loss ▂▂▂▂▂▂▂▂▁▂▄▄▂▁▂▁▆▃▃▃▃▄▇█▃▃▁▄▃▃▆▃▂▂▃▅█▇▃▆
wandb: 
wandb: Run summary:
wandb:          epoch 63
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.83507
wandb:     train_loss 6e-05
wandb:   val_accuracy 0.43778
wandb:       val_loss 6.52041
wandb: 
wandb: 🚀 View run sleek-sound-757 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/d855cysr
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_060224-d855cysr/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_061954-l0nzfhg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-smoke-759
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/l0nzfhg1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:25:40, 24.73s/it]  0%|          | 2/500 [00:48<3:18:37, 23.93s/it]  1%|          | 3/500 [01:12<3:21:44, 24.36s/it]  1%|          | 4/500 [01:36<3:19:33, 24.14s/it]  1%|          | 5/500 [02:00<3:17:22, 23.93s/it]  1%|          | 6/500 [02:29<3:30:50, 25.61s/it]  1%|▏         | 7/500 [02:57<3:38:38, 26.61s/it]  2%|▏         | 8/500 [03:27<3:44:49, 27.42s/it]  2%|▏         | 9/500 [03:55<3:47:43, 27.83s/it]  2%|▏         | 10/500 [04:23<3:47:42, 27.88s/it]  2%|▏         | 11/500 [04:47<3:36:20, 26.54s/it]  2%|▏         | 12/500 [05:10<3:27:51, 25.56s/it]  3%|▎         | 13/500 [05:38<3:33:06, 26.26s/it]  3%|▎         | 14/500 [06:06<3:37:32, 26.86s/it]  3%|▎         | 15/500 [06:35<3:40:45, 27.31s/it]  3%|▎         | 16/500 [06:58<3:30:19, 26.07s/it]  3%|▎         | 17/500 [07:26<3:35:45, 26.80s/it]  4%|▎         | 18/500 [07:55<3:39:20, 27.30s/it]  4%|▍         | 19/500 [08:18<3:30:11, 26.22s/it]  4%|▍         | 20/500 [08:49<3:41:25, 27.68s/it]  4%|▍         | 21/500 [09:13<3:31:21, 26.47s/it]  4%|▍         | 22/500 [09:36<3:23:20, 25.52s/it]  5%|▍         | 23/500 [10:00<3:19:16, 25.07s/it]  5%|▍         | 24/500 [10:24<3:15:13, 24.61s/it]  5%|▌         | 25/500 [10:53<3:24:31, 25.83s/it]  5%|▌         | 26/500 [11:21<3:29:56, 26.57s/it]  5%|▌         | 27/500 [11:50<3:34:47, 27.25s/it]  6%|▌         | 28/500 [12:18<3:37:38, 27.67s/it]  6%|▌         | 29/500 [12:42<3:26:56, 26.36s/it]  6%|▌         | 30/500 [13:11<3:32:31, 27.13s/it]  6%|▌         | 31/500 [13:39<3:35:28, 27.57s/it]  6%|▋         | 32/500 [14:03<3:25:52, 26.39s/it]  7%|▋         | 33/500 [14:31<3:30:15, 27.01s/it]  7%|▋         | 34/500 [14:55<3:21:16, 25.91s/it]  7%|▋         | 35/500 [15:18<3:14:36, 25.11s/it]  7%|▋         | 36/500 [15:46<3:20:10, 25.88s/it]  7%|▋         | 37/500 [16:09<3:14:07, 25.16s/it]  8%|▊         | 38/500 [16:32<3:09:17, 24.58s/it]  8%|▊         | 39/500 [16:56<3:06:30, 24.28s/it]  8%|▊         | 40/500 [17:22<3:10:21, 24.83s/it]  8%|▊         | 41/500 [17:46<3:08:02, 24.58s/it]  8%|▊         | 42/500 [18:10<3:05:07, 24.25s/it]  9%|▊         | 43/500 [18:38<3:14:54, 25.59s/it]  9%|▉         | 44/500 [19:07<3:21:21, 26.50s/it]  9%|▉         | 45/500 [19:30<3:13:49, 25.56s/it]  9%|▉         | 46/500 [19:54<3:09:12, 25.01s/it]  9%|▉         | 47/500 [20:17<3:04:48, 24.48s/it] 10%|▉         | 48/500 [20:40<3:01:42, 24.12s/it] 10%|▉         | 49/500 [21:08<3:09:38, 25.23s/it] 10%|█         | 50/500 [21:37<3:16:05, 26.15s/it] 10%|█         | 51/500 [22:05<3:21:37, 26.94s/it] 10%|█         | 52/500 [22:28<3:12:32, 25.79s/it] 11%|█         | 53/500 [22:57<3:17:33, 26.52s/it] 11%|█         | 54/500 [23:20<3:09:51, 25.54s/it] 11%|█         | 55/500 [23:43<3:04:22, 24.86s/it] 11%|█         | 56/500 [24:06<3:00:17, 24.36s/it] 11%|█▏        | 57/500 [24:30<2:57:29, 24.04s/it] 12%|█▏        | 58/500 [24:53<2:56:17, 23.93s/it] 12%|█▏        | 59/500 [25:17<2:54:26, 23.73s/it] 12%|█▏        | 60/500 [25:40<2:52:59, 23.59s/it] 12%|█▏        | 61/500 [26:13<3:13:27, 26.44s/it] 12%|█▏        | 62/500 [26:36<3:06:12, 25.51s/it] 13%|█▎        | 63/500 [27:00<3:01:25, 24.91s/it] 13%|█▎        | 64/500 [27:23<2:57:19, 24.40s/it] 13%|█▎        | 65/500 [27:47<2:54:48, 24.11s/it] 13%|█▎        | 66/500 [28:10<2:53:09, 23.94s/it] 13%|█▎        | 67/500 [28:33<2:51:37, 23.78s/it] 14%|█▎        | 68/500 [28:57<2:50:19, 23.66s/it] 14%|█▍        | 69/500 [29:20<2:49:23, 23.58s/it] 14%|█▍        | 70/500 [29:44<2:48:25, 23.50s/it] 14%|█▍        | 71/500 [30:07<2:47:31, 23.43s/it] 14%|█▍        | 72/500 [30:30<2:46:34, 23.35s/it] 15%|█▍        | 73/500 [30:53<2:46:06, 23.34s/it] 15%|█▍        | 74/500 [31:16<2:45:16, 23.28s/it] 15%|█▌        | 75/500 [31:40<2:45:59, 23.43s/it] 15%|█▌        | 76/500 [32:04<2:46:17, 23.53s/it] 15%|█▌        | 77/500 [32:27<2:45:48, 23.52s/it] 15%|█▌        | 77/500 [32:27<2:58:21, 25.30s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.140 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▁▅▂▂▁██████████████████████████████████
wandb:     train_loss ▃▃▁▁▄█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▄▅▁▆▆▅▅▅▆▅▇▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇
wandb:       val_loss ▂▂▃▂▅▄▇▆▅▂▁▆▂▂▃▅▁▅▅▃▆▁▃▁▇▁█▅▁▃▁▁▆▂█▁▃▅▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 76
wandb:  learning_rate 0.00021
wandb: train_accuracy 1.0
wandb:     train_loss 4e-05
wandb:   val_accuracy 0.48667
wandb:       val_loss 2.08675
wandb: 
wandb: 🚀 View run neat-smoke-759 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/l0nzfhg1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_061954-l0nzfhg1/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_065302-8w6fezbg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-darkness-761
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8w6fezbg
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:53, 25.84s/it]  0%|          | 2/500 [00:49<3:22:59, 24.46s/it]  1%|          | 3/500 [01:13<3:20:20, 24.19s/it]  1%|          | 4/500 [01:36<3:18:34, 24.02s/it]  1%|          | 5/500 [02:00<3:17:09, 23.90s/it]  1%|          | 6/500 [02:26<3:22:39, 24.61s/it]  1%|▏         | 7/500 [02:55<3:33:27, 25.98s/it]  2%|▏         | 8/500 [03:24<3:40:20, 26.87s/it]  2%|▏         | 9/500 [03:48<3:32:09, 25.93s/it]  2%|▏         | 10/500 [04:16<3:39:05, 26.83s/it]  2%|▏         | 11/500 [04:40<3:29:51, 25.75s/it]  2%|▏         | 12/500 [05:09<3:37:01, 26.68s/it]  3%|▎         | 13/500 [05:32<3:29:08, 25.77s/it]  3%|▎         | 14/500 [06:01<3:36:00, 26.67s/it]  3%|▎         | 15/500 [06:31<3:43:49, 27.69s/it]  3%|▎         | 16/500 [06:55<3:33:54, 26.52s/it]  3%|▎         | 17/500 [07:24<3:39:41, 27.29s/it]  4%|▎         | 18/500 [08:00<3:59:22, 29.80s/it]  4%|▍         | 19/500 [08:23<3:44:03, 27.95s/it]  4%|▍         | 20/500 [08:47<3:33:18, 26.66s/it]  4%|▍         | 21/500 [09:11<3:25:51, 25.79s/it]  4%|▍         | 22/500 [09:34<3:20:00, 25.11s/it]  5%|▍         | 23/500 [09:58<3:17:16, 24.82s/it]  5%|▍         | 24/500 [10:22<3:14:12, 24.48s/it]  5%|▌         | 25/500 [10:45<3:11:23, 24.18s/it]  5%|▌         | 26/500 [11:09<3:09:28, 23.98s/it]  5%|▌         | 27/500 [11:32<3:07:58, 23.84s/it]  6%|▌         | 28/500 [11:56<3:06:37, 23.72s/it]  6%|▌         | 29/500 [12:20<3:06:18, 23.73s/it]  6%|▌         | 30/500 [12:43<3:05:07, 23.63s/it]  6%|▌         | 31/500 [13:06<3:03:55, 23.53s/it]  6%|▋         | 32/500 [13:30<3:02:58, 23.46s/it]  7%|▋         | 33/500 [13:53<3:01:50, 23.36s/it]  7%|▋         | 34/500 [14:16<3:01:28, 23.37s/it]  7%|▋         | 35/500 [14:39<3:00:47, 23.33s/it]  7%|▋         | 36/500 [15:03<3:00:48, 23.38s/it]  7%|▋         | 37/500 [15:26<3:00:11, 23.35s/it]  8%|▊         | 38/500 [15:50<3:00:21, 23.42s/it]  8%|▊         | 39/500 [16:13<3:00:37, 23.51s/it]  8%|▊         | 40/500 [16:37<3:00:02, 23.48s/it]  8%|▊         | 41/500 [17:00<2:59:23, 23.45s/it]  8%|▊         | 42/500 [17:24<2:59:23, 23.50s/it]  9%|▊         | 43/500 [17:47<2:58:40, 23.46s/it]  9%|▉         | 44/500 [18:11<2:58:07, 23.44s/it]  9%|▉         | 45/500 [18:34<2:57:51, 23.45s/it]  9%|▉         | 46/500 [18:57<2:57:02, 23.40s/it]  9%|▉         | 47/500 [19:20<2:55:58, 23.31s/it] 10%|▉         | 48/500 [19:44<2:55:24, 23.28s/it] 10%|▉         | 49/500 [20:07<2:55:04, 23.29s/it] 10%|█         | 50/500 [20:30<2:54:55, 23.32s/it] 10%|█         | 51/500 [20:54<2:54:49, 23.36s/it] 10%|█         | 52/500 [21:18<2:55:24, 23.49s/it] 11%|█         | 53/500 [21:41<2:55:33, 23.56s/it] 11%|█         | 54/500 [22:05<2:54:39, 23.50s/it] 11%|█         | 54/500 [22:09<3:03:03, 24.63s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ███████▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁
wandb: train_accuracy ▁▂▂▃▃▃▃▄▅▅▄▆▄▆▅▇▇▇▇▇▇▇▇██▇██▃▆▇▇▇▇█▇████
wandb:     train_loss ▅▆▆▅▆▆▆█▅▄▅▄▆▃█▅▄▅▄▅▂▄▅▄▄▃█▂▄▂▃▆▂▄▁▃▃▄▂▁
wandb:   val_accuracy ▅▆▅▃▃▂▁▂▃▃▂▃▄▃▅▃▃▃▃▃▄▃▄▄▄▅▅▄█▃▄▅▅▃▅▇▅▆▅▇
wandb:       val_loss ▂▂▂▂▂▃▃▂▃▂▃▂▄▃▁▂▄▁▂▂▁▅▃▂▃▄▄▂▇▇▁▁█▁▇▂▄█▂▁
wandb: 
wandb: Run summary:
wandb:          epoch 53
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.80386
wandb:     train_loss 0.10979
wandb:   val_accuracy 0.38667
wandb:       val_loss 0.60131
wandb: 
wandb: 🚀 View run dulcet-darkness-761 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/8w6fezbg
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_065302-8w6fezbg/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_071558-q2cvahj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sun-763
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/q2cvahj1
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:32:32, 25.56s/it]  0%|          | 2/500 [00:48<3:20:56, 24.21s/it]  1%|          | 3/500 [01:12<3:19:46, 24.12s/it]  1%|          | 4/500 [01:36<3:16:47, 23.80s/it]  1%|          | 5/500 [02:01<3:21:21, 24.41s/it]  1%|          | 6/500 [02:25<3:20:05, 24.30s/it]  1%|▏         | 7/500 [02:49<3:17:57, 24.09s/it]  2%|▏         | 8/500 [03:12<3:16:07, 23.92s/it]  2%|▏         | 9/500 [03:42<3:29:10, 25.56s/it]  2%|▏         | 10/500 [04:05<3:23:01, 24.86s/it]  2%|▏         | 11/500 [04:28<3:18:48, 24.39s/it]  2%|▏         | 12/500 [04:56<3:26:23, 25.38s/it]  3%|▎         | 13/500 [05:19<3:21:27, 24.82s/it]  3%|▎         | 14/500 [05:51<3:37:16, 26.82s/it]  3%|▎         | 15/500 [06:15<3:29:44, 25.95s/it]  3%|▎         | 16/500 [06:43<3:36:03, 26.78s/it]  3%|▎         | 17/500 [07:15<3:46:09, 28.09s/it]  4%|▎         | 18/500 [07:44<3:47:36, 28.33s/it]  4%|▍         | 19/500 [08:07<3:35:50, 26.92s/it]  4%|▍         | 20/500 [08:31<3:28:04, 26.01s/it]  4%|▍         | 21/500 [08:55<3:22:32, 25.37s/it]  4%|▍         | 22/500 [09:19<3:19:02, 24.98s/it]  4%|▍         | 22/500 [09:19<3:22:36, 25.43s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁
wandb: train_accuracy ▁▁▁▁▁▃▅▄▆▂▄▄█▁▇▃▄█▇▃█▅
wandb:     train_loss ▂▂▂▂▃▁▂▂▂▅▃▂▁▃▁▁█▁▂▁▁█
wandb:   val_accuracy ▂▂▁▂▃▂▄▆▅▄▇▆▇▂▇▄▅▇▇▄█▆
wandb:       val_loss ▂▂▂▂▁▁▃▃▄▄▂▃▄█▂▄▆▁▁▂▂▃
wandb: 
wandb: Run summary:
wandb:          epoch 21
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.68351
wandb:     train_loss 6.23868
wandb:   val_accuracy 0.47111
wandb:       val_loss 2.42545
wandb: 
wandb: 🚀 View run driven-sun-763 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/q2cvahj1
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_071558-q2cvahj1/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_072603-2up8ye9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-star-765
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2up8ye9u
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:44, 25.22s/it]  0%|          | 2/500 [00:49<3:23:05, 24.47s/it]  1%|          | 3/500 [01:12<3:19:37, 24.10s/it]  1%|          | 4/500 [01:36<3:17:27, 23.89s/it]  1%|          | 5/500 [01:59<3:16:04, 23.77s/it]  1%|          | 6/500 [02:24<3:16:42, 23.89s/it]  1%|▏         | 7/500 [02:47<3:15:56, 23.85s/it]  2%|▏         | 8/500 [03:11<3:14:27, 23.72s/it]  2%|▏         | 9/500 [03:35<3:14:09, 23.73s/it]  2%|▏         | 10/500 [03:58<3:14:10, 23.78s/it]  2%|▏         | 11/500 [04:22<3:13:20, 23.72s/it]  2%|▏         | 12/500 [04:46<3:12:23, 23.66s/it]  3%|▎         | 13/500 [05:09<3:12:25, 23.71s/it]  3%|▎         | 14/500 [05:33<3:12:11, 23.73s/it]  3%|▎         | 15/500 [05:57<3:11:56, 23.74s/it]  3%|▎         | 16/500 [06:21<3:11:28, 23.74s/it]  3%|▎         | 17/500 [06:44<3:11:18, 23.77s/it]  4%|▎         | 18/500 [07:08<3:10:52, 23.76s/it]  4%|▍         | 19/500 [07:32<3:10:25, 23.75s/it]  4%|▍         | 20/500 [07:56<3:10:37, 23.83s/it]  4%|▍         | 21/500 [08:20<3:09:57, 23.80s/it]  4%|▍         | 22/500 [08:43<3:08:50, 23.70s/it]  5%|▍         | 23/500 [09:07<3:08:28, 23.71s/it]  5%|▍         | 24/500 [09:30<3:07:50, 23.68s/it]  5%|▌         | 25/500 [09:54<3:07:32, 23.69s/it]  5%|▌         | 26/500 [10:24<3:21:13, 25.47s/it]  5%|▌         | 27/500 [10:47<3:16:24, 24.91s/it]  6%|▌         | 28/500 [11:17<3:26:39, 26.27s/it]  6%|▌         | 29/500 [11:46<3:32:39, 27.09s/it]  6%|▌         | 30/500 [12:15<3:36:58, 27.70s/it]  6%|▌         | 31/500 [12:44<3:39:51, 28.13s/it]  6%|▋         | 32/500 [13:08<3:29:14, 26.82s/it]  7%|▋         | 33/500 [13:32<3:21:28, 25.89s/it]  7%|▋         | 34/500 [13:55<3:16:06, 25.25s/it]  7%|▋         | 35/500 [14:19<3:12:05, 24.79s/it]  7%|▋         | 36/500 [14:43<3:09:04, 24.45s/it]  7%|▋         | 36/500 [14:43<3:09:44, 24.54s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.019 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▁▇▁▇▁▃▆▄▅▅█▄▇▅▇▇████████▇██▇██████
wandb:     train_loss ▂▂▁▁▄▁▂█▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb:   val_accuracy ▂▃▃▁▁▆▃▄▇▄▆▄▇▅▇▅▇▇█▇▇▇▇██▇▇▇▇▇███▇██
wandb:       val_loss ▂▂▁▂▂▁▃▄▃▃▁▂▃▇▂▄▆▁▂▁▁▃▁▁▇▁▁▁▄▄█▁▁▄▇▃
wandb: 
wandb: Run summary:
wandb:          epoch 35
wandb:  learning_rate 0.00051
wandb: train_accuracy 1.0
wandb:     train_loss 2e-05
wandb:   val_accuracy 0.52667
wandb:       val_loss 4.51142
wandb: 
wandb: 🚀 View run twilight-star-765 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2up8ye9u
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_072603-2up8ye9u/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_074126-c64aq2dl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-morning-766
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c64aq2dl
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:48, 25.23s/it]  0%|          | 2/500 [00:48<3:19:31, 24.04s/it]  1%|          | 3/500 [01:12<3:17:20, 23.82s/it]  1%|          | 4/500 [01:35<3:17:10, 23.85s/it]  1%|          | 5/500 [01:59<3:15:51, 23.74s/it]  1%|          | 6/500 [02:22<3:14:12, 23.59s/it]  1%|▏         | 7/500 [02:46<3:13:07, 23.50s/it]  2%|▏         | 8/500 [03:09<3:12:02, 23.42s/it]  2%|▏         | 9/500 [03:32<3:11:06, 23.35s/it]  2%|▏         | 10/500 [03:55<3:10:33, 23.33s/it]  2%|▏         | 11/500 [04:18<3:09:23, 23.24s/it]  2%|▏         | 12/500 [04:42<3:09:07, 23.25s/it]  3%|▎         | 13/500 [05:05<3:08:47, 23.26s/it]  3%|▎         | 14/500 [05:28<3:08:32, 23.28s/it]  3%|▎         | 15/500 [05:52<3:08:43, 23.35s/it]  3%|▎         | 16/500 [06:15<3:08:51, 23.41s/it]  3%|▎         | 17/500 [06:39<3:08:32, 23.42s/it]  4%|▎         | 18/500 [07:02<3:07:55, 23.39s/it]  4%|▍         | 19/500 [07:25<3:07:32, 23.39s/it]  4%|▍         | 20/500 [07:49<3:06:24, 23.30s/it]  4%|▍         | 21/500 [08:12<3:05:56, 23.29s/it]  4%|▍         | 22/500 [08:38<3:12:44, 24.19s/it]  5%|▍         | 23/500 [09:02<3:11:11, 24.05s/it]  5%|▍         | 24/500 [09:31<3:23:28, 25.65s/it]  5%|▌         | 25/500 [09:55<3:18:15, 25.04s/it]  5%|▌         | 26/500 [10:18<3:14:16, 24.59s/it]  5%|▌         | 27/500 [10:45<3:17:35, 25.06s/it]  6%|▌         | 28/500 [11:08<3:12:43, 24.50s/it]  6%|▌         | 29/500 [11:31<3:09:27, 24.13s/it]  6%|▌         | 30/500 [12:01<3:22:08, 25.80s/it]  6%|▌         | 31/500 [12:24<3:16:21, 25.12s/it]  6%|▋         | 32/500 [12:53<3:25:30, 26.35s/it]  7%|▋         | 33/500 [13:17<3:19:05, 25.58s/it]  7%|▋         | 34/500 [13:41<3:14:58, 25.10s/it]  7%|▋         | 35/500 [14:05<3:10:51, 24.63s/it]  7%|▋         | 36/500 [14:28<3:07:21, 24.23s/it]  7%|▋         | 37/500 [14:51<3:05:06, 23.99s/it]  8%|▊         | 38/500 [15:15<3:03:09, 23.79s/it]  8%|▊         | 39/500 [15:38<3:01:27, 23.62s/it]  8%|▊         | 40/500 [16:02<3:01:17, 23.65s/it]  8%|▊         | 40/500 [16:02<3:04:25, 24.06s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.294 MB of 0.314 MB uploadedwandb: - 0.294 MB of 0.314 MB uploadedwandb: \ 0.294 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▃▄▄▄▄▄▄▆▅▅▅▇▆▄▆▆▅▆▆▆▇██▇▂▇▇▇█████▅▄▇█
wandb:     train_loss ▄▅▅▆▄▅▆▃▅▇▄▂▃▄▂▄▆▂▄▅▄▃▂▅▃▆▂▅▂▄▁▄▂▂▄▆▄█▃▂
wandb:   val_accuracy ▅▅▆▄▆▃▃▃▄▁▄▃▄▄▄▂▂▄▄▃▄▃▅▄▇▆▆▅▄▅▅▆▆▆▇▇█▅█▇
wandb:       val_loss ▂▃▃▂▃▂▃▃▃▂▃▃▃▄▂▃▄▂▃▂▂▄▂▁▃▂▃▃▆▃▆▂▃▃▅▄▂▂▄█
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.75929
wandb:     train_loss 0.3453
wandb:   val_accuracy 0.36444
wandb:       val_loss 3.60339
wandb: 
wandb: 🚀 View run bright-morning-766 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/c64aq2dl
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_074126-c64aq2dl/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_075811-43d3cah2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sun-768
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/43d3cah2
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:24, 25.78s/it]  0%|          | 2/500 [00:49<3:22:20, 24.38s/it]  1%|          | 3/500 [01:12<3:19:34, 24.09s/it]  1%|          | 4/500 [01:36<3:16:43, 23.80s/it]  1%|          | 5/500 [01:59<3:15:27, 23.69s/it]  1%|          | 6/500 [02:23<3:14:33, 23.63s/it]  1%|▏         | 7/500 [02:46<3:13:41, 23.57s/it]  2%|▏         | 8/500 [03:14<3:25:14, 25.03s/it]  2%|▏         | 9/500 [03:38<3:20:59, 24.56s/it]  2%|▏         | 10/500 [04:01<3:17:26, 24.18s/it]  2%|▏         | 11/500 [04:25<3:14:55, 23.92s/it]  2%|▏         | 12/500 [04:48<3:12:57, 23.72s/it]  3%|▎         | 13/500 [05:11<3:11:49, 23.63s/it]  3%|▎         | 14/500 [05:35<3:10:59, 23.58s/it]  3%|▎         | 15/500 [05:58<3:10:08, 23.52s/it]  3%|▎         | 16/500 [06:22<3:09:38, 23.51s/it]  3%|▎         | 17/500 [06:45<3:09:02, 23.48s/it]  4%|▎         | 18/500 [07:09<3:09:11, 23.55s/it]  4%|▍         | 19/500 [07:32<3:09:19, 23.62s/it]  4%|▍         | 20/500 [07:56<3:08:42, 23.59s/it]  4%|▍         | 21/500 [08:25<3:21:09, 25.20s/it]  4%|▍         | 22/500 [08:48<3:16:38, 24.68s/it]  5%|▍         | 23/500 [09:19<3:29:54, 26.40s/it]  5%|▍         | 24/500 [09:42<3:22:02, 25.47s/it]  5%|▌         | 25/500 [10:06<3:16:35, 24.83s/it]  5%|▌         | 26/500 [10:29<3:13:00, 24.43s/it]  5%|▌         | 27/500 [10:53<3:10:40, 24.19s/it]  6%|▌         | 28/500 [11:16<3:08:50, 24.00s/it]  6%|▌         | 29/500 [11:40<3:08:05, 23.96s/it]  6%|▌         | 30/500 [12:04<3:06:39, 23.83s/it]  6%|▌         | 31/500 [12:27<3:05:02, 23.67s/it]  6%|▋         | 32/500 [12:51<3:04:53, 23.70s/it]  7%|▋         | 33/500 [13:14<3:04:13, 23.67s/it]  7%|▋         | 34/500 [13:38<3:02:56, 23.55s/it]  7%|▋         | 35/500 [14:01<3:02:20, 23.53s/it]  7%|▋         | 36/500 [14:25<3:02:14, 23.57s/it]  7%|▋         | 37/500 [14:48<3:01:27, 23.52s/it]  8%|▊         | 38/500 [15:12<3:01:16, 23.54s/it]  8%|▊         | 39/500 [15:35<3:00:29, 23.49s/it]  8%|▊         | 40/500 [15:59<3:00:18, 23.52s/it]  8%|▊         | 41/500 [16:22<2:59:28, 23.46s/it]  8%|▊         | 42/500 [16:46<3:00:08, 23.60s/it]  9%|▊         | 43/500 [17:09<2:59:22, 23.55s/it]  9%|▉         | 44/500 [17:33<2:58:58, 23.55s/it]  9%|▉         | 45/500 [17:57<2:59:08, 23.62s/it]  9%|▉         | 46/500 [18:20<2:57:46, 23.49s/it]  9%|▉         | 47/500 [18:43<2:56:20, 23.36s/it] 10%|▉         | 48/500 [19:06<2:56:20, 23.41s/it] 10%|▉         | 49/500 [19:30<2:56:27, 23.48s/it] 10%|█         | 50/500 [19:54<2:56:23, 23.52s/it] 10%|█         | 51/500 [20:17<2:56:12, 23.55s/it] 10%|█         | 52/500 [20:41<2:55:41, 23.53s/it] 11%|█         | 53/500 [21:04<2:55:15, 23.52s/it] 11%|█         | 54/500 [21:28<2:54:33, 23.48s/it] 11%|█         | 55/500 [21:51<2:54:27, 23.52s/it] 11%|█         | 56/500 [22:15<2:54:01, 23.52s/it] 11%|█▏        | 57/500 [22:39<2:54:25, 23.62s/it] 12%|█▏        | 58/500 [23:02<2:53:11, 23.51s/it] 12%|█▏        | 59/500 [23:26<2:53:11, 23.56s/it] 12%|█▏        | 60/500 [23:49<2:52:56, 23.58s/it] 12%|█▏        | 61/500 [24:13<2:52:13, 23.54s/it] 12%|█▏        | 61/500 [24:18<2:54:56, 23.91s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.316 MB uploadedwandb: / 0.019 MB of 0.316 MB uploadedwandb: - 0.311 MB of 0.316 MB uploadedwandb: \ 0.316 MB of 0.316 MB uploadedwandb: | 0.316 MB of 0.316 MB uploadedwandb: / 0.316 MB of 0.316 MB uploadedwandb: - 0.316 MB of 0.316 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▂▂▁▁▄▆▂▄█▂▆▄▂▆▅▆▇▄▆▆▆▆▆▇▇▆▇▇██▇██▇██▇▆█▇
wandb:     train_loss ▂▂▂▃▃▁▇▁▁▂▁█▃▁▂▁▁▁▁▂▂▄▁▁▁▁▂▄▁▁▁▁▁▂▁▁▁▂▁▁
wandb:   val_accuracy ▁▁▁▁▁▅▂▆▇▃▅▇▂▇▆▆▇████▇▇█▇▇▇▇▇▇▆▇▇▇▆▇▆▅▆▅
wandb:       val_loss ▂▂▂▁▂▂▄▂▃▆▃▄▂▁▂▁▅▁▁▃▆▁▃▂▁▃▅▄▄▁▄▁▃▄▄▁█▃▇▁
wandb: 
wandb: Run summary:
wandb:          epoch 60
wandb:  learning_rate 0.00013
wandb: train_accuracy 0.89896
wandb:     train_loss 0.0
wandb:   val_accuracy 0.46667
wandb:       val_loss 0.05967
wandb: 
wandb: 🚀 View run happy-sun-768 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/43d3cah2
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_075811-43d3cah2/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_082313-192embjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-bird-770
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/192embjp
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:28:29, 25.07s/it]  0%|          | 2/500 [00:48<3:20:49, 24.20s/it]  1%|          | 3/500 [01:17<3:37:26, 26.25s/it]  1%|          | 4/500 [01:41<3:29:10, 25.30s/it]  1%|          | 5/500 [02:10<3:40:33, 26.73s/it]  1%|          | 6/500 [02:34<3:31:46, 25.72s/it]  1%|▏         | 7/500 [02:58<3:26:40, 25.15s/it]  2%|▏         | 8/500 [03:21<3:22:25, 24.69s/it]  2%|▏         | 9/500 [03:45<3:19:13, 24.34s/it]  2%|▏         | 10/500 [04:09<3:17:13, 24.15s/it]  2%|▏         | 11/500 [04:32<3:15:37, 24.00s/it]  2%|▏         | 12/500 [04:56<3:13:47, 23.83s/it]  3%|▎         | 13/500 [05:19<3:12:51, 23.76s/it]  3%|▎         | 14/500 [05:43<3:12:15, 23.74s/it]  3%|▎         | 15/500 [06:07<3:12:09, 23.77s/it]  3%|▎         | 16/500 [06:31<3:12:10, 23.82s/it]  3%|▎         | 17/500 [06:55<3:11:26, 23.78s/it]  4%|▎         | 18/500 [07:18<3:11:01, 23.78s/it]  4%|▍         | 19/500 [07:42<3:10:30, 23.76s/it]  4%|▍         | 20/500 [08:06<3:10:17, 23.79s/it]  4%|▍         | 21/500 [08:29<3:09:04, 23.68s/it]  4%|▍         | 22/500 [08:53<3:08:20, 23.64s/it]  5%|▍         | 23/500 [09:17<3:08:17, 23.68s/it]  5%|▍         | 24/500 [09:40<3:07:37, 23.65s/it]  5%|▌         | 25/500 [10:04<3:06:50, 23.60s/it]  5%|▌         | 26/500 [10:28<3:06:51, 23.65s/it]  5%|▌         | 27/500 [10:51<3:06:17, 23.63s/it]  6%|▌         | 28/500 [11:15<3:05:44, 23.61s/it]  6%|▌         | 29/500 [11:38<3:05:06, 23.58s/it]  6%|▌         | 30/500 [12:02<3:04:49, 23.60s/it]  6%|▌         | 31/500 [12:25<3:04:16, 23.58s/it]  6%|▋         | 32/500 [12:49<3:03:37, 23.54s/it]  7%|▋         | 33/500 [13:13<3:05:03, 23.78s/it]  7%|▋         | 34/500 [13:45<3:24:40, 26.35s/it]  7%|▋         | 35/500 [14:18<3:37:39, 28.08s/it]  7%|▋         | 36/500 [14:41<3:26:39, 26.72s/it]  7%|▋         | 37/500 [15:05<3:19:04, 25.80s/it]  8%|▊         | 38/500 [15:28<3:13:13, 25.10s/it]  8%|▊         | 39/500 [15:52<3:09:19, 24.64s/it]  8%|▊         | 39/500 [15:52<3:07:37, 24.42s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.313 MB uploadedwandb: | 0.010 MB of 0.313 MB uploadedwandb: / 0.020 MB of 0.313 MB uploadedwandb: - 0.313 MB of 0.313 MB uploadedwandb: \ 0.313 MB of 0.313 MB uploadedwandb: | 0.313 MB of 0.313 MB uploadedwandb: / 0.313 MB of 0.313 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▅▅▅▅▅▅▅▅▅▅▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▁▄▁▆▂▁▄▇▆▃▇▇▅█▇▇▇▇███▇▇████▄█▇████▇▆▆
wandb:     train_loss ▂▂▁▁▅▁▄█▂▁▁▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▄▅▅▁▅▇▆▅▇██▆██▆█▇█▇▇███▇▇███▇▇▇▇██▇█▇▇▆
wandb:       val_loss ▂▂▁▂▂▁▄▃▃▁▂▂▂▇▂▄▅▁▂▁▁▃▁▁█▁▁▁▄▅█▁▁▆▇▂▁▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 38
wandb:  learning_rate 0.00051
wandb: train_accuracy 0.8425
wandb:     train_loss 0.0227
wandb:   val_accuracy 0.43333
wandb:       val_loss 4.23118
wandb: 
wandb: 🚀 View run worldly-bird-770 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/192embjp
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_082313-192embjp/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_083947-5c0yf150
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-breeze-772
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5c0yf150
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:29:58, 25.25s/it]  0%|          | 2/500 [00:49<3:23:42, 24.54s/it]  1%|          | 3/500 [01:13<3:20:49, 24.24s/it]  1%|          | 4/500 [01:36<3:18:36, 24.02s/it]  1%|          | 5/500 [02:00<3:16:31, 23.82s/it]  1%|          | 6/500 [02:23<3:15:25, 23.73s/it]  1%|▏         | 7/500 [02:53<3:30:41, 25.64s/it]  2%|▏         | 8/500 [03:17<3:24:47, 24.97s/it]  2%|▏         | 9/500 [03:40<3:20:41, 24.52s/it]  2%|▏         | 10/500 [04:04<3:17:51, 24.23s/it]  2%|▏         | 11/500 [04:27<3:15:44, 24.02s/it]  2%|▏         | 12/500 [04:51<3:14:01, 23.86s/it]  3%|▎         | 13/500 [05:14<3:12:57, 23.77s/it]  3%|▎         | 14/500 [05:38<3:11:33, 23.65s/it]  3%|▎         | 15/500 [06:01<3:11:46, 23.72s/it]  3%|▎         | 16/500 [06:25<3:11:04, 23.69s/it]  3%|▎         | 17/500 [06:49<3:10:26, 23.66s/it]  4%|▎         | 18/500 [07:12<3:09:39, 23.61s/it]  4%|▍         | 19/500 [07:36<3:08:45, 23.55s/it]  4%|▍         | 20/500 [07:59<3:08:34, 23.57s/it]  4%|▍         | 21/500 [08:23<3:08:11, 23.57s/it]  4%|▍         | 22/500 [08:46<3:07:35, 23.55s/it]  5%|▍         | 23/500 [09:10<3:06:48, 23.50s/it]  5%|▍         | 24/500 [09:33<3:06:54, 23.56s/it]  5%|▌         | 25/500 [09:57<3:06:47, 23.60s/it]  5%|▌         | 26/500 [10:21<3:06:38, 23.63s/it]  5%|▌         | 27/500 [10:44<3:05:48, 23.57s/it]  6%|▌         | 28/500 [11:08<3:05:58, 23.64s/it]  6%|▌         | 29/500 [11:32<3:05:55, 23.68s/it]  6%|▌         | 30/500 [11:56<3:05:59, 23.74s/it]  6%|▌         | 31/500 [12:19<3:05:09, 23.69s/it]  6%|▋         | 32/500 [12:43<3:04:15, 23.62s/it]  7%|▋         | 33/500 [13:06<3:03:32, 23.58s/it]  7%|▋         | 34/500 [13:29<3:02:27, 23.49s/it]  7%|▋         | 35/500 [13:53<3:01:48, 23.46s/it]  7%|▋         | 36/500 [14:16<3:01:25, 23.46s/it]  7%|▋         | 37/500 [14:40<3:00:54, 23.44s/it]  8%|▊         | 38/500 [15:03<3:00:39, 23.46s/it]  8%|▊         | 39/500 [15:27<3:00:16, 23.46s/it]  8%|▊         | 40/500 [15:50<2:59:31, 23.42s/it]  8%|▊         | 40/500 [15:50<3:02:10, 23.76s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate █████████▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▁▂▃▄▄▅▄▄▄▆▅▅▅▆▆▅▆▆▅▆▆▆▇▇▇▅▇▇▇▆▇▇▇▇██▇██
wandb:     train_loss ▅▅▆▇▅▅▇▃▆█▄▂▂▅▃▆▄▂▄▅▇▂▂▃▂▇▃▂▁▃▂▄▁▁▄▅▂▂▅▂
wandb:   val_accuracy ▄▂▄▄▆▄▄▃▁▃▂▃▂▂▃▂▁▃▃▁▃▂▂▃▄▃▂▃▃▃▄▅▅▆▇▇▂██▇
wandb:       val_loss ▂▂▃▂▃▂▃▃▃▂▃▂▃▄▁▃▃▂▂▂▁▄▂▁▃▂▂▁▅▃▅▂▂▄▇▄▃▁▄█
wandb: 
wandb: Run summary:
wandb:          epoch 39
wandb:  learning_rate 4e-05
wandb: train_accuracy 0.81278
wandb:     train_loss 0.41952
wandb:   val_accuracy 0.38222
wandb:       val_loss 3.4219
wandb: 
wandb: 🚀 View run morning-breeze-772 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/5c0yf150
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_083947-5c0yf150/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_085618-kazme0v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-lion-774
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kazme0v6
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:34:10, 25.75s/it]  0%|          | 2/500 [00:49<3:22:19, 24.38s/it]  1%|          | 3/500 [01:20<3:47:01, 27.41s/it]  1%|          | 4/500 [01:49<3:52:00, 28.07s/it]  1%|          | 5/500 [02:21<4:02:30, 29.40s/it]  1%|          | 6/500 [02:50<4:01:10, 29.29s/it]  1%|▏         | 7/500 [03:13<3:45:27, 27.44s/it]  2%|▏         | 8/500 [03:44<3:54:22, 28.58s/it]  2%|▏         | 9/500 [04:08<3:41:36, 27.08s/it]  2%|▏         | 10/500 [04:32<3:32:35, 26.03s/it]  2%|▏         | 11/500 [04:55<3:26:18, 25.31s/it]  2%|▏         | 12/500 [05:19<3:21:30, 24.78s/it]  3%|▎         | 13/500 [05:42<3:17:43, 24.36s/it]  3%|▎         | 14/500 [06:06<3:16:08, 24.22s/it]  3%|▎         | 15/500 [06:31<3:16:38, 24.33s/it]  3%|▎         | 16/500 [06:55<3:14:47, 24.15s/it]  3%|▎         | 17/500 [07:18<3:12:28, 23.91s/it]  4%|▎         | 18/500 [07:41<3:11:00, 23.78s/it]  4%|▍         | 19/500 [08:05<3:09:43, 23.67s/it]  4%|▍         | 20/500 [08:29<3:09:43, 23.72s/it]  4%|▍         | 21/500 [08:52<3:08:30, 23.61s/it]  4%|▍         | 22/500 [09:15<3:07:51, 23.58s/it]  5%|▍         | 23/500 [09:39<3:08:02, 23.65s/it]  5%|▍         | 24/500 [10:03<3:07:51, 23.68s/it]  5%|▌         | 25/500 [10:26<3:06:34, 23.57s/it]  5%|▌         | 26/500 [10:50<3:06:02, 23.55s/it]  5%|▌         | 27/500 [11:13<3:05:14, 23.50s/it]  6%|▌         | 28/500 [11:36<3:04:05, 23.40s/it]  6%|▌         | 29/500 [12:05<3:15:01, 24.84s/it]  6%|▌         | 30/500 [12:29<3:12:27, 24.57s/it]  6%|▌         | 31/500 [12:52<3:09:39, 24.26s/it]  6%|▋         | 32/500 [13:16<3:08:17, 24.14s/it]  7%|▋         | 33/500 [13:40<3:06:58, 24.02s/it]  7%|▋         | 34/500 [14:03<3:04:56, 23.81s/it]  7%|▋         | 35/500 [14:26<3:02:49, 23.59s/it]  7%|▋         | 36/500 [14:50<3:02:54, 23.65s/it]  7%|▋         | 37/500 [15:18<3:12:52, 25.00s/it]  8%|▊         | 38/500 [15:46<3:20:23, 26.02s/it]  8%|▊         | 39/500 [16:10<3:14:32, 25.32s/it]  8%|▊         | 40/500 [16:34<3:11:01, 24.92s/it]  8%|▊         | 41/500 [16:58<3:07:52, 24.56s/it]  8%|▊         | 42/500 [17:21<3:05:14, 24.27s/it]  9%|▊         | 43/500 [17:45<3:02:49, 24.00s/it]  9%|▉         | 44/500 [18:09<3:01:45, 23.92s/it]  9%|▉         | 45/500 [18:32<3:00:16, 23.77s/it]  9%|▉         | 46/500 [18:56<3:00:55, 23.91s/it]  9%|▉         | 47/500 [19:20<2:59:38, 23.79s/it] 10%|▉         | 48/500 [19:43<2:58:55, 23.75s/it] 10%|▉         | 49/500 [20:07<2:57:53, 23.67s/it] 10%|▉         | 49/500 [20:07<3:05:12, 24.64s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.317 MB uploadedwandb: / 0.020 MB of 0.317 MB uploadedwandb: - 0.027 MB of 0.317 MB uploadedwandb: \ 0.027 MB of 0.317 MB uploadedwandb: | 0.027 MB of 0.317 MB uploadedwandb: / 0.317 MB of 0.317 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▂▂▁▂▄▅▆▂▃▆▄▅▆▆▆▇▇▇▇█▇▆█▆▆▇▇▇█▇▆▇█▇█████
wandb:     train_loss ▂▂▂▂▃▄▃▂█▄▂▂▂▇▁▁▁▁▁▁▁▂▁▁▃▂▁▂▁▁▁▁▁▁▄▁▁▁▁▁
wandb:   val_accuracy ▂▂▂▁▁▃█▇▄▅▆▅▇█▇▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▆▆▇▇▇▆▆▇▆▆
wandb:       val_loss ▂▂▂▂▁▂▂▂▃▂▃▇▁▃▁▁▁▂▁▁▇▁▁▄▂▇▁▃▆▂▁▂▇█▃▅▁▁▅█
wandb: 
wandb: Run summary:
wandb:          epoch 48
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.97028
wandb:     train_loss 0.0376
wandb:   val_accuracy 0.45556
wandb:       val_loss 14.1281
wandb: 
wandb: 🚀 View run distinctive-lion-774 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/kazme0v6
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_085618-kazme0v6/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_091708-dtxb2sar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-totem-775
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dtxb2sar
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:24<3:25:38, 24.73s/it]  0%|          | 2/500 [00:48<3:20:55, 24.21s/it]  1%|          | 3/500 [01:12<3:17:44, 23.87s/it]  1%|          | 4/500 [01:40<3:32:40, 25.73s/it]  1%|          | 5/500 [02:04<3:26:49, 25.07s/it]  1%|          | 6/500 [02:32<3:35:47, 26.21s/it]  1%|▏         | 7/500 [03:01<3:41:30, 26.96s/it]  2%|▏         | 8/500 [03:32<3:52:36, 28.37s/it]  2%|▏         | 9/500 [04:02<3:55:20, 28.76s/it]  2%|▏         | 10/500 [04:25<3:41:41, 27.15s/it]  2%|▏         | 11/500 [04:49<3:31:47, 25.99s/it]  2%|▏         | 12/500 [05:12<3:25:04, 25.21s/it]  3%|▎         | 13/500 [05:36<3:21:52, 24.87s/it]  3%|▎         | 14/500 [06:00<3:18:26, 24.50s/it]  3%|▎         | 15/500 [06:24<3:16:04, 24.26s/it]  3%|▎         | 16/500 [06:47<3:14:05, 24.06s/it]  3%|▎         | 17/500 [07:11<3:12:32, 23.92s/it]  3%|▎         | 17/500 [07:16<3:26:36, 25.67s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.307 MB uploadedwandb: / 0.010 MB of 0.307 MB uploadedwandb: - 0.228 MB of 0.307 MB uploadedwandb: \ 0.307 MB of 0.307 MB uploadedwandb: | 0.307 MB of 0.307 MB uploadedwandb: / 0.307 MB of 0.307 MB uploadedwandb: - 0.307 MB of 0.307 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██
wandb:  learning_rate █████████▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▃▆▅▂▂▁▆▅▇█████▇
wandb:     train_loss ▃▂▂▁▃▁▆█▁▂▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▄▂▇▅▂▃▁█▅█▄▆█▄▇▇
wandb:       val_loss ▁▁▁▂▂▁▄▄▅▃▁▆▂▅▁▃█
wandb: 
wandb: Run summary:
wandb:          epoch 16
wandb:  learning_rate 0.0008
wandb: train_accuracy 0.88559
wandb:     train_loss 0.02997
wandb:   val_accuracy 0.51333
wandb:       val_loss 10.13641
wandb: 
wandb: 🚀 View run resilient-totem-775 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/dtxb2sar
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_091708-dtxb2sar/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_092507-rey3oiro
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-cherry-776
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rey3oiro
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:25<3:30:31, 25.31s/it]  0%|          | 2/500 [00:49<3:23:39, 24.54s/it]  1%|          | 3/500 [01:13<3:20:37, 24.22s/it]  1%|          | 4/500 [01:42<3:35:42, 26.09s/it]  1%|          | 5/500 [02:06<3:29:34, 25.40s/it]  1%|          | 6/500 [02:30<3:25:16, 24.93s/it]  1%|▏         | 7/500 [02:54<3:22:07, 24.60s/it]  2%|▏         | 8/500 [03:18<3:20:23, 24.44s/it]  2%|▏         | 9/500 [03:42<3:18:52, 24.30s/it]  2%|▏         | 10/500 [04:06<3:18:24, 24.30s/it]  2%|▏         | 11/500 [04:30<3:17:15, 24.20s/it]  2%|▏         | 12/500 [04:54<3:16:22, 24.14s/it]  3%|▎         | 13/500 [05:18<3:15:30, 24.09s/it]  3%|▎         | 14/500 [05:42<3:14:08, 23.97s/it]  3%|▎         | 15/500 [06:05<3:12:52, 23.86s/it]  3%|▎         | 16/500 [06:32<3:18:37, 24.62s/it]  3%|▎         | 17/500 [06:56<3:17:06, 24.49s/it]  4%|▎         | 18/500 [07:20<3:15:23, 24.32s/it]  4%|▍         | 19/500 [07:44<3:15:06, 24.34s/it]  4%|▍         | 20/500 [08:08<3:14:01, 24.25s/it]  4%|▍         | 21/500 [08:32<3:13:17, 24.21s/it]  4%|▍         | 22/500 [08:56<3:12:19, 24.14s/it]  5%|▍         | 23/500 [09:20<3:10:55, 24.01s/it]  5%|▍         | 24/500 [09:44<3:10:22, 24.00s/it]  5%|▌         | 25/500 [10:08<3:09:18, 23.91s/it]  5%|▌         | 26/500 [10:32<3:08:56, 23.92s/it]  5%|▌         | 27/500 [10:55<3:08:13, 23.88s/it]  6%|▌         | 28/500 [11:20<3:08:10, 23.92s/it]  6%|▌         | 29/500 [11:50<3:23:44, 25.95s/it]  6%|▌         | 30/500 [12:19<3:31:01, 26.94s/it]  6%|▌         | 31/500 [12:44<3:24:34, 26.17s/it]  6%|▋         | 32/500 [13:08<3:19:46, 25.61s/it]  7%|▋         | 33/500 [13:32<3:15:38, 25.14s/it]  7%|▋         | 34/500 [13:57<3:13:52, 24.96s/it]  7%|▋         | 35/500 [14:21<3:11:26, 24.70s/it]  7%|▋         | 36/500 [14:45<3:09:22, 24.49s/it]  7%|▋         | 37/500 [15:09<3:07:16, 24.27s/it]  8%|▊         | 38/500 [15:32<3:05:36, 24.11s/it]  8%|▊         | 39/500 [15:56<3:05:11, 24.10s/it]  8%|▊         | 40/500 [16:21<3:05:00, 24.13s/it]  8%|▊         | 41/500 [16:45<3:04:23, 24.10s/it]  8%|▊         | 42/500 [17:09<3:03:35, 24.05s/it]  9%|▊         | 43/500 [17:33<3:03:04, 24.04s/it]  9%|▉         | 44/500 [17:57<3:02:59, 24.08s/it]  9%|▉         | 45/500 [18:21<3:02:50, 24.11s/it]  9%|▉         | 46/500 [18:45<3:02:43, 24.15s/it]  9%|▉         | 47/500 [19:09<3:02:00, 24.11s/it] 10%|▉         | 48/500 [19:33<3:01:43, 24.12s/it] 10%|▉         | 49/500 [19:58<3:01:52, 24.20s/it] 10%|█         | 50/500 [20:21<3:00:32, 24.07s/it] 10%|█         | 51/500 [20:45<2:59:30, 23.99s/it] 10%|█         | 52/500 [21:09<2:58:38, 23.93s/it] 11%|█         | 53/500 [21:33<2:57:33, 23.83s/it] 11%|█         | 54/500 [21:57<2:57:40, 23.90s/it] 11%|█         | 55/500 [22:21<2:57:13, 23.90s/it] 11%|█         | 56/500 [22:45<2:58:03, 24.06s/it] 11%|█▏        | 57/500 [23:09<2:58:11, 24.13s/it] 12%|█▏        | 58/500 [23:38<3:08:23, 25.57s/it] 12%|█▏        | 59/500 [24:03<3:05:27, 25.23s/it] 12%|█▏        | 60/500 [24:27<3:02:39, 24.91s/it] 12%|█▏        | 61/500 [24:51<3:00:41, 24.70s/it] 12%|█▏        | 62/500 [25:15<2:58:34, 24.46s/it] 13%|█▎        | 63/500 [25:39<2:56:41, 24.26s/it] 13%|█▎        | 64/500 [26:03<2:55:50, 24.20s/it] 13%|█▎        | 65/500 [26:27<2:54:23, 24.05s/it] 13%|█▎        | 66/500 [26:50<2:53:44, 24.02s/it] 13%|█▎        | 67/500 [27:14<2:53:06, 23.99s/it] 14%|█▎        | 68/500 [27:38<2:52:34, 23.97s/it] 14%|█▍        | 69/500 [28:02<2:52:17, 23.99s/it] 14%|█▍        | 70/500 [28:27<2:52:30, 24.07s/it] 14%|█▍        | 70/500 [28:27<2:54:46, 24.39s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.314 MB uploadedwandb: | 0.020 MB of 0.314 MB uploadedwandb: / 0.296 MB of 0.314 MB uploadedwandb: - 0.314 MB of 0.314 MB uploadedwandb: \ 0.314 MB of 0.314 MB uploadedwandb: | 0.314 MB of 0.314 MB uploadedwandb: / 0.314 MB of 0.314 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁
wandb: train_accuracy ▁▂▄▅▅▅▅▆▆▆▆▆▇▇▇▃▆▆▆▇▇▇▇▇███▇██▇▇▇███████
wandb:     train_loss ▅▆▆▆▃▇▄▃▄▅▃▅▃▅▄▄▅▂▄▃▅▃▅▄▃▄▁▃▃▂▂▄▃▄▂▁▁▆█▁
wandb:   val_accuracy ▆▆▆▃▃▄▁▃▂▃▃▆▃▄▂▄▄▇█▅▅▇▅▇▆▄▄▅▆▅▇▇█▇▆▆▃▇▇▇
wandb:       val_loss ▃▃▃▃▃▄▃▃▃▃▃▂▄▁▄▂▆▆▂▄▄▁▆▂▂▂█▂▆▂▁█▄▆▃▆▂▄▆▅
wandb: 
wandb: Run summary:
wandb:          epoch 69
wandb:  learning_rate 2e-05
wandb: train_accuracy 0.84993
wandb:     train_loss 0.04454
wandb:   val_accuracy 0.37778
wandb:       val_loss 2.20661
wandb: 
wandb: 🚀 View run sleek-cherry-776 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/rey3oiro
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_092507-rey3oiro/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_095415-co4j59z0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-music-778
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/co4j59z0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:32<4:33:47, 32.92s/it]  0%|          | 2/500 [00:55<3:44:30, 27.05s/it]  1%|          | 3/500 [01:19<3:30:13, 25.38s/it]  1%|          | 4/500 [01:41<3:20:40, 24.27s/it]  1%|          | 5/500 [02:04<3:15:13, 23.66s/it]  1%|          | 6/500 [02:27<3:12:50, 23.42s/it]  1%|▏         | 7/500 [02:50<3:10:55, 23.24s/it]  2%|▏         | 8/500 [03:13<3:09:32, 23.11s/it]  2%|▏         | 9/500 [03:35<3:07:48, 22.95s/it]  2%|▏         | 10/500 [03:58<3:07:05, 22.91s/it]  2%|▏         | 11/500 [04:21<3:06:03, 22.83s/it]  2%|▏         | 12/500 [04:44<3:06:13, 22.90s/it]  3%|▎         | 13/500 [05:06<3:04:20, 22.71s/it]  3%|▎         | 14/500 [05:28<3:03:18, 22.63s/it]  3%|▎         | 15/500 [05:51<3:02:43, 22.61s/it]  3%|▎         | 16/500 [06:13<3:01:26, 22.49s/it]  3%|▎         | 17/500 [06:35<3:00:35, 22.43s/it]  4%|▎         | 18/500 [06:58<3:00:18, 22.44s/it]  4%|▍         | 19/500 [07:21<3:00:26, 22.51s/it]  4%|▍         | 20/500 [07:43<3:00:18, 22.54s/it]  4%|▍         | 21/500 [08:06<2:59:19, 22.46s/it]  4%|▍         | 22/500 [08:33<3:11:37, 24.05s/it]  5%|▍         | 23/500 [08:56<3:07:30, 23.59s/it]  5%|▍         | 24/500 [09:18<3:04:12, 23.22s/it]  5%|▌         | 25/500 [09:48<3:18:54, 25.13s/it]  5%|▌         | 26/500 [10:17<3:27:14, 26.23s/it]  5%|▌         | 27/500 [10:39<3:16:49, 24.97s/it]  6%|▌         | 28/500 [11:01<3:09:37, 24.11s/it]  6%|▌         | 28/500 [11:06<3:07:14, 23.80s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.315 MB uploadedwandb: | 0.010 MB of 0.315 MB uploadedwandb: / 0.206 MB of 0.315 MB uploadedwandb: - 0.315 MB of 0.315 MB uploadedwandb: \ 0.315 MB of 0.315 MB uploadedwandb: | 0.315 MB of 0.315 MB uploadedwandb: / 0.315 MB of 0.315 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:  learning_rate █████████▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▂▃▂▁▁▂▅▆▃█▃▂▇▁▇▄▂█▂▆▅▃▅▄▆▆▄█
wandb:     train_loss ▂▂▂▂▃▂▃▂▅▁▃▇▂▃▂▁█▁▄▁▁▁▁▂▁▂▃▁
wandb:   val_accuracy ▁▁▁▁▂▂▃▅▂▇▅▂▇▂▇▆▄█▄▇▇▇▇▇▅▇▆▆
wandb:       val_loss ▂▂▂▂▂▂▃▃▃▁▂▁▃█▁▄▅▁▂▁▁▃▁▁█▁▁▂
wandb: 
wandb: Run summary:
wandb:          epoch 27
wandb:  learning_rate 0.00032
wandb: train_accuracy 0.8529
wandb:     train_loss 0.00094
wandb:   val_accuracy 0.49333
wandb:       val_loss 0.75766
wandb: 
wandb: 🚀 View run apricot-music-778 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/co4j59z0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_095415-co4j59z0/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_100613-h81ad47o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-snow-794
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/h81ad47o
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:15:22, 37.92s/it]  0%|          | 2/500 [01:01<4:02:38, 29.23s/it]  1%|          | 3/500 [01:33<4:12:35, 30.49s/it]  1%|          | 4/500 [02:04<4:15:58, 30.97s/it]  1%|          | 5/500 [02:33<4:08:32, 30.13s/it]  1%|          | 6/500 [03:03<4:06:47, 29.97s/it]  1%|▏         | 7/500 [03:34<4:09:35, 30.38s/it]  2%|▏         | 8/500 [03:57<3:50:41, 28.13s/it]  2%|▏         | 9/500 [04:20<3:38:00, 26.64s/it]  2%|▏         | 10/500 [04:51<3:46:45, 27.77s/it]  2%|▏         | 11/500 [05:14<3:35:11, 26.40s/it]  2%|▏         | 12/500 [05:44<3:44:40, 27.62s/it]  3%|▎         | 13/500 [06:14<3:49:42, 28.30s/it]  3%|▎         | 14/500 [06:44<3:53:11, 28.79s/it]  3%|▎         | 15/500 [07:15<3:56:53, 29.31s/it]  3%|▎         | 16/500 [07:45<3:58:48, 29.60s/it]  3%|▎         | 17/500 [08:16<4:01:32, 30.01s/it]  4%|▎         | 18/500 [08:47<4:03:44, 30.34s/it]  4%|▍         | 19/500 [09:19<4:07:25, 30.86s/it]  4%|▍         | 20/500 [09:59<4:28:23, 33.55s/it]  4%|▍         | 21/500 [10:22<4:02:49, 30.42s/it]  4%|▍         | 22/500 [10:53<4:04:00, 30.63s/it]  5%|▍         | 23/500 [11:22<3:58:38, 30.02s/it]  5%|▍         | 24/500 [11:53<4:00:37, 30.33s/it]  5%|▌         | 25/500 [12:24<4:02:25, 30.62s/it]  5%|▌         | 26/500 [12:53<3:56:52, 29.98s/it]  5%|▌         | 27/500 [13:24<4:00:32, 30.51s/it]  6%|▌         | 28/500 [13:58<4:07:15, 31.43s/it]  6%|▌         | 29/500 [14:21<3:47:40, 29.00s/it]  6%|▌         | 30/500 [14:50<3:46:09, 28.87s/it]  6%|▌         | 31/500 [15:13<3:32:24, 27.17s/it]  6%|▋         | 32/500 [15:37<3:23:19, 26.07s/it]  7%|▋         | 33/500 [16:00<3:16:10, 25.20s/it]  7%|▋         | 34/500 [16:23<3:10:47, 24.57s/it]  7%|▋         | 35/500 [16:46<3:07:08, 24.15s/it]  7%|▋         | 36/500 [17:09<3:04:46, 23.89s/it]  7%|▋         | 37/500 [17:33<3:02:51, 23.70s/it]  8%|▊         | 38/500 [17:56<3:01:37, 23.59s/it]  8%|▊         | 39/500 [18:19<3:00:29, 23.49s/it]  8%|▊         | 40/500 [18:42<2:59:02, 23.35s/it]  8%|▊         | 41/500 [19:05<2:57:47, 23.24s/it]  8%|▊         | 42/500 [19:28<2:56:54, 23.18s/it]  9%|▊         | 43/500 [19:58<3:11:29, 25.14s/it]  9%|▉         | 44/500 [20:29<3:24:38, 26.93s/it]  9%|▉         | 45/500 [21:00<3:33:31, 28.16s/it]  9%|▉         | 46/500 [21:23<3:21:38, 26.65s/it]  9%|▉         | 47/500 [21:46<3:13:21, 25.61s/it] 10%|▉         | 48/500 [22:19<3:28:07, 27.63s/it] 10%|▉         | 48/500 [22:27<3:31:26, 28.07s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.312 MB uploadedwandb: - 0.019 MB of 0.312 MB uploadedwandb: \ 0.118 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▂▃▃▆▅▇▅▅▅▇▆▇▆▆▇▇█▇▇▆█▇▇▇▆▇▇▇█▆▆▇▇███▇▇█
wandb:     train_loss ▂▂▂▂▂▁▁▂▁▇▁▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁▁▂▁▅▄▇▃▄▄▅▆▆▆▆▅▆▆▅▆▆█▆██▇▆▇▇▆█▆▆▆▇▇▇█▇▇
wandb:       val_loss ▂▂▂▂▂▂▂▂▁▃▃▄▃▂▆▁▁▃▆▅▄▄▅▅▂▃▁▁▃▂▆▃█▃▁▇▂▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.00041
wandb: train_accuracy 0.84695
wandb:     train_loss 0.00675
wandb:   val_accuracy 0.46889
wandb:       val_loss 0.00931
wandb: 
wandb: 🚀 View run genial-snow-794 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/h81ad47o
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_100613-h81ad47o/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_102923-fangoblx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-bush-795
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fangoblx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:37<5:08:57, 37.15s/it]  0%|          | 2/500 [01:00<4:00:42, 29.00s/it]  1%|          | 3/500 [01:23<3:36:51, 26.18s/it]  1%|          | 4/500 [01:50<3:40:43, 26.70s/it]  1%|          | 5/500 [02:13<3:29:40, 25.42s/it]  1%|          | 6/500 [02:44<3:44:24, 27.26s/it]  1%|▏         | 7/500 [03:15<3:53:28, 28.41s/it]  2%|▏         | 8/500 [03:46<3:58:30, 29.09s/it]  2%|▏         | 9/500 [04:16<4:02:06, 29.58s/it]  2%|▏         | 10/500 [04:55<4:25:09, 32.47s/it]  2%|▏         | 11/500 [05:18<4:00:48, 29.55s/it]  2%|▏         | 12/500 [05:49<4:02:49, 29.86s/it]  3%|▎         | 13/500 [06:20<4:05:04, 30.19s/it]  3%|▎         | 14/500 [07:00<4:29:55, 33.32s/it]  3%|▎         | 15/500 [07:23<4:04:17, 30.22s/it]  3%|▎         | 16/500 [07:46<3:45:33, 27.96s/it]  3%|▎         | 17/500 [08:18<3:54:56, 29.19s/it]  4%|▎         | 18/500 [08:48<3:55:56, 29.37s/it]  4%|▍         | 19/500 [09:10<3:39:22, 27.37s/it]  4%|▍         | 20/500 [09:33<3:27:33, 25.94s/it]  4%|▍         | 21/500 [09:56<3:19:00, 24.93s/it]  4%|▍         | 22/500 [10:26<3:31:05, 26.50s/it]  5%|▍         | 23/500 [10:54<3:34:53, 27.03s/it]  5%|▍         | 24/500 [11:24<3:40:09, 27.75s/it]  5%|▌         | 25/500 [11:46<3:27:17, 26.18s/it]  5%|▌         | 26/500 [12:08<3:17:53, 25.05s/it]  5%|▌         | 27/500 [12:31<3:11:31, 24.30s/it]  6%|▌         | 28/500 [12:54<3:07:36, 23.85s/it]  6%|▌         | 29/500 [13:17<3:05:21, 23.61s/it]  6%|▌         | 30/500 [13:39<3:02:26, 23.29s/it]  6%|▌         | 31/500 [14:02<3:00:12, 23.05s/it]  6%|▋         | 32/500 [14:25<2:58:52, 22.93s/it]  7%|▋         | 33/500 [14:47<2:57:27, 22.80s/it]  7%|▋         | 34/500 [15:09<2:56:06, 22.68s/it]  7%|▋         | 35/500 [15:32<2:55:24, 22.63s/it]  7%|▋         | 36/500 [15:54<2:54:38, 22.58s/it]  7%|▋         | 37/500 [16:17<2:54:12, 22.58s/it]  8%|▊         | 38/500 [16:39<2:53:42, 22.56s/it]  8%|▊         | 39/500 [17:02<2:52:57, 22.51s/it]  8%|▊         | 40/500 [17:25<2:53:06, 22.58s/it]  8%|▊         | 41/500 [17:54<3:08:47, 24.68s/it]  8%|▊         | 42/500 [18:18<3:05:14, 24.27s/it]  9%|▊         | 43/500 [18:40<3:00:55, 23.75s/it]  9%|▉         | 44/500 [19:03<2:58:04, 23.43s/it]  9%|▉         | 45/500 [19:25<2:55:48, 23.18s/it]  9%|▉         | 46/500 [19:55<3:09:52, 25.09s/it]  9%|▉         | 47/500 [20:30<3:33:11, 28.24s/it] 10%|▉         | 48/500 [20:54<3:20:59, 26.68s/it] 10%|▉         | 49/500 [21:16<3:11:09, 25.43s/it] 10%|█         | 50/500 [21:39<3:04:17, 24.57s/it] 10%|█         | 51/500 [22:01<2:59:05, 23.93s/it] 10%|█         | 52/500 [22:31<3:13:00, 25.85s/it] 11%|█         | 53/500 [22:59<3:16:49, 26.42s/it] 11%|█         | 54/500 [23:22<3:08:04, 25.30s/it] 11%|█         | 55/500 [23:50<3:13:36, 26.11s/it] 11%|█         | 56/500 [24:21<3:23:37, 27.52s/it] 11%|█▏        | 57/500 [24:51<3:30:07, 28.46s/it] 12%|█▏        | 58/500 [25:22<3:35:06, 29.20s/it] 12%|█▏        | 59/500 [25:52<3:35:50, 29.37s/it] 12%|█▏        | 60/500 [26:21<3:35:12, 29.35s/it] 12%|█▏        | 60/500 [26:29<3:14:16, 26.49s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.310 MB uploadedwandb: | 0.019 MB of 0.310 MB uploadedwandb: / 0.232 MB of 0.310 MB uploadedwandb: - 0.310 MB of 0.310 MB uploadedwandb: \ 0.310 MB of 0.310 MB uploadedwandb: | 0.310 MB of 0.310 MB uploadedwandb: / 0.310 MB of 0.310 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:  learning_rate ██████▆▆▆▆▆▆▆▅▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁
wandb: train_accuracy ▁▂▂▃▄▄▅▅▅▆▆▆▇▇▇▅▇▇▇▇▇▇▇▇▇██████▇▇▇█████▁
wandb:     train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:   val_accuracy ▂▂▁▂▇▇██▇▇▆▆▆▇▇▅▇▇▇▆▇▆▆▇▆█▇▇▆▇▇▆▆▇▇▇▇▇▆▁
wandb:       val_loss ▂▂▁▂▂▂▁▂▁▁▂▂▁▁▂▂▁▁▁▃▁▁▂▁▁▁▂▁▁▁▁▂▁▃▁▂▁▃▂█
wandb: 
wandb: Run summary:
wandb:          epoch 59
wandb:  learning_rate 3e-05
wandb: train_accuracy 0.33581
wandb:     train_loss 26.83063
wandb:   val_accuracy 0.32222
wandb:       val_loss 14.07363
wandb: 
wandb: 🚀 View run summer-bush-795 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/fangoblx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_102923-fangoblx/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_105634-ys0rvkvz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-surf-796
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ys0rvkvz
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:26<3:41:07, 26.59s/it]  0%|          | 2/500 [00:49<3:20:52, 24.20s/it]  1%|          | 3/500 [01:18<3:41:30, 26.74s/it]  1%|          | 4/500 [01:40<3:25:57, 24.91s/it]  1%|          | 5/500 [02:09<3:37:13, 26.33s/it]  1%|          | 6/500 [02:38<3:42:15, 26.99s/it]  1%|▏         | 7/500 [03:06<3:46:26, 27.56s/it]  2%|▏         | 8/500 [03:34<3:46:16, 27.59s/it]  2%|▏         | 9/500 [04:01<3:45:23, 27.54s/it]  2%|▏         | 10/500 [04:30<3:47:42, 27.88s/it]  2%|▏         | 11/500 [04:59<3:49:23, 28.15s/it]  2%|▏         | 12/500 [05:28<3:51:31, 28.47s/it]  3%|▎         | 13/500 [05:57<3:51:14, 28.49s/it]  3%|▎         | 14/500 [06:26<3:52:07, 28.66s/it]  3%|▎         | 15/500 [07:01<4:08:30, 30.74s/it]  3%|▎         | 16/500 [07:24<3:48:02, 28.27s/it]  3%|▎         | 17/500 [07:46<3:33:19, 26.50s/it]  4%|▎         | 18/500 [08:09<3:23:36, 25.34s/it]  4%|▍         | 19/500 [08:31<3:16:41, 24.54s/it]  4%|▍         | 20/500 [08:59<3:24:35, 25.57s/it]  4%|▍         | 21/500 [09:29<3:32:49, 26.66s/it]  4%|▍         | 22/500 [09:58<3:39:18, 27.53s/it]  5%|▍         | 23/500 [10:28<3:44:51, 28.28s/it]  5%|▍         | 24/500 [10:51<3:30:24, 26.52s/it]  5%|▌         | 25/500 [11:13<3:20:19, 25.30s/it]  5%|▌         | 26/500 [11:35<3:12:36, 24.38s/it]  5%|▌         | 27/500 [12:02<3:18:42, 25.21s/it]  6%|▌         | 28/500 [12:37<3:40:01, 27.97s/it]  6%|▌         | 29/500 [12:59<3:26:15, 26.27s/it]  6%|▌         | 30/500 [13:21<3:16:25, 25.08s/it]  6%|▌         | 31/500 [13:44<3:09:47, 24.28s/it]  6%|▋         | 32/500 [14:07<3:05:44, 23.81s/it]  7%|▋         | 33/500 [14:29<3:02:18, 23.42s/it]  7%|▋         | 34/500 [14:51<2:59:17, 23.08s/it]  7%|▋         | 35/500 [15:19<3:09:54, 24.50s/it]  7%|▋         | 36/500 [15:42<3:04:29, 23.86s/it]  7%|▋         | 37/500 [16:04<3:00:20, 23.37s/it]  8%|▊         | 38/500 [16:26<2:57:29, 23.05s/it]  8%|▊         | 39/500 [16:49<2:57:15, 23.07s/it]  8%|▊         | 40/500 [17:12<2:55:36, 22.90s/it]  8%|▊         | 41/500 [17:34<2:53:57, 22.74s/it]  8%|▊         | 42/500 [17:56<2:52:37, 22.61s/it]  9%|▊         | 43/500 [18:24<3:03:23, 24.08s/it]  9%|▉         | 44/500 [18:53<3:14:57, 25.65s/it]  9%|▉         | 45/500 [19:23<3:23:24, 26.82s/it]  9%|▉         | 46/500 [19:52<3:28:16, 27.53s/it]  9%|▉         | 47/500 [20:21<3:32:07, 28.10s/it] 10%|▉         | 48/500 [20:51<3:35:27, 28.60s/it] 10%|▉         | 48/500 [20:56<3:17:16, 26.19s/it]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.312 MB uploadedwandb: | 0.010 MB of 0.312 MB uploadedwandb: / 0.222 MB of 0.312 MB uploadedwandb: - 0.312 MB of 0.312 MB uploadedwandb: \ 0.312 MB of 0.312 MB uploadedwandb: | 0.312 MB of 0.312 MB uploadedwandb: / 0.312 MB of 0.312 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ████████▆▆▆▆▆▆▆▆▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▃▂▄▅▅▆▇▆▇▆▃▆▇▇▇▇██▇██▇███████▇██████▆██
wandb:     train_loss ▄▄▃▃▁▂▄▁▂▁▃▆▃▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▃▄▃▂▁▇▇▆▇██▅███▇█▆▇▇▇▅█▆▃▇▆▅█▆▂▇▆▄▅▃▆▁▇▆
wandb:       val_loss ▂▂▂▃▃▃▃▅▃▃▄█▁▃▇▂▂▁▄▁▆▁▂▁▇▇▁▁▃▂▂▁▄▆▁▁▁▁▁▃
wandb: 
wandb: Run summary:
wandb:          epoch 47
wandb:  learning_rate 0.0002
wandb: train_accuracy 0.99406
wandb:     train_loss 5e-05
wandb:   val_accuracy 0.45778
wandb:       val_loss 1.66496
wandb: 
wandb: 🚀 View run eternal-surf-796 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/ys0rvkvz
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_105634-ys0rvkvz/logs
Successfully processed 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_111815-u4ygirw8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-silence-797
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/u4ygirw8
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run hopeful-silence-797 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/u4ygirw8
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_111815-u4ygirw8/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_111856-1hub3i23
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-star-798
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1hub3i23
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run stellar-star-798 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1hub3i23
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_111856-1hub3i23/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_111927-h7hxusok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-puddle-799
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/h7hxusok
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run pleasant-puddle-799 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/h7hxusok
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_111927-h7hxusok/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_111957-1wlgv6ad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-fire-800
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1wlgv6ad
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run genial-fire-800 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1wlgv6ad
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_111957-1wlgv6ad/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112035-57qaah5t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-dawn-801
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/57qaah5t
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.025 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run prime-dawn-801 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/57qaah5t
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112035-57qaah5t/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112113-1fmvut5o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-glitter-802
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1fmvut5o
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run efficient-glitter-802 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/1fmvut5o
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112113-1fmvut5o/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112144-0veqx77l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-lion-803
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0veqx77l
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.010 MB of 0.031 MB uploadedwandb: - 0.025 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run neat-lion-803 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/0veqx77l
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112144-0veqx77l/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112216-9cgd8hmw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-rain-804
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9cgd8hmw
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.011 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run hearty-rain-804 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/9cgd8hmw
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112216-9cgd8hmw/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112247-nyu9q9jm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-yogurt-805
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nyu9q9jm
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run leafy-yogurt-805 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nyu9q9jm
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112247-nyu9q9jm/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112319-y3tcp1et
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-snow-806
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y3tcp1et
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.031 MB uploadedwandb: \ 0.019 MB of 0.031 MB uploadedwandb: | 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run classic-snow-806 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/y3tcp1et
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112319-y3tcp1et/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112351-qtouh1h4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-silence-807
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qtouh1h4
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:00<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.019 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run wise-silence-807 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/qtouh1h4
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112351-qtouh1h4/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112422-nhaz2uo9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sunset-808
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nhaz2uo9
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run laced-sunset-808 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/nhaz2uo9
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112422-nhaz2uo9/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112454-2hk7lwbx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-lake-809
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2hk7lwbx
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.011 MB of 0.031 MB uploadedwandb: / 0.026 MB of 0.031 MB uploadedwandb: 🚀 View run neat-lake-809 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/2hk7lwbx
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112454-2hk7lwbx/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112525-gfsowz77
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-lake-810
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gfsowz77
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: 🚀 View run royal-lake-810 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/gfsowz77
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112525-gfsowz77/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
wandb: Currently logged in as: anaiis (for-graduate-anaiis). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /data/Anaiis/anti_overfit/wandb/run-20240923_112556-epfw50f0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-glade-811
wandb: ⭐️ View project at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: 🚀 View run at https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/epfw50f0
/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 0/500 [00:01<?, ?it/s]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.031 MB uploadedwandb: | 0.010 MB of 0.031 MB uploadedwandb: / 0.025 MB of 0.031 MB uploadedwandb: 🚀 View run treasured-glade-811 at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch/runs/epfw50f0
wandb: ⭐️ View project at: https://wandb.ai/for-graduate-anaiis/0917-seed-gridsearch
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240923_112556-epfw50f0/logs
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "watch_grid_search.py", line 109, in <module>
    train(model, tr_loader, optimizer, scheduler, criterion, device, args.max_grad_norm)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 125, in train
    out = model(training_data)  # 前向传播
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 78, in forward
    x_theta = self.GAT_gamma(data['gamma'])
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/Anaiis/anti_overfit/model_gat_seed.py", line 29, in forward
    x = self.conv1(x, edge_index)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 341, in forward
    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/micro/anaconda3/envs/bob_env1/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 386, in message
    return alpha.unsqueeze(-1) * x_j
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.22 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 17.58 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error processing 2_20140419
grid_search.sh: 行 42: e：未找到命令
